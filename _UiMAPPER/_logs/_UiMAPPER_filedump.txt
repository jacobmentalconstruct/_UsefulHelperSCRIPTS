Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_UiMAPPER


--------------------------------------------------------------------------------
FILE: .gitignore
--------------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Python Caches & Bytecode
# -----------------------------------------------------------------------------
__pycache__/
*.py[cod]
*.pyo
*.pyd

# -----------------------------------------------------------------------------
# Virtual Environments
# -----------------------------------------------------------------------------
venv/
env/
.venv/
ENV/
env.bak/

# -----------------------------------------------------------------------------
# Build / Distribution Artifacts
# -----------------------------------------------------------------------------
build/
dist/
*.egg-info/
*.egg
.eggs/
pip-wheel-metadata/

# -----------------------------------------------------------------------------
# IDE / Editor Files
# -----------------------------------------------------------------------------
.vscode/
.vscode-test/
.idea/
*.code-workspace

# -----------------------------------------------------------------------------
# OS-Level Junk
# -----------------------------------------------------------------------------
Thumbs.db
Desktop.ini
.DS_Store
*.swp
*.tmp
*.bak

# -----------------------------------------------------------------------------
# Logs / Runtime Output
# -----------------------------------------------------------------------------
*.log
logs/
*.out

# -----------------------------------------------------------------------------
# Tkinter & App Temp Files
# -----------------------------------------------------------------------------
__appcache__/
*.db-journal

# -----------------------------------------------------------------------------
# Python-specific Dev Tools
# -----------------------------------------------------------------------------
pytest_cache/
.coverage
.tox/
.mypy_cache/
.pytest_cache/
.cache/

# -----------------------------------------------------------------------------
# Project-Specific Exclusions
# -----------------------------------------------------------------------------
# Prevent accidental inclusion of personal configurations or secrets.
local_settings.json
config.local.json
secrets.json

# -----------------------------------------------------------------------------
# Ignore the user’s entire Windows PATH junk accidentally copied in
# -----------------------------------------------------------------------------
$RECYCLE.BIN/

# -----------------------------------------------------------------------------
# If this helper script sits inside UsefulHelperScripts,
# ensure nothing outside this subfolder accidentally gets tracked.
# (Comment out i

--------------------------------------------------------------------------------
FILE: LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: README.md
--------------------------------------------------------------------------------
# UiMAPPER

UiMAPPER is a local, Tkinter-based application that scans a Python project and builds a structured map of its UI surface area.  
It is designed as an orchestrated, microservice-driven system that separates UI, backend orchestration, and analysis logic into clean, composable layers.

The goal is to produce a reliable **UI map + callback graph + report artifacts** from an existing codebase without embedding business logic into the interface layer.

---

## Architecture Overview

UiMAPPER is intentionally split into three top-level components:

### 1) `app.py` — Dumb Shell
- Creates the Tk root
- Instantiates backend + UI orchestrators
- Starts the main loop
- Contains **no business logic**

### 2) `ui.py` — UI Orchestrator
- Owns:
  - Tkinter theme
  - layout
  - widgets
  - user interaction
- Subscribes to progress events
- Polls backend session state
- Displays:
  - run status
  - counters
  - logs
  - reports
  - decision plans
- Contains **no analysis logic**

### 3) `backend.py` — Backend Orchestrator
- Owns the entire pipeline lifecycle
- Wires microservices together
- Runs work off the Tk main thread
- Emits progress events
- Maintains authoritative run state

---

## Microservice Model

All non-orchestration logic lives in microservices.  
Backend composes them into a deterministic pipeline.

### Core runtime services

| Category | Services |
|---|---|
| Session | RunSessionStateMS, CancellationTokenMS |
| Events | ProgressEventBusMS |
| Errors | ErrorNormalizerMS |
| Discovery | GitignoreFilterMS, ProjectCrawlMS, PythonFileEnumeratorMS |
| Entry points | EntrypointFinderMS |
| AST | AstParseCacheMS |
| UI Mapping | AstUiMapMS, TkWidgetDetectorMS |
| Graphing | CallbackGraphBuilderMS |
| Unknown handling | UnknownCaseCollectorMS |
| Inference | InferencePromptBuilderMS, OllamaClientMS, InferenceResultValidatorMS |
| HITL | HitlDecisionRouterMS |
| Modeling | UiMapModelMS |
| Reporting | ReportWriterMS, ReportSerializerMS |

---

## Pipeline Flow



--------------------------------------------------------------------------------
FILE: requirements.txt
--------------------------------------------------------------------------------
# UiMAPPER requirements
# This project is intentionally stdlib-only for runtime.
# (Tkinter ships with the standard Python installer on Windows/macOS and many Linux distros.)
# If you're on Linux and Tkinter is missing, install your distro package (e.g. python3-tk).

# No pip dependencies required.

--------------------------------------------------------------------------------
FILE: setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: src\app.py
--------------------------------------------------------------------------------
"""
app.py
------
Dumb shell for UiMAPPER.

Responsibilities:
- Create Tk root
- Apply icon/title if desired
- Instantiate backend orchestrator
- Instantiate UI orchestrator
- Start Tk mainloop

Non-goals:
- Any business logic
- Any pipeline code
- Any microservice wiring beyond "backend + ui orchestrators"
"""

from __future__ import annotations

import tkinter as tk

# Adjust imports if your package/module paths differ
from .backend import get_backend
from .ui import build_ui


APP_TITLE = "UiMAPPER"


def main() -> None:
    root = tk.Tk()
    root.title(APP_TITLE)

    # Optional: set minimum size + initial geometry
    root.minsize(1100, 720)
    root.geometry("1180x760")

    # Optional: icon
    # If you have an .ico in assets/icons, wire it here.
    # try:
    #     root.iconbitmap("assets/icons/uimapper.ico")
    # except Exception:
    #     pass

    backend = get_backend()
    build_ui(root, backend)

    root.mainloop()


if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
FILE: src\backend.py
--------------------------------------------------------------------------------
"""
backend.py
----------
Backend orchestrator for UiMAPPER.

Design goals:
- No Tkinter code here.
- Orchestrator only: wires microservices, owns run lifecycle, cancellation, threading.
- Emits progress events through ProgressEventBusMS.
- Maintains a RunSessionState (via RunSessionStateMS).
- Produces UiMap + reports (md/json/jsonl).
- Optional inference step using Ollama (HITL decisions are returned, not UI-rendered).

Expected project layout (typical):
src/
  app.py              # dumb shell
  ui.py               # UI orchestrator
  backend.py          # THIS FILE
  microservices/
    ...               # microservices imported below

Notes:
- This module is intentionally self-contained + defensive.
- Where your existing microservice names/paths differ, adjust imports at the top.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from threading import Thread, Lock
from typing import Any, Callable, Dict, List, Optional, Tuple
import time
import uuid


# -------------------------
# Imports: Microservices
# -------------------------
# Adjust these import paths to match your stamped project structure.

# Core run infra
from .microservices.CancellationTokenMS import CancellationTokenMS
from .microservices.ProgressEventBusMS import ProgressEventBusMS
from .microservices.RunSessionStateMS import RunSessionStateMS, RunSessionState
from .microservices.ErrorNormalizerMS import ErrorNormalizerMS

# Discovery
from .microservices.GitignoreFilterMS import GitignoreFilterMS
from .microservices.PythonFileEnumeratorMS import PythonFileEnumeratorMS, PythonEnumConfig
from .microservices.EntrypointFinderMS import EntrypointFinderMS

# AST + mapping
from .microservices.AstParseCacheMS import AstParseCacheMS
from .microservices.AstUiMapMS import AstUiMapMS
from .microservices.UnknownCaseCollectorMS import UnknownCaseCollectorMS

# Callback graph
from .microservices.CallbackGraphBuilderMS import CallbackGraphBuilderMS

# Canonical model + reports
from .microservices.UiMapModelMS import UiMapModelMS
from .microservices.ReportWriterMS import ReportWriterMS
from .microservices.ReportSerializerMS import ReportSerializerMS

# Optional inference/HITL
from .microservices.InferencePromptBuilderMS import InferencePromptBuilderMS
from .microservices.OllamaClientMS import OllamaClientMS, OllamaClientConfig
from .microservices.InferenceResultValidatorMS import InferenceResultValidatorMS
from .microservices.HitlDecisionRouterMS import HitlDecisionRouterMS, HitlPolicy


# -------------------------
# External dependency microservice (expected)
# -------------------------
# If you already stamped ProjectCrawlMS, import it here.
# Otherwise, implement/adjust to your crawl service interface:
#   crawl(project_root: Path, path_filter: Optional[Callable[[Path], bool]]) -> Iterable[CrawlEntry]
# where CrawlEntry has: abs_path: Path, rel_path: Path, is_dir: bool
try:
    from .microservices.ProjectCrawlMS import ProjectCrawlMS
except Exception:  # pragma: no cover
    ProjectCrawlMS = None  # type: ignore


# -------------------------
# Backend Settings
# -------------------------

@dataclass
class BackendSettings:
    # Output folder for reports; defaults to <project_root>/_uimapper_reports
    report_out_dir: Optional[Path] = None

    # Include .pyw in enumeration
    include_pyw: bool = True

    # Inference toggles
    enable_inference: bool = False
    ollama_base_url: str = "http://127.0.0.1:11434"
    ollama_model: str = ""  # e.g. "qwen2.5-coder:7b-instruct"
    ollama_timeout_sec: float = 30.0
    inference_max_cases: int = 10

    # HITL policy thresholds
    hitl_policy: HitlPolicy = field(default_factory=HitlPolicy)

    # Report formats
    write_md: bool = True
    write_json: bool = True
    write_jsonl: bool = True


# -------------------------
# Backend Orchestrator
# -------------------------

class BackendOrchestrator:
    """
    Threaded backend pipeline runner.

    Public surface:
    - start_run(project_root, settings) -> session_id
    - cancel_run(reason="user")
    - reset_session()
    - get_state_dict()
    - get_state()  (returns RunSessionState)
    - take_latest_decision_plan()  (returns plan once; then cleared)

    Progress + logs emitted through ProgressEventBusMS (provided or internal).
    """

    def __init__(
        self,
        *,
        event_bus: Optional[ProgressEventBusMS] = None,
    ):
        self.bus = event_bus or ProgressEventBusMS()
        self.token = CancellationTokenMS()
        self.state_ms = RunSessionStateMS()
        self.err_ms = ErrorNormalizerMS()

        self._state_lock = Lock()
        self._session: RunSessionState = self.state_ms.new_session(session_id=self._new_session_id())
        self._worker: Optional[Thread] = None
        self._latest_decision_plan: Optional[Dict[str, Any]] = None

        # Microservices (stateless or light state)
        self.py_enum_ms = PythonFileEnumeratorMS(PythonEnumConfig(include_pyw=True))
        self.ast_cache_ms = AstParseCacheMS()
        self.uimap_model_ms = UiMapModelMS()
        self.report_writer_ms = ReportWriterMS()
        self.report_serializer_ms = ReportSerializerMS()

        self.cb_graph_ms = CallbackGraphBuilderMS()
        self.unknown_collector_ms = UnknownCaseCollectorMS()

        self.infer_prompt_ms = InferencePromptBuilderMS()
        self.infer_validator_ms = InferenceResultValidatorMS()
        self.hitl_router_ms = HitlDecisionRouterMS()

        # Crawl + mapping are created per run (need project root)
        self._crawl_ms = ProjectCrawlMS() if ProjectCrawlMS is not None else None

    # -------------------------
    # Public API
    # -------------------------

    def start_run(self, project_root: Path, settings: Optional[BackendSettings] = None) -> str:
        """
        Start a new run. If a run is active, this will refuse and return current session_id.
        """
        settings = settings or BackendSettings()
        project_root = Path(project_root).resolve()

        with self._state_lock:
            if self._worker is not None and self._worker.is_alive():
                self.bus.emit(
                    type="backend",
                    message="Run already in progress; start_run ignored.",
                    level="warn",
                    meta={"session_id": self._session.session_id},
                )
                return self._session.session_id

            # Fresh session for each run
            self._session = self.state_ms.new_session(session_id=self._new_session_id())
            self._latest_decision_plan = None
            self.token.reset()
            self.ast_cache_ms.clear()

            self.state_ms.set_project_root(self._session, project_root)
            self.state_ms.set_status(self._session, "running")
            self._emit_stage("start", f"Starting run for: {project_root}")

            # Apply config updates
            self.py_enum_ms.config.include_pyw = bool(settings.include_pyw)
            self.hitl_router_ms.policy = settings.hitl_policy

            # Ensure crawl service exists
            if self._crawl_ms is None:
                self.state_ms.set_status(self._session, "error", error="ProjectCrawlMS not available")
                self.bus.emit(type="error", message="ProjectCrawlMS not available; cannot run.", level="error")
                return self._session.session_id

            # Spawn worker
            self._worker = Thread(
                target=self._run_worker,
                args=(project_root, settings),
                daemon=True,
                name=f"UiMapperBackend-{self._session.session_id}",
            )
            self._worker.start()
            return self._session.session_id

    def cancel_run(self, reason: str = "user") -> None:
        self.token.cancel(reason=reason)
        self.bus.emit(type="cancel", message=f"Cancellation requested: {reason}", level="warn")

    def reset_session(self) -> None:
        with self._state_lock:
            if self._worker is not None and self._worker.is_alive():
                self.cancel_run("reset_session")
            self.state_ms.reset(self._session)
            self._latest_decision_plan = None
            self.ast_cache_ms.clear()
            self.token.reset()
        self.bus.emit(type="backend", message="Session reset.", level="info")

    def get_state(self) -> RunSessionState:
        with self._state_lock:
            return self._session

    def get_state_dict(self) -> Dict[str, Any]:
        with self._state_lock:
            return self._session.to_dict()

    def take_latest_decision_plan(self) -> Optional[Dict[str, Any]]:
        """
        Returns the last decision plan (if any) once, then clears it.
        UI orchestrator calls this after run completes to present approvals.
        """
        with self._state_lock:
            plan = self._latest_decision_plan
            self._latest_decision_plan = None
            return plan

    # -------------------------
    # Worker
    # -------------------------

    def _run_worker(self, project_root: Path, settings: BackendSettings) -> None:
        t0 = time.time()
        try:
            cancel = self.token.predicate()

            # -------------------------
            # Stage 1: .gitignore filter
            # -------------------------
            self._emit_stage("gitignore", "Loading .gitignore rules...")
            gitignore_ms = GitignoreFilterMS(root=project_root)
            gitignore_ms.load()
            git_pred = gitignore_ms.predicate()

            if cancel():
                return self._finish_cancelled()

            # -------------------------
            # Stage 2: Crawl project
            # -------------------------
            self._emit_stage("crawl", "Crawling project...")
            crawl_entries = []
            for entry in self._crawl_ms.crawl(project_root, path_filter=git_pred):
                if cancel():
                    return self._finish_cancelled()
                crawl_entries.append(entry)
                # update session counters lightly
                try:
                    self.state_ms.add_entry(self._session, entry.rel_path, entry.is_dir)
                except Exception:
                    pass

            self._emit_stage("crawl", f"Crawl complete. Entries: {len(crawl_entries)}")

            # -------------------------
            # Stage 3: Enumerate Python files
            # -------------------------
            self._emit_stage("enumerate", "Enumerating Python files...")
            py_files = self.py_enum_ms.enumerate(crawl_entries)
            self.state_ms.set_py_files(self._session, py_files)
            self._emit_stage("enumerate", f"Python files: {len(py_files)}")

            if cancel():
                return self._finish_cancelled()

            # -------------------------
            # Stage 4: Entrypoint candidates
            # -------------------------
            self._emit_stage("entrypoints", "Finding entrypoint candidates...")
            ep_finder = EntrypointFinderMS(project_root=project_root)
            candidates = ep_finder.find_candidates(py_files)
            self.state_ms.set_entrypoints(self._session, candidates)
            self._emit_stage("entrypoints", f"Entrypoint candidates: {len(candidates)}")

            if cancel():
                return self._finish_cancelled()

            # -------------------------
            # Stage 5: AST parse
            # -------------------------
            self._emit_stage("ast", "Parsing ASTs...")
            ast_by_path: Dict[Path, Any] = {}
            for p in py_files:
                if cancel():
                    return self._finish_cancelled()
                res = self.ast_cache_ms.parse(p)
                if res.ok and res.tree is not None:
                    ast_by_path[p] = res.tree
                    self.state_ms.add_ast_ok(self._session, p)
                else:
                    err = res.error
                    self.state_ms.add_ast_error(
                        self._session,
                        {
                            "path": str(p),
                            "message": getattr(err, "message", "parse_error"),
                            "lineno": getattr(err, "lineno", None),
                            "col": getattr(err, "col_offset", None),
                        },
                    )

            self._emit_stage("ast", f"AST parsed. ok={self._session.counters.ast_ok} err={self._session.counters.ast_err}")

            if cancel():
                return self._finish_cancelled()

            # -------------------------
            # Stage 6: UI map
            # -------------------------
            self._emit_stage("map", "Building UI map...")
            mapper = AstUiMapMS(project_root=project_root)
            ui_map_obj = mapper.map_project(
                ast_by_path,
                cancel=cancel,
                log=self.bus.make_logger("map"),
            )

            # Consolidate unknowns (optional, but useful for inference selection)
            self.unknown_collector_ms.clear()
            for u in getattr(ui_map_obj, "unknowns", []) or []:
                where = getattr(u, "where", None)
                self.unknown_collector_ms.record(
                    kind=getattr(u, "kind", "unknown"),
                    detail=getattr(u, "detail", ""),
                    path=Path(getattr(where, "path", str(project_root))),
                    lineno=getattr(where, "lineno", None),
                    col=getattr(where, "col", None),
                    snippet=getattr(u, "snippet", None),
                    context=getattr(u, "context", None) or {},
                )

            # Normalize to dict for state storage
            ui_map_dict = ui_map_obj.to_dict() if hasattr(ui_map_obj, "to_dict") else ui_map_obj  # type: ignore
            self.state_ms.set_ui_map(self._session, ui_map_dict)
            self._emit_stage(
                "map",
                f"UI map built. windows={self._session.counters.windows} widgets={self._session.counters.widgets} unknowns={self._session.counters.unknowns}",
            )

            if cancel():
                return self._finish_cancelled()

            # -------------------------
            # Stage 7: Callback graph
            # -------------------------
            self._emit_stage("graph", "Building callback graph...")
            graph = self.cb_graph_ms.build(ast_by_path, ui_map_obj)
            # Store minimal callback graph for UI; keep it light
            graph_dict = {
                "nodes": {k: {"kind": v.kind, "key": v.key} for k, v in graph.nodes.items()},
                "edges": [
                    {"src": f"{e.src.kind}:{e.src.key}", "dst": f"{e.dst.kind}:{e.dst.key}", "kind": e.kind}
                    for e in graph.edges
                ],
                "unknowns": list(graph.unknowns),
            }
            with self._state_lock:
                self._session.callback_graph = graph_dict
            self._emit_stage("graph", f"Callback graph: nodes={len(graph.nodes)} edges={len(graph.edges)}")

            if cancel():
                return self._finish_cancelled()

            # -------------------------
            # Stage 8: Optional inference (prepare HITL plan)
            # -------------------------
            if settings.enable_inference:
                self._emit_stage("inference", "Preparing inference prompt...")
                unknown_cases = self.unknown_collector_ms.select_for_inference(max_items=settings.inference_max_cases)
                if not unknown_cases:
                    self._emit_stage("inference", "No unknown cases selected; skipping inference.")
                elif not settings.ollama_model.strip():
                    self._emit_stage("inference", "No ollama_model set; skipping inference.", level="warn")
                else:
                    prompt = self.infer_prompt_ms.build_prompt(
                        project_root=str(project_root),
                        unknown_cases=unknown_cases,
                    )

                    self._emit_stage("inference", f"Calling Ollama model: {settings.ollama_model}")
                    client = OllamaClientMS(
                        OllamaClientConfig(
                            base_url=settings.ollama_base_url,
                            timeout_sec=settings.ollama_timeout_sec,
                        )
                    )
                    resp = client.generate(
                        model=settings.ollama_model,
                        prompt=prompt,
                        format="json",
                        stream=False,
                    )
                    if not resp.ok or not resp.text:
                        self._emit_stage(
                            "inference",
                            "Ollama inference failed; continuing without applying results.",
                            level="warn",
                        )
                    else:
                        outcome = self.infer_validator_ms.validate_json_text(resp.text)
                        if not outcome.ok:
                            self._emit_stage(
                                "inference",
                                "Inference JSON invalid; continuing without applying results.",
                                level="warn",
                                meta={"errors": [e.message for e in outcome.errors]},
                            )
                        else:
                            plan = self.hitl_router_ms.build_plan(outcome.results)

                            # Auto-apply decisions conservatively (currently adds annotations)
                            auto_applied = 0
                            for item in plan.items:
                                if item.action == "auto_apply":
                                    self.uimap_model_ms.apply_inference_patch(
                                        ui_map_obj,
                                        case_id=item.case_id,
                                        classification=item.classification,
                                        extracted=item.extracted,
                                        notes=item.notes,
                                    )
                                    auto_applied += 1

                            # Store remaining decisions for UI to present (ask_user)
                            plan_dict = {
                                "stats": dict(plan.stats),
                                "items": [
                                    {
                                        "case_id": it.case_id,
                                        "classification": it.classification,
                                        "confidence": it.confidence,
                                        "action": it.action,
                                        "extracted": dict(it.extracted),
                                        "notes": it.notes,
                                    }
                                    for it in plan.items
                                ],
                            }
                            with self._state_lock:
                                self._latest_decision_plan = plan_dict

                            # Refresh state ui_map after annotation
                            ui_map_dict2 = ui_map_obj.to_dict() if hasattr(ui_map_obj, "to_dict") else ui_map_obj  # type: ignore
                            self.state_ms.set_ui_map(self._session, ui_map_dict2)

                            self._emit_stage(
                                "inference",
                                f"Inference complete. auto_applied={auto_applied} ask_user={plan.stats.get('ask_user', 0)}",
                            )

            if cancel():
                return self._finish_cancelled()

            # -------------------------
            # Stage 9: Reports
            # -------------------------
            out_dir = settings.report_out_dir
            if out_dir is None:
                out_dir = project_root / "_uimapper_reports"
            out_dir = Path(out_dir).resolve()
            out_dir.mkdir(parents=True, exist_ok=True)

            md_path = out_dir / "uimap_report.md"
            json_path = out_dir / "uimap_report.json"
            jsonl_path = out_dir / "uimap_report.jsonl"

            self._emit_stage("report", f"Writing reports to: {out_dir}")

            if settings.write_md:
                self.report_writer_ms.write_markdown(ui_map_obj, md_path)
            if settings.write_json:
                self.report_serializer_ms.write_json(ui_map_obj, json_path)
            if settings.write_jsonl:
                self.report_serializer_ms.write_jsonl(ui_map_obj, jsonl_path)

            self.state_ms.set_report_paths(
                self._session,
                md=md_path if settings.write_md else None,
                json_path=json_path if settings.write_json else None,
                jsonl=jsonl_path if settings.write_jsonl else None,
            )

            # Done
            dt = time.time() - t0
            with self._state_lock:
                self._session.meta["duration_sec"] = round(dt, 3)
            self.state_ms.set_status(self._session, "done")
            self._emit_stage("done", f"Run complete in {dt:.2f}s.")

        except Exception as e:
            ne = self.err_ms.normalize(e, include_traceback=False)
            self.state_ms.set_status(self._session, "error", error=ne.message)
            self.bus.emit(type="error", message=ne.message, level="error", meta=ne.to_dict())

    # -------------------------
    # Internal helpers
    # -------------------------

    def _finish_cancelled(self) -> None:
        self.state_ms.set_status(self._session, "cancelled", error=self.token.reason())
        self.bus.emit(type="cancelled", message="Run cancelled.", level="warn", meta={"reason": self.token.reason()})

    def _emit_stage(self, stage: str, message: str, *, level: str = "info", meta: Optional[Dict[str, Any]] = None) -> None:
        self.bus.emit(type="stage", message=f"{stage}: {message}", level=level, meta=meta or {})

    def _new_session_id(self) -> str:
        return uuid.uuid4().hex[:10]


# -------------------------
# Optional: module-level convenience singleton
# -------------------------

_backend_singleton: Optional[BackendOrchestrator] = None


def get_backend(event_bus: Optional[ProgressEventBusMS] = None) -> BackendOrchestrator:
    global _backend_singleton
    if _backend_singleton is None:
        _backend_singleton = BackendOrchestrator(event_bus=event_bus)
    return _backend_singleton



--------------------------------------------------------------------------------
FILE: src\ui.py
--------------------------------------------------------------------------------
"""
ui.py
-----
UI orchestrator for UiMAPPER.

Design goals:
- Own ALL Tkinter concerns: theme/colors, widgets, layout, user interactions.
- Be an orchestrator: no business logic / analysis logic inside.
- Talks to backend orchestrator via a small API:
    - start_run(project_root, settings)
    - cancel_run()
    - get_state_dict()
    - take_latest_decision_plan()
- Subscribes to ProgressEventBusMS and marshals events onto Tk thread.
- Maintains lightweight UI state (selected folder, settings toggles).

This file provides:
- UiOrchestrator class with:
    - build(root) -> Frame (main container)
    - set_backend(backend) (dependency injection)
    - attach_event_bus(bus) (subscribe)
"""

from __future__ import annotations

import json
import threading
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Callable, List

import tkinter as tk
from tkinter import ttk, filedialog, messagebox


# -------------------------
# Backend import
# -------------------------

# Adjust import to match your project.
from .backend import BackendOrchestrator, BackendSettings

# -------------------------
# Theme (match your helper scripts)
# -------------------------

@dataclass(frozen=True)
class Theme:
    bg: str = "#0f1117"
    panel: str = "#151a22"
    panel2: str = "#11151c"
    fg: str = "#e6edf3"
    muted: str = "#9aa4b2"
    accent: str = "#6cb6ff"
    warn: str = "#f2cc60"
    err: str = "#ff6b6b"
    ok: str = "#7ee787"
    border: str = "#243042"
    entry_bg: str = "#0b0e14"
    sel_bg: str = "#243b55"
    sel_fg: str = "#e6edf3"


THEME = Theme()


def apply_dark_ttk_style(root: tk.Tk) -> None:
    """
    Minimal ttk dark theme. This should be aligned with your microservice examples.
    """
    style = ttk.Style(root)
    try:
        style.theme_use("clam")
    except Exception:
        pass

    style.configure(".", font=("Segoe UI", 10), foreground=THEME.fg)
    style.configure("TFrame", background=THEME.bg)
    style.configure("Panel.TFrame", background=THEME.panel)
    style.configure("TLabel", background=THEME.bg, foreground=THEME.fg)
    style.configure("Muted.TLabel", background=THEME.bg, foreground=THEME.muted)

    style.configure("TButton", background=THEME.panel, foreground=THEME.fg, borderwidth=1)
    style.map(
        "TButton",
        background=[("active", THEME.panel2)],
        foreground=[("disabled", THEME.muted)],
    )

    style.configure("Accent.TButton", background=THEME.accent, foreground="#081018")
    style.map("Accent.TButton", background=[("active", THEME.accent)])

    style.configure("TEntry", fieldbackground=THEME.entry_bg, foreground=THEME.fg)
    style.configure("TCheckbutton", background=THEME.bg, foreground=THEME.fg)
    style.map("TCheckbutton", foreground=[("disabled", THEME.muted)])

    style.configure("TNotebook", background=THEME.bg, borderwidth=0)
    style.configure("TNotebook.Tab", background=THEME.panel, foreground=THEME.fg, padding=(10, 6))
    style.map("TNotebook.Tab", background=[("selected", THEME.panel2)])

    style.configure(
        "Treeview",
        background=THEME.panel,
        fieldbackground=THEME.panel,
        foreground=THEME.fg,
        bordercolor=THEME.border,
        lightcolor=THEME.border,
        darkcolor=THEME.border,
        rowheight=22,
    )
    style.map("Treeview", background=[("selected", THEME.sel_bg)], foreground=[("selected", THEME.sel_fg)])
    style.configure("Treeview.Heading", background=THEME.panel2, foreground=THEME.fg, relief="flat")

    style.configure("TScrollbar", background=THEME.panel, troughcolor=THEME.bg, bordercolor=THEME.bg, arrowcolor=THEME.fg)


# -------------------------
# UI Orchestrator
# -------------------------

class UiOrchestrator:
    def __init__(self, backend: BackendOrchestrator):
        self.backend = backend
        self.bus = backend.bus

        self.root: Optional[tk.Tk] = None
        self.container: Optional[ttk.Frame] = None

        # UI vars
        self.var_project_root = tk.StringVar(value="")
        self.var_enable_inference = tk.BooleanVar(value=False)
        self.var_model = tk.StringVar(value="")
        self.var_include_pyw = tk.BooleanVar(value=True)

        # Widgets we update
        self._status_lbl: Optional[ttk.Label] = None
        self._counts_lbl: Optional[ttk.Label] = None
        self._log_text: Optional[tk.Text] = None
        self._tree: Optional[ttk.Treeview] = None

        # Progress event queue (marshalled with after). Guard with a lock because callbacks may fire off-thread.
        self._event_queue: List[Dict[str, Any]] = []
        self._event_queue_limit = 500
        self._event_lock = threading.Lock()

        # Subscribe once
        self.bus.subscribe(self._on_progress_event)

    # -------------------------
    # Public
    # -------------------------

    def build(self, root: tk.Tk) -> ttk.Frame:
        self.root = root
        apply_dark_ttk_style(root)
        root.configure(bg=THEME.bg)

        self.container = ttk.Frame(root, style="TFrame")
        self.container.pack(fill="both", expand=True)

        self._build_header(self.container)
        self._build_body(self.container)
        self._build_footer(self.container)

        # Start periodic UI update loop
        self._tick()

        return self.container

    def destroy(self) -> None:
        # Unsubscribe
        try:
            self.bus.unsubscribe(self._on_progress_event)
        except Exception:
            pass

    # -------------------------
    # Layout
    # -------------------------

    def _build_header(self, parent: ttk.Frame) -> None:
        hdr = ttk.Frame(parent, style="Panel.TFrame")
        hdr.pack(fill="x", padx=10, pady=(10, 6))

        title = ttk.Label(hdr, text="UiMAPPER", font=("Segoe UI", 14, "bold"), background=THEME.panel, foreground=THEME.fg)
        title.grid(row=0, column=0, sticky="w", padx=10, pady=8)

        self._status_lbl = ttk.Label(hdr, text="idle", style="Muted.TLabel", background=THEME.panel)
        self._status_lbl.grid(row=0, column=1, sticky="e", padx=10)

        hdr.columnconfigure(0, weight=1)
        hdr.columnconfigure(1, weight=0)

    def _build_body(self, parent: ttk.Frame) -> None:
        body = ttk.Frame(parent, style="TFrame")
        body.pack(fill="both", expand=True, padx=10, pady=6)

        # Left: controls + log
        left = ttk.Frame(body, style="Panel.TFrame")
        left.pack(side="left", fill="y", padx=(0, 8), pady=0)

        self._build_controls(left)
        self._build_log(left)

        # Right: results
        right = ttk.Frame(body, style="Panel.TFrame")
        right.pack(side="left", fill="both", expand=True, padx=(8, 0), pady=0)

        self._build_results(right)

        left.configure(padding=10)
        right.configure(padding=10)

    def _build_controls(self, parent: ttk.Frame) -> None:
        box = ttk.Frame(parent, style="Panel.TFrame")
        box.pack(fill="x")

        # Project root
        ttk.Label(box, text="Project Root", background=THEME.panel, foreground=THEME.fg).grid(row=0, column=0, sticky="w")
        ent = ttk.Entry(box, textvariable=self.var_project_root, width=42)
        ent.grid(row=1, column=0, sticky="ew", pady=(4, 6))
        btn_browse = ttk.Button(box, text="Browse…", command=self._browse_folder)
        btn_browse.grid(row=1, column=1, sticky="e", padx=(8, 0), pady=(4, 6))

        # Options
        chk_pyw = ttk.Checkbutton(box, text="Include .pyw", variable=self.var_include_pyw)
        chk_pyw.grid(row=2, column=0, sticky="w", pady=(2, 2))

        chk_inf = ttk.Checkbutton(box, text="Enable inference (Ollama)", variable=self.var_enable_inference)
        chk_inf.grid(row=3, column=0, sticky="w", pady=(2, 6))

        ttk.Label(box, text="Ollama Model", style="Muted.TLabel", background=THEME.panel).grid(row=4, column=0, sticky="w")
        ent_model = ttk.Entry(box, textvariable=self.var_model)
        ent_model.grid(row=5, column=0, columnspan=2, sticky="ew", pady=(4, 10))

        # Actions
        btn_run = ttk.Button(box, text="Run", style="Accent.TButton", command=self._run_clicked)
        btn_run.grid(row=6, column=0, sticky="ew", pady=(0, 6))

        btn_cancel = ttk.Button(box, text="Cancel", command=self._cancel_clicked)
        btn_cancel.grid(row=7, column=0, sticky="ew", pady=(0, 6))

        btn_open_reports = ttk.Button(box, text="Open Report Folder", command=self._open_report_folder)
        btn_open_reports.grid(row=8, column=0, sticky="ew", pady=(0, 0))

        box.columnconfigure(0, weight=1)

        self._counts_lbl = ttk.Label(box, text="", style="Muted.TLabel", background=THEME.panel)
        self._counts_lbl.grid(row=9, column=0, sticky="w", pady=(10, 0))

    def _build_log(self, parent: ttk.Frame) -> None:
        ttk.Label(parent, text="Log", background=THEME.panel, foreground=THEME.fg).pack(anchor="w", pady=(10, 4))

        txt = tk.Text(
            parent,
            height=18,
            bg=THEME.entry_bg,
            fg=THEME.fg,
            insertbackground=THEME.fg,
            relief="flat",
            wrap="word",
        )
        txt.pack(fill="both", expand=False)
        txt.configure(state="disabled")
        self._log_text = txt

    def _build_results(self, parent: ttk.Frame) -> None:
        ttk.Label(parent, text="Results", background=THEME.panel, foreground=THEME.fg).pack(anchor="w", pady=(0, 6))

        cols = ("kind", "value")
        tree = ttk.Treeview(parent, columns=cols, show="headings", height=18)
        tree.heading("kind", text="Kind")
        tree.heading("value", text="Value")
        tree.column("kind", width=160, anchor="w")
        tree.column("value", width=520, anchor="w")
        tree.pack(fill="both", expand=True)

        self._tree = tree

        btns = ttk.Frame(parent, style="Panel.TFrame")
        btns.pack(fill="x", pady=(10, 0))

        ttk.Button(btns, text="View Decision Plan", command=self._view_decision_plan).pack(side="left")
        ttk.Button(btns, text="Copy UiMap JSON", command=self._copy_uimap_json).pack(side="left", padx=(8, 0))

    def _build_footer(self, parent: ttk.Frame) -> None:
        ftr = ttk.Frame(parent, style="TFrame")
        ftr.pack(fill="x", padx=10, pady=(6, 10))

        ttk.Label(
            ftr,
            text="Tip: inference results are staged as a decision plan. Auto-apply uses high confidence only; approvals are manual.",
            style="Muted.TLabel",
        ).pack(anchor="w")

    # -------------------------
    # Event handling / updates
    # -------------------------

    def _on_progress_event(self, event: Dict[str, Any]) -> None:
        # This may be called from a worker thread.
        with self._event_lock:
            if len(self._event_queue) >= self._event_queue_limit:
                self._event_queue.pop(0)
            self._event_queue.append(event)

    def _tick(self) -> None:
        """
        Periodic UI refresh:
        - consume progress events
        - poll backend state
        """
        if self.root is None:
            return

        self._drain_events()
        self._refresh_state_view()

        # keep ticking
        self.root.after(150, self._tick)

    def _drain_events(self) -> None:
        with self._event_lock:
            if not self._event_queue:
                return

            events = self._event_queue[:]
            self._event_queue.clear()

        for ev in events:
            self._append_log(self._format_event(ev))

    def _refresh_state_view(self) -> None:
        st = self.backend.get_state_dict()
        status = st.get("status", "<?>")
        err = st.get("last_error", None)

        if self._status_lbl is not None:
            txt = status if not err else f"{status}  (error: {err})"
            self._status_lbl.configure(text=txt)

        # counts
        c = (st.get("counters") or {})
        counts_line = (
            f"dirs={c.get('dirs_seen',0)}  files={c.get('files_seen',0)}  py={c.get('py_files',0)}  "
            f"ast_ok={c.get('ast_ok',0)}  ast_err={c.get('ast_err',0)}  "
            f"windows={c.get('windows',0)}  widgets={c.get('widgets',0)}  unknowns={c.get('unknowns',0)}"
        )
        if self._counts_lbl is not None:
            self._counts_lbl.configure(text=counts_line)

        # result tree summary
        if self._tree is not None:
            self._tree.delete(*self._tree.get_children())
            self._tree.insert("", "end", values=("session_id", st.get("session_id", "")))
            self._tree.insert("", "end", values=("project_root", st.get("project_root", "")))
            self._tree.insert("", "end", values=("status", status))
            if st.get("report_md_path"):
                self._tree.insert("", "end", values=("report_md", st.get("report_md_path")))
            if st.get("report_json_path"):
                self._tree.insert("", "end", values=("report_json", st.get("report_json_path")))
            if st.get("report_jsonl_path"):
                self._tree.insert("", "end", values=("report_jsonl", st.get("report_jsonl_path")))

            # entrypoint candidates (top 5)
            eps = st.get("entrypoint_candidates") or []
            for i, ep in enumerate(eps[:5], start=1):
                self._tree.insert("", "end", values=(f"entrypoint_{i}", ep.get("path", "")))

    # -------------------------
    # UI actions
    # -------------------------

    def _browse_folder(self) -> None:
        if self.root is None:
            return
        p = filedialog.askdirectory(title="Select project root")
        if p:
            self.var_project_root.set(p)

    def _run_clicked(self) -> None:
        p = self.var_project_root.get().strip()
        if not p:
            messagebox.showwarning("Missing", "Select a project root.")
            return

        settings = BackendSettings(
            include_pyw=bool(self.var_include_pyw.get()),
            enable_inference=bool(self.var_enable_inference.get()),
            ollama_model=self.var_model.get().strip(),
        )
        self.backend.start_run(Path(p), settings=settings)

    def _cancel_clicked(self) -> None:
        self.backend.cancel_run("user")

    def _open_report_folder(self) -> None:
        st = self.backend.get_state_dict()
        md = st.get("report_md_path") or st.get("report_json_path") or st.get("report_jsonl_path")
        if not md:
            messagebox.showinfo("No reports yet", "Run the mapper to generate reports.")
            return

        folder = str(Path(md).resolve().parent)
        try:
            import os, subprocess, sys
            if sys.platform.startswith("win"):
                os.startfile(folder)  # type: ignore
            elif sys.platform == "darwin":
                subprocess.Popen(["open", folder])
            else:
                subprocess.Popen(["xdg-open", folder])
        except Exception:
            messagebox.showinfo("Folder", folder)

    def _view_decision_plan(self) -> None:
        plan = self.backend.take_latest_decision_plan()
        if not plan:
            messagebox.showinfo("Decision Plan", "No decision plan available (either inference disabled or no items).")
            return

        # Simple viewer dialog
        top = tk.Toplevel(self.root)
        top.title("Decision Plan")
        top.configure(bg=THEME.bg)

        txt = tk.Text(top, bg=THEME.entry_bg, fg=THEME.fg, insertbackground=THEME.fg, relief="flat", wrap="none")
        txt.pack(fill="both", expand=True, padx=10, pady=10)

        txt.insert("1.0", json.dumps(plan, indent=2))
        txt.configure(state="disabled")

        ttk.Button(top, text="Close", command=top.destroy).pack(pady=(0, 10))

        top.geometry("900x600")

    def _copy_uimap_json(self) -> None:
        st = self.backend.get_state_dict()
        uimap = st.get("ui_map")
        if not uimap:
            messagebox.showinfo("UiMap", "No UiMap available yet.")
            return

        s = json.dumps(uimap, indent=2)
        self.root.clipboard_clear()
        self.root.clipboard_append(s)
        messagebox.showinfo("Copied", "UiMap JSON copied to clipboard.")

    # -------------------------
    # Log helpers
    # -------------------------

    def _append_log(self, line: str) -> None:
        if self._log_text is None:
            return
        self._log_text.configure(state="normal")
        self._log_text.insert("end", line + "\n")
        self._log_text.see("end")
        self._log_text.configure(state="disabled")

    def _format_event(self, ev: Dict[str, Any]) -> str:
        t = ev.get("type", "event")
        lvl = ev.get("level", "info")
        msg = ev.get("message", "")
        return f"[{lvl.upper():5}] {t}: {msg}"


# -------------------------
# Convenience builder for app.py
# -------------------------

def build_ui(root: tk.Tk, backend: BackendOrchestrator) -> UiOrchestrator:
    ui = UiOrchestrator(backend=backend)
    ui.build(root)
    return ui



--------------------------------------------------------------------------------
FILE: src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: src\microservices\AstParseCacheMS.py
--------------------------------------------------------------------------------
"""
AstParseCacheMS
---------------
Parse Python files into AST with caching + stable error reporting.

Responsibilities:
- Parse file content to ast.AST
- Cache by (path, mtime_ns, size) to avoid re-parsing
- Provide structured parse results (success or error)
- Never raise on syntax errors; return them as structured data

Non-goals:
- Project walking (ProjectCrawlMS)
- File filtering (.py selection)
- UI mapping logic (AstUiMapMS)
"""

from __future__ import annotations

import ast
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Tuple


# -------------------------
# Data Structures
# -------------------------

@dataclass(frozen=True)
class AstParseError:
    path: Path
    message: str
    lineno: Optional[int] = None
    col_offset: Optional[int] = None


@dataclass(frozen=True)
class AstParseResult:
    path: Path
    ok: bool
    tree: Optional[ast.AST] = None
    error: Optional[AstParseError] = None


@dataclass(frozen=True)
class _CacheKey:
    path: Path
    mtime_ns: int
    size: int


# -------------------------
# Service
# -------------------------

class AstParseCacheMS:
    """
    Usage:
        cache = AstParseCacheMS()
        res = cache.parse(path)
        if res.ok: use res.tree
        else: log res.error
    """

    def __init__(self):
        self._cache: Dict[_CacheKey, AstParseResult] = {}
        self._last_key_by_path: Dict[Path, _CacheKey] = {}

    def parse(self, path: Path) -> AstParseResult:
        p = Path(path).resolve()

        try:
            st = p.stat()
        except Exception as e:
            err = AstParseError(path=p, message=f"stat_failed: {e}")
            return AstParseResult(path=p, ok=False, error=err)

        key = _CacheKey(path=p, mtime_ns=st.st_mtime_ns, size=st.st_size)

        # If we previously cached a different version, remove it to keep cache small.
        prev_key = self._last_key_by_path.get(p)
        if prev_key is not None and prev_key != key:
            self._cache.pop(prev_key, None)

        self._last_key_by_path[p] = key

        cached = self._cache.get(key)
        if cached is not None:
            return cached

        result = self._parse_uncached(p)
        self._cache[key] = result
        return result

    def clear(self) -> None:
        self._cache.clear()
        self._last_key_by_path.clear()

    # -------------------------
    # Internal
    # -------------------------

    def _parse_uncached(self, path: Path) -> AstParseResult:
        try:
            text = path.read_text(encoding="utf-8", errors="replace")
        except Exception as e:
            err = AstParseError(path=path, message=f"read_failed: {e}")
            return AstParseResult(path=path, ok=False, error=err)

        try:
            tree = ast.parse(text, filename=str(path))
            return AstParseResult(path=path, ok=True, tree=tree)
        except SyntaxError as e:
            err = AstParseError(
                path=path,
                message=f"syntax_error: {e.msg}",
                lineno=getattr(e, "lineno", None),
                col_offset=getattr(e, "offset", None),
            )
            return AstParseResult(path=path, ok=False, error=err)
        except Exception as e:
            err = AstParseError(path=path, message=f"parse_failed: {e}")
            return AstParseResult(path=path, ok=False, error=err)


--------------------------------------------------------------------------------
FILE: src\microservices\AstUiMapMS.py
--------------------------------------------------------------------------------
"""
AstUiMapMS
----------
AST-driven UI mapping microservice.

Responsibilities:
- Consume parsed ASTs (from AstParseCacheMS) for a project
- Detect Tkinter UI constructs and extract a "UI Map" model:
    - windows / roots
    - widget constructions
    - geometry/layout calls (pack/grid/place)
    - config/style calls (configure, config, ttk.Style)
    - command/callback bindings (command=..., bind(...))
    - menu structures (Menu/add_command)
- Collect "unknown cases" for optional LLM/HITL inference

Non-goals:
- Project crawling / .gitignore filtering
- Threading / cancellation ownership (accepts a cancel predicate)
- LLM calls (backend orchestrator uses OllamaClientMS)
- UI (Tk) rendering

This is intentionally conservative: when uncertain, it records an UnknownCase.
"""

from __future__ import annotations

import ast
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple


# -------------------------
# Data Model
# -------------------------

@dataclass(frozen=True)
class SourceLoc:
    path: Path
    lineno: Optional[int] = None
    col: Optional[int] = None


@dataclass
class UiWidget:
    widget_id: str
    widget_type: str
    parent_id: Optional[str]
    created_at: SourceLoc
    kwargs: Dict[str, str] = field(default_factory=dict)
    layout_calls: List[str] = field(default_factory=list)
    config_calls: List[str] = field(default_factory=list)
    command_targets: List[str] = field(default_factory=list)
    bind_events: List[str] = field(default_factory=list)


@dataclass
class UiWindow:
    window_id: str
    created_at: SourceLoc
    title_calls: List[str] = field(default_factory=list)
    geometry_calls: List[str] = field(default_factory=list)
    config_calls: List[str] = field(default_factory=list)


@dataclass
class UnknownCase:
    kind: str
    detail: str
    where: SourceLoc
    snippet: Optional[str] = None


@dataclass
class UiMap:
    project_root: Path
    windows: Dict[str, UiWindow] = field(default_factory=dict)
    widgets: Dict[str, UiWidget] = field(default_factory=dict)
    unknowns: List[UnknownCase] = field(default_factory=list)
    parse_errors: List[str] = field(default_factory=list)


# -------------------------
# Config
# -------------------------

@dataclass
class AstUiMapConfig:
    # If True, include non-Tk UI constructs (future); currently unused.
    include_generic_ui: bool = False


# -------------------------
# Service
# -------------------------

class AstUiMapMS:
    def __init__(self, project_root: Path, config: Optional[AstUiMapConfig] = None):
        self.root = Path(project_root).resolve()
        self.config = config or AstUiMapConfig()

    def map_project(
        self,
        ast_by_path: Dict[Path, ast.AST],
        *,
        cancel: Optional[Callable[[], bool]] = None,
        log: Optional[Callable[[str], None]] = None,
    ) -> UiMap:
        """
        Build a UiMap from a dict of {path: ast_tree}.
        """
        cancel = cancel or (lambda: False)
        log = log or (lambda _msg: None)

        ui_map = UiMap(project_root=self.root)

        # Deterministic iteration
        for path in sorted(ast_by_path.keys(), key=lambda p: p.as_posix().lower()):
            if cancel():
                break
            tree = ast_by_path[path]
            try:
                self._map_file(path, tree, ui_map, log=log, cancel=cancel)
            except Exception as e:
                ui_map.unknowns.append(
                    UnknownCase(
                        kind="mapper_exception",
                        detail=str(e),
                        where=SourceLoc(path=path),
                    )
                )

        return ui_map

    # -------------------------
    # File mapping
    # -------------------------

    def _map_file(
        self,
        path: Path,
        tree: ast.AST,
        ui_map: UiMap,
        *,
        log: Callable[[str], None],
        cancel: Callable[[], bool],
    ) -> None:
        visitor = _TkAstVisitor(path=path, ui_map=ui_map, log=log, cancel=cancel)
        visitor.visit(tree)


# -------------------------
# AST Visitor
# -------------------------

class _TkAstVisitor(ast.NodeVisitor):
    """
    Conservative Tkinter/ttk UI mapper.

    It detects:
    - tk.Tk() / tkinter.Tk() root creation
    - ttk/tk widget constructors: Button/Frame/Label/Entry/etc.
    - .pack/.grid/.place calls
    - .configure/.config calls
    - `command=` callback names
    - .bind("...") events
    """

    TK_ROOT_NAMES = {"Tk"}
    LAYOUT_METHODS = {"pack", "grid", "place"}
    CONFIG_METHODS = {"config", "configure"}
    BIND_METHOD = "bind"

    # Common widget ctor names (heuristic)
    WIDGET_NAMES = {
        "Frame", "Label", "Button", "Entry", "Text", "Canvas", "Menu", "Scrollbar",
        "Listbox", "Toplevel", "Checkbutton", "Radiobutton", "Spinbox", "Scale",
        "PanedWindow", "LabelFrame", "Message",
        # ttk
        "Combobox", "Treeview", "Notebook", "Separator", "Progressbar",
    }

    def __init__(
        self,
        *,
        path: Path,
        ui_map: UiMap,
        log: Callable[[str], None],
        cancel: Callable[[], bool],
    ):
        self.path = path
        self.ui_map = ui_map
        self.log = log
        self.cancel = cancel

        # Basic symbol tracking
        self._imports: Dict[str, str] = {}  # alias -> module (e.g., tk -> tkinter)
        self._assigned_ids: Dict[str, str] = {}  # varname -> widget_id/window_id

        self._next_id = 1

    # -------------------------
    # Helpers
    # -------------------------

    def _new_id(self, prefix: str) -> str:
        i = self._next_id
        self._next_id += 1
        return f"{prefix}{i}"

    def _loc(self, node: ast.AST) -> SourceLoc:
        return SourceLoc(
            path=self.path,
            lineno=getattr(node, "lineno", None),
            col=getattr(node, "col_offset", None),
        )

    def _expr_to_str(self, node: ast.AST) -> str:
        """
        Best-effort compact string for an AST expression.
        Conservative: returns placeholders when complex.
        """
        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Attribute):
            base = self._expr_to_str(node.value)
            return f"{base}.{node.attr}"
        if isinstance(node, ast.Constant):
            return repr(node.value)
        if isinstance(node, ast.Call):
            fn = self._expr_to_str(node.func)
            return f"{fn}(...)"
        if isinstance(node, ast.Subscript):
            return f"{self._expr_to_str(node.value)}[...]"
        return node.__class__.__name__

    def _call_name(self, call: ast.Call) -> str:
        return self._expr_to_str(call.func)

    def _is_tk_root_call(self, call: ast.Call) -> bool:
        # tk.Tk(), tkinter.Tk(), or bare Tk() if imported
        fn = call.func
        if isinstance(fn, ast.Attribute) and fn.attr in self.TK_ROOT_NAMES:
            return True
        if isinstance(fn, ast.Name) and fn.id in self.TK_ROOT_NAMES:
            return True
        return False

    def _is_widget_ctor_call(self, call: ast.Call) -> Optional[str]:
        """
        Return widget type name if this call looks like a widget constructor.
        """
        fn = call.func
        # ttk.Button / tk.Frame
        if isinstance(fn, ast.Attribute) and fn.attr in self.WIDGET_NAMES:
            return fn.attr
        # bare Button() (from tkinter import Button)
        if isinstance(fn, ast.Name) and fn.id in self.WIDGET_NAMES:
            return fn.id
        return None

    def _extract_parent_id(self, call: ast.Call) -> Optional[str]:
        """
        First positional argument for widget constructors is usually parent.
        If it's a known var, map to widget_id/window_id.
        """
        if not call.args:
            return None
        a0 = call.args[0]
        if isinstance(a0, ast.Name):
            return self._assigned_ids.get(a0.id)
        return None

    def _extract_kwargs(self, call: ast.Call) -> Dict[str, str]:
        out: Dict[str, str] = {}
        for kw in call.keywords:
            if kw.arg is None:
                continue
            out[kw.arg] = self._expr_to_str(kw.value)
        return out

    def _maybe_record_command(self, widget: UiWidget, kwargs: Dict[str, str]) -> None:
        cmd = kwargs.get("command")
        if cmd:
            widget.command_targets.append(cmd)

    # -------------------------
    # Visit nodes
    # -------------------------

    def visit_Import(self, node: ast.Import) -> Any:
        for alias in node.names:
            name = alias.name  # e.g., tkinter
            asname = alias.asname or name
            self._imports[asname] = name
        self.generic_visit(node)

    def visit_ImportFrom(self, node: ast.ImportFrom) -> Any:
        # from tkinter import ttk as ttk
        mod = node.module or ""
        for alias in node.names:
            asname = alias.asname or alias.name
            self._imports[asname] = f"{mod}.{alias.name}" if mod else alias.name
        self.generic_visit(node)

    def visit_Assign(self, node: ast.Assign) -> Any:
        # var = Call(...)
        if self.cancel():
            return None

        if isinstance(node.value, ast.Call):
            call = node.value

            # Root window
            if self._is_tk_root_call(call):
                win_id = self._new_id("win")
                w = UiWindow(window_id=win_id, created_at=self._loc(node))
                self.ui_map.windows[win_id] = w

                for t in node.targets:
                    if isinstance(t, ast.Name):
                        self._assigned_ids[t.id] = win_id

                self.generic_visit(node)
                return None

            # Widget ctor
            wtype = self._is_widget_ctor_call(call)
            if wtype:
                wid = self._new_id("w")
                parent_id = self._extract_parent_id(call)
                kwargs = self._extract_kwargs(call)

                w = UiWidget(
                    widget_id=wid,
                    widget_type=wtype,
                    parent_id=parent_id,
                    created_at=self._loc(node),
                    kwargs=kwargs,
                )
                self._maybe_record_command(w, kwargs)
                self.ui_map.widgets[wid] = w

                for t in node.targets:
                    if isinstance(t, ast.Name):
                        self._assigned_ids[t.id] = wid

                self.generic_visit(node)
                return None

        self.generic_visit(node)
        return None

    def visit_Call(self, node: ast.Call) -> Any:
        if self.cancel():
            return None

        # method calls like: widget.pack(...), root.title(...), widget.configure(...)
        if isinstance(node.func, ast.Attribute):
            attr = node.func.attr
            receiver = node.func.value

            recv_id = None
            if isinstance(receiver, ast.Name):
                recv_id = self._assigned_ids.get(receiver.id)

            # Layout calls
            if attr in self.LAYOUT_METHODS and recv_id and recv_id in self.ui_map.widgets:
                w = self.ui_map.widgets[recv_id]
                w.layout_calls.append(f"{attr}({self._args_sig(node)})")
                self.generic_visit(node)
                return None

            # Config calls
            if attr in self.CONFIG_METHODS and recv_id:
                sig = f"{attr}({self._args_sig(node)})"
                if recv_id in self.ui_map.widgets:
                    self.ui_map.widgets[recv_id].config_calls.append(sig)
                elif recv_id in self.ui_map.windows:
                    self.ui_map.windows[recv_id].config_calls.append(sig)
                else:
                    self.ui_map.unknowns.append(
                        UnknownCase(
                            kind="config_on_unknown_receiver",
                            detail=f"{self._expr_to_str(receiver)}.{attr}",
                            where=self._loc(node),
                        )
                    )
                self.generic_visit(node)
                return None

            # Window title / geometry
            if recv_id and recv_id in self.ui_map.windows:
                if attr == "title":
                    self.ui_map.windows[recv_id].title_calls.append(self._call_render(node))
                elif attr == "geometry":
                    self.ui_map.windows[recv_id].geometry_calls.append(self._call_render(node))

            # bind events
            if attr == self.BIND_METHOD and recv_id and recv_id in self.ui_map.widgets:
                w = self.ui_map.widgets[recv_id]
                ev = self._extract_bind_event(node)
                w.bind_events.append(ev)
                self.generic_visit(node)
                return None

        self.generic_visit(node)
        return None

    # -------------------------
    # Call rendering helpers
    # -------------------------

    def _args_sig(self, call: ast.Call) -> str:
        """
        Compact signature string: positional + keyword names.
        """
        parts: List[str] = []
        for a in call.args:
            parts.append(self._expr_to_str(a))
        for kw in call.keywords:
            if kw.arg is None:
                parts.append("**kwargs")
            else:
                parts.append(f"{kw.arg}={self._expr_to_str(kw.value)}")
        return ", ".join(parts)

    def _call_render(self, call: ast.Call) -> str:
        return f"{self._call_name(call)}({self._args_sig(call)})"

    def _extract_bind_event(self, call: ast.Call) -> str:
        """
        bind("<Button-1>", handler) => "<Button-1> -> handler"
        """
        event = "<?>"
        handler = "<?>"
        if call.args:
            event = self._expr_to_str(call.args[0])
        if len(call.args) >= 2:
            handler = self._expr_to_str(call.args[1])
        return f"{event} -> {handler}"


--------------------------------------------------------------------------------
FILE: src\microservices\base_service.py
--------------------------------------------------------------------------------
import logging
from typing import Dict, Any

class BaseService:
    """
    Standard parent class for all microservices. 
    Provides consistent logging and identity management.
    """
    def __init__(self, name: str):
        self._service_info = {
            "name": name, 
            "id": name.lower().replace(" ", "_")
        }
        
        # Setup standard logging
        logging.basicConfig(
            level=logging.INFO, 
            format='%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(name)

    def log_info(self, message: str):
        self.logger.info(message)

    def log_error(self, message: str):
        self.logger.error(message)

    def log_warning(self, message: str):
        self.logger.warning(message)

--------------------------------------------------------------------------------
FILE: src\microservices\CallbackGraphBuilderMS.py
--------------------------------------------------------------------------------
"""
CallbackGraphBuilderMS
----------------------
Build a callback/call graph focused on UI event wiring.

Goal:
- Given ASTs and UI mapping data, build a directed graph showing:
    - widgets/events -> handler functions
    - handler functions -> other functions they call
- This enables "what triggers what" exploration.

Responsibilities:
- Extract handler targets from:
    - widget ctor kwargs: command=...
    - .bind(event, handler)
    - Menu.add_command(command=...)
- Build function index per module:
    - def name(...)
    - methods: ClassName.method
- Build intra-file call edges using conservative name/attr call extraction
- Provide a simple graph model and unknowns list

Non-goals:
- Perfect static analysis (imports, dynamic dispatch, lambdas)
- Cross-module resolution via import aliasing (future)
- Execution / tracing
"""

from __future__ import annotations

import ast
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple


# -------------------------
# Data Model
# -------------------------

@dataclass(frozen=True)
class GraphNode:
    """
    Node kinds:
    - "event": UI event source (widget_id + event kind)
    - "func": function or method symbol
    """
    kind: str
    key: str  # stable identifier


@dataclass(frozen=True)
class GraphEdge:
    src: GraphNode
    dst: GraphNode
    kind: str  # "event_to_handler" | "calls"


@dataclass
class CallbackGraph:
    nodes: Dict[str, GraphNode] = field(default_factory=dict)
    edges: List[GraphEdge] = field(default_factory=list)
    unknowns: List[str] = field(default_factory=list)


# -------------------------
# Service
# -------------------------

class CallbackGraphBuilderMS:
    """
    Build a callback graph from:
        - ast_by_path: {Path: ast.AST}
        - ui_map-like info:
            - widgets with command_targets and bind_events
            - (optional) mapping var->widget_id already done by AstUiMapMS

    This service does not depend on a specific UiMap class; it uses duck-typing.
    """

    def build(
        self,
        ast_by_path: Dict[Path, ast.AST],
        ui_map: object,
    ) -> CallbackGraph:
        graph = CallbackGraph()

        # 1) Index functions
        func_index = self._index_functions(ast_by_path)

        # 2) Add event->handler edges from ui_map
        self._add_event_edges(ui_map, func_index, graph)

        # 3) Add function call edges by scanning bodies
        self._add_call_edges(ast_by_path, func_index, graph)

        return graph

    # -------------------------
    # Function indexing
    # -------------------------

    def _index_functions(self, ast_by_path: Dict[Path, ast.AST]) -> Dict[str, Tuple[Path, ast.AST]]:
        """
        Returns:
            symbol_key -> (path, node)
        symbol_key formats:
            - modulepath::funcname
            - modulepath::ClassName.method
        """
        idx: Dict[str, Tuple[Path, ast.AST]] = {}

        for path in sorted(ast_by_path.keys(), key=lambda p: p.as_posix().lower()):
            tree = ast_by_path[path]
            modkey = self._module_key(path)
            visitor = _FunctionIndexVisitor(modkey=modkey)
            visitor.visit(tree)
            for k, n in visitor.found.items():
                idx[k] = (path, n)

        return idx

    def _module_key(self, path: Path) -> str:
        return path.as_posix()

    # -------------------------
    # UI event edges
    # -------------------------

    def _add_event_edges(self, ui_map: object, func_index: Dict[str, Tuple[Path, ast.AST]], graph: CallbackGraph) -> None:
        """
        ui_map.widgets is expected to be a dict-like of widget objects:
            widget.widget_id
            widget.command_targets: List[str]
            widget.bind_events: List[str] entries like "<Button-1> -> handler"
        """
        widgets = getattr(ui_map, "widgets", {}) or {}

        for wid, w in widgets.items():
            widget_id = getattr(w, "widget_id", str(wid))

            # command= callbacks
            for target in getattr(w, "command_targets", []) or []:
                self._link_event_to_handler(
                    graph,
                    event_key=f"{widget_id}:command",
                    handler_expr=target,
                    func_index=func_index,
                )

            # bind callbacks: "<Event> -> handler"
            for bind in getattr(w, "bind_events", []) or []:
                # stored as string; parse conservatively
                try:
                    ev, handler = bind.split("->", 1)
                    ev = ev.strip()
                    handler = handler.strip()
                except Exception:
                    graph.unknowns.append(f"unparseable_bind:{widget_id}:{bind}")
                    continue

                self._link_event_to_handler(
                    graph,
                    event_key=f"{widget_id}:bind:{ev}",
                    handler_expr=handler,
                    func_index=func_index,
                )

    def _link_event_to_handler(
        self,
        graph: CallbackGraph,
        *,
        event_key: str,
        handler_expr: str,
        func_index: Dict[str, Tuple[Path, ast.AST]],
    ) -> None:
        ev_node = self._get_node(graph, "event", event_key)

        # handler_expr might be "self.on_click" or "on_click" or "lambda ..."
        handler_symbol = self._resolve_handler_expr(handler_expr, func_index)

        if handler_symbol is None:
            graph.unknowns.append(f"unresolved_handler:{event_key}:{handler_expr}")
            return

        fn_node = self._get_node(graph, "func", handler_symbol)
        graph.edges.append(GraphEdge(src=ev_node, dst=fn_node, kind="event_to_handler"))

    def _resolve_handler_expr(
        self,
        handler_expr: str,
        func_index: Dict[str, Tuple[Path, ast.AST]],
    ) -> Optional[str]:
        s = handler_expr.strip()

        if s.startswith("lambda"):
            return None

        # If already looks like module-qualified symbol key, accept.
        if "::" in s:
            return s if s in func_index else None

        # Normalize "self.foo" -> try any "::Class.method" and "::foo"
        if s.startswith("self."):
            meth = s.split(".", 1)[1]
            # Try any method match ending with ".meth"
            candidates = [k for k in func_index.keys() if k.endswith(f".{meth}")]
            if len(candidates) == 1:
                return candidates[0]
            if len(candidates) > 1:
                return None
            # Try free function name
            candidates = [k for k in func_index.keys() if k.endswith(f"::{meth}")]
            if len(candidates) == 1:
                return candidates[0]
            return None

        # Bare name: find unique match
        candidates = [k for k in func_index.keys() if k.endswith(f"::{s}")]
        if len(candidates) == 1:
            return candidates[0]
        if len(candidates) > 1:
            return None

        # Attribute form "obj.fn"
        if "." in s:
            tail = s.split(".")[-1]
            candidates = [k for k in func_index.keys() if k.endswith(f"::{tail}") or k.endswith(f".{tail}")]
            if len(candidates) == 1:
                return candidates[0]

        return None

    # -------------------------
    # Call edges
    # -------------------------

    def _add_call_edges(self, ast_by_path: Dict[Path, ast.AST], func_index: Dict[str, Tuple[Path, ast.AST]], graph: CallbackGraph) -> None:
        """
        Build edges between functions based on ast.Call nodes inside each function body.
        """
        for sym, (path, node) in func_index.items():
            fn_calls = _CallCollectorVisitor()
            fn_calls.visit(node)

            src = self._get_node(graph, "func", sym)

            for called in fn_calls.called_names:
                dst_sym = self._resolve_called_name(called, func_index)
                if dst_sym is None:
                    # unknown external call; ignore quietly or record
                    continue
                dst = self._get_node(graph, "func", dst_sym)
                graph.edges.append(GraphEdge(src=src, dst=dst, kind="calls"))

    def _resolve_called_name(self, called: str, func_index: Dict[str, Tuple[Path, ast.AST]]) -> Optional[str]:
        """
        called may be:
            - foo
            - self.foo
            - mod.foo
        Resolve conservatively to a unique symbol in index.
        """
        s = called.strip()

        if s.startswith("self."):
            tail = s.split(".", 1)[1]
            candidates = [k for k in func_index.keys() if k.endswith(f".{tail}")]
            if len(candidates) == 1:
                return candidates[0]
            return None

        # bare function name
        if "." not in s:
            candidates = [k for k in func_index.keys() if k.endswith(f"::{s}")]
            if len(candidates) == 1:
                return candidates[0]
            return None

        # attribute call: mod.foo or obj.foo
        tail = s.split(".")[-1]
        candidates = [k for k in func_index.keys() if k.endswith(f"::{tail}") or k.endswith(f".{tail}")]
        if len(candidates) == 1:
            return candidates[0]

        return None

    # -------------------------
    # Graph helpers
    # -------------------------

    def _get_node(self, graph: CallbackGraph, kind: str, key: str) -> GraphNode:
        nk = f"{kind}:{key}"
        node = graph.nodes.get(nk)
        if node is None:
            node = GraphNode(kind=kind, key=key)
            graph.nodes[nk] = node
        return node


# -------------------------
# Visitors
# -------------------------

class _FunctionIndexVisitor(ast.NodeVisitor):
    def __init__(self, modkey: str):
        self.modkey = modkey
        self.class_stack: List[str] = []
        self.found: Dict[str, ast.AST] = {}

    def visit_ClassDef(self, node: ast.ClassDef) -> None:
        self.class_stack.append(node.name)
        self.generic_visit(node)
        self.class_stack.pop()

    def visit_FunctionDef(self, node: ast.FunctionDef) -> None:
        key = self._make_key(node.name)
        self.found[key] = node
        self.generic_visit(node)

    def visit_AsyncFunctionDef(self, node: ast.AsyncFunctionDef) -> None:
        key = self._make_key(node.name)
        self.found[key] = node
        self.generic_visit(node)

    def _make_key(self, fn_name: str) -> str:
        if self.class_stack:
            cls = self.class_stack[-1]
            return f"{self.modkey}::{cls}.{fn_name}"
        return f"{self.modkey}::{fn_name}"


class _CallCollectorVisitor(ast.NodeVisitor):
    """
    Collect simple call target strings.
    """
    def __init__(self):
        self.called_names: Set[str] = set()

    def visit_Call(self, node: ast.Call) -> None:
        self.called_names.add(self._call_to_str(node.func))
        self.generic_visit(node)

    def _call_to_str(self, node: ast.AST) -> str:
        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Attribute):
            # self.foo / mod.foo
            base = self._call_to_str(node.value)
            return f"{base}.{node.attr}"
        return node.__class__.__name__


--------------------------------------------------------------------------------
FILE: src\microservices\CancellationTokenMS.py
--------------------------------------------------------------------------------
"""
CancellationTokenMS
-------------------
Thread-safe cancellation token for long-running operations.

Responsibilities:
- Provide a shared cancellation state (set/cancel/reset)
- Provide polling-friendly predicate: token.is_cancelled()
- Provide optional "reason" string
- Provide a lightweight context manager for scoped cancellation reset

Non-goals:
- Threading orchestration (Backend orchestrator owns threads)
- Timeouts / scheduling
"""

from __future__ import annotations

from dataclasses import dataclass
from threading import Event, Lock
from typing import Optional


@dataclass(frozen=True)
class CancelState:
    cancelled: bool
    reason: Optional[str] = None


class CancellationTokenMS:
    def __init__(self):
        self._event = Event()
        self._lock = Lock()
        self._reason: Optional[str] = None

    # -------------------------
    # Control
    # -------------------------

    def cancel(self, reason: Optional[str] = None) -> None:
        with self._lock:
            self._reason = reason
            self._event.set()

    def reset(self) -> None:
        with self._lock:
            self._reason = None
            self._event.clear()

    # -------------------------
    # Query
    # -------------------------

    def is_cancelled(self) -> bool:
        return self._event.is_set()

    def reason(self) -> Optional[str]:
        with self._lock:
            return self._reason

    def snapshot(self) -> CancelState:
        with self._lock:
            return CancelState(cancelled=self._event.is_set(), reason=self._reason)

    # -------------------------
    # Convenience
    # -------------------------

    def predicate(self):
        """
        Return a no-arg callable usable as cancel() injection in services:
            cancel = token.predicate()
            if cancel(): ...
        """
        return self.is_cancelled

    def __enter__(self):
        self.reset()
        return self

    def __exit__(self, exc_type, exc, tb):
        # Do not auto-cancel; just leave state as-is.
        return False


--------------------------------------------------------------------------------
FILE: src\microservices\document_utils.py
--------------------------------------------------------------------------------
from ._ContentExtractorMS import ContentExtractorMS

# Singleton instance to reuse the extractor logic
_extractor = ContentExtractorMS()

def extract_text_from_pdf(blob: bytes) -> str:
    """Proxy to ContentExtractorMS PDF logic."""
    return _extractor._extract_pdf(blob)

def extract_text_from_html(html_text: str) -> str:
    """Proxy to ContentExtractorMS HTML logic."""
    return _extractor._extract_html(html_text)

--------------------------------------------------------------------------------
FILE: src\microservices\EntrypointFinderMS.py
--------------------------------------------------------------------------------
"""
EntrypointFinderMS
------------------
Find likely entrypoint files in a Python project.

Responsibilities:
- Scan a list of Python files and identify candidates for "entrypoint"
- Provide deterministic scoring + reasons
- Support both script-style entrypoints and package entrypoints

Heuristics (scored):
- Contains `if __name__ == "__main__":`
- Imports `tkinter` / `ttk` and creates `tk.Tk()` or `tkinter.Tk()`
- Defines a `main()` function
- Filename hints: app.py, main.py, run.py, __main__.py
- Located near project root (shorter relative path)

Non-goals:
- AST-accurate program understanding (fast text scan)
- Executing code
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import re


# -------------------------
# Data Structures
# -------------------------

@dataclass(frozen=True)
class EntrypointCandidate:
    path: Path
    score: int
    reasons: Tuple[str, ...]


@dataclass
class EntrypointFinderConfig:
    max_candidates: int = 20
    read_bytes_limit: int = 512_000  # 512KB per file (fast, safe default)

    # filename hints
    strong_names: Tuple[str, ...] = (
        "app.py",
        "main.py",
        "run.py",
        "__main__.py",
        "start.py",
        "launcher.py",
        "cli.py",
    )


# -------------------------
# Service
# -------------------------

class EntrypointFinderMS:
    def __init__(self, project_root: Path, config: Optional[EntrypointFinderConfig] = None):
        self.root = Path(project_root).resolve()
        self.config = config or EntrypointFinderConfig()

        self._re_main_guard = re.compile(r"""if\s+__name__\s*==\s*["']__main__["']\s*:""")
        self._re_def_main = re.compile(r"""^\s*def\s+main\s*\(""", re.MULTILINE)

        # Tk hints (simple text; AST service will do real mapping later)
        self._re_tk_import = re.compile(r"""^\s*(import\s+tkinter\b|from\s+tkinter\s+import\b)""", re.MULTILINE)
        self._re_tk_root = re.compile(r"""\b(tkinter\.)?Tk\s*\(""")
        self._re_ttk = re.compile(r"""\bttk\b""")

    # -------------------------
    # Public API
    # -------------------------

    def find_candidates(self, py_files: List[Path]) -> List[EntrypointCandidate]:
        """
        Returns ranked candidates (highest score first).
        """
        cands: List[EntrypointCandidate] = []

        for p in py_files:
            try:
                score, reasons = self._score_file(p)
            except Exception:
                # If unreadable, ignore
                continue

            if score > 0:
                cands.append(EntrypointCandidate(path=p, score=score, reasons=tuple(reasons)))

        # deterministic sort: score desc, then path
        cands.sort(key=lambda c: (-c.score, c.path.as_posix().lower()))
        return cands[: self.config.max_candidates]

    # -------------------------
    # Internals
    # -------------------------

    def _score_file(self, path: Path) -> Tuple[int, List[str]]:
        score = 0
        reasons: List[str] = []

        name = path.name.lower()

        if name in (n.lower() for n in self.config.strong_names):
            score += 25
            reasons.append(f"filename_hint:{name}")

        rel_depth = self._rel_depth(path)
        # closer to root gets more points
        depth_bonus = max(0, 10 - rel_depth)
        if depth_bonus:
            score += depth_bonus
            reasons.append(f"near_root:depth={rel_depth}")

        text = self._read_text_head(path)

        if self._re_main_guard.search(text):
            score += 40
            reasons.append("__main__guard")

        if self._re_def_main.search(text):
            score += 15
            reasons.append("defines_main()")

        tk_import = bool(self._re_tk_import.search(text))
        tk_root = bool(self._re_tk_root.search(text))
        if tk_import:
            score += 12
            reasons.append("imports_tkinter")
        if tk_root:
            score += 12
            reasons.append("creates_Tk()")

        if tk_import and tk_root:
            score += 10
            reasons.append("tkinter_entrypoint_signal")

        if self._re_ttk.search(text):
            score += 3
            reasons.append("mentions_ttk")

        # package-style entrypoint: package/__main__.py
        if name == "__main__.py":
            score += 30
            reasons.append("package___main__")

        return score, reasons

    def _rel_depth(self, path: Path) -> int:
        try:
            rel = path.resolve().relative_to(self.root)
            # depth = number of parents in rel (0 if in root)
            return len(rel.parts) - 1
        except Exception:
            return 99

    def _read_text_head(self, path: Path) -> str:
        """
        Reads up to read_bytes_limit bytes, decodes as utf-8 with replacement.
        """
        data = path.read_bytes()
        if len(data) > self.config.read_bytes_limit:
            data = data[: self.config.read_bytes_limit]
        return data.decode("utf-8", errors="replace")


--------------------------------------------------------------------------------
FILE: src\microservices\ErrorNormalizerMS.py
--------------------------------------------------------------------------------
"""
ErrorNormalizerMS
-----------------
Normalize exceptions + error payloads into stable, UI-friendly strings/dicts.

Responsibilities:
- Convert exceptions (and arbitrary error-like objects) into:
    - short code (category)
    - human message
    - detail string (traceback optional)
- Provide deterministic formatting
- Optionally include traceback for logs, not for UI toast

Non-goals:
- Logging sink (ProgressEventBusMS can publish)
- Retrying logic
"""

from __future__ import annotations

import traceback
from dataclasses import dataclass
from typing import Any, Dict, Optional, Tuple


@dataclass(frozen=True)
class NormalizedError:
    code: str
    message: str
    detail: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        out = {"code": self.code, "message": self.message}
        if self.detail:
            out["detail"] = self.detail
        return out


class ErrorNormalizerMS:
    def normalize(
        self,
        err: Any,
        *,
        include_traceback: bool = False,
        tb_limit: int = 20,
    ) -> NormalizedError:
        """
        Convert err into NormalizedError. Never raises.
        """
        if err is None:
            return NormalizedError(code="none", message="No error")

        # Already normalized
        if isinstance(err, NormalizedError):
            return err

        # String error
        if isinstance(err, str):
            return NormalizedError(code="error", message=err)

        # Exception
        if isinstance(err, BaseException):
            code = err.__class__.__name__
            msg = str(err) or code
            detail = None
            if include_traceback:
                detail = "".join(traceback.format_exception(type(err), err, err.__traceback__, limit=tb_limit)).strip()
            return NormalizedError(code=code, message=msg, detail=detail)

        # Dict-like error
        if isinstance(err, dict):
            code = str(err.get("code", "error"))
            msg = str(err.get("message", "Unknown error"))
            detail = err.get("detail", None)
            if detail is not None:
                detail = str(detail)
            return NormalizedError(code=code, message=msg, detail=detail)

        # Fallback
        return NormalizedError(code=err.__class__.__name__, message=str(err))


--------------------------------------------------------------------------------
FILE: src\microservices\GitignoreFilterMS.py
--------------------------------------------------------------------------------
"""
GitignoreFilterMS
-----------------
Deterministic .gitignore-style path filter.

Responsibilities:
- Load ignore rules from:
    - <root>/.gitignore
    - <root>/.git/info/exclude (if present)
    - optional extra ignore files
- Evaluate paths (absolute) relative to root
- Support a practical subset of gitignore semantics:
    - blank lines + comments (#) ignored
    - negation rules via leading "!"
    - directory rules via trailing "/"
    - rooted patterns via leading "/"
    - glob wildcards: "*", "?", "[...]" (fnmatch semantics)
    - "**" treated as "match any path segments" (approx via fnmatch on normalized paths)

Non-goals:
- Full gitignore edge-case parity (this is "good enough" for project crawling).
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Callable, List, Optional, Tuple
import fnmatch
import os


# -------------------------
# Data Structures
# -------------------------

@dataclass(frozen=True)
class GitignoreRule:
    raw: str
    is_negation: bool
    dir_only: bool
    rooted: bool
    pattern: str  # normalized for matching


# -------------------------
# Service
# -------------------------

class GitignoreFilterMS:
    """
    Produces a predicate usable by ProjectCrawlMS:

        keep = gitignore_filter.predicate()
        keep(abs_path: Path, is_dir: bool) -> bool

    Semantics:
    - If a rule matches -> toggles ignore state
        - normal rule => ignored = True
        - negation rule => ignored = False
    - Last matching rule wins (gitignore behavior)
    """

    def __init__(
        self,
        root: Path,
        extra_ignore_files: Optional[List[Path]] = None,
    ):
        self.root = Path(root).resolve()
        self.extra_ignore_files = extra_ignore_files or []

        self._rules: List[GitignoreRule] = []
        self._loaded_sources: List[Path] = []

    # -------------------------
    # Public API
    # -------------------------

    def load(self) -> None:
        """
        Load ignore rules from known locations + any extra ignore files.
        Safe to call multiple times; it replaces existing rules.
        """
        self._rules = []
        self._loaded_sources = []

        sources: List[Path] = []

        root_gitignore = self.root / ".gitignore"
        if root_gitignore.exists():
            sources.append(root_gitignore)

        git_info_exclude = self.root / ".git" / "info" / "exclude"
        if git_info_exclude.exists():
            sources.append(git_info_exclude)

        for p in self.extra_ignore_files:
            pp = Path(p).resolve()
            if pp.exists():
                sources.append(pp)

        for src in sources:
            self._loaded_sources.append(src)
            self._rules.extend(self._parse_file(src))

    def predicate(self) -> Callable[[Path, bool], bool]:
        """
        Returns a predicate function:
            (abs_path: Path, is_dir: bool) -> keep?
        """
        # Ensure loaded at least once (zero rules is fine)
        if not self._loaded_sources and not self._rules:
            self.load()

        def _keep(abs_path: Path, is_dir: bool) -> bool:
            return not self.is_ignored(abs_path, is_dir)

        return _keep

    def is_ignored(self, abs_path: Path, is_dir: bool) -> bool:
        """
        Returns True if gitignore rules ignore this path.
        """
        try:
            rel = Path(abs_path).resolve().relative_to(self.root)
        except Exception:
            # Path outside root: do not ignore by these rules
            return False

        rel_posix = rel.as_posix()

        ignored = False
        for rule in self._rules:
            if rule.dir_only and not is_dir:
                continue
            if self._rule_matches(rule, rel_posix, rel, is_dir):
                ignored = not rule.is_negation

        return ignored

    # -------------------------
    # Internals: parsing
    # -------------------------

    def _parse_file(self, path: Path) -> List[GitignoreRule]:
        rules: List[GitignoreRule] = []
        text = path.read_text(encoding="utf-8", errors="replace")
        for line in text.splitlines():
            rule = self._parse_line(line)
            if rule is not None:
                rules.append(rule)
        return rules

    def _parse_line(self, line: str) -> Optional[GitignoreRule]:
        s = line.strip("\n\r")

        # Skip blanks
        if not s.strip():
            return None

        # Skip comments (but allow escaped \#)
        stripped = s.lstrip()
        if stripped.startswith("#"):
            return None

        # Unescape leading "\#" (keep "#")
        if s.startswith(r"\#"):
            s = s[1:]

        is_negation = False
        if s.startswith("!"):
            is_negation = True
            s = s[1:]

        # After removing "!", empty means nothing
        if not s:
            return None

        dir_only = s.endswith("/")
        if dir_only:
            s = s[:-1]

        rooted = s.startswith("/")
        if rooted:
            s = s[1:]

        # Normalize to POSIX-ish matching.
        # Gitignore treats backslashes specially on Windows; we normalize to "/".
        pattern = s.replace("\\", "/")

        return GitignoreRule(
            raw=line,
            is_negation=is_negation,
            dir_only=dir_only,
            rooted=rooted,
            pattern=pattern,
        )

    # -------------------------
    # Internals: matching
    # -------------------------

    def _rule_matches(self, rule: GitignoreRule, rel_posix: str, rel_path: Path, is_dir: bool) -> bool:
        """
        A pragmatic matcher:
        - If rule contains "/" => match against full relative path (posix)
        - Else => match against basename in any directory (like gitignore)
        - If rule.rooted => anchor at repo root (i.e., compare from start)
        """
        pat = rule.pattern

        # Directory match: if rule matches a dir path, also match contents by prefix
        # (gitignore-style). We'll check both exact dir and prefix.
        if rule.dir_only:
            # If this is a dir, match itself; if it's a file (won't be here), ignore.
            # Also if a dir rule matches "a/b", it should ignore "a/b/**".
            if self._match_path(rule, rel_posix, pat):
                return True
            if rel_posix.startswith(pat.rstrip("/") + "/"):
                return True
            return False

        # If pattern has a "/" then treat as path-glob, else basename-glob
        if "/" in pat or rule.rooted:
            return self._match_path(rule, rel_posix, pat)

        # Basename-style: match on name at any depth
        name = rel_path.name
        return fnmatch.fnmatchcase(name, pat)

    def _match_path(self, rule: GitignoreRule, rel_posix: str, pat: str) -> bool:
        """
        Path matching:
        - If rooted: only match from root (we already have rel_posix)
        - Else:
            - If pat contains "/" we allow matching at any depth by trying:
                - exact full-path match
                - "**/pat" match
        """
        # Exact match
        if fnmatch.fnmatchcase(rel_posix, pat):
            return True

        if rule.rooted:
            return False

        # Allow match at any depth
        # e.g., pat "foo/bar.py" should match "x/y/foo/bar.py"
        if "/" in pat:
            deep_pat = f"**/{pat}"
            if fnmatch.fnmatchcase(rel_posix, deep_pat):
                return True

        return False


--------------------------------------------------------------------------------
FILE: src\microservices\HitlDecisionRouterMS.py
--------------------------------------------------------------------------------
"""
HitlDecisionRouterMS
--------------------
Human-in-the-loop decision router for applying inference results.

Responsibilities:
- Take validated inference results and decide:
    - auto-apply (high confidence)
    - require human approval
    - reject / keep unknown
- Provide a deterministic decision plan for the UI orchestrator:
    - list of decisions with required UI actions

Non-goals:
- Rendering dialogs (UI orchestrator / HitlDialogMS)
- Applying changes to UiMap (backend orchestrator does that)
- Retrying LLM prompts (backend orchestrator decides)

Design:
- Decisions are policy-based and purely functional (no side effects).
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Dict, List, Optional


# -------------------------
# Data Structures
# -------------------------

@dataclass(frozen=True)
class HitlPolicy:
    """
    Policy thresholds:
    - auto_apply_at: confidence >= this -> auto apply
    - ask_user_at: confidence >= this -> ask user (approve/reject)
    - below ask_user_at -> reject (keep unknown)
    """
    auto_apply_at: float = 0.90
    ask_user_at: float = 0.60

    # If True, even high confidence requires user approval for these classifications.
    force_approval_for: List[str] = field(default_factory=lambda: ["callback_target"])


@dataclass(frozen=True)
class DecisionItem:
    case_id: str
    classification: str
    confidence: float
    action: str  # "auto_apply" | "ask_user" | "reject"
    extracted: Dict[str, Optional[str]]
    notes: str


@dataclass(frozen=True)
class DecisionPlan:
    items: List[DecisionItem]
    stats: Dict[str, int]


# -------------------------
# Service
# -------------------------

class HitlDecisionRouterMS:
    def __init__(self, policy: Optional[HitlPolicy] = None):
        self.policy = policy or HitlPolicy()

    def build_plan(self, validated_results: List[object]) -> DecisionPlan:
        """
        validated_results: duck-typed ValidatedResult:
            - case_id, classification, confidence, extracted, notes
        """
        items: List[DecisionItem] = []
        stats = {"auto_apply": 0, "ask_user": 0, "reject": 0}

        for r in validated_results:
            case_id = getattr(r, "case_id", "")
            classification = getattr(r, "classification", "unknown")
            confidence = float(getattr(r, "confidence", 0.0))
            extracted = getattr(r, "extracted", {}) or {}
            notes = getattr(r, "notes", "") or ""

            action = self._decide_action(classification, confidence)

            items.append(
                DecisionItem(
                    case_id=case_id,
                    classification=classification,
                    confidence=confidence,
                    action=action,
                    extracted=dict(extracted),
                    notes=notes,
                )
            )
            stats[action] += 1

        # Deterministic ordering: prioritize ask_user first, then auto_apply, then reject
        order = {"ask_user": 0, "auto_apply": 1, "reject": 2}
        items.sort(key=lambda it: (order.get(it.action, 9), it.case_id))

        return DecisionPlan(items=items, stats=stats)

    # -------------------------
    # Policy logic
    # -------------------------

    def _decide_action(self, classification: str, confidence: float) -> str:
        # Force approval for certain classes regardless of confidence
        if classification in set(self.policy.force_approval_for):
            return "ask_user" if confidence >= self.policy.ask_user_at else "reject"

        if confidence >= self.policy.auto_apply_at:
            return "auto_apply"
        if confidence >= self.policy.ask_user_at:
            return "ask_user"
        return "reject"


--------------------------------------------------------------------------------
FILE: src\microservices\InferencePromptBuilderMS.py
--------------------------------------------------------------------------------
"""
InferencePromptBuilderMS
------------------------
Build deterministic prompts for LLM inference over UnknownCase items.

Responsibilities:
- Convert UnknownCase + local context into a compact, structured prompt
- Provide a strict expected JSON response schema for downstream validation
- Support batching multiple UnknownCase items into one prompt

Non-goals:
- Calling the LLM (OllamaClientMS does that)
- Validating the LLM output (InferenceResultValidatorMS does that)
- UI/HITL decisions (HitlDecisionRouterMS / UI orchestrator does that)

Design:
- Always emit the same format for the same inputs (deterministic ordering)
- Keep prompts small by limiting snippet/context sizes
"""

from __future__ import annotations

from dataclasses import dataclass
from typing import Dict, List, Optional


# -------------------------
# Config
# -------------------------

@dataclass
class InferencePromptConfig:
    max_snippet_chars: int = 1200
    max_context_kv_chars: int = 1200
    max_items_per_prompt: int = 10


# -------------------------
# Service
# -------------------------

class InferencePromptBuilderMS:
    def __init__(self, config: Optional[InferencePromptConfig] = None):
        self.config = config or InferencePromptConfig()

    def build_prompt(
        self,
        *,
        project_root: str,
        unknown_cases: List[object],
        goal: str = "Resolve unknown UI mapping cases conservatively.",
    ) -> str:
        """
        unknown_cases: duck-typed UnknownCase:
            - kind, detail, path, lineno, col, snippet, context (dict)
        """
        items = unknown_cases[: self.config.max_items_per_prompt]

        header = self._header(project_root=project_root, goal=goal)
        schema = self._response_schema()
        body = self._items_block(items)

        return "\n".join([header, schema, body]).strip() + "\n"

    # -------------------------
    # Prompt blocks
    # -------------------------

    def _header(self, *, project_root: str, goal: str) -> str:
        return (
            "You are a static-analysis assistant for a Tkinter UI mapper.\n"
            "Your job is to classify uncertain code patterns into safe, conservative interpretations.\n"
            "Only use information present in the provided case records.\n"
            "If you are not confident, return an 'unknown' classification.\n"
            "\n"
            f"ProjectRoot: {project_root}\n"
            f"Goal: {goal}\n"
        )

    def _response_schema(self) -> str:
        # Keep this concise but strict.
        return (
            "Return ONLY valid JSON matching this schema:\n"
            "{\n"
            '  "results": [\n'
            "    {\n"
            '      "case_id": "string",\n'
            '      "classification": "widget_ctor|layout_call|config_call|bind_call|menu_call|callback_target|window_call|unknown",\n'
            '      "confidence": 0.0,\n'
            '      "extracted": {\n'
            '        "widget_type": "string|null",\n'
            '        "parent": "string|null",\n'
            '        "event": "string|null",\n'
            '        "handler": "string|null",\n'
            '        "method": "string|null"\n'
            "      },\n"
            '      "notes": "string"\n'
            "    }\n"
            "  ]\n"
            "}\n"
            "Rules:\n"
            "- confidence is 0.0 to 1.0\n"
            "- extracted values must be null if not applicable\n"
            "- never hallucinate file contents; rely on snippet/context only\n"
            "- be conservative: prefer 'unknown' over guessing\n"
        )

    def _items_block(self, items: List[object]) -> str:
        lines: List[str] = []
        lines.append("Cases:")
        for idx, uc in enumerate(items, start=1):
            lines.extend(self._render_case(idx, uc))
        return "\n".join(lines)

    def _render_case(self, idx: int, uc: object) -> List[str]:
        # stable case_id
        path = str(getattr(uc, "path", "<?>"))
        kind = str(getattr(uc, "kind", "<?>"))
        detail = str(getattr(uc, "detail", "<?>"))
        lineno = getattr(uc, "lineno", None)
        col = getattr(uc, "col", None)

        snippet = getattr(uc, "snippet", None)
        context = getattr(uc, "context", None) or {}

        case_id = f"case_{idx}"

        out: List[str] = []
        out.append(f"- case_id: {case_id}")
        out.append(f"  kind: {kind}")
        out.append(f"  where: {path}:{lineno if lineno is not None else '?'}:{col if col is not None else '?'}")
        out.append(f"  detail: {detail}")

        if snippet:
            out.append("  snippet: |")
            clipped = self._clip(str(snippet), self.config.max_snippet_chars)
            out.extend(self._indent_block(clipped, indent="    "))

        if context:
            out.append("  context:")
            ctx_text = self._render_context(context)
            out.extend(self._indent_block(ctx_text, indent="    "))

        return out

    # -------------------------
    # Utilities
    # -------------------------

    def _render_context(self, ctx: Dict[str, str]) -> str:
        # deterministic ordering
        items = sorted(((str(k), str(v)) for k, v in ctx.items()), key=lambda kv: kv[0].lower())
        parts: List[str] = []
        total = 0
        for k, v in items:
            line = f"{k}: {v}"
            parts.append(line)
            total += len(line)
            if total >= self.config.max_context_kv_chars:
                parts.append("... (context truncated)")
                break
        return "\n".join(parts)

    def _clip(self, s: str, limit: int) -> str:
        if len(s) <= limit:
            return s
        return s[:limit] + "\n... (truncated)"

    def _indent_block(self, s: str, indent: str) -> List[str]:
        return [indent + line for line in s.splitlines()]


--------------------------------------------------------------------------------
FILE: src\microservices\InferenceResultValidatorMS.py
--------------------------------------------------------------------------------
"""
InferenceResultValidatorMS
--------------------------
Validate and normalize LLM outputs produced for UnknownCase inference.

Responsibilities:
- Parse JSON safely (string -> dict)
- Validate schema:
    {
      "results": [
        {
          "case_id": str,
          "classification": oneof(...),
          "confidence": float [0..1],
          "extracted": {
            "widget_type": str|null,
            "parent": str|null,
            "event": str|null,
            "handler": str|null,
            "method": str|null
          },
          "notes": str
        }
      ]
    }
- Normalize types (confidence clamp, missing keys -> null/empty)
- Provide clear error info for UI/HITL display

Non-goals:
- Applying results to UiMap (orchestrator does that)
- Retrying prompts / repair (HitlDecisionRouterMS or orchestrator does that)
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple


# -------------------------
# Data Structures
# -------------------------

_ALLOWED_CLASSIFICATIONS = {
    "widget_ctor",
    "layout_call",
    "config_call",
    "bind_call",
    "menu_call",
    "callback_target",
    "window_call",
    "unknown",
}

_EXTRACTED_KEYS = {"widget_type", "parent", "event", "handler", "method"}


@dataclass(frozen=True)
class ValidationError:
    message: str
    detail: Optional[str] = None


@dataclass(frozen=True)
class ValidatedResult:
    case_id: str
    classification: str
    confidence: float
    extracted: Dict[str, Optional[str]]
    notes: str


@dataclass(frozen=True)
class ValidationOutcome:
    ok: bool
    results: List[ValidatedResult]
    errors: List[ValidationError]


# -------------------------
# Service
# -------------------------

class InferenceResultValidatorMS:
    def validate_json_text(self, text: str) -> ValidationOutcome:
        """
        Parse + validate. Never raises.
        """
        if text is None:
            return ValidationOutcome(ok=False, results=[], errors=[ValidationError("empty_text")])

        s = text.strip()
        if not s:
            return ValidationOutcome(ok=False, results=[], errors=[ValidationError("empty_text")])

        try:
            obj = json.loads(s)
        except Exception as e:
            return ValidationOutcome(ok=False, results=[], errors=[ValidationError("json_parse_error", detail=str(e))])

        return self.validate_obj(obj)

    def validate_obj(self, obj: Any) -> ValidationOutcome:
        errors: List[ValidationError] = []
        results: List[ValidatedResult] = []

        if not isinstance(obj, dict):
            return ValidationOutcome(ok=False, results=[], errors=[ValidationError("root_not_object")])

        res_list = obj.get("results", None)
        if not isinstance(res_list, list):
            return ValidationOutcome(ok=False, results=[], errors=[ValidationError("missing_or_invalid_results_list")])

        for i, item in enumerate(res_list):
            vr, item_errors = self._validate_item(item, idx=i)
            if item_errors:
                errors.extend(item_errors)
            if vr is not None:
                results.append(vr)

        ok = len(errors) == 0 and len(results) > 0
        if len(results) == 0 and len(errors) == 0:
            errors.append(ValidationError("no_results_returned"))
            ok = False

        return ValidationOutcome(ok=ok, results=results, errors=errors)

    # -------------------------
    # Internal validation
    # -------------------------

    def _validate_item(self, item: Any, idx: int) -> Tuple[Optional[ValidatedResult], List[ValidationError]]:
        errs: List[ValidationError] = []

        if not isinstance(item, dict):
            return None, [ValidationError("result_item_not_object", detail=f"index={idx}")]

        case_id = item.get("case_id")
        if not isinstance(case_id, str) or not case_id.strip():
            errs.append(ValidationError("invalid_case_id", detail=f"index={idx}"))

        classification = item.get("classification")
        if not isinstance(classification, str) or classification not in _ALLOWED_CLASSIFICATIONS:
            errs.append(ValidationError("invalid_classification", detail=f"index={idx} value={classification!r}"))

        confidence = item.get("confidence")
        conf_val: float = 0.0
        if isinstance(confidence, (int, float)):
            conf_val = float(confidence)
        else:
            errs.append(ValidationError("invalid_confidence_type", detail=f"index={idx}"))

        conf_val = self._clamp(conf_val, 0.0, 1.0)

        extracted = item.get("extracted", {})
        extracted_norm: Dict[str, Optional[str]] = {k: None for k in _EXTRACTED_KEYS}
        if extracted is None:
            extracted = {}
        if not isinstance(extracted, dict):
            errs.append(ValidationError("invalid_extracted_type", detail=f"index={idx}"))
            extracted = {}

        # normalize extracted keys
        for k in _EXTRACTED_KEYS:
            v = extracted.get(k, None)
            if v is None:
                extracted_norm[k] = None
            elif isinstance(v, str):
                extracted_norm[k] = v
            else:
                # Coerce non-strings to string only if it's safe-ish; otherwise null
                try:
                    extracted_norm[k] = str(v)
                except Exception:
                    extracted_norm[k] = None
                    errs.append(ValidationError("extracted_value_unstringifiable", detail=f"index={idx} key={k}"))

        notes = item.get("notes", "")
        if notes is None:
            notes = ""
        if not isinstance(notes, str):
            try:
                notes = str(notes)
            except Exception:
                notes = ""
                errs.append(ValidationError("invalid_notes_type", detail=f"index={idx}"))

        # If critical fields invalid, drop this result (but keep errs)
        if errs and (not isinstance(case_id, str) or not isinstance(classification, str)):
            return None, errs

        vr = ValidatedResult(
            case_id=case_id.strip(),
            classification=classification,
            confidence=conf_val,
            extracted=extracted_norm,
            notes=notes,
        )
        return vr, errs

    def _clamp(self, x: float, lo: float, hi: float) -> float:
        if x < lo:
            return lo
        if x > hi:
            return hi
        return x


--------------------------------------------------------------------------------
FILE: src\microservices\microservice_std_lib.py
--------------------------------------------------------------------------------
"""
LIBRARY: Microservice Standard Lib
VERSION: 2.1.0
ROLE: Provides decorators for tagging Python classes as AI-discoverable services.

Change (2.1.0):
- Split dependencies into:
    internal_dependencies: local modules / microservices to vendor with the app
    external_dependencies: pip-installable packages (requirements.txt)
- Keep legacy "dependencies" as an alias for external_dependencies for backward compatibility.
- Accept unknown keyword args in @service_metadata(...) to prevent older/newer services from crashing
  (e.g. when a runner passes additional fields).
"""

import functools
import inspect
from typing import Dict, List, Any, Optional, Type

# ==============================================================================
# DECORATORS (The "Writer" Tools)
# ==============================================================================

def service_metadata(
    name: str,
    version: str,
    description: str,
    tags: List[str],
    capabilities: Optional[List[str]] = None,

    # Legacy field (kept for backward compatibility):
    # Historically this mixed stdlib + pip deps. Going forward, treat this as *external* deps.
    dependencies: Optional[List[str]] = None,

    # New fields (preferred):
    internal_dependencies: Optional[List[str]] = None,
    external_dependencies: Optional[List[str]] = None,

    # Side effects / operational hints
    side_effects: Optional[List[str]] = None,

    # Forward-compat: ignore unknown keyword args instead of crashing older/newer services
    **_ignored_kwargs: Any,
):
    """
    Class Decorator.
    Labels a Microservice class with high-level metadata for the Catalog.

    Dependency semantics:
      - internal_dependencies: local modules and/or other microservice modules that must be shipped with an app
      - external_dependencies: third-party pip packages (requirements.txt)
      - dependencies (legacy): treated as external_dependencies when external_dependencies is not provided
    """
    # Prefer explicit new key, otherwise fall back to legacy dependencies
    ext = external_dependencies if external_dependencies is not None else (dependencies or [])
    intl = internal_dependencies or []

    def decorator(cls):
        cls._is_microservice = True
        cls._service_info = {
            "name": name,
            "version": version,
            "description": description,
            "tags": tags,
            "capabilities": capabilities or [],

            # New keys
            "internal_dependencies": intl,
            "external_dependencies": ext,

            # Legacy alias (keep existing tooling working)
            "dependencies": ext,

            "side_effects": side_effects or []
        }
        return cls
    return decorator


def service_endpoint(
    inputs: Dict[str, str],
    outputs: Dict[str, str],
    description: str,
    tags: Optional[List[str]] = None,
    side_effects: Optional[List[str]] = None,
    mode: str = "sync",
):
    """
    Method Decorator.
    Defines the 'Socket' that the AI Architect can plug into.

    :param inputs: Dict of {arg_name: type_string} (e.g. {"query": "str"})
    :param outputs: Dict of {return_name: type_string}
    :param description: What the endpoint does
    :param tags: List of categories (e.g. ["read", "write"])
    :param side_effects: List of side effects (e.g. ["filesystem:write", "db:write"])
    :param mode: "sync" or "async" (informational unless your runtime uses it)
    """

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)

        # Attach metadata to the function object itself
        wrapper._is_endpoint = True
        wrapper._endpoint_info = {
            "name": func.__name__,
            "inputs": inputs,
            "outputs": outputs,
            "description": description,
            "tags": tags or [],
            "side_effects": side_effects or [],
            "mode": mode
        }
        return wrapper
    return decorator


# ==============================================================================
# INTROSPECTION (The "Reader" Tools)
# ==============================================================================

def extract_service_schema(service_cls: Type) -> Dict[str, Any]:
    """
    Scans a decorated Service Class and returns a JSON-serializable schema
    of its metadata and all its exposed endpoints.

    This is what the AI Agent uses to 'read' the manual.
    """
    if not getattr(service_cls, "_is_microservice", False):
        raise ValueError(f"Class {service_cls.__name__} is not decorated with @service_metadata")

    schema = {
        "meta": getattr(service_cls, "_service_info", {}),
        "endpoints": []
    }

    # Inspect all methods of the class
    for _, method in inspect.getmembers(service_cls, predicate=inspect.isfunction):
        endpoint_info = getattr(method, "_endpoint_info", None)
        if endpoint_info:
            schema["endpoints"].append(endpoint_info)

    return schema

--------------------------------------------------------------------------------
FILE: src\microservices\OllamaClientMS.py
--------------------------------------------------------------------------------
"""
OllamaClientMS
--------------
Minimal, dependency-free Ollama HTTP client using stdlib (urllib).

Responsibilities:
- List local models: GET /api/tags
- Generate text/JSON: POST /api/generate
- Provide timeouts + clear error reporting
- Optional streaming support (line-delimited JSON chunks)

Non-goals:
- UI
- Prompt building (InferencePromptBuilderMS)
- Output validation (InferenceResultValidatorMS)
"""

from __future__ import annotations

import json
import socket
import urllib.request
import urllib.error
from dataclasses import dataclass
from typing import Dict, Generator, List, Optional, Tuple, Union


# -------------------------
# Data Structures
# -------------------------

@dataclass(frozen=True)
class OllamaError:
    message: str
    status: Optional[int] = None
    detail: Optional[str] = None


@dataclass(frozen=True)
class OllamaResponse:
    ok: bool
    text: Optional[str] = None
    raw: Optional[Dict] = None
    error: Optional[OllamaError] = None


@dataclass
class OllamaClientConfig:
    base_url: str = "http://127.0.0.1:11434"
    timeout_sec: float = 30.0
    user_agent: str = "UiMapper/OllamaClientMS"


# -------------------------
# Service
# -------------------------

class OllamaClientMS:
    def __init__(self, config: Optional[OllamaClientConfig] = None):
        self.config = config or OllamaClientConfig()

    # -------------------------
    # Public API
    # -------------------------

    def list_models(self) -> OllamaResponse:
        """
        GET /api/tags
        Returns raw JSON plus convenience 'text' (None).
        """
        url = self._url("/api/tags")
        return self._get_json(url)

    def generate(
        self,
        *,
        model: str,
        prompt: str,
        system: Optional[str] = None,
        format: Optional[str] = None,  # e.g. "json"
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        num_predict: Optional[int] = None,
        stream: bool = False,
        options: Optional[Dict] = None,
    ) -> Union[OllamaResponse, Generator[OllamaResponse, None, None]]:
        """
        POST /api/generate

        If stream=False:
            returns OllamaResponse with .text containing the full response field.

        If stream=True:
            returns generator yielding OllamaResponse per chunk (ok=True, raw=chunk, text=chunk.get("response")).
        """
        url = self._url("/api/generate")

        payload: Dict = {
            "model": model,
            "prompt": prompt,
            "stream": bool(stream),
        }
        if system is not None:
            payload["system"] = system
        if format is not None:
            payload["format"] = format
        if options is not None:
            payload["options"] = dict(options)

        # Convenience options
        if temperature is not None:
            payload.setdefault("options", {})
            payload["options"]["temperature"] = temperature
        if top_p is not None:
            payload.setdefault("options", {})
            payload["options"]["top_p"] = top_p
        if num_predict is not None:
            payload.setdefault("options", {})
            payload["options"]["num_predict"] = num_predict

        if stream:
            return self._post_stream_json(url, payload)
        return self._post_json(url, payload)

    # -------------------------
    # Internal HTTP helpers
    # -------------------------

    def _url(self, path: str) -> str:
        base = self.config.base_url.rstrip("/")
        if not path.startswith("/"):
            path = "/" + path
        return base + path

    def _headers(self) -> Dict[str, str]:
        return {
            "Content-Type": "application/json",
            "Accept": "application/json",
            "User-Agent": self.config.user_agent,
        }

    def _get_json(self, url: str) -> OllamaResponse:
        req = urllib.request.Request(url=url, headers=self._headers(), method="GET")
        try:
            with urllib.request.urlopen(req, timeout=self.config.timeout_sec) as resp:
                data = resp.read().decode("utf-8", errors="replace")
                raw = json.loads(data) if data.strip() else {}
                return OllamaResponse(ok=True, raw=raw)
        except urllib.error.HTTPError as e:
            return OllamaResponse(
                ok=False,
                error=OllamaError(
                    message="http_error",
                    status=getattr(e, "code", None),
                    detail=self._safe_read_http_error(e),
                ),
            )
        except urllib.error.URLError as e:
            return OllamaResponse(ok=False, error=OllamaError(message="url_error", detail=str(e)))
        except socket.timeout:
            return OllamaResponse(ok=False, error=OllamaError(message="timeout", detail=f">{self.config.timeout_sec}s"))
        except Exception as e:
            return OllamaResponse(ok=False, error=OllamaError(message="exception", detail=str(e)))

    def _post_json(self, url: str, payload: Dict) -> OllamaResponse:
        body = json.dumps(payload).encode("utf-8")
        req = urllib.request.Request(url=url, headers=self._headers(), data=body, method="POST")
        try:
            with urllib.request.urlopen(req, timeout=self.config.timeout_sec) as resp:
                data = resp.read().decode("utf-8", errors="replace")
                raw = json.loads(data) if data.strip() else {}
                # Ollama returns "response" for generate
                text = raw.get("response")
                return OllamaResponse(ok=True, raw=raw, text=text)
        except urllib.error.HTTPError as e:
            return OllamaResponse(
                ok=False,
                error=OllamaError(
                    message="http_error",
                    status=getattr(e, "code", None),
                    detail=self._safe_read_http_error(e),
                ),
            )
        except urllib.error.URLError as e:
            return OllamaResponse(ok=False, error=OllamaError(message="url_error", detail=str(e)))
        except socket.timeout:
            return OllamaResponse(ok=False, error=OllamaError(message="timeout", detail=f">{self.config.timeout_sec}s"))
        except Exception as e:
            return OllamaResponse(ok=False, error=OllamaError(message="exception", detail=str(e)))

    def _post_stream_json(self, url: str, payload: Dict) -> Generator[OllamaResponse, None, None]:
        """
        Stream responses. Ollama streams JSON objects, one per line.
        """
        body = json.dumps(payload).encode("utf-8")
        req = urllib.request.Request(url=url, headers=self._headers(), data=body, method="POST")

        try:
            with urllib.request.urlopen(req, timeout=self.config.timeout_sec) as resp:
                for line in resp:
                    try:
                        s = line.decode("utf-8", errors="replace").strip()
                        if not s:
                            continue
                        raw = json.loads(s)
                        yield OllamaResponse(ok=True, raw=raw, text=raw.get("response"))
                        if raw.get("done") is True:
                            break
                    except Exception as e:
                        yield OllamaResponse(ok=False, error=OllamaError(message="stream_parse_error", detail=str(e)))
                        break
        except urllib.error.HTTPError as e:
            yield OllamaResponse(
                ok=False,
                error=OllamaError(
                    message="http_error",
                    status=getattr(e, "code", None),
                    detail=self._safe_read_http_error(e),
                ),
            )
        except urllib.error.URLError as e:
            yield OllamaResponse(ok=False, error=OllamaError(message="url_error", detail=str(e)))
        except socket.timeout:
            yield OllamaResponse(ok=False, error=OllamaError(message="timeout", detail=f">{self.config.timeout_sec}s"))
        except Exception as e:
            yield OllamaResponse(ok=False, error=OllamaError(message="exception", detail=str(e)))

    def _safe_read_http_error(self, e: urllib.error.HTTPError) -> str:
        try:
            data = e.read().decode("utf-8", errors="replace")
            return data[:4000]
        except Exception:
            return ""


--------------------------------------------------------------------------------
FILE: src\microservices\ProgressEventBusMS.py
--------------------------------------------------------------------------------
"""
ProgressEventBusMS
------------------
A tiny pub/sub event bus for progress + logging events.

Responsibilities:
- Allow microservices/orchestrators to publish progress events
- Allow UI layer to subscribe/unsubscribe handlers
- Keep it thread-safe enough for "publish from worker thread, handle on UI thread"
  (UI should marshal events to Tk via after(); this bus just delivers callbacks)

Event Model:
- Each event is a dict with at minimum:
    {"type": "...", "message": "...", "level": "info|warn|error", "meta": {...}}

Non-goals:
- Tkinter after() marshalling
- Persistent logging
- Complex routing / filtering (keep it minimal + deterministic)
"""

from __future__ import annotations

from dataclasses import dataclass
from threading import Lock
from typing import Any, Callable, Dict, List, Optional


ProgressHandler = Callable[[Dict[str, Any]], None]


@dataclass(frozen=True)
class ProgressEvent:
    type: str
    message: str
    level: str = "info"
    meta: Dict[str, Any] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "type": self.type,
            "message": self.message,
            "level": self.level,
            "meta": dict(self.meta or {}),
        }


class ProgressEventBusMS:
    def __init__(self):
        self._lock = Lock()
        self._subs: List[ProgressHandler] = []

    # -------------------------
    # Subscribe / Unsubscribe
    # -------------------------

    def subscribe(self, handler: ProgressHandler) -> None:
        with self._lock:
            if handler not in self._subs:
                self._subs.append(handler)

    def unsubscribe(self, handler: ProgressHandler) -> None:
        with self._lock:
            if handler in self._subs:
                self._subs.remove(handler)

    def clear(self) -> None:
        with self._lock:
            self._subs.clear()

    # -------------------------
    # Publish
    # -------------------------

    def publish(self, event: Dict[str, Any]) -> None:
        """
        Publish a raw dict event. This makes the bus flexible.
        Handlers should be resilient to missing keys.
        """
        with self._lock:
            subs = list(self._subs)

        for h in subs:
            try:
                h(event)
            except Exception:
                # Bus is intentionally silent; logging belongs elsewhere.
                pass

    def emit(
        self,
        *,
        type: str,
        message: str,
        level: str = "info",
        meta: Optional[Dict[str, Any]] = None,
    ) -> None:
        self.publish(
            {
                "type": type,
                "message": message,
                "level": level,
                "meta": dict(meta or {}),
            }
        )

    # -------------------------
    # Convenience helpers
    # -------------------------

    def make_logger(self, prefix: str = "") -> Callable[[str], None]:
        """
        Returns a simple function you can pass as `log=` to other services:
            log = bus.make_logger("crawl")
            log("hello") -> emits {"type":"log", "message":"[crawl] hello", ...}
        """
        pfx = f"[{prefix}] " if prefix else ""

        def _log(msg: str) -> None:
            self.emit(type="log", message=pfx + str(msg), level="info")

        return _log


--------------------------------------------------------------------------------
FILE: src\microservices\ProjectCrawlMS.py
--------------------------------------------------------------------------------
"""
ProjectCrawlMS
--------------
Deterministic project filesystem crawler.

Responsibilities:
- Validate project root
- Walk directory tree in stable order
- Yield normalized relative/absolute paths
- Provide extension points for filter services (ignore rules, file-type filters)

Design notes:
- No UI
- No threading
- Pure IO + structure
- Orchestrator decides what to do with yielded paths
"""

from __future__ import annotations

import os
from dataclasses import dataclass
from pathlib import Path
from typing import Callable, Generator, Iterable, List, Optional


# -------------------------
# Data Structures
# -------------------------

@dataclass(frozen=True)
class CrawlEntry:
    abs_path: Path
    rel_path: Path
    is_dir: bool


@dataclass
class CrawlConfig:
    root: Path
    follow_symlinks: bool = False
    include_hidden: bool = False
    stable_sort: bool = True


# -------------------------
# Service
# -------------------------

class ProjectCrawlMS:
    """
    Core filesystem crawler.

    The service itself does NOT know about:
    - gitignore
    - file extensions
    - AST
    - UI

    It simply emits filesystem structure.

    Filtering is injected via:
        path_filters: Callable[[Path, bool], bool]
            (path, is_dir) -> keep?
    """

    def __init__(self, config: CrawlConfig):
        self.config = config
        self.root = config.root.resolve()

        self._validate_root()

    # -------------------------
    # Validation
    # -------------------------

    def _validate_root(self) -> None:
        if not self.root.exists():
            raise FileNotFoundError(f"Project root does not exist: {self.root}")
        if not self.root.is_dir():
            raise NotADirectoryError(f"Project root is not a directory: {self.root}")

    # -------------------------
    # Public API
    # -------------------------

    def crawl(
        self,
        path_filters: Optional[List[Callable[[Path, bool], bool]]] = None,
    ) -> Generator[CrawlEntry, None, None]:
        """
        Walk the filesystem from root and yield CrawlEntry objects.

        path_filters:
            List of predicates. If any returns False -> entry skipped.
            Signature: (abs_path: Path, is_dir: bool) -> bool
        """

        filters = path_filters or []

        for abs_dir, dirs, files in os.walk(
            self.root,
            followlinks=self.config.follow_symlinks,
        ):
            abs_dir_path = Path(abs_dir)

            # Stable ordering for deterministic mapping
            if self.config.stable_sort:
                dirs.sort()
                files.sort()

            # Emit directory itself
            rel_dir = abs_dir_path.relative_to(self.root)

            if self._passes_filters(abs_dir_path, True, filters):
                yield CrawlEntry(
                    abs_path=abs_dir_path,
                    rel_path=rel_dir,
                    is_dir=True,
                )

            # Control directory traversal (filter dirs in-place)
            dirs[:] = [
                d for d in dirs
                if self._dir_allowed(abs_dir_path / d, filters)
            ]

            # Emit files
            for file_name in files:
                abs_file = abs_dir_path / file_name

                if not self.config.include_hidden and file_name.startswith("."):
                    continue

                if self._passes_filters(abs_file, False, filters):
                    yield CrawlEntry(


--------------------------------------------------------------------------------
FILE: src\microservices\PythonFileEnumeratorMS.py
--------------------------------------------------------------------------------
"""
PythonFileEnumeratorMS
----------------------
Filter + enumerate Python files from a ProjectCrawlMS stream.

Responsibilities:
- Consume CrawlEntry stream
- Emit only .py files (optionally include .pyw)
- Optionally exclude common virtualenv / cache dirs via fast heuristics
- Provide stable ordering

Non-goals:
- .gitignore parsing (use GitignoreFilterMS)
- AST parsing
- UI logic
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Generator, Iterable, List, Optional, Set, Tuple


# -------------------------
# Data Structures
# -------------------------

@dataclass
class PythonEnumConfig:
    include_pyw: bool = True
    stable_sort: bool = True

    # Fast path pruning based on path parts (not gitignore-accurate, just practical)
    exclude_dir_names: Set[str] = None

    def __post_init__(self) -> None:
        if self.exclude_dir_names is None:
            self.exclude_dir_names = {
                ".git",
                ".hg",
                ".svn",
                "__pycache__",
                ".mypy_cache",
                ".pytest_cache",
                ".ruff_cache",
                ".tox",
                ".nox",
                "venv",
                ".venv",
                "env",
                ".env",
                "node_modules",
                "dist",
                "build",
                ".idea",
                ".vscode",
            }


# -------------------------
# Service
# -------------------------

class PythonFileEnumeratorMS:
    """
    Emits absolute Paths for Python files from a crawl stream.

    Expected upstream:
        ProjectCrawlMS.crawl(...) yielding CrawlEntry(abs_path, rel_path, is_dir)

    Typical usage:
        entries = crawl.crawl(path_filters=[gitignore.predicate(), ...])
        py_files = PythonFileEnumeratorMS(cfg).enumerate(entries)
    """

    def __init__(self, config: Optional[PythonEnumConfig] = None):
        self.config = config or PythonEnumConfig()

    def enumerate(self, crawl_entries: Iterable[object]) -> List[Path]:
        """
        Returns a list (optionally stable-sorted) of Python file absolute paths.
        """
        out: List[Path] = []
        for entry in crawl_entries:
            # Duck-typed to avoid importing CrawlEntry directly
            abs_path: Path = entry.abs_path
            rel_path: Path = entry.rel_path
            is_dir: bool = entry.is_dir

            if is_dir:
                # Heuristic: if any part of rel path is an excluded dir, skip
                if self._contains_excluded_dir(rel_path):
                    continue
                continue

            if self._contains_excluded_dir(rel_path.parent):
                continue

            if self._is_python_file(abs_path):
                out.append(abs_path)

        if self.config.stable_sort:
            out.sort(key=lambda p: p.as_posix().lower())

        return out

    # -------------------------
    # Internals
    # -------------------------

    def _is_python_file(self, path: Path) -> bool:
        s = path.name.lower()
        if s.endswith(".py"):
            return True
        if self.config.include_pyw and s.endswith(".pyw"):
            return True
        return False

    def _contains_excluded_dir(self, rel: Path) -> bool:
        if rel is None:
            return False
        for part in rel.parts:
            if part in self.config.exclude_dir_names:
                return True
        return False


--------------------------------------------------------------------------------
FILE: src\microservices\ReportSerializerMS.py
--------------------------------------------------------------------------------
"""
ReportSerializerMS
------------------
Serialize UiMap artifacts to JSON (and optionally JSONL) in a deterministic way.

Responsibilities:
- Write UiMap.to_dict() output to:
    - JSON (pretty)
    - JSON (compact)
    - JSONL (optional): stream records (widgets/windows/unknowns) for tooling
- Provide stable ordering for deterministic diffs

Non-goals:
- Markdown rendering (ReportWriterMS)
- Choosing output paths (UI orchestrator)
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple


# -------------------------
# Config
# -------------------------

@dataclass
class ReportSerializerConfig:
    pretty: bool = True
    indent: int = 2
    ensure_ascii: bool = False
    sort_keys: bool = True


# -------------------------
# Service
# -------------------------

class ReportSerializerMS:
    def __init__(self, config: Optional[ReportSerializerConfig] = None):
        self.config = config or ReportSerializerConfig()

    # -------------------------
    # JSON
    # -------------------------

    def dumps_json(self, ui_map: object) -> str:
        data = self._as_dict(ui_map)
        if self.config.pretty:
            return json.dumps(
                data,
                indent=self.config.indent,
                ensure_ascii=self.config.ensure_ascii,
                sort_keys=self.config.sort_keys,
            ) + "\n"
        return json.dumps(
            data,
            ensure_ascii=self.config.ensure_ascii,
            sort_keys=self.config.sort_keys,
            separators=(",", ":"),
        ) + "\n"

    def write_json(self, ui_map: object, out_path: Path) -> Path:
        out_path = Path(out_path)
        out_path.write_text(self.dumps_json(ui_map), encoding="utf-8")
        return out_path

    # -------------------------
    # JSONL (optional tooling export)
    # -------------------------

    def write_jsonl(self, ui_map: object, out_path: Path) -> Path:
        """
        Writes multiple JSONL record types:
            {"type":"meta", ...}
            {"type":"window", "id":..., ...}
            {"type":"widget", "id":..., ...}
            {"type":"unknown", ...}
            {"type":"parse_error", ...}
        """
        out_path = Path(out_path)
        data = self._as_dict(ui_map)

        with out_path.open("w", encoding="utf-8") as f:
            # meta
            meta = {"type": "meta", "project_root": data.get("project_root", "")}
            f.write(json.dumps(meta, ensure_ascii=self.config.ensure_ascii) + "\n")

            windows = data.get("windows", {}) or {}
            for win_id in sorted(windows.keys(), key=lambda x: str(x).lower()):
                rec = {"type": "window", "id": win_id}
                rec.update(windows[win_id])
                f.write(json.dumps(rec, ensure_ascii=self.config.ensure_ascii) + "\n")

            widgets = data.get("widgets", {}) or {}
            for wid in sorted(widgets.keys(), key=lambda x: str(x).lower()):
                rec = {"type": "widget", "id": wid}
                rec.update(widgets[wid])
                f.write(json.dumps(rec, ensure_ascii=self.config.ensure_ascii) + "\n")

            unknowns = data.get("unknowns", []) or []
            for u in unknowns:
                rec = {"type": "unknown"}
                rec.update(u)
                f.write(json.dumps(rec, ensure_ascii=self.config.ensure_ascii) + "\n")

            parse_errors = data.get("parse_errors", []) or []
            for e in parse_errors:
                rec = {"type": "parse_error", "error": e}
                f.write(json.dumps(rec, ensure_ascii=self.config.ensure_ascii) + "\n")

        return out_path

    # -------------------------
    # Internal
    # -------------------------

    def _as_dict(self, ui_map: object) -> Dict[str, Any]:
        if isinstance(ui_map, dict):
            return ui_map
        to_dict = getattr(ui_map, "to_dict", None)
        if callable(to_dict):
            return to_dict()
        # best-effort attribute extraction
        out: Dict[str, Any] = {}
        for k in ("project_root", "windows", "widgets", "unknowns", "parse_errors", "callback_edges"):
            out[k] = getattr(ui_map, k, None)
        return out


--------------------------------------------------------------------------------
FILE: src\microservices\ReportWriterMS.py
--------------------------------------------------------------------------------
"""
ReportWriterMS
--------------
Generate human-readable reports from UiMap.

Responsibilities:
- Produce Markdown (primary) report from UiMapModelMS.UiMap (or compatible dict)
- Keep output deterministic (stable ordering)
- Provide a compact executive summary + detailed sections:
    - windows
    - widgets (grouped by type)
    - layout/config calls
    - callbacks/binds
    - unknown cases
    - parse errors

Non-goals:
- File dialogs / choosing output paths (UI orchestrator)
- JSON serialization (ReportSerializerMS can do raw export)
"""

from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
import json


# -------------------------
# Config
# -------------------------

@dataclass
class ReportWriterConfig:
    include_timestamp: bool = True
    max_unknown_examples: int = 50
    max_call_items_per_widget: int = 50


# -------------------------
# Service
# -------------------------

class ReportWriterMS:
    def __init__(self, config: Optional[ReportWriterConfig] = None):
        self.config = config or ReportWriterConfig()

    def build_markdown(self, ui_map: object) -> str:
        """
        ui_map: duck-typed UiMap or dict-like
        """
        m = self._as_dict(ui_map)

        lines: List[str] = []
        lines.append("# UI Mapper Report")
        if self.config.include_timestamp:
            lines.append(f"_Generated: {datetime.now().isoformat(timespec='seconds')}_")
        lines.append("")
        lines.append(f"**Project Root:** `{m.get('project_root', '')}`")
        lines.append("")

        # Summary
        windows = m.get("windows", {}) or {}
        widgets = m.get("widgets", {}) or {}
        unknowns = m.get("unknowns", []) or []
        parse_errors = m.get("parse_errors", []) or []

        lines.append("## Summary")
        lines.append(f"- Windows detected: **{len(windows)}**")
        lines.append(f"- Widgets detected: **{len(widgets)}**")
        lines.append(f"- Unknown cases: **{len(unknowns)}**")
        lines.append(f"- Parse errors: **{len(parse_errors)}**")
        lines.append("")

        # Windows
        lines.append("## Windows")
        if not windows:
            lines.append("_None detected._")
        else:
            for win_id in sorted(windows.keys(), key=lambda x: str(x).lower()):
                w = windows[win_id]
                lines.extend(self._render_window(win_id, w))
        lines.append("")

        # Widgets grouped by type
        lines.append("## Widgets")
        if not widgets:
            lines.append("_None detected._")
        else:
            by_type: Dict[str, List[Tuple[str, Dict[str, Any]]]] = {}
            for wid, w in widgets.items():
                wt = str(w.get("widget_type", "<?>"))
                by_type.setdefault(wt, []).append((wid, w))

            for wt in sorted(by_type.keys(), key=lambda x: str(x).lower()):
                lines.append(f"### {wt} ({len(by_type[wt])})")
                for wid, w in sorted(by_type[wt], key=lambda t: str(t[0]).lower()):
                    lines.extend(self._render_widget(wid, w))
                lines.append("")
        lines.append("")

        # Unknowns
        lines.append("## Unknown Cases")
        if not unknowns:
            lines.append("_None._")
        else:
            # sort by file/line/kind
            unknowns_sorted = sorted(
                unknowns,
                key=lambda u: (
                    str(u.get("where", {}).get("path", "")).lower(),
                    int(u.get("where", {}).get("lineno") or 0),
                    str(u.get("kind", "")).lower(),
                    str(u.get("detail", "")).lower(),
                ),
            )
            shown = unknowns_sorted[: self.config.max_unknown_examples]
            lines.append(f"_Showing {len(shown)} of {len(unknowns_sorted)}._")
            lines.append("")
            for u in shown:
                lines.extend(self._render_unknown(u))
        lines.append("")

        # Parse errors
        lines.append("## Parse Errors")
        if not parse_errors:
            lines.append("_None._")
        else:
            for e in parse_errors:
                lines.append(f"- {e}")
        lines.append("")

        return "\n".join(lines).rstrip() + "\n"

    def write_markdown(self, ui_map: object, out_path: Path) -> Path:
        out_path = Path(out_path)
        md = self.build_markdown(ui_map)
        out_path.write_text(md, encoding="utf-8")
        return out_path

    # -------------------------
    # Render helpers
    # -------------------------

    def _render_window(self, win_id: str, w: Dict[str, Any]) -> List[str]:
        lines: List[str] = []
        created_at = w.get("created_at", {}) or {}
        where = self._render_loc(created_at)

        lines.append(f"### {win_id}")
        lines.append(f"- Created at: {where}")

        titles = w.get("title_calls", []) or []
        geos = w.get("geometry_calls", []) or []
        cfgs = w.get("config_calls", []) or []

        if titles:
            lines.append("- Title calls:")
            for t in titles[: self.config.max_call_items_per_widget]:
                lines.append(f"  - `{t}`")
        if geos:
            lines.append("- Geometry calls:")
            for g in geos[: self.config.max_call_items_per_widget]:
                lines.append(f"  - `{g}`")
        if cfgs:
            lines.append("- Config calls:")
            for c in cfgs[: self.config.max_call_items_per_widget]:
                lines.append(f"  - `{c}`")

        return lines

    def _render_widget(self, wid: str, w: Dict[str, Any]) -> List[str]:
        lines: List[str] = []
        created_at = w.get("created_at", {}) or {}
        where = self._render_loc(created_at)

        parent = w.get("parent_id", None)
        lines.append(f"- **{wid}** (parent: `{parent}`) — created at {where}")

        kwargs = w.get("kwargs", {}) or {}
        if kwargs:
            lines.append("  - kwargs:")
            for k in sorted(kwargs.keys(), key=lambda x: str(x).lower()):
                lines.append(f"    - `{k}` = `{kwargs[k]}`")

        layouts = w.get("layout_calls", []) or []
        if layouts:
            lines.append("  - layout:")
            for lc in layouts[: self.config.max_call_items_per_widget]:
                lines.append(f"    - `{lc}`")

        cfgs = w.get("config_calls", []) or []
        if cfgs:
            lines.append("  - config:")
            for c in cfgs[: self.config.max_call_items_per_widget]:
                lines.append(f"    - `{c}`")

        cmds = w.get("command_targets", []) or []
        if cmds:
            lines.append("  - commands:")
            for c in cmds:
                lines.append(f"    - `{c}`")

        binds = w.get("bind_events", []) or []
        if binds:
            lines.append("  - binds:")
            for b in binds:
                lines.append(f"    - `{b}`")

        return lines

    def _render_unknown(self, u: Dict[str, Any]) -> List[str]:
        lines: List[str] = []
        kind = u.get("kind", "unknown")
        detail = u.get("detail", "")
        where = self._render_loc(u.get("where", {}) or {})
        lines.append(f"- **{kind}** @ {where}")
        if detail:
            lines.append(f"  - detail: {detail}")

        snippet = u.get("snippet", None)
        if snippet:
            lines.append("  - snippet:")
            lines.append("```")
            lines.extend(self._clip_lines(str(snippet), 80))
            lines.append("```")

        ctx = u.get("context", {}) or {}
        if ctx:
            lines.append("  - context:")
            for k in sorted(ctx.keys(), key=lambda x: str(x).lower()):
                v = ctx[k]
                if v is None:
                    v = ""
                lines.append(f"    - {k}: {v}")

        return lines

    def _render_loc(self, loc: Dict[str, Any]) -> str:
        p = loc.get("path", "<?>")
        ln = loc.get("lineno", None)
        col = loc.get("col", None)
        if ln is None and col is None:
            return f"`{p}`"
        return f"`{p}:{ln if ln is not None else '?'}:{col if col is not None else '?'}`"

    def _clip_lines(self, s: str, max_lines: int) -> List[str]:
        lines = s.splitlines()
        if len(lines) <= max_lines:
            return lines
        return lines[:max_lines] + ["... (truncated)"]

    def _as_dict(self, ui_map: object) -> Dict[str, Any]:
        """
        Accept UiMap dataclass with to_dict(), or a dict.
        """
        if isinstance(ui_map, dict):
            return ui_map
        to_dict = getattr(ui_map, "to_dict", None)
        if callable(to_dict):
            return to_dict()
        # last resort: best-effort attribute extraction
        out: Dict[str, Any] = {}
        for k in ("project_root", "windows", "widgets", "unknowns", "parse_errors", "callback_edges"):
            out[k] = getattr(ui_map, k, None)
        return out


--------------------------------------------------------------------------------
FILE: src\microservices\RunSessionStateMS.py
--------------------------------------------------------------------------------
"""
RunSessionStateMS
-----------------
State container for a single "run" of the UI mapping pipeline.

Why:
- Orchestrators need a shared object to store:
    - selected project root
    - discovered file lists
    - parse results
    - ui_map artifacts
    - report output paths
    - timings + counters
- UI layer needs to observe state changes (polling or events)

Responsibilities:
- Hold session fields in a structured dataclass
- Provide reset/clear methods
- Provide lightweight update helpers + counters
- Keep it UI-agnostic (no Tk variables)

Non-goals:
- Persistence (save/load) — could be another microservice later
- Threading
"""

from __future__ import annotations

from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional


@dataclass
class RunCounters:
    dirs_seen: int = 0
    files_seen: int = 0
    py_files: int = 0
    ast_ok: int = 0
    ast_err: int = 0
    widgets: int = 0
    windows: int = 0
    unknowns: int = 0


@dataclass
class RunSessionState:
    session_id: str

    project_root: Optional[str] = None

    # Discovery
    all_entries: List[str] = field(default_factory=list)
    py_files: List[str] = field(default_factory=list)
    entrypoint_candidates: List[Dict[str, Any]] = field(default_factory=list)

    # AST
    ast_ok_paths: List[str] = field(default_factory=list)
    ast_error_items: List[Dict[str, Any]] = field(default_factory=list)

    # Mapping outputs
    ui_map: Optional[Dict[str, Any]] = None  # usually UiMap.to_dict()
    callback_graph: Optional[Dict[str, Any]] = None

    # Reports
    report_md_path: Optional[str] = None
    report_json_path: Optional[str] = None
    report_jsonl_path: Optional[str] = None

    # Timing + counters
    counters: RunCounters = field(default_factory=RunCounters)
    meta: Dict[str, Any] = field(default_factory=dict)

    # Errors / status
    status: str = "idle"  # idle|running|cancelled|done|error
    last_error: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        return asdict(self)


class RunSessionStateMS:
    """
    Provides session creation + mutation helpers.
    """

    def new_session(self, session_id: str) -> RunSessionState:
        return RunSessionState(session_id=session_id)

    def reset(self, s: RunSessionState) -> None:
        sid = s.session_id
        s.__dict__.clear()
        # Re-init minimal
        fresh = RunSessionState(session_id=sid)
        s.__dict__.update(fresh.__dict__)

    # -------------------------
    # Update helpers
    # -------------------------

    def set_project_root(self, s: RunSessionState, root: Path) -> None:
        s.project_root = str(Path(root).resolve())

    def set_status(self, s: RunSessionState, status: str, error: Optional[str] = None) -> None:
        s.status = status
        s.last_error = error

    def add_entry(self, s: RunSessionState, rel_path: Path, is_dir: bool) -> None:
        s.all_entries.append(rel_path.as_posix())
        if is_dir:
            s.counters.dirs_seen += 1
        else:
            s.counters.files_seen += 1

    def set_py_files(self, s: RunSessionState, py_files: List[Path]) -> None:
        s.py_files = [p.as_posix() for p in py_files]
        s.counters.py_files = len(py_files)

    def set_entrypoints(self, s: RunSessionState, candidates: List[object]) -> None:
        """
        candidates: duck-typed EntrypointCandidate(path, score, reasons)
        """
        out: List[Dict[str, Any]] = []
        for c in candidates:
            out.append(
                {
                    "path": getattr(c, "path").as_posix() if getattr(c, "path", None) else "",
                    "score": int(getattr(c, "score", 0)),
                    "reasons": list(getattr(c, "reasons", []) or []),
                }
            )
        s.entrypoint_candidates = out

    def add_ast_ok(self, s: RunSessionState, path: Path) -> None:
        s.ast_ok_paths.append(path.as_posix())
        s.counters.ast_ok += 1

    def add_ast_error(self, s: RunSessionState, err_item: Dict[str, Any]) -> None:
        s.ast_error_items.append(dict(err_item))
        s.counters.ast_err += 1

    def set_ui_map(self, s: RunSessionState, ui_map_dict: Dict[str, Any]) -> None:
        s.ui_map = dict(ui_map_dict)
        # attempt to update counters
        s.counters.windows = len((ui_map_dict.get("windows") or {}))
        s.counters.widgets = len((ui_map_dict.get("widgets") or {}))
        s.counters.unknowns = len((ui_map_dict.get("unknowns") or []))

    def set_report_paths(
        self,
        s: RunSessionState,
        *,
        md: Optional[Path] = None,
        json_path: Optional[Path] = None,
        jsonl: Optional[Path] = None,
    ) -> None:
        if md is not None:
            s.report_md_path = Path(md).as_posix()
        if json_path is not None:
            s.report_json_path = Path(json_path).as_posix()
        if jsonl is not None:
            s.report_jsonl_path = Path(jsonl).as_posix()


--------------------------------------------------------------------------------
FILE: src\microservices\TkWidgetDetectorMS.py
--------------------------------------------------------------------------------
"""
TkWidgetDetectorMS
------------------
AST utility microservice for detecting Tkinter/ttk widget construction patterns.

Why this exists (separation of concerns):
- AstUiMapMS should orchestrate "mapping" and aggregation.
- TkWidgetDetectorMS should provide tight, testable detection utilities.

Responsibilities:
- Identify whether an ast.Call looks like:
    - root window creation (Tk())
    - widget constructor call (Frame/Button/etc.)
    - Menu construction
    - layout call (pack/grid/place)
    - config call (configure/config)
    - bind call (bind)
- Extract:
    - widget type name
    - parent expression
    - keyword args (best-effort stringification)
    - command callback targets

Non-goals:
- Whole-file traversal
- Project mapping
- LLM inference
"""

from __future__ import annotations

import ast
from dataclasses import dataclass
from typing import Dict, List, Optional, Set, Tuple


# -------------------------
# Data Structures
# -------------------------

@dataclass(frozen=True)
class DetectedWidgetCall:
    widget_type: str
    parent_expr: Optional[str]
    kwargs: Dict[str, str]
    command_target: Optional[str]


@dataclass(frozen=True)
class DetectedMethodCall:
    method: str
    receiver_expr: str
    args_sig: str


# -------------------------
# Service
# -------------------------

class TkWidgetDetectorMS:
    """
    Stateless detection + extraction helpers.
    """

    # Conservative common set; extend later.
    TK_ROOT_NAMES: Set[str] = {"Tk"}
    LAYOUT_METHODS: Set[str] = {"pack", "grid", "place"}
    CONFIG_METHODS: Set[str] = {"config", "configure"}
    BIND_METHOD: str = "bind"
    MENU_METHODS: Set[str] = {"add_command", "add_separator", "add_cascade"}

    WIDGET_NAMES: Set[str] = {
        # tkinter
        "Frame", "Label", "Button", "Entry", "Text", "Canvas", "Menu", "Scrollbar",
        "Listbox", "Toplevel", "Checkbutton", "Radiobutton", "Spinbox", "Scale",
        "PanedWindow", "LabelFrame", "Message",
        # ttk
        "Combobox", "Treeview", "Notebook", "Separator", "Progressbar",
    }

    # -------------------------
    # Primary detectors
    # -------------------------

    def is_tk_root_call(self, call: ast.Call) -> bool:
        """
        tk.Tk(), tkinter.Tk(), or bare Tk()
        """
        fn = call.func
        if isinstance(fn, ast.Attribute) and fn.attr in self.TK_ROOT_NAMES:
            return True
        if isinstance(fn, ast.Name) and fn.id in self.TK_ROOT_NAMES:
            return True
        return False

    def detect_widget_ctor(self, call: ast.Call) -> Optional[DetectedWidgetCall]:
        """
        If call is a widget constructor, return extracted details.
        """
        widget_type = self._widget_type(call)
        if not widget_type:
            return None

        parent_expr = self._extract_parent_expr(call)
        kwargs = self._extract_kwargs(call)
        cmd = kwargs.get("command")

        return DetectedWidgetCall(
            widget_type=widget_type,
            parent_expr=parent_expr,
            kwargs=kwargs,
            command_target=cmd,
        )

    def detect_layout_call(self, call: ast.Call) -> Optional[DetectedMethodCall]:
        """
        widget.pack(...) / widget.grid(...) / widget.place(...)
        """
        return self._detect_attr_call(call, self.LAYOUT_METHODS)

    def detect_config_call(self, call: ast.Call) -> Optional[DetectedMethodCall]:
        """
        widget.configure(...) / widget.config(...)
        """
        return self._detect_attr_call(call, self.CONFIG_METHODS)

    def detect_bind_call(self, call: ast.Call) -> Optional[DetectedMethodCall]:
        """
        widget.bind(event, handler, ...)
        """
        if not isinstance(call.func, ast.Attribute):
            return None
        if call.func.attr != self.BIND_METHOD:
            return None
        recv = self.expr_to_str(call.func.value)
        return DetectedMethodCall(
            method=self.BIND_METHOD,
            receiver_expr=recv,
            args_sig=self.call_args_sig(call),
        )

    def detect_menu_method(self, call: ast.Call) -> Optional[DetectedMethodCall]:
        """
        menu.add_command(...) / add_separator / add_cascade
        """
        return self._detect_attr_call(call, self.MENU_METHODS)

    # -------------------------
    # Extraction utilities
    # -------------------------

    def expr_to_str(self, node: ast.AST) -> str:
        """
        Conservative stringification.
        Designed for diagnostics + mapping, not round-tripping.
        """
        if isinstance(node, ast.Name):
            return node.id
        if isinstance(node, ast.Attribute):
            return f"{self.expr_to_str(node.value)}.{node.attr}"
        if isinstance(node, ast.Constant):
            return repr(node.value)
        if isinstance(node, ast.Call):
            return f"{self.expr_to_str(node.func)}(...)"
        if isinstance(node, ast.Subscript):
            return f"{self.expr_to_str(node.value)}[...]"
        if isinstance(node, ast.JoinedStr):
            return "f'...'"
        return node.__class__.__name__

    def call_args_sig(self, call: ast.Call) -> str:
        parts: List[str] = []
        for a in call.args:
            parts.append(self.expr_to_str(a))
        for kw in call.keywords:
            if kw.arg is None:
                parts.append("**kwargs")
            else:
                parts.append(f"{kw.arg}={self.expr_to_str(kw.value)}")
        return ", ".join(parts)

    # -------------------------
    # Internal helpers
    # -------------------------

    def _widget_type(self, call: ast.Call) -> Optional[str]:
        fn = call.func
        if isinstance(fn, ast.Attribute) and fn.attr in self.WIDGET_NAMES:
            return fn.attr
        if isinstance(fn, ast.Name) and fn.id in self.WIDGET_NAMES:
            return fn.id
        return None

    def _extract_parent_expr(self, call: ast.Call) -> Optional[str]:
        if not call.args:
            return None
        return self.expr_to_str(call.args[0])

    def _extract_kwargs(self, call: ast.Call) -> Dict[str, str]:
        out: Dict[str, str] = {}
        for kw in call.keywords:
            if kw.arg is None:
                continue
            out[kw.arg] = self.expr_to_str(kw.value)
        return out

    def _detect_attr_call(self, call: ast.Call, allowed: Set[str]) -> Optional[DetectedMethodCall]:
        if not isinstance(call.func, ast.Attribute):
            return None
        if call.func.attr not in allowed:
            return None
        recv = self.expr_to_str(call.func.value)
        return DetectedMethodCall(
            method=call.func.attr,
            receiver_expr=recv,
            args_sig=self.call_args_sig(call),
        )


--------------------------------------------------------------------------------
FILE: src\microservices\UiMapModelMS.py
--------------------------------------------------------------------------------
"""
UiMapModelMS
------------
Canonical data model + helpers for UI mapping results.

Why:
- Multiple services (AstUiMapMS, CallbackGraphBuilderMS, ReportWriterMS, UI)
  need a shared, stable schema.
- This microservice owns:
    - dataclasses
    - merging/patching helpers
    - deterministic ID assignment helpers (optional)
    - lightweight query helpers

Responsibilities:
- Define UiMap, UiWindow, UiWidget, UnknownCase, SourceLoc
- Provide safe merge/patch operations (e.g., apply inference result)
- Provide export-ready dict conversion (pure python primitives)

Non-goals:
- AST traversal
- Report formatting (ReportWriterMS)
- LLM prompts/validation
"""

from __future__ import annotations

from dataclasses import dataclass, field, asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


# -------------------------
# Core Structures
# -------------------------

@dataclass(frozen=True)
class SourceLoc:
    path: str
    lineno: Optional[int] = None
    col: Optional[int] = None


@dataclass
class UiWindow:
    window_id: str
    created_at: SourceLoc
    title_calls: List[str] = field(default_factory=list)
    geometry_calls: List[str] = field(default_factory=list)
    config_calls: List[str] = field(default_factory=list)


@dataclass
class UiWidget:
    widget_id: str
    widget_type: str
    parent_id: Optional[str]
    created_at: SourceLoc

    kwargs: Dict[str, str] = field(default_factory=dict)

    layout_calls: List[str] = field(default_factory=list)
    config_calls: List[str] = field(default_factory=list)

    # Callbacks / event wiring
    command_targets: List[str] = field(default_factory=list)
    bind_events: List[str] = field(default_factory=list)


@dataclass
class UnknownCase:
    kind: str
    detail: str
    where: SourceLoc
    snippet: Optional[str] = None
    context: Dict[str, str] = field(default_factory=dict)


@dataclass
class UiMap:
    project_root: str

    windows: Dict[str, UiWindow] = field(default_factory=dict)
    widgets: Dict[str, UiWidget] = field(default_factory=dict)

    unknowns: List[UnknownCase] = field(default_factory=list)
    parse_errors: List[str] = field(default_factory=list)

    # Optional graphs / enrichments
    callback_edges: List[Dict[str, str]] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to JSON-serializable dict deterministically.
        """
        def _sort_dict(d: Dict[str, Any]) -> Dict[str, Any]:
            return {k: d[k] for k in sorted(d.keys(), key=lambda x: str(x).lower())}

        out = asdict(self)

        # Deterministic ordering for dict fields
        out["windows"] = _sort_dict(out.get("windows", {}))
        out["widgets"] = _sort_dict(out.get("widgets", {}))

        # Deterministic ordering for unknowns (by file/line/kind)
        out["unknowns"] = sorted(
            out.get("unknowns", []),
            key=lambda u: (
                str(u.get("where", {}).get("path", "")).lower(),
                int(u.get("where", {}).get("lineno") or 0),
                str(u.get("kind", "")).lower(),
                str(u.get("detail", "")).lower(),
            ),
        )
        return out


# -------------------------
# Service
# -------------------------

class UiMapModelMS:
    """
    Helper operations around UiMap.
    """

    # -------------------------
    # Construction helpers
    # -------------------------

    def new_map(self, project_root: Path) -> UiMap:
        return UiMap(project_root=str(Path(project_root).resolve()))

    def loc(self, path: Path, node: Optional[object] = None) -> SourceLoc:
        return SourceLoc(
            path=str(Path(path).resolve()),
            lineno=getattr(node, "lineno", None) if node is not None else None,
            col=getattr(node, "col_offset", None) if node is not None else None,
        )

    # -------------------------
    # Unknown handling
    # -------------------------

    def add_unknown(
        self,
        ui_map: UiMap,
        *,
        kind: str,
        detail: str,
        where: SourceLoc,
        snippet: Optional[str] = None,
        context: Optional[Dict[str, str]] = None,
    ) -> None:
        ui_map.unknowns.append(
            UnknownCase(
                kind=kind,
                detail=detail,
                where=where,
                snippet=snippet,
                context=context or {},
            )
        )

    # -------------------------
    # Patch / merge helpers
    # -------------------------

    def apply_inference_patch(
        self,
        ui_map: UiMap,
        *,
        case_id: str,
        classification: str,
        extracted: Dict[str, Optional[str]],
        notes: str = "",
    ) -> None:
        """
        Apply a validated inference result conservatively.
        This function does NOT remove unknowns automatically. It can annotate.

        Strategy:
        - For now, we simply append an annotation unknown with kind="inference_applied"
          so downstream report can show what happened.
        - Later, you can map case_id -> specific unknown index and "resolve" it.
        """
        self.add_unknown(
            ui_map,
            kind="inference_applied",
            detail=f"{case_id} classification={classification} notes={notes}".strip(),
            where=SourceLoc(path=ui_map.project_root),
            snippet=None,
            context={k: (v if v is not None else "") for k, v in extracted.items()},
        )

    def merge_maps(self, base: UiMap, other: UiMap) -> UiMap:
        """
        Merge two UiMaps. Deterministic, conservative:
        - windows/widgets merged by id (other overwrites on key collision)
        - unknowns concatenated
        - parse_errors concatenated
        """
        out = UiMap(project_root=base.project_root)

        out.windows = dict(base.windows)
        out.windows.update(other.windows)

        out.widgets = dict(base.widgets)
        out.widgets.update(other.widgets)

        out.unknowns = list(base.unknowns) + list(other.unknowns)
        out.parse_errors = list(base.parse_errors) + list(other.parse_errors)
        out.callback_edges = list(base.callback_edges) + list(other.callback_edges)

        return out

    # -------------------------
    # Query helpers
    # -------------------------

    def widgets_by_type(self, ui_map: UiMap, widget_type: str) -> List[UiWidget]:
        wt = widget_type.strip()
        return [w for w in ui_map.widgets.values() if w.widget_type == wt]

    def root_windows(self, ui_map: UiMap) -> List[UiWindow]:
        return list(ui_map.windows.values())


--------------------------------------------------------------------------------
FILE: src\microservices\UnknownCaseCollectorMS.py
--------------------------------------------------------------------------------
"""
UnknownCaseCollectorMS
----------------------
Collect and normalize "unknown cases" encountered during UI/AST mapping.

Why:
- Many analysis steps are intentionally conservative.
- When we cannot confidently classify something (dynamic widget factory, lambda callback,
  indirect assignment, getattr, etc.), we emit an UnknownCase.
- This service centralizes:
    - creation helpers (consistent fields)
    - dedupe / grouping / ranking
    - export-friendly summaries for HITL + LLM prompting

Responsibilities:
- Provide methods to record unknown cases with stable keys
- Support deduping similar unknowns (same kind + same file + same line + same detail)
- Provide grouped views (by kind, by file)
- Provide "top unknowns" selection for LLM/HITL

Non-goals:
- Performing inference
- UI dialogs
- AST traversal (callers use this to record)
"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple


# -------------------------
# Data Structures
# -------------------------

@dataclass(frozen=True)
class UnknownCase:
    kind: str
    detail: str
    path: Path
    lineno: Optional[int] = None
    col: Optional[int] = None
    snippet: Optional[str] = None

    # Optional context payloads (keep small; avoid megabyte dumps)
    context: Dict[str, str] = field(default_factory=dict)

    def key(self) -> Tuple[str, str, str, int, int]:
        """
        Stable dedupe key.
        """
        return (
            self.kind,
            self.detail.strip(),
            self.path.as_posix(),
            int(self.lineno or 0),
            int(self.col or 0),
        )


@dataclass
class UnknownSummary:
    kind: str
    count: int
    examples: List[UnknownCase] = field(default_factory=list)


# -------------------------
# Service
# -------------------------

class UnknownCaseCollectorMS:
    def __init__(self):
        self._by_key: Dict[Tuple[str, str, str, int, int], UnknownCase] = {}
        self._counts: Dict[Tuple[str, str, str, int, int], int] = {}

    # -------------------------
    # Record API
    # -------------------------

    def record(
        self,
        *,
        kind: str,
        detail: str,
        path: Path,
        lineno: Optional[int] = None,
        col: Optional[int] = None,
        snippet: Optional[str] = None,
        context: Optional[Dict[str, str]] = None,
    ) -> None:
        uc = UnknownCase(
            kind=kind,
            detail=detail,
            path=Path(path).resolve(),
            lineno=lineno,
            col=col,
            snippet=snippet,
            context=context or {},
        )
        k = uc.key()
        if k not in self._by_key:
            self._by_key[k] = uc
            self._counts[k] = 1
        else:
            self._counts[k] += 1

    # Convenience helpers

    def record_ast_node(
        self,
        *,
        kind: str,
        detail: str,
        path: Path,
        node: object,
        snippet: Optional[str] = None,
        context: Optional[Dict[str, str]] = None,
    ) -> None:
        lineno = getattr(node, "lineno", None)
        col = getattr(node, "col_offset", None)
        self.record(
            kind=kind,
            detail=detail,
            path=path,
            lineno=lineno,
            col=col,
            snippet=snippet,
            context=context,
        )

    # -------------------------
    # Retrieval
    # -------------------------

    def all_cases(self) -> List[UnknownCase]:
        return list(self._by_key.values())

    def count(self, case: UnknownCase) -> int:
        return self._counts.get(case.key(), 1)

    def clear(self) -> None:
        self._by_key.clear()
        self._counts.clear()

    # -------------------------
    # Grouping / Summaries
    # -------------------------

    def summarize_by_kind(self, max_examples_per_kind: int = 3) -> List[UnknownSummary]:
        buckets: Dict[str, List[UnknownCase]] = {}
        for uc in self._by_key.values():
            buckets.setdefault(uc.kind, []).append(uc)

        summaries: List[UnknownSummary] = []
        for kind, cases in buckets.items():
            # deterministic ordering: file/line then detail
            cases_sorted = sorted(
                cases,
                key=lambda c: (c.path.as_posix().lower(), int(c.lineno or 0), c.detail.lower()),
            )
            summaries.append(
                UnknownSummary(
                    kind=kind,
                    count=sum(self._counts[c.key()] for c in cases_sorted),
                    examples=cases_sorted[:max_examples_per_kind],
                )
            )

        # sort summaries by total count desc, then kind
        summaries.sort(key=lambda s: (-s.count, s.kind.lower()))
        return summaries

    def summarize_by_file(self, max_examples_per_file: int = 5) -> Dict[str, List[UnknownCase]]:
        buckets: Dict[str, List[UnknownCase]] = {}
        for uc in self._by_key.values():
            key = uc.path.as_posix()
            buckets.setdefault(key, []).append(uc)

        for k in list(buckets.keys()):
            buckets[k] = sorted(
                buckets[k],
                key=lambda c: (c.kind.lower(), int(c.lineno or 0), c.detail.lower()),
            )[:max_examples_per_file]

        return dict(sorted(buckets.items(), key=lambda kv: kv[0].lower()))

    # -------------------------
    # Selection for HITL/LLM
    # -------------------------

    def select_for_inference(
        self,
        *,
        max_items: int = 20,
        kind_priority: Optional[List[str]] = None,
    ) -> List[UnknownCase]:
        """
        Returns a prioritized list of unknown cases to send to an LLM/HITL.
        Strategy:
        - Prefer kinds listed in kind_priority (in order)
        - Otherwise, prefer cases with higher occurrence counts
        - Stable tie-break by file/line
        """
        kind_priority = kind_priority or []

        cases = list(self._by_key.values())

        def prio(uc: UnknownCase) -> Tuple[int, int, str, int]:
            # smaller kind_index is higher priority
            if uc.kind in kind_priority:
                kind_idx = kind_priority.index(uc.kind)
                kind_score = -1000 + kind_idx  # push ahead of all others
            else:
                kind_score = 0

            cnt = self._counts.get(uc.key(), 1)
            return (kind_score, -cnt, uc.path.as_posix().lower(), int(uc.lineno or 0))

        cases.sort(key=prio)
        return cases[:max_items]


--------------------------------------------------------------------------------
FILE: src\microservices\_TkinterAppShellMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterAppShellMS
ENTRY_POINT: _TkinterAppShellMS.py
INTERNAL_DEPENDENCIES: _TkinterThemeManagerMS, microservice_std_lib, base_service
EXTERNAL_DEPENDENCIES: None

This module provides the root Tkinter application shell. It owns the
Tkinter root window, manages global theme propagation and hosts
embedded UI components.
"""

import tkinter as tk
from tkinter import ttk
import logging
from typing import Dict, Any, Optional

from .microservice_std_lib import service_metadata, service_endpoint
from .base_service import BaseService

# Attempt to import our theme manager.
try:
    from ._TkinterThemeManagerMS import TkinterThemeManagerMS  # type: ignore
except Exception as ex:
    TkinterThemeManagerMS = None
    logging.getLogger("AppShell").warning(
        "Theme manager could not be imported: %s. Falling back to defaults.", ex
    )

logger = logging.getLogger("AppShell")


@service_metadata(
    name="TkinterAppShell",
    version="2.1.1",
    description="The Mother Ship: Root UI container that manages the lifecycle of recursive Cell windows.",
    tags=["ui", "shell", "container"],
    capabilities=["ui:gui", "window-management"],
    internal_dependencies=["_TkinterThemeManagerMS", "microservice_std_lib", "base_service"],
)
class TkinterAppShellMS(BaseService):
    """
    The Mother Ship.
    Owns the Tk root window and provides a stable lifecycle contract for the app.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("TkinterAppShell")
        self.config = config or {}

        # 1) Root Window
        self.root = tk.Tk()
        self.root.title(self.config.get("title", "_theCELL - Idea Ingestor"))
        self.root.geometry(self.config.get("geometry", "1000x800"))

        # 2) Theme Management
        self.theme_manager = None
        self.colors = {
            "background": "#1e1e1e",
            "foreground": "#cccccc",
            "panel_bg": "#252526",
            "accent": "#007acc",
            # optional UI keys some screens may ask for:
            "entry_bg": "#1e1e1e",
            "entry_fg": "#cccccc",
            "select_bg": "#264f78",
            "button_fg": "#cccccc",
        }

        if TkinterThemeManagerMS:
            theme_pref = (self.config.get("theme") or "Dark").strip().title()
            self.theme_manager = TkinterThemeManagerMS({"theme": theme_pref})
            try:
                self.colors = self.theme_manager.get_theme()
            except Exception:
                # fallback to defaults if theme manager is partially implemented
                pass
            self.log_info(f"Theme Manager initialized with '{theme_pref}' theme.")

        self.root.configure(bg=self.colors.get("background"))

        # 3) Main Container (the docking zone)
        self.main_container = tk.Frame(self.root, bg=self.colors.get("background"))
        self.main_container.pack(fill="both", expand=True)

        # If any build withdraws to avoid a flash, we still safely deiconify in launch()
        # (no-op if not withdrawn).
        # self.root.withdraw()

    # -------------------------------------------------------------------------
    # LIFECYCLE (IMPORTANT: app.py expects shell.launch())
    # -------------------------------------------------------------------------

    @service_endpoint(
        inputs={},
        outputs={},
        description="Starts the GUI main loop.",
        tags=["lifecycle", "start"],
        side_effects=["ui:block"],
    )
    # ROLE: Starts the GUI main loop.
    # INPUTS: {}
    # OUTPUTS: {}
    def launch(self) -> None:
        """Ignition sequence start."""
        try:
            self.root.deiconify()
        except Exception:
            pass

        try:
            self.log_info("AppShell Launched.")
        except Exception:
            logger.info("AppShell Launched.")

        self.root.mainloop()

    @service_endpoint(
        inputs={},
        outputs={},
        description="Gracefully shuts down the application.",
        tags=["lifecycle", "stop"],
        side_effects=["ui:close"],
    )
    # ROLE: Gracefully shuts down the application.
    # INPUTS: {}
    # OUTPUTS: {}
    def shutdown(self) -> None:
        """Closes the Tkinter event loop and destroys the root window."""
        self.log_info("Shutting down Application Shell.")
        try:
            # quit() exits mainloop; destroy() removes windows
            self.root.quit()
            self.root.destroy()
        except Exception:
            pass

    # -------------------------------------------------------------------------
    # THEME
    # -------------------------------------------------------------------------

    @service_endpoint(
        inputs={"theme_name": "str"},
        outputs={"success": "bool"},
        description="Switches the global application theme (e.g., Dark, Light).",
        tags=["ui", "theme"],
        side_effects=["ui:refresh"],
    )
    # ROLE: Switches the global application theme (e.g., Dark, Light).
    # INPUTS: {"theme_name": "str"}
    # OUTPUTS: {"success": "bool"}
    def set_theme(self, theme_name: str) -> bool:
        """Updates the theme and propagates colors to the shell."""
        if not self.theme_manager:
            return False

        theme_name = (theme_name or "Dark").strip().title()
        success = False
        try:
            success = bool(self.theme_manager.set_theme(theme_name))
        except Exception:
            success = False

        if success:
            try:
                self.colors = self.theme_manager.get_theme()
            except Exception:
                pass
            self.root.configure(bg=self.colors.get("background"))
            self.main_container.configure(bg=self.colors.get("background"))
            self.log_info(f"Theme switched to {theme_name}")

        return success

    # -------------------------------------------------------------------------
    # LAYOUT + WINDOWS
    # -------------------------------------------------------------------------

    @service_endpoint(
        inputs={},
        outputs={"container": "tk.Frame"},
        description="Returns the main content area for other services to dock into.",
        tags=["ui", "layout"],
    )
    # ROLE: Returns the main content area for other services to dock into.
    # INPUTS: {}
    # OUTPUTS: {"container": "tk.Frame"}
    def get_main_container(self) -> tk.Frame:
        """Other services call this to know where to pack() themselves."""
        return self.main_container

    @service_endpoint(
        inputs={"title": "str", "geometry": "str"},
        outputs={"window": "tk.Toplevel"},
        description="Spawns a new top-level window for a child cell.",
        tags=["ui", "lifecycle"],
    )
    # ROLE: Spawns a new top-level window for a child cell.
    # INPUTS: {"geometry": "str", "title": "str"}
    # OUTPUTS: {"window": "tk.Toplevel"}
    def spawn_window(self, title: str = "Child Cell", geometry: str = "1000x800") -> tk.Toplevel:
        """Creates a new Toplevel window that inherits the shell's theme."""
        new_window = tk.Toplevel(self.root)
        new_window.title(title)
        new_window.geometry(geometry)
        bg = self.colors.get("background", "#1e1e1e")
        new_window.configure(bg=bg)
        self.log_info(f"Spawned new window: {title}")
        return new_window


if __name__ == "__main__":
    # Test Harness
    logging.basicConfig(level=logging.INFO)
    shell = TkinterAppShellMS({"title": "Shell Test Window"})
    print(f"Shell Ready: {shell._service_info['id']}")
    shell.launch()

--------------------------------------------------------------------------------
FILE: src\microservices\_TkinterSmartExplorerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterSmartExplorerMS
ENTRY_POINT: _TkinterSmartExplorerMS.py
INTERNAL_DEPENDENCIES: microservice_std_lib
EXTERNAL_DEPENDENCIES: None
"""
import tkinter as tk
from tkinter import ttk
from typing import Dict, Any, Optional, List
from .microservice_std_lib import service_metadata, service_endpoint
from .base_service import BaseService

@service_metadata(name='TkinterSmartExplorer', version='1.0.0', description='A hierarchical tree viewer capable of displaying file systems or JSON data structures.', tags=['ui', 'widget', 'explorer'], capabilities=['ui:gui'], internal_dependencies=['microservice_std_lib'], external_dependencies=[])
class TkinterSmartExplorerMS(tk.Frame, BaseService):
    """
    The Navigator.
    A TreeView widget that expects standard 'Node' dictionaries (name, type, children).
    """

    def __init__(self, config: Optional[Dict[str, Any]]=None):
        self.config = config or {}
        parent = self.config.get('parent')
        
        # Multi-inheritance initialization
        BaseService.__init__(self, 'TkinterSmartExplorer')
        theme = self.config.get('theme', {})
        tk.Frame.__init__(self, parent, bg=theme.get('panel_bg', '#252526'))
        self.tree = ttk.Treeview(self, show='tree headings', selectmode='browse')
        self.tree.heading('#0', text='Explorer', anchor='w')
        vsb = ttk.Scrollbar(self, orient='vertical', command=self.tree.yview)
        hsb = ttk.Scrollbar(self, orient='horizontal', command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        self.tree.pack(side='left', fill='both', expand=True)
        vsb.pack(side='right', fill='y')
        self.icons = {'folder': '📁', 'file': '📄', 'web': '🌐', 'unknown': '❓'}

    @service_endpoint(inputs={'data': 'Dict'}, outputs={}, description="Populates the tree view with a nested dictionary structure (Standard 'Node' format).", tags=['ui', 'update'], side_effects=['ui:update'])
    # ROLE: Populates the tree view with a nested dictionary structure (Standard 'Node' format).
    # INPUTS: {"data": "Dict"}
    # OUTPUTS: {}
    def load_data(self, data: Dict[str, Any]):
        """
        Ingests a dictionary tree (like from _ScoutMS or _TreeMapperMS).
        """
        for item in self.tree.get_children():
            self.tree.delete(item)
        self._build_node('', data)

    def _build_node(self, parent_id, node_data):
        ntype = node_data.get('type', 'unknown')
        icon = self.icons.get(ntype, self.icons['unknown'])
        text = f"{icon} {node_data.get('name', '???')}"
        item_id = self.tree.insert(parent_id, 'end', text=text, open=True)
        for child in node_data.get('children', []):
            self._build_node(item_id, child)
if __name__ == '__main__':
    root = tk.Tk()
    explorer = TkinterSmartExplorerMS({'parent': root})
    explorer.pack(fill='both', expand=True)
    dummy_data = {'name': 'Project Root', 'type': 'folder', 'children': [{'name': 'src', 'type': 'folder', 'children': []}, {'name': 'README.md', 'type': 'file'}]}
    explorer.load_data(dummy_data)
    root.mainloop()


--------------------------------------------------------------------------------
FILE: src\microservices\_TkinterThemeManagerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterThemeManagerMS
ENTRY_POINT: _TkinterThemeManagerMS.py
INTERNAL_DEPENDENCIES: microservice_std_lib
EXTERNAL_DEPENDENCIES: None

This module defines and manages the colour palette used throughout the
application.  A `TkinterThemeManagerMS` class encapsulates two
predefined themes—`Dark` and `Light`—inspired by the Visual Studio
Code default themes.  Themes can be swapped at runtime and are
returned as mutable dictionaries so that UI components can reference
their values directly and respond to changes.

To add a new theme, extend the `THEMES` dictionary with the
appropriate keys.  See the definitions of `DARK_THEME` and
`LIGHT_THEME` for guidance on required keys.
"""

from typing import Dict, Any, Optional
from .microservice_std_lib import service_metadata, service_endpoint


"""
Defines colour palettes for supported themes.

These palettes draw inspiration from the Visual Studio Code default
dark and light themes to provide a comfortable and familiar
development environment.  Colours are carefully chosen to avoid
high‑contrast combinations that can lead to eye strain while still
maintaining adequate contrast for accessibility.  Should additional
themes be added in the future, follow the same structure and include
keys for all UI elements consumed throughout the UI.
"""

# Visual Studio Code inspired dark theme.  The underlying palette
# uses neutral greys and blue accents similar to VS Code’s default
# dark theme.  Colours have been adjusted to be less harsh while
# maintaining sufficient contrast.
DARK_THEME: Dict[str, Any] = {
    'name': 'Dark',
    # Primary backgrounds and foregrounds
    'background': '#1e1e1e',        # main window background
    'foreground': '#d4d4d4',        # primary text colour
    'panel_bg': '#252526',          # toolbar, config panels
    'border': '#3c3c3c',            # borders and separators
    'accent': '#007acc',            # accent colour for buttons and highlights
    'error': '#f44747',             # error messages / destructive actions
    'success': '#89d185',           # success messages

    # Fonts (kept here for completeness but rarely overridden)
    'font_main': ('Segoe UI', 10),
    'font_mono': ('Consolas', 11),

    # Button styling
    'button_bg': '#0e639c',         # primary button background
    'button_fg': '#ffffff',         # primary button text colour

    # Input/entry styling
    'entry_bg': '#1e1e1e',          # entry and text box background
    'entry_fg': '#d4d4d4',          # entry text colour

    # Selection colours
    'select_bg': '#264f78',         # selection background (lists/text)
    'select_fg': '#ffffff',         # selection text colour

    # Table/heading styling
    'heading_bg': '#3c3c3c',        # table headings background
    'heading_fg': '#ffffff',        # table headings text colour
    'heading_font': ('Segoe UI', 12, 'bold'),
}

# Visual Studio Code inspired light theme.  The palette uses soft
# greys with a blue accent, mirroring VS Code’s light theme while
# avoiding stark white backgrounds.  Text colours are dark greys
# to maintain readability without excessive contrast.
LIGHT_THEME: Dict[str, Any] = {
    'name': 'Light',
    # Primary backgrounds and foregrounds
    'background': '#ffffff',        # main window background (pure white)
    'foreground': '#333333',        # primary text colour (dark grey)
    'panel_bg': '#f3f3f3',          # toolbar, config panels
    'border': '#dcdcdc',            # borders and separators
    'accent': '#0066b8',            # accent colour for buttons and highlights
    'error': '#d13438',             # error messages / destructive actions
    'success': '#107c10',           # success messages

    # Fonts
    'font_main': ('Segoe UI', 10),
    'font_mono': ('Consolas', 11),

    # Button styling
    'button_bg': '#e7e7e7',         # primary button background
    'button_fg': '#333333',         # primary button text colour

    # Input/entry styling
    'entry_bg': '#ffffff',          # entry and text box background
    'entry_fg': '#333333',          # entry text colour

    # Selection colours
    'select_bg': '#add6ff',         # selection background (lists/text)
    'select_fg': '#000000',         # selection text colour

    # Table/heading styling
    'heading_bg': '#e2e2e2',        # table headings background
    'heading_fg': '#333333',        # table headings text colour
    'heading_font': ('Segoe UI', 12, 'bold'),
}

# Registry of all supported themes.  Keys should be title‑cased to
# simplify lookups when user preferences are normalised by
# `TkinterThemeManagerMS.set_theme()`.
THEMES: Dict[str, Dict[str, Any]] = {
    'Dark': DARK_THEME,
    'Light': LIGHT_THEME,
}


@service_metadata(
    name='TkinterThemeManager',
    version='1.2.0',
    description='Centralised configuration for UI colours and fonts.',
    tags=['ui', 'config', 'theme'],
    capabilities=['ui:style'],
    internal_dependencies=['microservice_std_lib'],
    external_dependencies=[],
)
class TkinterThemeManagerMS:
    """
    The Stylist: Holds the colour palette and font settings.

    All UI components query this service to decide how to draw themselves.
    The palette is returned as a mutable dictionary so that callers can
    hold a reference and receive updates in place when switching themes.

    Configuration options:
      - ``theme``: one of the keys defined in ``THEMES`` (default ``'Dark'``)
      - ``overrides``: a dictionary of key/value pairs used to override
        default theme values.  Overrides persist across theme changes.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}

        # Determine the requested base theme, falling back to Dark if unknown.
        requested = self.config.get('theme', 'Dark')
        requested = (requested or 'Dark').strip().title()
        if requested not in THEMES:
            requested = 'Dark'

        # Active theme name and palette.  ``self.theme`` is mutable and
        # updated in place on theme changes to preserve object identity.
        self.theme_name: str = requested
        # copy() to ensure modifications on ``self.theme`` do not affect
        # the global template stored in ``THEMES``.
        self.theme: Dict[str, Any] = THEMES[self.theme_name].copy()

        # Persist any user overrides.  If overrides are supplied they
        # should override defaults on initialisation and on subsequent
        # theme changes.  A shallow copy is sufficient because values
        # should be primitives or tuples.
        self._overrides = dict(self.config.get('overrides', {}))
        if self._overrides:
            self.theme.update(self._overrides)

    @service_endpoint(
        inputs={},
        outputs={'theme': 'Dict'},
        description='Returns the current active theme dictionary.',
        tags=['ui', 'read'],
    )
    # ROLE: Returns the current active theme dictionary.
    # INPUTS: {}
    # OUTPUTS: {"theme": "Dict"}
    def get_theme(self) -> Dict[str, Any]:
        """Return the current theme palette."""
        return self.theme

    @service_endpoint(
        inputs={},
        outputs={'theme_name': 'str'},
        description='Returns the current active theme name.',
        tags=['ui', 'read'],
    )
    # ROLE: Returns the current active theme name.
    # INPUTS: {}
    # OUTPUTS: {"theme_name": "str"}
    def get_theme_name(self) -> str:
        """Return the name of the current theme (e.g. ``'Dark'``)."""
        return self.theme_name

    @service_endpoint(
        inputs={'theme_name': 'str'},
        outputs={'applied': 'bool'},
        description='Switches the active theme (Dark/Light).',
        tags=['ui', 'write'],
        side_effects=['ui:refresh'],
    )
    # ROLE: Switches the active theme (Dark/Light).
    # INPUTS: {"theme_name": "str"}
    # OUTPUTS: {"applied": "bool"}
    def set_theme(self, theme_name: str) -> bool:
        """
        Switch the current theme to ``theme_name``.

        Unknown theme names fall back to ``'Dark'``.  Overrides stored
        during initialisation are re‑applied after the base palette is
        swapped so that user customisations persist across theme changes.
        The method always returns ``True``.
        """
        name = (theme_name or 'Dark').strip().title()
        if name not in THEMES:
            name = 'Dark'
        self.theme_name = name

        # Build a fresh palette from the base and reapply overrides.  The
        # resulting palette is merged into the existing ``self.theme``
        # dictionary to preserve its identity for any UI components holding
        # references.  This ensures calls like ``refresh_theme()`` only need
        # to reconfigure widget properties rather than replace entire dicts.
        new_theme: Dict[str, Any] = THEMES[self.theme_name].copy()
        if self._overrides:
            new_theme.update(self._overrides)

        # Update the existing dict in place rather than reassigning.
        self.theme.clear()
        self.theme.update(new_theme)
        return True

    @service_endpoint(
        inputs={'key': 'str', 'value': 'Any'},
        outputs={},
        description='Updates a specific theme attribute (e.g., changing accent colour).',
        tags=['ui', 'write'],
        side_effects=['ui:refresh'],
    )
    # ROLE: Updates a specific theme attribute (e.g., changing accent colour).
    # INPUTS: {"key": "str", "value": "Any"}
    # OUTPUTS: {}
    def update_key(self, key: str, value: Any) -> None:
        """
        Update an individual key in the current theme.

        Overrides are persisted so that subsequent calls to
        ``set_theme()`` do not wipe out the change.  A missing key will
        simply be added to the palette.
        """
        # Update the current palette
        self.theme[key] = value
        # Persist override for future theme switches
        self._overrides[key] = value


if __name__ == '__main__':  # pragma: no cover
    # Simple manual test: print palette names and accents
    svc = TkinterThemeManagerMS({'theme': 'Dark'})
    print('Initial:', svc.get_theme_name(), svc.get_theme()['accent'])
    svc.set_theme('Light')
    print('After switch:', svc.get_theme_name(), svc.get_theme()['accent'])
--------------------------------------------------------------------------------
FILE: src\microservices\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: tools\check_ms_inits.py
--------------------------------------------------------------------------------
from __future__ import annotations

import sys
from pathlib import Path
import importlib
import inspect
import pkgutil


def main() -> None:
    repo_root = Path(__file__).resolve().parents[1]
    if str(repo_root) not in sys.path:
        sys.path.insert(0, str(repo_root))

    ms_dir = repo_root / "src" / "microservices"
    if not ms_dir.exists():
        raise SystemExit(f"Cannot find microservices folder at: {ms_dir}")

    pkg_name = "src.microservices"

    print(f"Scanning: {ms_dir}")
    print("")

    for m in pkgutil.iter_modules([str(ms_dir)]):
        mod_name = f"{pkg_name}.{m.name}"
        try:
            mod = importlib.import_module(mod_name)
        except Exception as e:
            print(f"[IMPORT FAIL] {mod_name}: {e}")
            continue

        for name, obj in vars(mod).items():
            if isinstance(obj, type) and name.endswith("MS"):
                try:
                    sig = inspect.signature(obj.__init__)
                except Exception:
                    continue

                params = list(sig.parameters.values())[1:]  # skip self
                required = [
                    p for p in params
                    if p.default is inspect._empty
                    and p.kind in (p.POSITIONAL_OR_KEYWORD, p.KEYWORD_ONLY)
                ]
                if required:
                    print(f"{name}.__init__ requires: {[p.name for p in required]}  ({mod_name})")


if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------
FILE: tools\fix.py
--------------------------------------------------------------------------------
from __future__ import annotations

import ast
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Tuple, List


@dataclass
class RuleSet:
    package_dir: str = "src"
    # Modules that should be treated as internal-to-src when imported without prefix.
    internal_top_level: Tuple[str, ...] = (
        "backend",
        "ui",
        "microservices",
    )


def is_under(path: Path, parent: Path) -> bool:
    try:
        path.resolve().relative_to(parent.resolve())
        return True
    except Exception:
        return False


def rewrite_imports_in_file(py_path: Path, rules: RuleSet) -> bool:
    """
    Returns True if file changed.
    Only rewrites when file is under src/ and import matches internal_top_level.
    """
    text = py_path.read_text(encoding="utf-8")
    try:
        tree = ast.parse(text)
    except SyntaxError:
        return False

    changed = False
    lines = text.splitlines(keepends=True)

    # Collect node spans to rewrite via naive line slicing (stable enough for ImportFrom).
    # We only rewrite 'from X import Y' style for now.
    edits: List[Tuple[int, int, str]] = []  # (lineno0, endlineno0, replacement)

    for node in ast.walk(tree):
        if isinstance(node, ast.ImportFrom):
            # Skip already-relative imports
            if node.level and node.level > 0:
                continue

            if not node.module:
                continue

            mod = node.module

            # If it starts with internal module roots, rewrite to relative.
            # Examples:
            #   from backend import X         -> from .backend import X
            #   from ui import build_ui       -> from .ui import build_ui
            #   from microservices.X import Y -> from .microservices.X import Y
            root = mod.split(".")[0]
            if root not in rules.internal_top_level:
                continue

            # Rewrite by adding leading dot
            new_mod = "." + mod

            # Reconstruct the exact import statement (preserve imported names + aliases)
            names = []
            for a in node.names:
                if a.asname:
                    names.append(f"{a.name} as {a.asname}")
                else:
                    names.append(a.name)
            names_str = ", ".join(names)

            # Preserve "from __future__" (should not match our roots anyway)
            replacement = f"from {new_mod} import {names_str}\n"

            # Node lineno/end_lineno are 1-based
            if hasattr(node, "lineno") and hasattr(node, "end_lineno"):
                start = node.lineno - 1
                end = node.end_lineno - 1
                # Only rewrite single-line imports (keeps it safe)
                if start == end:
                    original_line = lines[start]
                    # Keep indentation of original line
                    indent = original_line[: len(original_line) - len(original_line.lstrip(" \t"))]
                    edits.append((start, start, indent + replacement))
                    changed = True

    if not changed:
        return False

    # Apply edits from bottom to top so indices stay valid
    for start, end, repl in sorted(edits, key=lambda e: e[0], reverse=True):
        lines[start : end + 1] = [repl]

    new_text = "".join(lines)
    if new_text != text:
        py_path.write_text(new_text, encoding="utf-8")
        return True
    return False


def main() -> None:
    rules = RuleSet()
    repo_root = Path(__file__).resolve().parent
    src_dir = repo_root / rules.package_dir
    if not src_dir.exists():
        raise SystemExit(f"Cannot find {src_dir}")

    changed_files = 0
    for py_path in src_dir.rglob("*.py"):
        if rewrite_imports_in_file(py_path, rules):
            changed_files += 1
            print(f"rewrote: {py_path}")

    print(f"\nDone. Files changed: {changed_files}")


if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------
FILE: tools\__init__.py
--------------------------------------------------------------------------------
