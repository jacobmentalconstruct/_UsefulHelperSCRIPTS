Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS


--------------------------------------------------------------------------------
FILE: .gitignore
--------------------------------------------------------------------------------
# ===========================
# üêç Python Essentials
# ===========================
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# ===========================
#  virtualenv (Local Environments)
# ===========================
# Typical names for virtual environments
venv/
.venv/
env/
ENV/
env.bak/
venv.bak/

# ===========================
# üõ°Ô∏è Secrets & Security (NEVER COMMIT THESE)
# ===========================
.env
.env.local
.env.development.local
.env.test.local
.env.production.local
secrets.json
api_keys.txt
token.json

# ===========================
# üíæ Databases & Local Data
# ===========================
# You usually don't want to commit your actual RAG databases
*.sqlite
*.sqlite3
*.db
# Unless strictly necessary for the app structure
!distributable_data.db

# ===========================
# ü§ñ AI / ML Artifacts
# ===========================
# Large model weights (too big for standard Git)
*.ckpt
*.safetensors
*.pt
*.pth
*.gguf
*.bin
*.h5

# Local model folders (if you download Ollama models locally)
models/
model_cache/

# ===========================
# üñºÔ∏è Generated Media
# ===========================
# Don't commit thousands of test images from _FirstHOME
generated_images/
output_images/
temp_generations/

# ===========================
# üîß IDE & OS Settings
# ===========================
# Windows
Thumbs.db
ehthumbs.db
Desktop.ini

# Mac
.DS_Store
.AppleDouble
.LSOverride

# VS Code
.vscode/
*.code-workspace

# ===========================
# üöß WIP / GHOST APPS
# ===========================
# Add the folder names of apps you are working on but 
# aren't ready to push to the suite yet.

____mini-scripts-collection-bin____
____toy-chest-of-experiments____
____WIP____
--------------------------------------------------------------------------------
FILE: LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: MENU.bat
--------------------------------------------------------------------------------
@echo off
:: Launches the Menu App without opening a lingering command prompt window
start "" "pyw" "_UsefulHelperScriptsMENU\src\app.pyw"
--------------------------------------------------------------------------------
FILE: _GitPUSHER\.gitignore
--------------------------------------------------------------------------------
# -----------------------------------------------------------------------------
# Python Caches & Bytecode
# -----------------------------------------------------------------------------
__pycache__/
*.py[cod]
*.pyo
*.pyd

# -----------------------------------------------------------------------------
# Virtual Environments
# -----------------------------------------------------------------------------
venv/
env/
.venv/
ENV/
env.bak/

# -----------------------------------------------------------------------------
# Build / Distribution Artifacts
# -----------------------------------------------------------------------------
build/
dist/
*.egg-info/
*.egg
.eggs/
pip-wheel-metadata/

# -----------------------------------------------------------------------------
# IDE / Editor Files
# -----------------------------------------------------------------------------
.vscode/
.vscode-test/
.idea/
*.code-workspace

# -----------------------------------------------------------------------------
# OS-Level Junk
# -----------------------------------------------------------------------------
Thumbs.db
Desktop.ini
.DS_Store
*.swp
*.tmp
*.bak

# -----------------------------------------------------------------------------
# Logs / Runtime Output
# -----------------------------------------------------------------------------
*.log
logs/
*.out

# -----------------------------------------------------------------------------
# Tkinter & App Temp Files
# -----------------------------------------------------------------------------
__appcache__/
*.db-journal

# -----------------------------------------------------------------------------
# Python-specific Dev Tools
# -----------------------------------------------------------------------------
pytest_cache/
.coverage
.tox/
.mypy_cache/
.pytest_cache/
.cache/

# -----------------------------------------------------------------------------
# Project-Specific Exclusions
# -----------------------------------------------------------------------------
# Prevent accidental inclusion of personal configurations or secrets.
local_settings.json
config.local.json
secrets.json

# -----------------------------------------------------------------------------
# Ignore the user‚Äôs entire Windows PATH junk accidentally copied in
# -----------------------------------------------------------------------------
$RECYCLE.BIN/

# -----------------------------------------------------------------------------
# If this helper script sits inside UsefulHelperScripts,
# ensure nothing outside this subfolder accidentally gets tracked.
# (Comment out i

--------------------------------------------------------------------------------
FILE: _GitPUSHER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _GitPUSHER\README.md
--------------------------------------------------------------------------------
Based on the code provided, here is a professional, comprehensive, and clean `README.md` file. It highlights the dual nature of the tool (GUI + CLI), its zero-dependency architecture, and its safety features.

-----

# Git Commit & Push Helper

A lightweight, zero-dependency Python tool designed to streamline the `git add .` $\to$ `git commit` $\to$ `git push` workflow. It features a modern Dark Mode GUI for desktop use and a fully functional CLI for automation scripts.

## üöÄ Features

  * **Hybrid Interface:** Run it as a GUI application or a Command Line utility.
  * **Workflow Automation:** Performs `add`, `commit`, and `push` in a single action.
  * **Safety First:**
      * Validates that the target folder is a Git repository.
      * **Stop-gap Logic:** Warns you (or blocks operation) if a `.gitignore` file is missing, preventing the accidental commit of virtual environments or build artifacts.
  * **Zero Dependencies:** Built entirely with the Python Standard Library (`tkinter`, `subprocess`, etc.). No `pip install` required.
  * **Dark Mode UI:** A custom-styled Tkinter interface designed for low-eye-strain environments.
  * **Recursion Detection:** Intelligently detects if you are using the tool to commit changes to the tool's own repository.

## üìã Prerequisites

  * **Python 3.x**
  * **Git** (Must be installed and accessible via system PATH)

## üõ†Ô∏è Installation

1.  Clone this repository or download `app.py`.
2.  That's it. There are no external requirements to install.

## üñ•Ô∏è Usage: GUI Mode

Simply run the script without arguments:

```bash
python app.py
```

### interface Controls

  * **Repository:** Defaults to the current working directory. You can type a path or use the **"‚Ä¶"** button to browse.
  * **Commit Message:** Enter your message here. Press `<Enter>` to trigger the commit.
  * **Log Window:** Displays real-time output from the Git subprocesses.

> **Note:** If you launch the app from within a git repository, it will automatically detect the root and prepopulate the path.

## ‚å®Ô∏è Usage: CLI Mode

You can use the tool in headless environments or build scripts by passing arguments.

### Basic Commit & Push

```bash
python app.py -m "Refactored the core engine"
```

### Specify a different repository

```bash
python app.py --repo "C:/Projects/MyWebsite" -m "Update CSS"
```

### Push Only (Skip commit)

```bash
python app.py --push-only
```

### Force commit without .gitignore

By default, the CLI will fail if `.gitignore` is missing. You can override this:

```bash
python app.py -m "Initial commit" --force-without-gitignore
```

### CLI Arguments Reference

| Argument | Description |
| :--- | :--- |
| `-r`, `--repo` | Path to the target repository (Default: current dir). |
| `-m`, `--message` | The commit message (Required unless using `--push-only`). |
| `--push-only` | Skips `add` and `commit`, executes only `git push`. |
| `--force-without-gitignore` | Bypasses the safety check for missing `.gitignore` files. |

## üõ°Ô∏è Safety Mechanisms

### The `.gitignore` Check

One of the most common mistakes in rapid development is running `git add .` inside a folder containing a `venv/` or `node_modules/`.

  * **GUI:** Prompts a Human-in-the-Loop (HITL) warning dialog asking for confirmation before proceeding.
  * **CLI:** Aborts immediately unless the `--force-without-gitignore` flag is used.

### Porcelain Status

The tool utilizes `git status --porcelain` to programmatically ensure the working tree actually has changes before attempting a commit, preventing empty commit errors.

## üìÑ License

Open Source. Feel free to modify and integrate into your own workflows.
--------------------------------------------------------------------------------
FILE: _GitPUSHER\requirements.txt
--------------------------------------------------------------------------------
tk>=0.1.0
--------------------------------------------------------------------------------
FILE: _GitPUSHER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _GitPUSHER\src\app.py
--------------------------------------------------------------------------------
import sys
import os
import argparse
import shutil
import subprocess
import tkinter as tk
from tkinter import filedialog, messagebox, font
from pathlib import Path

# ==============================================================================
# 1. CORE ENGINE (Business Logic)
# ==============================================================================

def _norm_path(p: str) -> str:
    """
    Normalize paths for robust equality comparisons on Windows/macOS/Linux.
    """
    try:
        return os.path.normcase(os.path.realpath(os.path.abspath(p)))
    except Exception:
        return os.path.normcase(os.path.abspath(p))


def find_git_root(start_path: str) -> str | None:
    """
    Walk upward from start_path to find a directory containing `.git`.
    Returns the repo root path or None.
    """
    try:
        p = Path(start_path).resolve()
    except Exception:
        p = Path(start_path)

    # If a file is passed, start from its parent
    if p.is_file():
        p = p.parent

    for parent in [p] + list(p.parents):
        if (parent / ".git").is_dir():
            return str(parent)
    return None


class GitOpsEngine:
    """
    Encapsulates Git-related operations for a single repository.
    """
    def __init__(self, repo_path=None):
        self.repo_path = repo_path or os.getcwd()

    # --- Environment Checks ---------------------------------------------------

    def is_git_available(self) -> bool:
        """Return True if `git` is available on PATH."""
        return shutil.which("git") is not None

    def is_valid_repo(self) -> bool:
        """Return True if repo_path contains a .git directory."""
        if not self.repo_path or not os.path.isdir(self.repo_path):
            return False
        git_dir = os.path.join(self.repo_path, ".git")
        return os.path.isdir(git_dir)

    def has_gitignore(self) -> bool:
        """Return True if a .gitignore exists in the repo root."""
        gitignore_path = os.path.join(self.repo_path, ".gitignore")
        return os.path.exists(gitignore_path)

    # --- Low-Level Git Runner -------------------------------------------------

    def _run_git(self, args, log_callback=None):
        """
        Run a git command inside repo_path.

        args: list of arguments, e.g. ["git", "status", "--porcelain"]
        log_callback: optional function that accepts a string for UI logging.
        """
        if log_callback is None:
            def log_callback(_: str):
                return

        try:
            result = subprocess.run(
                args,
                cwd=self.repo_path,
                capture_output=True,
                text=True
            )
        except FileNotFoundError:
            log_callback("ERROR: Git executable not found.\n")
            return 1, "", "Git executable not found."

        stdout = result.stdout.strip()
        stderr = result.stderr.strip()

        if stdout:
            log_callback(stdout + "\n")
        if stderr:
            log_callback(stderr + "\n")

        return result.returncode, stdout, stderr

    # --- Status Helpers -------------------------------------------------------

    def get_status_porcelain(self, log_callback=None) -> str | None:
        """
        Return the porcelain status output (possibly empty if clean),
        or None on error.
        """
        code, out, _ = self._run_git(["git", "status", "--porcelain"], log_callback)
        if code != 0:
            return None
        return out

    # --- Core Operation -------------------------------------------------------

    def commit_and_push(
        self,
        message: str,
        allow_without_gitignore: bool = False,
        log_callback=None
    ) -> bool:
        """
        Execute: git add ., git commit -m message, git push.

        Returns True on success, False on failure.
        """
        if log_callback is None:
            log_callback = lambda s: None

        if not message.strip():
            log_callback("ERROR: Commit message is empty.\n")
            return False

        if not self.is_git_available():
            log_callback("ERROR: Git not found on PATH.\n")
            return False

        if not self.is_valid_repo():
            log_callback("ERROR: Selected folder is not a valid Git repository.\n")
            return False

        if not allow_without_gitignore and not self.has_gitignore():
            log_callback("WARNING: No .gitignore detected; operation blocked by policy.\n")
            return False

        status_out = self.get_status_porcelain(log_callback)
        if status_out is None:
            log_callback("ERROR: Unable to determine git status.\n")
            return False

        if not status_out.strip():
            log_callback("INFO: Nothing to commit (working tree clean).\n")
            return False

        log_callback("Running: git add .\n")
        code, _, _ = self._run_git(["git", "add", "."], log_callback)
        if code != 0:
            log_callback("ERROR: git add failed.\n")
            return False

        log_callback("Running: git commit\n")
        code, commit_out, commit_err = self._run_git(
            ["git", "commit", "-m", message],
            log_callback
        )
        if code != 0:
            combined = (commit_out + "\n" + commit_err).lower()
            if "nothing to commit" in combined:
                log_callback("INFO: Nothing to commit after git add.\n")
            else:
                log_callback("ERROR: git commit failed.\n")
                return False

        log_callback("Running: git push\n")
        code, _, _ = self._run_git(["git", "push"], log_callback)
        if code != 0:
            log_callback("ERROR: git push failed.\n")
            return False

        log_callback("SUCCESS: Commit & push completed.\n")
        return True

    def push_only(self, log_callback=None) -> bool:
        """Execute: git push."""
        if log_callback is None:
            log_callback = lambda s: None

        if not self.is_git_available():
            log_callback("ERROR: Git not found on PATH.\n")
            return False

        if not self.is_valid_repo():
            log_callback("ERROR: Selected folder is not a valid Git repository.\n")
            return False

        log_callback("Running: git push\n")
        code, _, _ = self._run_git(["git", "push"], log_callback)
        if code != 0:
            log_callback("ERROR: git push failed.\n")
            return False

        log_callback("SUCCESS: Push completed.\n")
        return True


# ==============================================================================
# 2. GUI LAYER (The Visual Cortex)
# ==============================================================================

class GitCommitGUI:
    """
    Small dark-themed Tk GUI for commit + push operations.
    """
    def __init__(self, root, engine: GitOpsEngine):
        self.root = root
        self.engine = engine

        self.root.title("Git Commit & Push Helper")
        self.root.geometry("600x260")
        self.root.configure(bg="#050505")
        self.root.resizable(False, False)

        # --- FONTS ---
        self.f_mono = font.Font(family="Consolas", size=10)
        self.f_ui = font.Font(family="Segoe UI", size=9)

        # --- STATE ---
        self.repo_var = tk.StringVar(value=self.engine.repo_path)
        self.msg_var = tk.StringVar(value="")

        # --- Recursion UX state ---
        self._self_repo_root = find_git_root(Path(__file__).resolve().parent)
        self._self_repo_note_shown_for = None  # normalized path or None
        self._autofill_message = "Self-test / recursion check"

        self._build_ui()

        # React to repo path edits (manual typing or folder picker)
        self.repo_var.trace_add("write", self._on_repo_change)

        # Apply initial self-repo behavior on startup (if launching in repo root)
        self._on_repo_change()

    # UI Construction ----------------------------------------------------------

    def _build_ui(self):
        self.status_var = tk.StringVar(value="Ready.")
        status_label = tk.Label(
            self.root,
            textvariable=self.status_var,
            bg="#222222",
            fg="#888888",
            bd=1,
            relief=tk.SUNKEN,
            anchor="w",
            font=self.f_ui
        )
        status_label.pack(side=tk.BOTTOM, fill=tk.X)

        container = tk.Frame(self.root, bg="#050505")
        container.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        # REPO ROW
        repo_frame = tk.Frame(container, bg="#050505")
        repo_frame.pack(fill=tk.X, pady=(0, 8))

        tk.Label(repo_frame, text="Repository:", bg="#050505", fg="#f0f0f0", font=self.f_ui).pack(side=tk.LEFT)

        tk.Entry(
            repo_frame,
            textvariable=self.repo_var,
            bg="#111111",
            fg="#f0f0f0",
            insertbackground="#f0f0f0",
            relief="flat",
            highlightthickness=1,
            highlightbackground="#444444",
            font=self.f_mono
        ).pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(8, 4))

        tk.Button(
            repo_frame,
            text="‚Ä¶",
            width=3,
            bg="#333333",
            fg="#f0f0f0",
            activebackground="#555555",
            activeforeground="#ffffff",
            relief="flat",
            command=self._browse_repo
        ).pack(side=tk.LEFT)

        # COMMIT MESSAGE ROW
        msg_frame = tk.Frame(container, bg="#050505")
        msg_frame.pack(fill=tk.X, pady=(0, 8))

        tk.Label(msg_frame, text="Commit message:", bg="#050505", fg="#f0f0f0", font=self.f_ui).pack(side=tk.LEFT)

        entry_msg = tk.Entry(
            msg_frame,
            textvariable=self.msg_var,
            bg="#111111",
            fg="#f0f0f0",
            insertbackground="#f0f0f0",
            relief="flat",
            highlightthickness=1,
            highlightbackground="#444444",
            font=self.f_mono
        )
        entry_msg.pack(side=tk.LEFT, fill=tk.X, expand=True, padx=(8, 0))
        entry_msg.bind("<Return>", self._on_commit_push)

        # BUTTON ROW
        btn_frame = tk.Frame(container, bg="#050505")
        btn_frame.pack(fill=tk.X, pady=(0, 8))

        self.btn_commit = tk.Button(
            btn_frame,
            text="Commit & Push",
            bg="#333333",
            fg="#f0f0f0",
            activebackground="#555555",
            activeforeground="#ffffff",
            relief="flat",
            command=self._on_commit_push
        )
        self.btn_commit.pack(side=tk.LEFT)

        tk.Button(
            btn_frame,
            text="Close",
            bg="#333333",
            fg="#f0f0f0",
            activebackground="#555555",
            activeforeground="#ffffff",
            relief="flat",
            command=self.root.destroy
        ).pack(side=tk.RIGHT)

        # LOG AREA
        log_frame = tk.Frame(container, bg="#050505")
        log_frame.pack(fill=tk.BOTH, expand=True)

        tk.Label(log_frame, text="Log:", bg="#050505", fg="#f0f0f0", font=self.f_ui).pack(anchor="w")

        self.txt_log = tk.Text(
            log_frame,
            height=6,
            bg="#101010",
            fg="#f0f0f0",
            insertbackground="#f0f0f0",
            relief="flat",
            highlightthickness=1,
            highlightbackground="#444444",
            wrap="word",
            font=self.f_mono
        )
        self.txt_log.pack(fill=tk.BOTH, expand=True)

        self._log("Git Commit & Push Helper ready.\n")

    # Helpers ------------------------------------------------------------------

    def _browse_repo(self):
        initial = self.repo_var.get() or os.getcwd()
        folder = filedialog.askdirectory(initialdir=initial)
        if folder:
            self.repo_var.set(folder)

    def _log(self, text: str):
        self.txt_log.configure(state="normal")
        self.txt_log.insert("end", text)
        if not text.endswith("\n"):
            self.txt_log.insert("end", "\n")
        self.txt_log.see("end")
        self.txt_log.configure(state="disabled")

    def _update_status(self, text: str):
        self.status_var.set(text)
        self.root.update_idletasks()

    # Recursion UX -------------------------------------------------------------

    def _is_self_repo_selected(self) -> bool:
        """
        True if the selected repo (or its git root) equals this script's repo root.
        """
        if not self._self_repo_root:
            return False

        selected = self.repo_var.get().strip()
        if not selected:
            return False

        selected_root = find_git_root(selected)
        if not selected_root:
            return False

        return _norm_path(selected_root) == _norm_path(self._self_repo_root)

    def _on_repo_change(self, *_):
        """
        Triggered when repo_var changes. Handles:
        - Logging a note if operating on its own repo (informational only).
        - Autofilling commit message gracefully.
        """
        if self._is_self_repo_selected():
            norm_self = _norm_path(self._self_repo_root)
            if self._self_repo_note_shown_for != norm_self:
                self._log("NOTE: Self-repo detected (operating on this tool's own repository).\n")
                self._self_repo_note_shown_for = norm_self
                self._update_status("Self-repo detected.")

            # Autofill only if helpful (don't override custom messages)
            current_msg = self.msg_var.get().strip()
            if current_msg == "" or current_msg == self._autofill_message:
                self.msg_var.set(self._autofill_message)
        else:
            self._update_status("Ready.")

    # Main action --------------------------------------------------------------

    def _on_commit_push(self, event=None):
        self.txt_log.configure(state="normal")
        self.txt_log.delete("1.0", "end")
        self.txt_log.configure(state="disabled")

        repo = self.repo_var.get().strip()
        msg = self.msg_var.get().strip()
        self.engine.repo_path = repo

        def log_cb(s: str):
            self._log(s)

        if not self.engine.is_git_available():
            messagebox.showerror("Error", "Git is not available on PATH.")
            self._log("ERROR: Git not found on PATH.\n")
            self._update_status("Error: git missing.")
            return

        if not repo or not os.path.isdir(repo):
            messagebox.showerror("Error", "Repository folder does not exist.")
            self._log("ERROR: Invalid repository path.\n")
            self._update_status("Error: invalid repo path.")
            return

        if not self.engine.is_valid_repo():
            messagebox.showerror("Error", "Selected folder is not a Git repository (missing .git).")
            self._log("ERROR: No .git directory found in selected folder.\n")
            self._update_status("Error: not a git repo.")
            return

        if not msg:
            messagebox.showwarning("Missing commit message", "Please enter a commit message.")
            self._log("WARNING: Commit message is empty.\n")
            self._update_status("Awaiting commit message.")
            return

        # .gitignore HITL
        if not self.engine.has_gitignore():
            self._log("WARNING: No .gitignore detected.\n")
            proceed = messagebox.askyesno(
                "No .gitignore found",
                "No .gitignore file detected.\n\n"
                "This will add and commit ALL files, including build artifacts, virtualenvs, etc.\n\n"
                "Continue anyway?"
            )
            if not proceed:
                self._log("User aborted: no .gitignore present.\n")
                self._update_status("Aborted (no .gitignore).")
                return

        self._log(f"Using repo: {repo}\n")
        self._log(f"Commit message: {msg}\n")
        self._log("-" * 40 + "\n")

        self.btn_commit.configure(state="disabled")
        self._update_status("Running git operations...")

        try:
            status_out = self.engine.get_status_porcelain(log_cb)
            if status_out is None:
                messagebox.showerror("Error", "Failed to run 'git status'. See log for details.")
                self._update_status("Error: git status failed.")
                return

            if not status_out.strip():
                self._log("No local changes detected (working tree clean).\n")
                push_anyway = messagebox.askyesno(
                    "No changes to commit",
                    "No changes detected to commit.\n\n"
                    "Do you still want to run 'git push'?"
                )
                if not push_anyway:
                    self._log("User aborted: no changes to commit; push skipped.\n")
                    self._update_status("Aborted (nothing to commit).")
                    return

                success = self.engine.push_only(log_cb)
                if success:
                    messagebox.showinfo("Success", "Push completed successfully (no new commit).")
                    self._update_status("Push complete (no new commit).")
                else:
                    messagebox.showerror("Push failed", "git push failed. See log for details.")
                    self._update_status("Push failed.")
                return

            success = self.engine.commit_and_push(
                message=msg,
                allow_without_gitignore=True,
                log_callback=log_cb
            )

            if success:
                messagebox.showinfo("Success", "Commit & push completed successfully.")
                self._update_status("Commit & push complete.")
            else:
                messagebox.showerror("Error", "Commit and/or push failed. See log for details.")
                self._update_status("Commit/push failed.")
        finally:
            self.btn_commit.configure(state="normal")


# ==============================================================================
# 3. CLI LAYER (Utility)
# ==============================================================================

def run_cli():
    parser = argparse.ArgumentParser(description="Git Commit & Push Helper CLI")
    parser.add_argument("-r", "--repo", default=os.getcwd(), help="Path to the Git repository.")
    parser.add_argument("-m", "--message", required=True, help="Commit message.")
    parser.add_argument("--force-without-gitignore", action="store_true",
                        help="Allow commit/push even if .gitignore is missing.")
    parser.add_argument("--push-only", action="store_true", help="Skip commit and just run git push.")

    args = parser.parse_args()
    engine = GitOpsEngine(repo_path=args.repo)

    def log_cb(s: str):
        sys.stdout.write(s)
        if not s.endswith("\n"):
            sys.stdout.write("\n")
        sys.stdout.flush()

    if not engine.is_git_available():
        print("ERROR: Git is not available on PATH.", file=sys.stderr)
        sys.exit(1)

    if not engine.is_valid_repo():
        print("ERROR: Selected folder is not a Git repository (missing .git).", file=sys.stderr)
        sys.exit(1)

    if args.push_only:
        ok = engine.push_only(log_cb)
        sys.exit(0 if ok else 1)

    if not args.force_without_gitignore and not engine.has_gitignore():
        print("ERROR: No .gitignore found. Use --force-without-gitignore to override.", file=sys.stderr)
        sys.exit(1)

    ok = engine.commit_and_push(
        message=args.message,
        allow_without_gitignore=args.force_without_gitignore,
        log_callback=log_cb
    )
    sys.exit(0 if ok else 1)


# ==============================================================================
# 4. ENTRY POINT
# ==============================================================================

def run_gui():
    engine = GitOpsEngine()
    root = tk.Tk()
    GitCommitGUI(root, engine)
    root.mainloop()

def main():
    if len(sys.argv) > 1:
        run_cli()
    else:
        run_gui()

if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------
FILE: _GitPUSHER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _GraphFORGE\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _GraphFORGE\README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _GraphFORGE\requirements.txt
--------------------------------------------------------------------------------
# Standard Library dependencies only:
# tkinter, argparse, json, ast, threading, os, sys
#
# No external pip packages required.
--------------------------------------------------------------------------------
FILE: _GraphFORGE\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _GraphFORGE\src\app.py
--------------------------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
== Generic Python Module/CLI Boilerplate ==

This is a generic template for a Python file that can be:
1.  Imported as a module by other scripts (e.g., `import generic_module`).
2.  Run as a standalone command-line script (e.g., `$ python generic_module.py --input data.txt`).

How to use this template:
1.  Rename this file to match your new tool (e.g., `my_data_processor.py`).
2.  Update this docstring to describe what your tool does.
3.  Fill in the "CORE FUNCTIONALITY" section with your app's logic.
4.  Go to the `main()` function to define your CLI arguments.
5.  In `main()`, add the code to call your core functions using the parsed arguments.
"""

# 1. IMPORTS
# Standard library imports
import sys
import os
import argparse  # For parsing command-line arguments

# Third-party imports (if any)
# e.g., import requests

# Local/application imports (if any)
# e.g., from . import my_other_module


# 2. CONSTANTS
# TODO: Define any constants your application needs.
SOME_DEFAULT_SETTING = "default_value"


# 3. CORE FUNCTIONALITY (The "Importable" Module)
#
# These functions make up the "core logic" of your application.
# They can be imported and used by other Python scripts.
# They should be self-contained and not rely on command-line arguments.

def core_logic_function(data: any, setting: str = SOME_DEFAULT_SETTING) -> any:
    """
    TODO: Replace this with your main logic function.
    
    This function should perform the primary task of your module.
    
    Args:
        data (any): The input data to process.
        setting (str, optional): An example of an optional setting.
                                 Defaults to SOME_DEFAULT_SETTING.

    Returns:
        any: The processed data.
    """
    print(f"[Core Logic] Processing data with setting: {setting}")
    
    # --- TODO: Your actual logic goes here ---
    # Example:
    try:
        processed_data = f"Processed data: {str(data).upper()}"
        print("[Core Logic] Processing complete.")
        return processed_data
    except Exception as e:
        print(f"[Core Logic] Error during processing: {e}", file=sys.stderr)
        # Re-raise the exception to be handled by the caller
        raise


def helper_function(value: int) -> str:
    """
    TODO: Add any helper functions your core logic needs.
    
    This is an example of a helper that might be called by
    core_logic_function or also be importable.
    
    Args:
        value (int): An input value.

    Returns:
        str: A formatted string.
    """
    return f"Helper processed value: {value * 2}"


# 4. CLI (Command-Line Interface) LOGIC
#
# This code only runs when the script is executed directly.
# It should handle parsing arguments and calling the core functions.

def main():
    """
    Main function to run the script from the command line.
    
    It parses arguments, calls core functions, and handles CLI-specific
    input/output and error handling.
    """
    
    # --- Argument Parsing ---
    # Set up the argument parser
    # TODO: Update the description to match your tool.
    parser = argparse.ArgumentParser(
        description="A generic CLI tool. TODO: Describe your tool here.",
        epilog="Example: python generic_module.py my_input.txt -o my_output.txt -v"
    )
    
    # --- TODO: Define your arguments ---
    
    # Example of a required positional argument
    parser.add_argument(
        "input_path",  # The name of the argument
        type=str,
        help="TODO: Describe this required input (e.g., path to an input file)."
    )
    
    # Example of an optional argument (e.g., -o or --output)
    parser.add_argument(
        "-o", "--output",
        type=str,
        default=None,  # Default to None if not provided
        help="TODO: Describe this optional argument (e.g., path to an output file)."
    )
    
    # Example of a "flag" argument (stores True if present)
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",  # This makes it a boolean flag
        help="Enable verbose output."
    )
    
    # Parse the arguments from the command line (e.g., sys.argv)
    args = parser.parse_args()

    # --- Main Application Flow ---
    
    # Use the 'verbose' flag to control print statements
    if args.verbose:
        print("Verbose mode enabled.", file=sys.stderr)
        print(f"Arguments received: {args}", file=sys.stderr)

    try:
        # 1. Load data (CLI-specific task)
        #    TODO: Replace this with your actual data loading
        if args.verbose:
            print(f"Loading data from {args.input_path}...", file=sys.stderr)
        # This is just an example. You'd likely load a file here.
        input_data = f"Content of {args.input_path}" 

        # 2. Call core logic (the "importable" part)
        if args.verbose:
            print("Calling core logic...", file=sys.stderr)
        
        # Here we pass the CLI arguments to the core function
        processed_data = core_logic_function(input_data)
        
        # 3. Handle output (CLI-specific task)
        if args.output:
            # Save to a file
            if args.verbose:
                print(f"Saving processed data to {args.output}...", file=sys.stderr)
            # TODO: Add file-saving logic here
            # with open(args.output, 'w') as f:
            #     f.write(processed_data)
            print(f"Success: Output saved to {args.output}")
        else:
            # Print to standard output
            if args.verbose:
                print("Printing processed data to stdout:", file=sys.stderr)
            print(processed_data)
        
        # Exit with a success code
        sys.exit(0)

    except FileNotFoundError as e:
        print(f"\nError: Input file not found.", file=sys.stderr)
        print(f"Details: {e}", file=sys.stderr)
        sys.exit(1) # Exit with a non-zero error code
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
        sys.exit(1)


# This "magic" line is the key to the whole pattern:
#
# - If you run `python generic_module.py ...`, Python sets
#   __name__ = "__main__", and the main() function is called.
#
# - If you `import generic_module` in another script, __name__
#   is "generic_module", so this block is SKIPPED.
#
if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: _GraphFORGE\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _LineNUMBERIZER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _LineNUMBERIZER\README.md
--------------------------------------------------------------------------------
# LineNumberizer

LineNumberizer is a tool designed to make text files and codebases "Agent-Friendly" by adding stable, parseable line numbers, or generating abstract syntax trees (AST) for Python code.

It is useful for feeding code to LLMs (Large Language Models) that need to reference specific line numbers when suggesting edits.

## Features

* **Annotate**: Add line numbers with various styles (Pipe `|`, Colon `:`, Bracket `[L#]`).
* **Strip**: Safely remove line numbers added by this tool without modifying the code content.
* **AST Export**: Generate a JSON representation of Python code structure (Tree, Flat, or Semantic blocks).
* **Line Map**: Generate a JSON map of line numbers to content hashes for integrity checking.

## How to Run

### Windows
1.  Double-click `setup_env.bat`.
2.  This will create a virtual environment, set up the project, and launch the GUI.

### Manual Run
```bash
# Run the GUI
python -m src.app

# Run the CLI directly
python src/linenumberizer.py --help
--------------------------------------------------------------------------------
FILE: _LineNUMBERIZER\requirements.txt
--------------------------------------------------------------------------------
# Standard Library dependencies only:
# tkinter, argparse, json, ast, threading, os, sys
#
# No external pip packages required.
--------------------------------------------------------------------------------
FILE: _LineNUMBERIZER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo Setting up LineNumberizer Environment...
python -m venv .venv
call .venv\Scripts\activate
pip install -r requirements.txt
echo Setup Complete.
echo ----------------------------------------------------------------------
echo  Starting LineNumberizer GUI...
echo ----------------------------------------------------------------------
python -m src.app
pause
--------------------------------------------------------------------------------
FILE: _LineNUMBERIZER\src\app.py
--------------------------------------------------------------------------------
from __future__ import annotations

import os
import threading
import tkinter as tk
from tkinter import filedialog, messagebox
from tkinter import ttk

# Import the CLI from the sibling module
try:
    # Try relative import (standard when running via 'python -m src.app')
    from .linenumberizer import main as ln_main
except ImportError:
    try:
        # Fallback for direct script execution ('python app.py')
        import linenumberizer
        ln_main = linenumberizer.main
    except ImportError as e:
        raise SystemExit("Could not import linenumberizer.py. Ensure it sits next to this GUI file.\n" + str(e))

APP_TITLE = "LineNumberizer ‚Äì Helper"

STYLES = ("pipe", "colon", "bracket")
AST_MODES = ("tree", "flat", "semantic")

# ----------------------------
# Path utilities
# ----------------------------

def split_stem_ext(path: str):
    base = os.path.basename(path)
    stem, ext = os.path.splitext(base)
    return stem, ext


def default_output_for(input_path: str, op: str, style: str, ast_mode: str) -> str:
    """Return the suggested output path per user preference (underscore tag)."""
    if not input_path:
        return ""
    folder = os.path.dirname(os.path.abspath(input_path))
    stem, ext = split_stem_ext(input_path)
    if op == "annotate":
        # Example: foo.py -> foo._lineNUMBERED.pipe.py
        out_name = f"{stem}._lineNUMBERED.{style}{ext}"
    elif op == "strip":
        out_name = f"{stem}._stripped{ext}"
    elif op == "map":
        out_name = f"{stem}._linemap.json"
    elif op == "ast":
        out_name = f"{stem}._ast.{ast_mode}.json"
    else:
        out_name = f"{stem}.out"
    return os.path.join(folder, out_name)


# ----------------------------
# Worker
# ----------------------------

def run_cli_async(argv, on_done):
    def _worker():
        try:
            rc = ln_main(argv)
        except Exception as e:
            on_done(False, f"error: {e}")
            return
        ok = (rc == 0)
        on_done(ok, f"Completed with exit code {rc}")
    t = threading.Thread(target=_worker, daemon=True)
    t.start()


# ----------------------------
# GUI
# ----------------------------

class App(ttk.Frame):
    def __init__(self, master: tk.Tk):
        super().__init__(master, padding=12)
        self.master = master
        self.grid(sticky="nsew")

        # Vars
        self.var_file = tk.StringVar()
        self.var_out = tk.StringVar()
        self.var_op = tk.StringVar(value="annotate")
        self.var_style = tk.StringVar(value=STYLES[0])
        self.var_start = tk.IntVar(value=1)
        self.var_width = tk.IntVar(value=0)
        self.var_ast_mode = tk.StringVar(value=AST_MODES[0])

        # Layout config
        master.title(APP_TITLE)
        master.minsize(600, 320)
        master.columnconfigure(0, weight=1)
        master.rowconfigure(0, weight=1)

        self.columnconfigure(1, weight=1)
        self.rowconfigure(12, weight=1)

        # File input
        ttk.Label(self, text="Input file").grid(row=0, column=0, sticky="w")
        row0 = ttk.Frame(self)
        row0.grid(row=0, column=1, sticky="ew", pady=(0, 6))
        row0.columnconfigure(0, weight=1)
        ttk.Entry(row0, textvariable=self.var_file).grid(row=0, column=0, sticky="ew")
        ttk.Button(row0, text="Browse‚Ä¶", command=self.pick_file).grid(row=0, column=1, padx=(6,0))

        # Operation
        ttk.Label(self, text="Operation").grid(row=1, column=0, sticky="w")
        row1 = ttk.Frame(self)
        row1.grid(row=1, column=1, sticky="w", pady=(0, 6))
        for i, (val, text) in enumerate((
            ("annotate", "Annotate"),
            ("strip", "Strip"),
            ("map", "Map"),
            ("ast", "AST (Python)"),
        )):
            rb = ttk.Radiobutton(row1, value=val, text=text, variable=self.var_op, command=self._update_out_suggestion)
            rb.grid(row=0, column=i, padx=(0,12))

        # Annotate options
        self.annot_frame = ttk.LabelFrame(self, text="Annotate options")
        self.annot_frame.grid(row=2, column=0, columnspan=2, sticky="ew", pady=(0, 6))
        for c in range(0, 6):
            self.annot_frame.columnconfigure(c, weight=0)
        self.annot_frame.columnconfigure(3, weight=1)

        ttk.Label(self.annot_frame, text="Style").grid(row=0, column=0, sticky="w")
        style_cb = ttk.Combobox(self.annot_frame, values=STYLES, textvariable=self.var_style, width=10, state="readonly")
        style_cb.grid(row=0, column=1, sticky="w", padx=(6, 18))
        style_cb.bind("<<ComboboxSelected>>", lambda e: self._update_out_suggestion())

        ttk.Label(self.annot_frame, text="Start").grid(row=0, column=2, sticky="e")
        ttk.Spinbox(self.annot_frame, from_=1, to=10_000_000, textvariable=self.var_start, width=8).grid(row=0, column=3, sticky="w", padx=(6, 18))

        ttk.Label(self.annot_frame, text="Min Width (0=auto)").grid(row=0, column=4, sticky="e")
        ttk.Spinbox(self.annot_frame, from_=0, to=12, textvariable=self.var_width, width=6).grid(row=0, column=5, sticky="w")

        # AST options
        self.ast_frame = ttk.LabelFrame(self, text="AST options (Python)")
        self.ast_frame.grid(row=3, column=0, columnspan=2, sticky="ew", pady=(0, 6))
        self.ast_frame.columnconfigure(1, weight=1)
        ttk.Label(self.ast_frame, text="Mode").grid(row=0, column=0, sticky="w")
        ast_cb = ttk.Combobox(self.ast_frame, values=AST_MODES, textvariable=self.var_ast_mode, width=10, state="readonly")
        ast_cb.grid(row=0, column=1, sticky="w", padx=(6, 18))
        ast_cb.bind("<<ComboboxSelected>>", lambda e: self._update_out_suggestion())

        # Output
        ttk.Label(self, text="Output path").grid(row=4, column=0, sticky="w")
        row3 = ttk.Frame(self)
        row3.grid(row=4, column=1, sticky="ew", pady=(0, 6))
        row3.columnconfigure(0, weight=1)
        ttk.Entry(row3, textvariable=self.var_out).grid(row=0, column=0, sticky="ew")
        ttk.Button(row3, text="Change‚Ä¶", command=self.pick_out).grid(row=0, column=1, padx=(6,0))

        # Action buttons
        bar = ttk.Frame(self)
        bar.grid(row=5, column=0, columnspan=2, sticky="ew", pady=(6, 6))
        bar.columnconfigure(0, weight=1)
        self.btn_run = ttk.Button(bar, text="Run", command=self.on_run)
        self.btn_run.grid(row=0, column=1, sticky="e")

        # Log output
        ttk.Label(self, text="Log").grid(row=6, column=0, sticky="w")
        self.log = tk.Text(self, height=8, wrap="word")
        self.log.grid(row=7, column=0, columnspan=2, sticky="nsew")
        self.log.configure(state="disabled")

        self._update_controls()

    # ------------- UI helpers -------------
    def pick_file(self):
        path = filedialog.askopenfilename(title="Choose a text file")
        if path:
            self.var_file.set(path)
            self._update_out_suggestion()

    def pick_out(self):
        base = self.var_out.get() or default_output_for(self.var_file.get(), self.var_op.get(), self.var_style.get(), self.var_ast_mode.get())
        initialdir = os.path.dirname(base) if base else None
        initialfile = os.path.basename(base) if base else None
        path = filedialog.asksaveasfilename(title="Save output as", initialdir=initialdir, initialfile=initialfile)
        if path:
            self.var_out.set(path)

    def _update_out_suggestion(self):
        self.var_out.set(default_output_for(self.var_file.get(), self.var_op.get(), self.var_style.get(), self.var_ast_mode.get()))
        self._update_controls()

    def _update_controls(self):
        # Enable annotate options only for annotate op
        annot = (self.var_op.get() == "annotate")
        for child in self.annot_frame.winfo_children():
            try:
                child.configure(state=("!disabled" if annot else "disabled"))
            except tk.TclError:
                pass
        # Enable AST options only for ast op
        ast_enabled = (self.var_op.get() == "ast")
        for child in self.ast_frame.winfo_children():
            try:
                child.configure(state=("!disabled" if ast_enabled else "disabled"))
            except tk.TclError:
                pass

    def _append_log(self, msg: str):
        self.log.configure(state="normal")
        self.log.insert("end", msg.rstrip()+"\n")
        self.log.see("end")
        self.log.configure(state="disabled")

    def on_run(self):
        infile = self.var_file.get().strip()
        if not infile:
            messagebox.showerror(APP_TITLE, "Please choose an input file.")
            return
        if not os.path.isfile(infile):
            messagebox.showerror(APP_TITLE, "Input path does not exist or is not a file.")
            return

        op = self.var_op.get()
        out = self.var_out.get().strip() or default_output_for(infile, op, self.var_style.get(), self.var_ast_mode.get())
        argv = [op, infile]

        if op == "annotate":
            argv += ["--out", out, "--style", self.var_style.get(), "--start", str(self.var_start.get()), "--width", str(self.var_width.get())]
        elif op == "strip":
            argv += ["--out", out]
        elif op == "map":
            argv += ["--out", out]
        elif op == "ast":
            argv += ["--out", out, "--mode", self.var_ast_mode.get()]
        else:
            messagebox.showerror(APP_TITLE, f"Unknown operation: {op}")
            return

        self.btn_run.configure(state="disabled")
        self._append_log(f"Running: linenumberizer {' '.join(argv)}")

        def done(ok: bool, msg: str):
            self.after(0, self._on_done, ok, msg)

        run_cli_async(argv, done)

    def _on_done(self, ok: bool, msg: str):
        self.btn_run.configure(state="normal")
        self._append_log(msg)
        if ok:
            messagebox.showinfo(APP_TITLE, "Done.")
        else:
            messagebox.showerror(APP_TITLE, "Failed ‚Äì see log.")


# ----------------------------
# Entrypoint
# ----------------------------

def main():
    root = tk.Tk()
    try:
        root.tk.call("tk", "scaling", 1.2)
    except tk.TclError:
        pass
    style = ttk.Style(root)
    if "clam" in style.theme_names():
        style.theme_use("clam")
    App(root)
    root.mainloop()


if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: _LineNUMBERIZER\src\linenumberizer.py
--------------------------------------------------------------------------------
#!/usr/bin/env python3
"""
LineNumberizer ‚Äî make diffs agent-friendly by annotating files with stable, parseable line numbers.

New in this version
  ‚Ä¢ AST export (Python): `linenumberizer ast FILE.py --out FILE._ast.json [--mode tree|flat]`
    - Gracefully logs and emits a stub JSON when the input is NOT a Python file.
    - On Python SyntaxError, logs the error with the exact line number and exits non‚Äëzero.

Core features
  ‚Ä¢ annotate: write a numbered copy with a consistent prefix format.
  ‚Ä¢ strip: remove previously added numbers safely.
  ‚Ä¢ map: export a JSON line-map (line ‚Üí SHA-256 of raw content) for sanity checks.

Format
  Default each line is prefixed as:  "{LN:>W}‚îÇ "  (e.g., "   42‚îÇ ") where the bar is U+2502.

(c) 2025 ‚Äî MIT License
"""
from __future__ import annotations

import argparse
import io
import json
import os
import re
import sys
import hashlib
import ast
from dataclasses import dataclass
from typing import Iterable, Tuple, Any, Dict

# ----------------------------
# Core formatting primitives
# ----------------------------

@dataclass(frozen=True)
class PrefixStyle:
    name: str
    pattern: re.Pattern
    def make(self, n: int, width: int) -> str:
        raise NotImplementedError

class PipeStyle(PrefixStyle):
    def make(self, n: int, width: int) -> str:
        return f"{n:>{width}}‚îÇ "  # U+2502

class ColonStyle(PrefixStyle):
    def make(self, n: int, width: int) -> str:
        return f"{n:>{width}}: "

class BracketStyle(PrefixStyle):
    def make(self, n: int, width: int) -> str:
        return f"[L{n:0{width}d}] "

# Regexes that recognize our own prefixes ONLY (conservative stripping)
PIPE_RE   = re.compile(r"^(?P<prefix>\s*\d+\u2502\s)")
COLON_RE  = re.compile(r"^(?P<prefix>\s*\d+:\s)")
BRACK_RE  = re.compile(r"^(?P<prefix>\s*\[L\d+\]\s)")

STYLES = {
    "pipe": PipeStyle("pipe", PIPE_RE),
    "colon": ColonStyle("colon", COLON_RE),
    "bracket": BracketStyle("bracket", BRACK_RE),
}

# ----------------------------
# I/O helpers
# ----------------------------

def open_text_maybe(path: str) -> io.TextIOBase:
    if path == "-":
        return io.TextIOWrapper(sys.stdin.buffer, encoding="utf-8", newline="")
    return open(path, "r", encoding="utf-8", newline="")

def create_text_maybe(path: str) -> io.TextIOBase:
    if path == "-":
        return io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", newline="")
    os.makedirs(os.path.dirname(os.path.abspath(path)) or ".", exist_ok=True)
    return open(path, "w", encoding="utf-8", newline="")

# ----------------------------
# Core operations
# ----------------------------

def detect_total_lines(path: str) -> int:
    total = 0
    with open_text_maybe(path) as fh:
        for _ in fh:
            total += 1
    return total

def annotate_lines(lines: Iterable[str], start: int, width: int, style: PrefixStyle) -> Iterable[str]:
    n = start
    for line in lines:
        yield f"{style.make(n, width)}{line}"
        n += 1

def strip_lines(lines: Iterable[str]) -> Iterable[str]:
    for line in lines:
        m = PIPE_RE.match(line) or COLON_RE.match(line) or BRACK_RE.match(line)
        if m:
            yield line[m.end("prefix"):]
        else:
            yield line

def line_hash(s: str) -> str:
    # Hash raw line content (no newline normalization; keep bytes as is)
    return hashlib.sha256(s.encode("utf-8")).hexdigest()

def build_map(lines: Iterable[str]) -> Tuple[int, list]:
    total = 0
    entries = []
    for ln, line in enumerate(lines, start=1):
        total += 1
        entries.append({"n": ln, "hash": line_hash(line)})
    return total, entries

# ----------------------------
# Python AST export
# ----------------------------

AST_SAFE_FIELDS = (
    "lineno", "col_offset", "end_lineno", "end_col_offset",
    "name", "id", "arg", "attr",
)

def _node_span(d: Dict[str, Any]) -> Dict[str, Any]:
    # ... (this function remains the same)
    out = {}
    for k in AST_SAFE_FIELDS:
        if k in d:
            out[k] = d[k]
    return out

def _ast_node_to_dict(node: ast.AST) -> Dict[str, Any]:
    # ... (this function remains the same)
    d: Dict[str, Any] = {"type": type(node).__name__}
    for k, v in ast.iter_fields(node):
        if isinstance(v, ast.AST):
            continue
        if isinstance(v, list) and v and all(isinstance(x, ast.AST) for x in v):
            continue
        if isinstance(v, (str, int, float, bool)) and k in AST_SAFE_FIELDS:
            d[k] = v
    d.update(_node_span(getattr(node, "__dict__", {})))

    children = []
    for child in ast.iter_child_nodes(node):
        children.append(_ast_node_to_dict(child))
    if children:
        d["children"] = children
    return d

def build_py_ast(text: str, mode: str = "tree") -> Dict[str, Any]:
    # ... (this function remains the same)
    root = ast.parse(text)
    tree = _ast_node_to_dict(root)
    if mode == "tree":
        return {"language": "python", "mode": "tree", "root": tree}

    # flat mode
    flat, stack = [], [(tree, -1)]
    while stack:
        node, parent = stack.pop()
        idx = len(flat)
        entry = {k: v for k, v in node.items() if k != "children"}
        entry["parent"] = parent
        flat.append(entry)
        for ch in reversed(node.get("children", [])):
            stack.append((ch, idx))
    return {"language": "python", "mode": "flat", "nodes": flat}


# In linenumberizer.py, replace the existing SemanticVisitor class

class SemanticVisitor(ast.NodeVisitor):
    """
    An AST visitor that builds a list of "logical blocks" from the source.
    Implements suggestions from user feedback.
    """
    def __init__(self, depth: str = 'top'):
        self.depth = depth
        self.blocks: list[Dict[str, Any]] = []

    def _get_signature(self, node: ast.FunctionDef) -> Dict[str, Any]:
        # ... (this helper function remains the same as before)
        sig = {"params": [], "returns": None}
        if node.returns:
            sig["returns"] = ast.unparse(node.returns)
        
        defaults = [None] * (len(node.args.args) - len(node.args.defaults)) + [ast.unparse(d) for d in node.args.defaults]
        for arg, default_val in zip(node.args.args, defaults):
            param = {"name": arg.arg}
            if arg.annotation:
                param["annotation"] = ast.unparse(arg.annotation)
            if default_val is not None:
                param["default"] = default_val
            sig["params"].append(param)
        return sig

    def _process_node(self, node: ast.AST, is_top_level: bool = False):
        if not hasattr(node, 'lineno'):
            return

        # Suggestion 1: Use ast.unparse for robust source capture
        source_segment = ast.unparse(node)

        block = {
            "type": type(node).__name__,
            "span": (node.lineno, node.end_lineno),
        }
        
        node_name = getattr(node, 'name', '')
        if node_name:
            block['name'] = node_name

        # Process different node types
        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
            block["decorators"] = [ast.unparse(d) for d in node.decorator_list]
            if isinstance(node, ast.FunctionDef):
                 block["signature"] = self._get_signature(node)
        elif isinstance(node, ast.Import):
            block["names"] = [alias.name for alias in node.names]
        elif isinstance(node, ast.ImportFrom):
            block["module"] = node.module or ""
            block["names"] = [alias.name for alias in node.names]
        
        # Add source and hash after all fields are gathered
        block["source"] = source_segment
        block["hash"] = line_hash(source_segment)

        # Suggestion 4: Add a stable block ID
        hash_short = block['hash'][:8]
        block['id'] = f"{block['type']}:{node_name}:{block['span'][0]}:{block['span'][1]}:{hash_short}"

        # Decide whether to add the block and whether to recurse
        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
            self.blocks.append(block)
            if self.depth == 'all':
                for child in node.body:
                    self._process_node(child, is_top_level=False)
        
        # Suggestion 3: Capture imports inside functions if depth is 'all'
        elif isinstance(node, (ast.Import, ast.ImportFrom)):
            if is_top_level or self.depth == 'all':
                self.blocks.append(block)

        elif is_top_level:
             # Capture other top-level statements like assignments
             self.blocks.append(block)

    def visit_Module(self, node: ast.Module):
        for child in node.body:
            self._process_node(child, is_top_level=True)

# You will also need to update the `build_semantic_model` function slightly
# to remove the 'source_text' argument from the visitor's constructor.

def build_semantic_model(text: str, depth: str) -> Dict[str, Any]:
    """Builds a high-level list of logical blocks (functions, classes, etc.)."""
    root = ast.parse(text)
    visitor = SemanticVisitor(depth) # <-- Correctly calls with only 'depth'
    visitor.visit(root)
    sorted_blocks = sorted(visitor.blocks, key=lambda b: b['span'][0])
    return {"blocks": sorted_blocks}
    
# ----------------------------
# Command handlers
# ----------------------------

def cmd_annotate(args: argparse.Namespace) -> int:
    style = STYLES[args.style]
    total = detect_total_lines(args.file)
    width = max(args.width or 0, len(str(args.start + total - 1)), 3)

    out_path = args.out or suggest_out_path(args.file, suffix=numbered_suffix(args.style))
    with open_text_maybe(args.file) as inp:
        processed = annotate_lines(inp, start=args.start, width=width, style=style)
        if args.dry_run:
            for chunk in processed:
                sys.stdout.write(chunk)
        else:
            if args.inplace:
                tmp_path = out_path + ".tmp"
                with create_text_maybe(tmp_path) as out:
                    for chunk in processed:
                        out.write(chunk)
                os.replace(tmp_path, args.file)
                print(f"Annotated in-place: {args.file} (style={args.style}, width={width})")
            else:
                with create_text_maybe(out_path) as out:
                    for chunk in processed:
                        out.write(chunk)
                print(f"Annotated ‚Üí {out_path} (style={args.style}, width={width})")

    if args.map:
        with open_text_maybe(out_path if not args.inplace else args.file) as fh:
            total2, entries = build_map(strip_prefix_for_map(fh))
        payload = {
            "source": os.path.abspath(args.file),
            "annotated": os.path.abspath(out_path if not args.inplace else args.file),
            "style": args.style,
            "width": width,
            "start": args.start,
            "total_lines": total2,
            "lines": entries,
        }
        with create_text_maybe(args.map) as m:
            json.dump(payload, m, indent=2)
        print(f"Map written ‚Üí {args.map}")

    return 0

def strip_prefix_for_map(lines: Iterable[str]) -> Iterable[str]:
    for line in lines:
        m = PIPE_RE.match(line) or COLON_RE.match(line) or BRACK_RE.match(line)
        yield line[m.end("prefix"): ] if m else line


def cmd_strip(args: argparse.Namespace) -> int:
    with open_text_maybe(args.file) as inp:
        processed = strip_lines(inp)
        out_path = args.out or suggest_out_path(args.file, suffix=".stripped")
        if args.dry_run:
            for chunk in processed:
                sys.stdout.write(chunk)
        else:
            if args.inplace:
                tmp = out_path + ".tmp"
                with create_text_maybe(tmp) as out:
                    for chunk in processed:
                        out.write(chunk)
                os.replace(tmp, args.file)
                print(f"Stripped in-place: {args.file}")
            else:
                with create_text_maybe(out_path) as out:
                    for chunk in processed:
                        out.write(chunk)
                print(f"Stripped ‚Üí {out_path}")
    return 0


def cmd_map(args: argparse.Namespace) -> int:
    with open_text_maybe(args.file) as fh:
        total, entries = build_map(strip_prefix_for_map(fh))
    payload = {
        "source": os.path.abspath(args.file),
        "total_lines": total,
        "lines": entries,
    }
    out_path = args.out or suggest_out_path(args.file, suffix=".linemap.json")
    with create_text_maybe(out_path) as out:
        json.dump(payload, out, indent=2)
    print(f"Map written ‚Üí {out_path}")
    return 0


def cmd_ast(args: argparse.Namespace) -> int:
    """Export a Python AST as JSON. Gracefully handles non-Python inputs."""
    try:
        with open_text_maybe(args.file) as fh:
            src = fh.read()
    except FileNotFoundError as e:
        print(f"error: {e}", file=sys.stderr)
        return 2

    out_path = args.out or suggest_out_path(args.file, suffix=f"._ast.{args.mode}.json")
    ext = os.path.splitext(args.file)[1].lower()

    if ext not in {".py", ""}:
        # Graceful no-op with stub payload for non-Python files
        stub = {
            "language": "unknown",
            "note": f"AST export currently supports Python (.py). Skipping '{args.file}'.",
            "supported": False,
        }
        with create_text_maybe(out_path) as out:
            json.dump(stub, out, indent=2)
        print(f"note: Non-Python file detected; wrote stub AST ‚Üí {out_path}")
        return 0

    try:
        if args.mode in ("tree", "flat"):
            payload = build_py_ast(src, mode=args.mode)
        elif args.mode == "semantic":
            # Suggestion 4: Emit file-level metadata
            payload = {
                "language": "python",
                "mode": "semantic",
                "file_metadata": {
                    "path": os.path.abspath(args.file),
                    "total_lines": src.count('\n') + 1,
                    "last_modified": os.path.getmtime(args.file),
                }
            }
            semantic_data = build_semantic_model(src, depth=args.depth)
            payload.update(semantic_data)
        else:
            raise ValueError(f"Unknown AST mode: {args.mode}")

        with create_text_maybe(out_path) as out:
            json.dump(payload, out, indent=2)
        print(f"AST written ‚Üí {out_path}")
        return 0
    except SyntaxError as e:
        line = getattr(e, 'lineno', '?')
        print(f"error: Python syntax error at line {line}: {e}", file=sys.stderr)
        return 3

# ----------------------------
# Utilities
# ----------------------------

def numbered_suffix(style: str) -> str:
    return f".numbered.{style}"

def suggest_out_path(src: str, suffix: str) -> str:
    base = os.path.abspath(src)
    return base + suffix

# ----------------------------
# CLI
# ----------------------------

def build_parser() -> argparse.ArgumentParser:
    p = argparse.ArgumentParser(
        prog="linenumberizer",
        description="Annotate files with parseable line numbers; export maps and Python AST.",
    )
    sub = p.add_subparsers(dest="cmd", required=True)

    # annotate
    a = sub.add_parser("annotate", help="Write a numbered copy of FILE")
    a.add_argument("file", help="Path to input file or '-' for stdin")
    a.add_argument("--out", "-o", help="Output path (default: FILE.numbered.<style> or stdout if --dry-run)")
    a.add_argument("--style", choices=STYLES.keys(), default="pipe", help="Prefix style")
    a.add_argument("--start", type=int, default=1, help="Starting line number (default: 1)")
    a.add_argument("--width", type=int, default=0, help="Minimum number width (auto if 0)")
    a.add_argument("--map", help="Also write a JSON line map to this path")
    a.add_argument("--dry-run", action="store_true", help="Write to stdout instead of a file")
    a.add_argument("--inplace", action="store_true", help="Replace FILE in-place (writes to temp and moves over)")
    a.set_defaults(func=cmd_annotate)

    # strip
    s = sub.add_parser("strip", help="Remove previously added line number prefixes")
    s.add_argument("file", help="Path to input file or '-' for stdin")
    s.add_argument("--out", "-o", help="Output path (default: FILE.stripped)")
    s.add_argument("--dry-run", action="store_true", help="Write to stdout instead of a file")
    s.add_argument("--inplace", action="store_true", help="Replace FILE in-place (writes to temp and moves over)")
    s.set_defaults(func=cmd_strip)

    # map
    m = sub.add_parser("map", help="Emit a JSON line‚Üíhash map for the (raw) content")
    m.add_argument("file", help="Path to input file or '-' for stdin")
    m.add_argument("--out", "-o", help="Output path (default: FILE.linemap.json)")
    m.set_defaults(func=cmd_map)

    # ast (Python)
    astd = sub.add_parser("ast", help="Export a Python AST (JSON)")
    astd.add_argument("file", help="Path to input file (Python .py recommended)")
    astd.add_argument("--out", "-o", help="Output JSON path (default: FILE._ast.json)")
    astd.add_argument("--mode", choices=("tree", "flat", "semantic"), default="tree", help="Tree(nested), flat list, or semantic blocks")
    astd.add_argument("--depth", choices=("top", "all"), default="top", help="For semantic mode: 'top' level blocks only, or 'all' nested blocks.")
    astd.set_defaults(func=cmd_ast)
    

    return p


def main(argv: list[str] | None = None) -> int:
    parser = build_parser()
    args = parser.parse_args(argv)
    try:
        return args.func(args)
    except BrokenPipeError:
        try:
            sys.stdout.close()
        except Exception:
            pass
        return 0
    except FileNotFoundError as e:
        print(f"error: {e}", file=sys.stderr)
        return 2
    except Exception as e:
        print(f"error: {e}", file=sys.stderr)
        return 1


if __name__ == "__main__":
    raise SystemExit(main())

--------------------------------------------------------------------------------
FILE: _LineNUMBERIZER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\requirements.txt
--------------------------------------------------------------------------------
# Standard Library dependencies only:
# tkinter, argparse, json, ast, threading, os, sys
#
# No external pip packages required.
--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\src\app.py
--------------------------------------------------------------------------------
import sys
import os
import tkinter as tk
from tkinter import ttk, messagebox, filedialog
from pathlib import Path
import json

# --- 1. PATH SETUP ---
# Ensure we can import from the root 'src' folder downwards
current_dir = os.path.dirname(os.path.abspath(__file__)) # src/
root_dir = os.path.dirname(current_dir)                  # ProjectRoot/
sys.path.append(root_dir)

# --- 2. MICROSERVICE IMPORTS ---
try:
    from src.microservices._TkinterAppShellMS import TkinterAppShellMS
    from src.microservices._TkinterThemeManagerMS import TkinterThemeManagerMS
    from src.microservices._ServiceRegistryMS import ServiceRegistryMS
    from src.microservices._ContextPackerMS import ContextPackerMS
except ImportError as e:
    # Fallback print if imports fail, helpful for debugging
    print(f"CRITICAL IMPORT ERROR: {e}")
    print(f"sys.path: {sys.path}")
    sys.exit(1)

# Configuration
# (Update this path if your library is stored elsewhere)
DEFAULT_LIBRARY_PATH = r"C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_MicroserviceLIBRARY"

class MicroserviceBrowserApp:
    def __init__(self):
        # 1. Initialize Managers
        self.theme_mgr = TkinterThemeManagerMS()
        self.registry_svc = ServiceRegistryMS()
        self.packer_svc = ContextPackerMS()
        
        # 2. State
        self.library_root = Path(DEFAULT_LIBRARY_PATH)
        self.services_map = {} # { "ServiceName": {metadata} }
        self.cart = set() # Set of selected Service Names for export
        self.show_line_numbers = tk.BooleanVar(value=True)

        # 3. Setup Shell
        self.app = TkinterAppShellMS({
            "theme_manager": self.theme_mgr,
            "title": "Microservice Context Composer",
            "geometry": "1400x900"
        })
        
        self.build_ui()
        self.refresh_library()

    def build_ui(self):
        container = self.app.get_main_container()
        
        # --- Top Toolbar ---
        toolbar = ttk.Frame(container, padding=5)
        toolbar.pack(fill="x")
        
        ttk.Label(toolbar, text="Library Path:").pack(side="left")
        self.lbl_path = ttk.Label(toolbar, text=str(self.library_root), foreground="cyan")
        self.lbl_path.pack(side="left", padx=10)
        ttk.Button(toolbar, text="Change...", command=self.change_library).pack(side="left")
        ttk.Button(toolbar, text="Refresh", command=self.refresh_library).pack(side="left", padx=5)
        
        # --- Main Layout (Paned) ---
        paned = tk.PanedWindow(container, orient="horizontal", bg="#2b2b2b", sashwidth=4)
        paned.pack(fill="both", expand=True, pady=5)
        
        # LEFT: Service List
        frame_left = ttk.Frame(paned)
        paned.add(frame_left, width=300)
        
        ttk.Label(frame_left, text="Available Services", font=("Segoe UI", 10, "bold")).pack(anchor="w", pady=5)
        self.tree = ttk.Treeview(frame_left, columns=("type"), show="tree", selectmode="extended")
        self.tree.pack(fill="both", expand=True)
        self.tree.bind("<<TreeviewSelect>>", self.on_service_select)
        
        # CENTER: Detail View (Code & Metadata)
        frame_center = ttk.Frame(paned)
        paned.add(frame_center, width=600)
        
        # Tabs for Verbatim vs Info
        self.notebook = ttk.Notebook(frame_center)
        self.notebook.pack(fill="both", expand=True)
        
        # Tab 1: Code View
        tab_code = ttk.Frame(self.notebook)
        self.notebook.add(tab_code, text="Source Code")
        
        # Controls for Code View
        code_controls = ttk.Frame(tab_code)
        code_controls.pack(fill="x", pady=2)
        ttk.Checkbutton(code_controls, text="Show Line Numbers", variable=self.show_line_numbers, command=self.refresh_code_view).pack(side="left")
        ttk.Button(code_controls, text="Copy Content", command=self.copy_current_code).pack(side="right")
        
        self.txt_code = tk.Text(tab_code, font=("Consolas", 10), bg="#1e1e1e", fg="#d4d4d4", wrap="none")
        self.txt_code.pack(fill="both", expand=True)

        # Tab 2: Metadata
        tab_meta = ttk.Frame(self.notebook)
        self.notebook.add(tab_meta, text="Metadata / Signatures")
        self.txt_meta = tk.Text(tab_meta, font=("Consolas", 10), bg="#1e1e1e", fg="#569cd6", wrap="word")
        self.txt_meta.pack(fill="both", expand=True)

        # RIGHT: The "Context Cart" (Export)
        frame_right = ttk.Frame(paned)
        paned.add(frame_right, width=300)
        
        ttk.Label(frame_right, text="Context Cart", font=("Segoe UI", 10, "bold")).pack(anchor="w", pady=5)
        
        self.lst_cart = tk.Listbox(frame_right, bg="#252526", fg="white", height=20)
        self.lst_cart.pack(fill="x", pady=5)
        
        btn_box = ttk.Frame(frame_right)
        btn_box.pack(fill="x")
        ttk.Button(btn_box, text="Add Selected", command=self.add_to_cart).pack(fill="x", pady=2)
        ttk.Button(btn_box, text="Clear Cart", command=self.clear_cart).pack(fill="x", pady=2)
        
        ttk.Separator(frame_right, orient="horizontal").pack(fill="x", pady=10)
        
        ttk.Label(frame_right, text="Export Options").pack(anchor="w")
        ttk.Button(frame_right, text="Copy Merged to Clipboard", command=self.export_clipboard).pack(fill="x", pady=2)
        ttk.Button(frame_right, text="Save to .txt File", command=self.export_file).pack(fill="x", pady=2)

    # --- Logic ---

    def change_library(self):
        path = filedialog.askdirectory(title="Select Microservice Library Folder")
        if path:
            self.library_root = Path(path)
            self.lbl_path.config(text=str(self.library_root))
            self.refresh_library()

    def refresh_library(self):
        # 1. Update Registry Service root
        self.registry_svc.root = self.library_root
        
        # 2. Scan
        self.tree.delete(*self.tree.get_children())
        self.services_map.clear()
        
        # Using the Registry MS to scan the folder dynamically
        # It returns a list of dictionaries
        registry_data = self.registry_svc.scan(save_to="registry.json") 
        
        for item in registry_data:
            name = item['name']
            self.services_map[name] = item
            # Insert into tree
            self.tree.insert("", "end", iid=name, text=name, values=("Microservice"))

    def on_service_select(self, event):
        selected = self.tree.selection()
        if not selected: return
        
        service_name = selected[0]
        data = self.services_map.get(service_name)
        if not data: return
        
        # 1. Show Metadata
        self.txt_meta.delete("1.0", "end")
        self.txt_meta.insert("1.0", json.dumps(data, indent=2))
        
        # 2. Load File Content
        file_path = self.library_root / data['path']
        if file_path.exists():
            try:
                content = file_path.read_text(encoding="utf-8")
                self.current_code_content = content
                self.refresh_code_view()
            except Exception as e:
                self.txt_code.delete("1.0", "end")
                self.txt_code.insert("1.0", f"Error reading file: {e}")

    def refresh_code_view(self):
        if not hasattr(self, 'current_code_content'): return
        
        self.txt_code.delete("1.0", "end")
        content = self.current_code_content
        
        if self.show_line_numbers.get():
            lines = content.splitlines()
            numbered = []
            for idx, line in enumerate(lines):
                # Format: 001 | code
                numbered.append(f"{idx+1:03d} | {line}")
            self.txt_code.insert("1.0", "\n".join(numbered))
        else:
            self.txt_code.insert("1.0", content)

    def copy_current_code(self):
        content = self.txt_code.get("1.0", "end")
        self.app.root.clipboard_clear()
        self.app.root.clipboard_append(content)
        messagebox.showinfo("Copied", "Source code copied to clipboard.")

    # --- Cart & Export Logic ---

    def add_to_cart(self):
        selected = self.tree.selection()
        for s in selected:
            if s not in self.cart:
                self.cart.add(s)
                self.lst_cart.insert("end", s)

    def clear_cart(self):
        self.cart.clear()
        self.lst_cart.delete(0, "end")

    def _generate_export_text(self):
        """Compiles the cart into a single string."""
        if not self.cart: return None
        
        output = []
        output.append("CONTEXT PACKET: MICROSERVICES")
        output.append("="*40)
        
        for name in self.cart:
            data = self.services_map.get(name)
            if not data: continue
            
            path = self.library_root / data['path']
            if path.exists():
                content = path.read_text(encoding="utf-8")
                
                output.append(f"\nSTART FILE: {data['path']}")
                output.append("-" * 40)
                output.append(content)
                output.append("-" * 40)
                output.append(f"END FILE: {data['path']}\n")
                
        return "\n".join(output)

    def export_clipboard(self):
        text = self._generate_export_text()
        if text:
            self.app.root.clipboard_clear()
            self.app.root.clipboard_append(text)
            messagebox.showinfo("Export", f"Copied {len(self.cart)} services to clipboard.")
        else:
            messagebox.showwarning("Empty", "Add services to the cart first.")

    def export_file(self):
        text = self._generate_export_text()
        if not text:
            messagebox.showwarning("Empty", "Add services to the cart first.")
            return
            
        f = filedialog.asksaveasfilename(defaultextension=".txt", filetypes=[("Text Files", "*.txt")])
        if f:
            with open(f, "w", encoding="utf-8") as file:
                file.write(text)

    def run(self):
        self.app.launch()

if __name__ == "__main__":
    browser = MicroserviceBrowserApp()
    browser.run()
--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\src\microservices\microservice_std_lib.py
--------------------------------------------------------------------------------
"""
LIBRARY: Microservice Standard Lib
VERSION: 2.0.0
ROLE: Provides decorators for tagging Python classes as AI-discoverable services.
"""

import functools
import inspect
from typing import Dict, List, Any, Optional, Type

# ==============================================================================
# DECORATORS (The "Writer" Tools)
# ==============================================================================

def service_metadata(name: str, version: str, description: str, tags: List[str], capabilities: List[str] = None, dependencies: List[str] = None, side_effects: List[str] = None):
    """
    Class Decorator.
    Labels a Microservice class with high-level metadata for the Catalog.
    """
    def decorator(cls):
        cls._is_microservice = True
        cls._service_info = {
            "name": name,
            "version": version,
            "description": description,
            "tags": tags,
            "capabilities": capabilities or [],
            "dependencies": dependencies or [],
            "side_effects": side_effects or []
        }
        return cls
    return decorator

def service_endpoint(inputs: Dict[str, str], outputs: Dict[str, str], description: str, tags: List[str] = None, side_effects: List[str] = None, mode: str = "sync"):
    """
    Method Decorator.
    Defines the 'Socket' that the AI Architect can plug into.
    
    :param inputs: Dict of {arg_name: type_string} (e.g. {"query": "str"})
    :param outputs: Dict of {return_name: type_string} (e.g. {"results": "List[Dict]"})
    :param description: What this specific function does.
    :param tags: Keywords for searching (e.g. ["search", "read-only"])
    :param side_effects: List of impact types (e.g. ["network:outbound", "disk:write"])
    :param mode: 'sync', 'async', or 'ui_event'
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        
        # Attach metadata to the function object itself
        wrapper._endpoint_info = {
            "name": func.__name__,
            "inputs": inputs,
            "outputs": outputs,
            "description": description,
            "tags": tags or [],
            "side_effects": side_effects or [],
            "mode": mode
        }
        return wrapper
    return decorator

# ==============================================================================
# INTROSPECTION (The "Reader" Tools)
# ==============================================================================

def extract_service_schema(service_cls: Type) -> Dict[str, Any]:
    """
    Scans a decorated Service Class and returns a JSON-serializable schema 
    of its metadata and all its exposed endpoints.
    
    This is what the AI Agent uses to 'read' the manual.
    """
    if not getattr(service_cls, "_is_microservice", False):
        raise ValueError(f"Class {service_cls.__name__} is not decorated with @service_metadata")

    schema = {
        "meta": getattr(service_cls, "_service_info", {}),
        "endpoints": []
    }

    # Inspect all methods of the class
    for name, method in inspect.getmembers(service_cls, predicate=inspect.isfunction):
        # Unwrap decorators if necessary to find our tags
        # (Though usually the wrapper has the tag attached)
        endpoint_info = getattr(method, "_endpoint_info", None)
        
        if endpoint_info:
            schema["endpoints"].append(endpoint_info)

    return schema

--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\src\microservices\_ContextPackerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ContextPackerMS
ENTRY_POINT: _ContextPackerMS.py
DEPENDENCIES: None
"""

import os
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Set

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    'venv', '.venv', 'dist', 'build', '.DS_Store', 'file-dump.txt'
}

logger = logging.getLogger("ContextPacker")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="ContextPacker",
    version="1.0.0",
    description="Flattens a directory of source code into a single text file (useful for LLM context stuffing).",
    tags=["filesystem", "export", "utility"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ContextPackerMS:
    """
    The Packer: Walks a directory and dumps all text-readable files 
    into a single monolithic text file with delimiters.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"root_path": "str", "output_filename": "str", "additional_excludes": "Set[str]"},
        outputs={"output_path": "str", "file_count": "int"},
        description="Packs directory contents into a single text file.",
        tags=["export", "dump"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def pack_directory(self, 
                       root_path: str, 
                       output_filename: str = "context_dump.txt", 
                       additional_excludes: Optional[Set[str]] = None) -> Dict[str, Any]:
        """
        Walks the directory and writes file contents to the output file.
        """
        root = Path(root_path).resolve()
        output_file = root / output_filename
        
        # Merge excludes
        excludes = DEFAULT_EXCLUDES.copy()
        if additional_excludes:
            excludes.update(additional_excludes)
            
        # Ensure we don't pack the output file itself if it already exists
        excludes.add(output_filename)

        count = 0
        logger.info(f"Packing context from {root} into {output_filename}...")

        try:
            with open(output_file, 'w', encoding='utf-8') as out_f:
                # Add Header
                out_f.write(f"CONTEXT PACKER DUMP\n")
                out_f.write(f"SOURCE: {root}\n")
                out_f.write("="*60 + "\n\n")

                for current_dir, dirs, files in os.walk(root):
                    # In-place modification of dirs to skip excluded folders during walk
                    dirs[:] = [d for d in dirs if d not in excludes and not d.startswith('.')]
                    
                    for file in files:
                        if file in excludes or file.startswith('.'):
                            continue
                            
                        file_path = Path(current_dir) / file
                        
                        # Process the file
                        self._append_file(file_path, root, out_f)
                        count += 1
                        
            return {
                "output_path": str(output_file),
                "file_count": count
            }
            
        except Exception as e:
            logger.error(f"Packing failed: {e}")
            raise

    def _append_file(self, file_path: Path, root: Path, out_f):
        """Helper to append a single file's content to the dump."""
        rel_path = file_path.relative_to(root)
        
        try:
            # Try reading as text
            content = file_path.read_text(encoding='utf-8')
            
            out_f.write(f"==================================================\n")
            out_f.write(f"FILE: {rel_path}\n")
            out_f.write(f"==================================================\n")
            out_f.write(content + "\n\n")
            
        except UnicodeDecodeError:
            # It's a binary file (image, pyc, etc.)
            out_f.write(f"==================================================\n")
            out_f.write(f"FILE: {rel_path} [SKIPPED - BINARY]\n")
            out_f.write(f"==================================================\n\n")
        except Exception as e:
            logger.warning(f"Could not read {rel_path}: {e}")


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    packer = ContextPackerMS()
    print("Service ready:", packer)
    
    # Run a test pack on the current folder
    print("\n--- Packing Current Directory ---")
    result = packer.pack_directory(".", "test_dump.txt")
    print(f"Packed {result['file_count']} files to: {result['output_path']}")

--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\src\microservices\_ServiceRegistryMS.py
--------------------------------------------------------------------------------
import ast
import json
import uuid
import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logger = logging.getLogger("ServiceRegistry")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="ServiceRegistry",
    version="1.0.0",
    description="Scans a library of Python microservices and generates standardized JSON Service Tokens.",
    tags=["introspection", "registry", "parsing"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ServiceRegistryMS:
    """
    The Tokenizer (v2): Scans a library of Python microservices and generates
    standardized JSON 'Service Tokens'.
    Feature: Hybrid AST/Regex parsing for maximum robustness.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Default to current directory if not specified
        self.root = Path(self.config.get("root_path", ".")).resolve()
        self.registry = []

    @service_endpoint(
        inputs={"save_to": "str"},
        outputs={"registry": "List[Dict]"},
        description="Scans the file system for microservices and builds a registry.",
        tags=["introspection", "scan"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict[str, Any]]:
        logger.info(f"Scanning for microservices in: {self.root}")
        self.registry = [] # Reset registry
        
        # 1. Walk directories/files
        if self.root.exists():
            for item in self.root.iterdir():
                # Check for Service Folders (e.g. _AuthMS)
                if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                    self._process_folder(item)
                # Check for Service Files (e.g. __AuthMS.py)
                elif item.is_file() and item.name.startswith("_") and item.name.endswith("MS.py"):
                    token = self._tokenize_file(item)
                    if token:
                        self.registry.append(token)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            logger.info(f"‚úÖ Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            logger.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_folder(self, folder: Path):
        # Find the main .py file (usually matches folder name, or is the only .py file)
        candidates = list(folder.glob("*.py"))
        for file in candidates:
            # Usually entry points start with __ inside the folder
            if file.name.startswith("__") or len(candidates) == 1:
                token = self._tokenize_file(file)
                if token:
                    self.registry.append(token)
                    logger.info(f"  + Tokenized: {token['name']}")
                    break 

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source = f.read()
            
            # Attempt 1: Strict AST Parsing (The "Right" Way)
            try:
                return self._ast_parse(source, file_path)
            except Exception:
                # Attempt 2: Regex Fallback (The "Survival" Way)
                return self._regex_parse(source, file_path)
                
        except Exception as e:
            logger.warning(f"  - Failed to read {file_path.name}: {e}")
            return None

    def _ast_parse(self, source: str, file_path: Path):
        tree = ast.parse(source)
        target_class = None
        
        # Find class ending in 'MS'
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name.endswith("MS"):
                target_class = node
                break
        
        if not target_class: return None

        # Extract Metadata
        return self._build_token(
            name=target_class.name,
            doc=ast.get_docstring(target_class) or "",
            methods=[
                (n.name, [a.arg for a in n.args.args if a.arg != 'self'], ast.get_docstring(n) or "")
                for n in target_class.body if isinstance(n, ast.FunctionDef) and not n.name.startswith("_")
            ],
            deps=self._extract_ast_imports(tree),
            file_path=file_path
        )

    def _regex_parse(self, source: str, file_path: Path):
        # Find class definition
        class_match = re.search(r'class\s+(\w+MS)', source)
        if not class_match: return None
        name = class_match.group(1)
        
        # Find methods (def name(args):)
        methods = []
        for match in re.finditer(r'def\s+(\w+)\s*\((.*?)\):', source):
            m_name = match.group(1)
            if not m_name.startswith("_"):
                # Rough args parsing
                args = [a.strip().split(':')[0] for a in match.group(2).split(',') if a.strip() != 'self']
                methods.append((m_name, args, "Regex extracted"))
                
        return self._build_token(name, "Parsed via Regex", methods, [], file_path)

    def _build_token(self, name, doc, methods, deps, file_path):
        # Generate deterministic ID
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, name).hex[:8].upper()}"
        
        method_dict = {
            m_name: {"args": m_args, "doc": m_doc.strip()} 
            for m_name, m_args, m_doc in methods
        }
        
        try:
            rel_path = str(file_path.relative_to(self.root)).replace('\\', '/')
        except ValueError:
            rel_path = file_path.name

        return {
            "token_id": token_id,
            "name": name,
            "path": rel_path,
            "description": doc.strip(),
            "methods": method_dict,
            "dependencies": sorted(deps)
        }

    def _extract_ast_imports(self, tree):
        deps = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: deps.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: deps.add(node.module.split('.')[0])
        return list(deps)


if __name__ == "__main__":
    # Setup logging for independent test
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    
    svc = ServiceRegistryMS()
    print("Service ready:", svc)
    # Perform a test scan of the current directory
    svc.scan()
--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\src\microservices\_TkinterAppShellMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterAppShellMS
ENTRY_POINT: _TkinterAppShellMS.py
DEPENDENCIES: None
"""
import tkinter as tk
from tkinter import ttk
import logging
from typing import Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# Updated Import: Single Underscore + 'Tkinter' prefix
try:
    from _TkinterThemeManagerMS import TkinterThemeManagerMS
except ImportError:
    TkinterThemeManagerMS = None

logger = logging.getLogger("AppShell")

@service_metadata(
    name="TkinterAppShell",
    version="2.0.0",
    description="The Application Container. Manages the root window, main loop, and global layout.",
    tags=["ui", "core", "lifecycle"],
    capabilities=["ui:root", "ui:gui"]
)
class TkinterAppShellMS:
    """
    The Mother Ship.
    Owns the Tkinter Root. All other UI microservices dock into this.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.root = tk.Tk()
        self.root.withdraw() # Hide until launch
        
        # Load Theme (Inject dependency or create new)
        self.theme_svc = self.config.get("theme_manager")
        if not self.theme_svc and TkinterThemeManagerMS:
            self.theme_svc = TkinterThemeManagerMS()
            
        self.colors = self.theme_svc.get_theme() if self.theme_svc else {}
        self._configure_root()
        
    def _configure_root(self):
        self.root.title(self.config.get("title", "Microservice OS"))
        self.root.geometry(self.config.get("geometry", "1200x800"))
        
        # Apply Base Theme
        bg = self.colors.get('background', '#1e1e1e')
        self.root.configure(bg=bg)
        
        # Configure TTK Styles globally
        style = ttk.Style()
        style.theme_use('clam')
        
        # Standard Frames
        style.configure('TFrame', background=bg)
        style.configure('TLabel', background=bg, foreground=self.colors.get('foreground', '#ccc'))
        style.configure('TButton', background=self.colors.get('panel_bg', '#333'), foreground='white')
        
        # Main Container (Grid or Pack)
        self.main_container = tk.Frame(self.root, bg=bg)
        self.main_container.pack(fill="both", expand=True, padx=5, pady=5)

    @service_endpoint(
        inputs={},
        outputs={},
        description="Starts the GUI Main Loop.",
        tags=["lifecycle", "start"],
        mode="sync",
        side_effects=["ui:block"]
    )
    def launch(self):
        """Ignition sequence start."""
        self.root.deiconify()
        logger.info("AppShell Launched.")
        self.root.mainloop()

    @service_endpoint(
        inputs={},
        outputs={"container": "tk.Frame"},
        description="Returns the main content area for other services to dock into.",
        tags=["ui", "layout"]
    )
    def get_main_container(self):
        """Other services call this to know where to .pack() themselves."""
        return self.main_container

    @service_endpoint(
        inputs={},
        outputs={},
        description="Gracefully shuts down the application.",
        tags=["lifecycle", "stop"],
        side_effects=["ui:close"]
    )
    def shutdown(self):
        self.root.quit()

if __name__ == "__main__":
    shell = TkinterAppShellMS({"title": "Test Shell"})
    shell.launch()
--------------------------------------------------------------------------------
FILE: _MicroserviceBROWSER\src\microservices\_TkinterThemeManagerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterThemeManagerMS
ENTRY_POINT: _TkinterThemeManagerMS.py
DEPENDENCIES: None
"""
from typing import Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# Default "Dark Modern" Theme
DEFAULT_THEME = {
    'background': '#1e1e1e',
    'foreground': '#d4d4d4',
    'panel_bg':   '#252526',
    'border':     '#3c3c3c',
    'accent':     '#007acc',
    'error':      '#f48771',
    'success':    '#89d185',
    'font_main':  ('Segoe UI', 10),
    'font_mono':  ('Consolas', 10)
}

@service_metadata(
    name="TkinterThemeManager",
    version="1.0.0",
    description="Centralized configuration for UI colors and fonts.",
    tags=["ui", "config", "theme"],
    capabilities=["ui:style"]
)
class TkinterThemeManagerMS:
    """
    The Stylist: Holds the color palette and font settings.
    All UI components query this service to decide how to draw themselves.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.theme = DEFAULT_THEME.copy()
        
        # Allow override from config
        if "overrides" in self.config:
            self.theme.update(self.config["overrides"])

    @service_endpoint(
        inputs={},
        outputs={"theme": "Dict"},
        description="Returns the current active theme dictionary.",
        tags=["ui", "read"]
    )
    def get_theme(self) -> Dict[str, Any]:
        return self.theme

    @service_endpoint(
        inputs={"key": "str", "value": "Any"},
        outputs={},
        description="Updates a specific theme attribute (e.g., changing accent color).",
        tags=["ui", "write"],
        side_effects=["ui:refresh"]
    )
    def update_key(self, key: str, value: Any):
        self.theme[key] = value

if __name__ == "__main__":
    svc = TkinterThemeManagerMS()
    print("Theme Ready:", svc.get_theme()['accent'])
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\base_service.py
--------------------------------------------------------------------------------
import logging
from typing import Dict, Any

class BaseService:
    """
    Standard parent class for all microservices. 
    Provides consistent logging and identity management.
    """
    def __init__(self, name: str):
        self._service_info = {
            "name": name, 
            "id": name.lower().replace(" ", "_")
        }
        
        # Setup standard logging
        logging.basicConfig(
            level=logging.INFO, 
            format='%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(name)

    def log_info(self, message: str):
        self.logger.info(message)

    def log_error(self, message: str):
        self.logger.error(message)

    def log_warning(self, message: str):
        self.logger.warning(message)

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\boiler_plate.py
--------------------------------------------------------------------------------
from microservice_std_lib import service_metadata, service_endpoint
from typing import Dict, Any, Optional

@service_metadata(
    name="YourServiceName",
    version="1.0.0",
    description="Briefly describe the 'Purpose' here.",
    tags=["category", "utility"],
    capabilities=["filesystem:read"] # Optional: what it actually touches
)
class YourServiceMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Initialize your core logic/engines here

    @service_endpoint(
        inputs={"param1": "str", "param2": "int"},
        outputs={"result": "str"},
        description="Detailed description of what this specific method does.",
        tags=["action"],
        side_effects=["filesystem:write"] # Be explicit for the AI safety
    )
    def perform_action(self, param1: str, param2: int = 10) -> Dict[str, Any]:
        # Your translated logic goes here
        return {"result": f"Processed {param1}"}

if __name__ == "__main__":
    # Standard independent test block for the catalogue
    svc = YourServiceMS()
    print("Service ready:", svc)
    # Add a print test of your logic here
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\document_utils.py
--------------------------------------------------------------------------------
from _ContentExtractorMS import ContentExtractorMS

# Singleton instance to reuse the extractor logic
_extractor = ContentExtractorMS()

def extract_text_from_pdf(blob: bytes) -> str:
    """Proxy to ContentExtractorMS PDF logic."""
    return _extractor._extract_pdf(blob)

def extract_text_from_html(html_text: str) -> str:
    """Proxy to ContentExtractorMS HTML logic."""
    return _extractor._extract_html(html_text)

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\LIBRARY_CATALOGUE.md
--------------------------------------------------------------------------------
# üìö Microservice Library Card Catalogue
> **Generated**: 2025-12-23 08:50
> **Total Services**: 53
> **Swarm Configuration**: `4` Workers (`qwen2.5-coder:1.5b-cpu`), 1 Architect (`qwen2.5-coder:3b-cpu`)

## üß† System Architecture Overview
System analysis failed.

## üìá Index
- **[ArchiveBotMS](#archivebotms)**: 
- **[AuthMS](#authms)**: ROLE: Simple authentication microservice providing username/password login
- **[CartridgeServiceMS](#cartridgeservicems)**: The Source of Truth.
- **[ChalkBoardMS](#chalkboardms)**: 
- **[ChunkingRouterMS](#chunkingrouterms)**: The Editor: A 'Recursive' text splitter.
- **[CodeChunkerMS](#codechunkerms)**: The Surgeon (Pure Python Edition): Splits code into semantic blocks
- **[CodeFormatterMS](#codeformatterms)**: The Architect.
- **[CodeGrapherMS](#codegrapherms)**: The Cartographer of Logic: Parses Python code to extract high-level 
- **[CodeJanitorMS](#codejanitorms)**: 
- **[CognitiveMemoryMS](#cognitivememoryms)**: The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
- **[ContentExtractorMS](#contentextractorms)**: The Decoder.
- **[ContextPackerMS](#contextpackerms)**: The Packer: Walks a directory and dumps all text-readable files 
- **[EnvironmentManagerMS](#environmentmanagerms)**: The Operator.
- **[ExplorerWidgetMS](#explorerwidgetms)**: A standalone file system tree viewer.
- **[HeuristicSumMS](#heuristicsumms)**: The Skimmer: Generates quick summaries of code/text files without AI.
- **[IngestEngineMS](#ingestenginems)**: The Heavy Lifter: Reads files, chunks text, fetches embeddings,
- **[IntakeServiceMS](#intakeservicems)**: The Vacuum. 
- **[IsoProcessMS](#isoprocessms)**: The Safety Valve: Spawns isolated processes with real-time logging feedback.
- **[LexicalSearchMS](#lexicalsearchms)**: The Librarian's Index: A lightweight, AI-free search engine.
- **[LibrarianMS](#librarianms)**: The Swarm Librarian.
- **[LibrarianMS](#librarianms)**: 
- **[LogViewMS](#logviewms)**: The Console: A professional log viewer widget.
- **[MonacoHostMS](#monacohostms)**: Hosts the Monaco Editor.
- **[NetworkLayoutMS](#networklayoutms)**: The Topologist: Calculates visual coordinates for graph nodes using
- **[NeuralGraphEngineMS](#neuralgraphenginems)**: ‚ú® This Python class implements a neural graph rendering engine using Pygame, pro
- **[NeuralGraphViewerMS](#neuralgraphviewerms)**: ‚ú® This Python class is a Tkinter-based UI component that hosts the neural graph 
- **[NeuralServiceMS](#neuralservicems)**: The Brain Interface: Orchestrates local AI operations via Ollama for inference a
- **[ProjectForgeMS](#projectforgems)**: The Blacksmith.
- **[PromptOptimizerMS](#promptoptimizerms)**: The Tuner: Uses an LLM to refine prompts or generate variations.
- **[PromptVaultMS](#promptvaultms)**: The Vault: A persistent SQLite store for managing, versioning, 
- **[PythonChunkerMS](#pythonchunkerms)**: Specialized Python AST Chunker.
- **[RefineryServiceMS](#refineryservicems)**: The Night Shift.
- **[RegexWeaverMS](#regexweaverms)**: The Weaver: A fault-tolerant dependency extractor.
- **[RoleManagerMS](#rolemanagerms)**: The Casting Director: Manages Agent Personas (Roles).
- **[SandboxManagerMS](#sandboxmanagerms)**: The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
- **[ScannerMS](#scannerms)**: The Scanner: Walks the file system, filters junk, and detects binary files.
- **[ScoutMS](#scoutms)**: The Scanner: Walks file systems OR crawls websites (Depth-Aware).
- **[SearchEngineMS](#searchenginems)**: The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
- **[SemanticChunkerMS](#semanticchunkerms)**: Intelligent Code Splitter.
- **[ServiceRegistryMS](#serviceregistryms)**: The Tokenizer (v2): Scans a library of Python microservices and generates
- **[SpinnerThingyMaBobberMS](#spinnerthingymabobberms)**: The Visualizer: An interactive spinner widget.
- **[SysInspectorMS](#sysinspectorms)**: The Auditor: Gathers hardware and environment statistics.
- **[TasklistVaultMS](#tasklistvaultms)**: The Taskmaster: A persistent SQLite engine for hierarchical task management.
- **[TelemetryServiceMS](#telemetryservicems)**: The Nervous System.
- **[TextChunkerMS](#textchunkerms)**: The Butcher: A unified service for splitting text into digestible chunks
- **[ThoughtStreamMS](#thoughtstreamms)**: The Neural Inspector: A UI widget for displaying a stream of AI thoughts/logs
- **[TkinterAppShellMS](#tkinterappshellms)**: The Mother Ship.
- **[TkinterSmartExplorerMS](#tkintersmartexplorerms)**: The Navigator.
- **[TkinterThemeManagerMS](#tkinterthememanagerms)**: The Stylist: Holds the color palette and font settings.
- **[TkinterUniButtonMS](#tkinterunibuttonms)**: A generic button group that can merge ANY two actions.
- **[TreeMapperMS](#treemapperms)**: The Cartographer: Generates ASCII-art style directory maps.
- **[VectorFactoryMS](#vectorfactoryms)**: The Switchboard: Returns the appropriate VectorStore implementation
- **[WebScraperMS](#webscraperms)**: The Reader: Fetches URLs and extracts the main content using Readability.

---

### ArchiveBotMS
**File**: `_ArchiveBotMS.py`
**Description**: 

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `create_backup` | `source_path, output_dir, extra_exclusions, use_default_exclusions` |  |

---

### AuthMS
**File**: `_AuthMS.py`
**Description**: ROLE: Simple authentication microservice providing username/password login
      and signed session tokens.

INPUTS:
  - config: Optional configuration dict. Recognized keys:
      - 'secret_key': Secret used to sign tokens.

OUTPUTS:
  - Exposes `login` and `validate_session` endpoints for use in pipelines.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `login` | `username, password` | Attempt to log in with the provided username and password. |
| `validate_session` | `token` | Check if a serialized token is valid and not expired. |

---

### CartridgeServiceMS
**File**: `_CartridgeServiceMS.py`
**Description**: The Source of Truth.
Manages the Unified Neural Cartridge Format (UNCF v1.0).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `get_vector_dim` | `None` | Retrieves the expected vector dimension from the manifest spec. |
| `initialize_manifest` | `None` | Populates the boot sector with strict RagFORGE Cartridge Schema (UNCF) v1.1. |
| `set_manifest` | `key, value` | Upsert metadata key. |
| `get_manifest` | `key` | Retrieve metadata key. |
| `validate_cartridge` | `None` | Quality Control: Checks if the cartridge is Agent-Safe. |
| `store_file` | `vfs_path, origin_path, content, blob, mime_type, origin_type` | The Universal Input Method.  |
| `get_pending_files` | `limit` | Fetches files waiting for the Refinery. |
| `update_status` | `file_id, status, metadata` |  |
| `ensure_directory` | `vfs_path` | Idempotent insert for VFS directories. |
| `get_status_flags` | `None` | Returns key manifest status flags in a single call. |
| `list_files` | `prefix, status, limit` | Enumerate files in the cartridge (optionally filtered by VFS prefix and/or status). |
| `get_file_record` | `vfs_path` | Fetch a single file record by VFS path. |
| `list_directories` | `prefix` | Enumerate directories in the cartridge VFS. |
| `get_directory_tree` | `root` | Builds a nested directory tree starting at `root` ("" for full tree). |
| `get_status_summary` | `None` | Counts files by status and provides a quick cartridge overview. |
| `add_node` | `node_id, node_type, label, data` |  |
| `add_edge` | `source, target, relation, weight` |  |
| `search_embeddings` | `query_vector, limit` | Performs semantic search using sqlite-vec. |

---

### ChalkBoardMS
**File**: `_ChalkBoardMS.py`
**Description**: 

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `loaded` | `None` | Called by JS when the page is ready. |
| `log_action` | `action_name` | Called by JS when user interacts. |
| `update_sign` | `text, theme` | Updates the embedded HTML via JS injection. |
| `trigger_effect` | `effect` | Triggers CSS animations like 'shake'. |

---

### ChunkingRouterMS
**File**: `_ChunkingRouterMS.py`
**Description**: The Editor: A 'Recursive' text splitter.
It respects the natural structure of text (Paragraphs -> Sentences -> Words)
rather than just hacking it apart by character count.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `chunk_file` | `text, filename, max_size, overlap` | Extension-aware router. |

---

### CodeChunkerMS
**File**: `_CodeChunkerMS.py`
**Description**: The Surgeon (Pure Python Edition): Splits code into semantic blocks
(Classes, Functions) using indentation and regex heuristics.

Advantages: Zero dependencies. Works on any machine.
Disadvantages: Slightly less precise than Tree-Sitter for messy code.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `chunk_file` | `file_path, max_chars` | Reads a file and breaks it into logical blocks based on indentation. |

---

### CodeFormatterMS
**File**: `_CodeFormatterMS.py`
**Description**: The Architect.
Uses the WhitespaceEngine to enforce strict indentation rules, 
fixing 'staircase' formatting and mixed tabs/spaces.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `normalize_code` | `content, use_tabs, spaces` | Pure logic endpoint: Takes string, returns string + patch. |
| `format_file` | `file_path, use_tabs, spaces` | Filesystem endpoint: In-place repair of a file. |

---

### CodeGrapherMS
**File**: `_CodeGrapherMS.py`
**Description**: The Cartographer of Logic: Parses Python code to extract high-level 
symbols (classes, functions) and maps their 'Call' relationships.

Output: A graph structure (Nodes + Edges) suitable for visualization 
or dependency analysis.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `scan_directory` | `root_path` | Recursively scans a directory for .py files and builds the graph. |

---

### CodeJanitorMS
**File**: `_CodeJanitorMS.py`
**Description**: 

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `enforce_standards` | `dry_run` |  |

---

### CognitiveMemoryMS
**File**: `_CognitiveMemoryMS.py`
**Description**: The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
flushing to Long-Term Memory (Vector Store).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `add_entry` | `role, content, metadata` | Adds an item to working memory and persists it. |
| `get_context` | `limit` | Returns the most recent conversation history formatted for an LLM. |
| `get_full_history` | `None` | Returns the raw list of memory objects. |
| `commit_turn` | `None` | Signal that a "Turn" (User + AI response) is complete. |

---

### ContentExtractorMS
**File**: `_ContentExtractorMS.py`
**Description**: The Decoder.
A standalone utility microservice that separates the concern of 
document parsing from ingestion logic.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `get_health` | `None` | Returns the operational status and library availability. |
| `extract_text` | `blob, mime_type` | Main routing logic for extraction.  |

---

### ContextPackerMS
**File**: `_ContextPackerMS.py`
**Description**: The Packer: Walks a directory and dumps all text-readable files 
into a single monolithic text file with delimiters.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `pack_directory` | `root_path, output_filename, additional_excludes` | Walks the directory and writes file contents to the output file. |

---

### EnvironmentManagerMS
**File**: `_EnvironmentManagerMS.py`
**Description**: The Operator.
Finds the right Python interpreter (System vs Venv) and launches processes.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `resolve_python` | `project_path, config_override` | Priority: |
| `launch_script` | `project_path, script_rel_path, env_vars` |  |

---

### ExplorerWidgetMS
**File**: `_ExplorerWidgetMS.py`
**Description**: A standalone file system tree viewer.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `refresh_tree` | `None` |  |
| `get_selected_paths` | `None` |  |
| `process_gui_queue` | `None` |  |

---

### HeuristicSumMS
**File**: `_HeuristicSumMS.py`
**Description**: The Skimmer: Generates quick summaries of code/text files without AI.
Scans for high-value lines (headers, signatures, docstrings) and concatenates them.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `summarize` | `text, filename, max_chars` | Generates a summary string from the provided text. |

---

### IngestEngineMS
**File**: `_IngestEngineMS.py`
**Description**: The Heavy Lifter: Reads files, chunks text, fetches embeddings,
populates the Graph Nodes, and weaves Graph Edges.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `abort` | `None` |  |
| `check_ollama_connection` | `None` |  |
| `get_available_models` | `None` |  |
| `process_files` | `file_paths, model_name` |  |

---

### IntakeServiceMS
**File**: `_IntakeServiceMS.py`
**Description**: The Vacuum. 
Now supports two-phase ingestion:
1. Scan -> Build Tree (with .gitignore respect)
2. Ingest -> Process selected paths

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `get_health` | `None` | Returns the operational status of the IntakeServiceMS. |
| `ingest_source` | `source_path` | Headless/CLI Entry point: Scans and Ingests in one go. |
| `scan_path` | `root_path, web_depth` | Unified Scanner Interface. |
| `ingest_selected` | `file_list, root_path` | Ingests only the specific files passed in the list. |
| `save_persistence` | `root_path, checked_map` | Saves user selections into the Cartridge Manifest (Portable). |

---

### IsoProcessMS
**File**: `_IsoProcessMS.py`
**Description**: The Safety Valve: Spawns isolated processes with real-time logging feedback.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `execute` | `payload, config` |  |

---

### LexicalSearchMS
**File**: `_LexicalSearchMS.py`
**Description**: The Librarian's Index: A lightweight, AI-free search engine.

Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
Ideal for environments where installing PyTorch/Transformers is impossible
or overkill.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `add_document` | `doc_id, text, metadata` | Adds or updates a document in the index. |
| `search` | `query, top_k` | Performs a BM25 Ranked Search. |

---

### LibrarianMS
**File**: `_LibrarianMS.py`
**Description**: The Swarm Librarian.
Spawns concurrent AI workers to scan the codebase and create a system manifest.
Optimized for Ryzen CPUs and 32GB RAM.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `generate_catalog` | `output_file` | Main entry point. Uses ThreadPoolExecutor for parallel processing. |

---

### LibrarianMS
**File**: `_LibrarianServiceMS.py`
**Description**: 

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `generate_catalog` | `output_file` |  |

---

### LogViewMS
**File**: `_LogViewMS.py`
**Description**: The Console: A professional log viewer widget.
Features:
- Thread-safe (consumes from a Queue).
- Message Consolidation ("Error occurred (x5)").
- Level Filtering (Toggle INFO/DEBUG/ERROR).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `clear` | `None` |  |
| `save` | `None` |  |

---

### MonacoHostMS
**File**: `_MonacoHostMS.py`
**Description**: Hosts the Monaco Editor.
This service spawns a GUI window and cannot be run in headless environments.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `launch` | `title, width, height, func` | Create and launch the window. |
| `set_save_callback` | `callback` | Sets the function to trigger when Ctrl+S is pressed in the editor. |
| `open_file` | `filepath, content` | Opens a file in the editor (must be called from a background thread or callback). |

---

### NetworkLayoutMS
**File**: `_NetworkLayoutMS.py`
**Description**: The Topologist: Calculates visual coordinates for graph nodes using
server-side algorithms (NetworkX). 
Useful for generating static map snapshots or pre-calculating positions 
to offload client-side rendering.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `calculate_layout` | `nodes, edges, algorithm` | Computes (x, y) coordinates for the given graph. |

---

### NeuralGraphEngineMS
**File**: `_NeuralGraphEngineMS.py`
**Description**: ‚ú® This Python class implements a neural graph rendering engine using Pygame, providing real-time visualization of complex relationships in a 2D force-directed graph. It includes functionalities for camera control, asset management, data manipulation, and physics simulation to ensure smooth rendering and interaction with the user.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `get_health` | `None` | Returns the operational status of the NeuralGraphEngineMS. |
| `resize` | `width, height` |  |
| `set_data` | `nodes, links` |  |
| `screen_to_world` | `sx, sy` |  |
| `get_node_at` | `sx, sy` |  |
| `handle_mouse_down` | `x, y` |  |
| `handle_mouse_move` | `x, y, is_dragging` |  |
| `handle_mouse_up` | `None` |  |
| `pan` | `dx, dy` |  |
| `zoom_camera` | `amount, mouse_x, mouse_y` |  |
| `highlight_nodes` | `node_ids` | Highlights specific nodes by ID. |
| `step_physics` | `None` |  |
| `get_image_bytes` | `None` |  |

---

### NeuralGraphViewerMS
**File**: `_NeuralGraphViewerMS.py`
**Description**: ‚ú® This Python class is a Tkinter-based UI component that hosts the neural graph engine and provides search/highlighting overlays. It includes features for searching, highlighting, and rendering graphs in a user-friendly interface.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `bind_services` | `cartridge, neural` |  |
| `run_search` | `event` |  |
| `load_from_db` | `db_path` | Loads graph data from SQLite. |
| `on_resize` | `event` |  |
| `on_double_click` | `event` |  |
| `on_click` | `event` |  |
| `on_release` | `event` |  |
| `on_drag` | `event` |  |
| `on_hover` | `event` |  |
| `on_zoom` | `amount` |  |
| `on_windows_scroll` | `event` |  |
| `animate` | `None` | The Heartbeat Loop. |

---

### NeuralServiceMS
**File**: `_NeuralServiceMS.py`
**Description**: The Brain Interface: Orchestrates local AI operations via Ollama for inference and embeddings.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `update_models` | `fast_model, smart_model, embed_model` | Called by the UI Settings Modal to change models on the fly. |
| `get_available_models` | `None` | Fetches list from Ollama for the UI dropdown. |
| `check_connection` | `None` | Pings Ollama to see if it's alive. |
| `get_embedding` | `text` | Generates a vector using the configured embedding model. |
| `request_inference` | `prompt, tier, format_json` | Synchronous inference request. |
| `process_parallel` | `items, worker_func` | Helper to run a function across many items using a ThreadPool. |

---

### ProjectForgeMS
**File**: `_ProjectForgeMS.py`
**Description**: The Blacksmith.
Creates directory structures, stamps out boilerplate code, and injects dependencies.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `forge_project` | `parent_path, project_name, dependencies, project_type` | Stamps out a new project folder. |

---

### PromptOptimizerMS
**File**: `_PromptOptimizerMS.py`
**Description**: The Tuner: Uses an LLM to refine prompts or generate variations.
Requires an 'inference_func' to be passed in the config, which accepts a string
and returns a string (simulating an LLM call).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `refine_prompt` | `draft_prompt, feedback` | Rewrites a prompt based on feedback. |
| `generate_variations` | `draft_prompt, num_variations, context_data` | Generates multiple versions of a prompt for testing. |

---

### PromptVaultMS
**File**: `_PromptVaultMS.py`
**Description**: The Vault: A persistent SQLite store for managing, versioning, 
and rendering AI prompts.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `create_template` | `slug, title, content, author, tags` | Creates a new prompt template with an initial version 1. |
| `add_version` | `slug, content, author` | Adds a new version to an existing template. |
| `get_template` | `slug` | Retrieves a full template with all history. |
| `render` | `slug, context` | Fetches the latest version and renders it with Jinja2. |
| `list_slugs` | `None` |  |

---

### PythonChunkerMS
**File**: `_PythonChunkerMS.py`
**Description**: Specialized Python AST Chunker.
Focuses exclusively on identifying classes and functions to preserve code logic.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `get_health` | `None` | Returns the operational status of the PythonChunkerMS. |
| `chunk` | `content` | Parses Python source into semantic CodeChunks. |

---

### RefineryServiceMS
**File**: `_RefineryServiceMS.py`
**Description**: The Night Shift.
Polls the DB for 'RAW' files and processes them into Chunks and Graph Nodes.

Graph Enrichment:
- Code: function/class nodes, resolved import edges when possible.
- Docs: section/chapter nodes for long-form text (md/txt/rst).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `get_health` | `None` | Returns the operational status of the RefineryServiceMS. |
| `process_pending` | `batch_size` | Main loop. Returns number of files processed. |

---

### RegexWeaverMS
**File**: `_RegexWeaverMS.py`
**Description**: The Weaver: A fault-tolerant dependency extractor.
Uses Regex to find imports, making it faster and more permissive
than AST parsers (works on broken code).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `extract_dependencies` | `content, language` | Scans code content for import statements. |

---

### RoleManagerMS
**File**: `_RoleManagerMS.py`
**Description**: The Casting Director: Manages Agent Personas (Roles).
Persists configuration for System Prompts, Attached KBs, and Memory Settings.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `create_role` | `name, system_prompt, description, kbs` | Creates a new Agent Persona. |
| `get_role` | `name_or_id` | Retrieves a role by Name or ID. |
| `list_roles` | `None` |  |
| `delete_role` | `name` |  |

---

### SandboxManagerMS
**File**: `_SandboxManagerMS.py`
**Description**: The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
Allows for safe experimentation, diffing, and atomic promotion of changes.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `init_sandbox` | `force` | Creates or resets the sandbox by mirroring the live project. |
| `reset_sandbox` | `None` | Discards all sandbox changes and re-syncs from live. |
| `get_diff` | `None` | Compares Sandbox vs Live. Returns added, modified, and deleted files. |
| `promote_changes` | `None` | Applies changes from Sandbox to Live. |

---

### ScannerMS
**File**: `_ScannerMS.py`
**Description**: The Scanner: Walks the file system, filters junk, and detects binary files.
Generates the tree structure used by the UI.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `is_binary` | `file_path` | Determines if a file is binary using two heuristics: |
| `scan_directory` | `root_path` | Recursively scans a directory and returns a JSON-compatible tree. |
| `flatten_tree` | `tree_node` | Helper to extract all valid file paths from a tree node  |

---

### ScoutMS
**File**: `_ScoutMS.py`
**Description**: The Scanner: Walks file systems OR crawls websites (Depth-Aware).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `is_binary` | `file_path` |  |
| `scan_directory` | `root_path, web_depth` | Main Entry Point. |
| `flatten_tree` | `tree_node` |  |

---

### SearchEngineMS
**File**: `_SearchEngineMS.py`
**Description**: The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).

Architecture:
1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `search` | `db_path, query, limit` | Main entry point. Returns a list of results sorted by relevance. |

---

### SemanticChunkerMS
**File**: `_SemanticChunkerMS.py`
**Description**: Intelligent Code Splitter.
Parses source code into logical units (Classes, Functions) 
rather than arbitrary text windows.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `chunk_file` | `content, filename` | Splits file content into chunks. |

---

### ServiceRegistryMS
**File**: `_ServiceRegistryMS.py`
**Description**: The Tokenizer (v2): Scans a library of Python microservices and generates
standardized JSON 'Service Tokens'.
Feature: Hybrid AST/Regex parsing for maximum robustness.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `scan` | `save_to` |  |

---

### SpinnerThingyMaBobberMS
**File**: `_SpinnerThingyMaBobberMS.py`
**Description**: The Visualizer: An interactive spinner widget.
Useful for "Processing..." screens or OBS overlays.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `launch` | `None` | Starts the Tkinter main event loop. |
| `handle_keypress` | `event` |  |
| `get_neon_color` | `offset` |  |
| `draw_arc` | `cx, cy, radius, width, start, extent, color` |  |
| `animate` | `None` |  |

---

### SysInspectorMS
**File**: `_SysInspectorMS.py`
**Description**: The Auditor: Gathers hardware and environment statistics.
Supports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `generate_report` | `None` | Runs the full audit and returns a formatted string report. |

---

### TasklistVaultMS
**File**: `_TasklistVaultMS.py`
**Description**: The Taskmaster: A persistent SQLite engine for hierarchical task management.
Supports infinite nesting of sub-tasks and status tracking.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `create_list` | `name` | Creates a new task list and returns its ID. |
| `get_lists` | `None` | Returns metadata for all task lists. |
| `add_task` | `list_id, content, parent_id` | Adds a task (or sub-task) to a list. |
| `update_task` | `task_id, content, status, result` | Updates a task's details. |
| `get_full_tree` | `list_id` | Fetches a list and reconstructs the full hierarchy of tasks. |
| `delete_list` | `list_id` |  |

---

### TelemetryServiceMS
**File**: `_TelemetryServiceMS.py`
**Description**: The Nervous System.
Watches the thread-safe LogQueue and updates the GUI Panels.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `get_health` | `None` | Returns the operational status of the TelemetryServiceMS. |
| `start` | `None` | Begins the GUI update loop. |
| `ping` | `None` | Allows an agent to verify the pulse of the UI loop. |

---

### TextChunkerMS
**File**: `_TextChunkerMS.py`
**Description**: The Butcher: A unified service for splitting text into digestible chunks
for RAG (Retrieval Augmented Generation).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `chunk_by_chars` | `text, chunk_size, chunk_overlap` | Standard Sliding Window. Best for prose/documentation. |
| `chunk_by_lines` | `text, max_lines, max_chars` | Line-Preserving Chunker. Best for Code. |

---

### ThoughtStreamMS
**File**: `_ThoughtStreamMS.py`
**Description**: The Neural Inspector: A UI widget for displaying a stream of AI thoughts/logs
visualized as 'bubbles' with sparklines.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `add_thought_bubble` | `filename, chunk_id, content, vector_preview, color` | Mimics the 'InspectorFrame' from your React code. |

---

### TkinterAppShellMS
**File**: `_TkinterAppShellMS.py`
**Description**: The Mother Ship.
Owns the Tkinter Root. All other UI microservices dock into this.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `launch` | `None` | Ignition sequence start. |
| `get_main_container` | `None` | Other services call this to know where to .pack() themselves. |
| `shutdown` | `None` |  |

---

### TkinterSmartExplorerMS
**File**: `_TkinterSmartExplorerMS.py`
**Description**: The Navigator.
A TreeView widget that expects standard 'Node' dictionaries (name, type, children).

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `load_data` | `data` | Ingests a dictionary tree (like from _ScoutMS or _TreeMapperMS). |

---

### TkinterThemeManagerMS
**File**: `_TkinterThemeManagerMS.py`
**Description**: The Stylist: Holds the color palette and font settings.
All UI components query this service to decide how to draw themselves.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `get_theme` | `None` |  |
| `update_key` | `key, value` |  |

---

### TkinterUniButtonMS
**File**: `_TkinterUniButtonMS.py`
**Description**: A generic button group that can merge ANY two actions.
Pass the visual/functional definitions in via the config objects.

---

### TreeMapperMS
**File**: `_TreeMapperMS.py`
**Description**: The Cartographer: Generates ASCII-art style directory maps.
Useful for creating context snapshots for LLMs.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `generate_tree` | `root_path, additional_exclusions, use_default_exclusions` |  |

---

### VectorFactoryMS
**File**: `_VectorFactoryMS.py`
**Description**: The Switchboard: Returns the appropriate VectorStore implementation
based on configuration.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `create` | `backend, config` | :param backend: 'faiss' or 'chroma' |

---

### WebScraperMS
**File**: `_WebScraperMS.py`
**Description**: The Reader: Fetches URLs and extracts the main content using Readability.
Strips ads, navbars, and boilerplate to return clean text for LLMs.

| Endpoint | Inputs | Summary |
| :--- | :--- | :--- |
| `scrape` | `url` | Synchronous wrapper for fetching and cleaning a URL. |

---

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\microservice_std_lib.py
--------------------------------------------------------------------------------
"""
LIBRARY: Microservice Standard Lib
VERSION: 2.0.0
ROLE: Provides decorators for tagging Python classes as AI-discoverable services.
"""

import functools
import inspect
from typing import Dict, List, Any, Optional, Type

# ==============================================================================
# DECORATORS (The "Writer" Tools)
# ==============================================================================

def service_metadata(name: str, version: str, description: str, tags: List[str], capabilities: List[str] = None, dependencies: List[str] = None, side_effects: List[str] = None):
    """
    Class Decorator.
    Labels a Microservice class with high-level metadata for the Catalog.
    """
    def decorator(cls):
        cls._is_microservice = True
        cls._service_info = {
            "name": name,
            "version": version,
            "description": description,
            "tags": tags,
            "capabilities": capabilities or [],
            "dependencies": dependencies or [],
            "side_effects": side_effects or []
        }
        return cls
    return decorator

def service_endpoint(inputs: Dict[str, str], outputs: Dict[str, str], description: str, tags: List[str] = None, side_effects: List[str] = None, mode: str = "sync"):
    """
    Method Decorator.
    Defines the 'Socket' that the AI Architect can plug into.
    
    :param inputs: Dict of {arg_name: type_string} (e.g. {"query": "str"})
    :param outputs: Dict of {return_name: type_string} (e.g. {"results": "List[Dict]"})
    :param description: What this specific function does.
    :param tags: Keywords for searching (e.g. ["search", "read-only"])
    :param side_effects: List of impact types (e.g. ["network:outbound", "disk:write"])
    :param mode: 'sync', 'async', or 'ui_event'
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        
        # Attach metadata to the function object itself
        wrapper._endpoint_info = {
            "name": func.__name__,
            "inputs": inputs,
            "outputs": outputs,
            "description": description,
            "tags": tags or [],
            "side_effects": side_effects or [],
            "mode": mode
        }
        return wrapper
    return decorator

# ==============================================================================
# INTROSPECTION (The "Reader" Tools)
# ==============================================================================

def extract_service_schema(service_cls: Type) -> Dict[str, Any]:
    """
    Scans a decorated Service Class and returns a JSON-serializable schema 
    of its metadata and all its exposed endpoints.
    
    This is what the AI Agent uses to 'read' the manual.
    """
    if not getattr(service_cls, "_is_microservice", False):
        raise ValueError(f"Class {service_cls.__name__} is not decorated with @service_metadata")

    schema = {
        "meta": getattr(service_cls, "_service_info", {}),
        "endpoints": []
    }

    # Inspect all methods of the class
    for name, method in inspect.getmembers(service_cls, predicate=inspect.isfunction):
        # Unwrap decorators if necessary to find our tags
        # (Though usually the wrapper has the tag attached)
        endpoint_info = getattr(method, "_endpoint_info", None)
        
        if endpoint_info:
            schema["endpoints"].append(endpoint_info)

    return schema

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\registry.json
--------------------------------------------------------------------------------
[
  {
    "token_id": "MS_82159302",
    "name": "ArchiveBotMS",
    "path": "_ArchiveBotMS.py",
    "description": "",
    "methods": {
      "create_backup": {
        "args": [
          "source_path",
          "output_dir",
          "extra_exclusions",
          "use_default_exclusions"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "datetime",
      "fnmatch",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "tarfile",
      "tempfile",
      "typing"
    ]
  },
  {
    "token_id": "MS_712B803A",
    "name": "AuthMS",
    "path": "_AuthMS.py",
    "description": "ROLE: Simple authentication microservice providing username/password login\n      and signed session tokens.\n\nINPUTS:\n  - config: Optional configuration dict. Recognized keys:\n      - 'secret_key': Secret used to sign tokens.\n\nOUTPUTS:\n  - Exposes `login` and `validate_session` endpoints for use in pipelines.",
    "methods": {
      "login": {
        "args": [
          "username",
          "password"
        ],
        "doc": "Attempt to log in with the provided username and password.\n\n:param username: Login identifier.\n:param password: Plain-text password.\n:returns: Signed session token if successful, None otherwise."
      },
      "validate_session": {
        "args": [
          "token"
        ],
        "doc": "Check if a serialized token is valid and not expired.\n\n:param token: Session token string.\n:returns: True if token is valid and not expired, False otherwise."
      }
    },
    "dependencies": [
      "base64",
      "hashlib",
      "json",
      "logging",
      "microservice_std_lib",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_B53FC606",
    "name": "CartridgeServiceMS",
    "path": "_CartridgeServiceMS.py",
    "description": "The Source of Truth.\nManages the Unified Neural Cartridge Format (UNCF v1.0).",
    "methods": {
      "get_vector_dim": {
        "args": [],
        "doc": "Retrieves the expected vector dimension from the manifest spec."
      },
      "initialize_manifest": {
        "args": [],
        "doc": "Populates the boot sector with strict RagFORGE Cartridge Schema (UNCF) v1.1."
      },
      "set_manifest": {
        "args": [
          "key",
          "value"
        ],
        "doc": "Upsert metadata key."
      },
      "get_manifest": {
        "args": [
          "key"
        ],
        "doc": "Retrieve metadata key."
      },
      "validate_cartridge": {
        "args": [],
        "doc": "Quality Control: Checks if the cartridge is Agent-Safe."
      },
      "store_file": {
        "args": [
          "vfs_path",
          "origin_path",
          "content",
          "blob",
          "mime_type",
          "origin_type"
        ],
        "doc": "The Universal Input Method. \nStores raw data. If file exists, updates it and resets status to 'RAW' for re-refining."
      },
      "get_pending_files": {
        "args": [
          "limit"
        ],
        "doc": "Fetches files waiting for the Refinery."
      },
      "update_status": {
        "args": [
          "file_id",
          "status",
          "metadata"
        ],
        "doc": ""
      },
      "ensure_directory": {
        "args": [
          "vfs_path"
        ],
        "doc": "Idempotent insert for VFS directories."
      },
      "get_status_flags": {
        "args": [],
        "doc": "Returns key manifest status flags in a single call."
      },
      "list_files": {
        "args": [
          "prefix",
          "status",
          "limit"
        ],
        "doc": "Enumerate files in the cartridge (optionally filtered by VFS prefix and/or status)."
      },
      "get_file_record": {
        "args": [
          "vfs_path"
        ],
        "doc": "Fetch a single file record by VFS path."
      },
      "list_directories": {
        "args": [
          "prefix"
        ],
        "doc": "Enumerate directories in the cartridge VFS."
      },
      "get_directory_tree": {
        "args": [
          "root"
        ],
        "doc": "Builds a nested directory tree starting at `root` (\"\" for full tree)."
      },
      "get_status_summary": {
        "args": [],
        "doc": "Counts files by status and provides a quick cartridge overview."
      },
      "add_node": {
        "args": [
          "node_id",
          "node_type",
          "label",
          "data"
        ],
        "doc": ""
      },
      "add_edge": {
        "args": [
          "source",
          "target",
          "relation",
          "weight"
        ],
        "doc": ""
      },
      "search_embeddings": {
        "args": [
          "query_vector",
          "limit"
        ],
        "doc": "Performs semantic search using sqlite-vec."
      }
    },
    "dependencies": [
      "datetime",
      "json",
      "microservice_std_lib",
      "os",
      "pathlib",
      "sqlite3",
      "sqlite_vec",
      "struct",
      "tempfile",
      "time",
      "typing",
      "uuid"
    ]
  },
  {
    "token_id": "MS_D14BCEE8",
    "name": "ChalkBoardMS",
    "path": "_ChalkBoardMS.py",
    "description": "",
    "methods": {
      "loaded": {
        "args": [],
        "doc": "Called by JS when the page is ready."
      },
      "log_action": {
        "args": [
          "action_name"
        ],
        "doc": "Called by JS when user interacts."
      },
      "update_sign": {
        "args": [
          "text",
          "theme"
        ],
        "doc": "Updates the embedded HTML via JS injection."
      },
      "trigger_effect": {
        "args": [
          "effect"
        ],
        "doc": "Triggers CSS animations like 'shake'."
      }
    },
    "dependencies": [
      "json",
      "microservice_std_lib",
      "os",
      "webview"
    ]
  },
  {
    "token_id": "MS_0E8A6CD8",
    "name": "ChunkingRouterMS",
    "path": "_ChunkingRouterMS.py",
    "description": "The Editor: A 'Recursive' text splitter.\nIt respects the natural structure of text (Paragraphs -> Sentences -> Words)\nrather than just hacking it apart by character count.",
    "methods": {
      "chunk_file": {
        "args": [
          "text",
          "filename",
          "max_size",
          "overlap"
        ],
        "doc": "Extension-aware router."
      }
    },
    "dependencies": [
      "_PythonChunkerMS",
      "microservice_std_lib",
      "re",
      "typing"
    ]
  },
  {
    "token_id": "MS_D918BB0A",
    "name": "CodeChunkerMS",
    "path": "_CodeChunkerMS.py",
    "description": "The Surgeon (Pure Python Edition): Splits code into semantic blocks\n(Classes, Functions) using indentation and regex heuristics.\n\nAdvantages: Zero dependencies. Works on any machine.\nDisadvantages: Slightly less precise than Tree-Sitter for messy code.",
    "methods": {
      "chunk_file": {
        "args": [
          "file_path",
          "max_chars"
        ],
        "doc": "Reads a file and breaks it into logical blocks based on indentation."
      }
    },
    "dependencies": [
      "microservice_std_lib",
      "os",
      "pathlib",
      "re",
      "tempfile",
      "typing"
    ]
  },
  {
    "token_id": "MS_286694E7",
    "name": "CodeFormatterMS",
    "path": "_CodeFormatterMS.py",
    "description": "The Architect.\nUses the WhitespaceEngine to enforce strict indentation rules, \nfixing 'staircase' formatting and mixed tabs/spaces.",
    "methods": {
      "normalize_code": {
        "args": [
          "content",
          "use_tabs",
          "spaces"
        ],
        "doc": "Pure logic endpoint: Takes string, returns string + patch.\nDoes not touch the filesystem."
      },
      "format_file": {
        "args": [
          "file_path",
          "use_tabs",
          "spaces"
        ],
        "doc": "Filesystem endpoint: In-place repair of a file."
      }
    },
    "dependencies": [
      "json",
      "logging",
      "microservice_std_lib",
      "pathlib",
      "re",
      "typing"
    ]
  },
  {
    "token_id": "MS_7A85FF81",
    "name": "CodeGrapherMS",
    "path": "_CodeGrapherMS.py",
    "description": "The Cartographer of Logic: Parses Python code to extract high-level \nsymbols (classes, functions) and maps their 'Call' relationships.\n\nOutput: A graph structure (Nodes + Edges) suitable for visualization \nor dependency analysis.",
    "methods": {
      "scan_directory": {
        "args": [
          "root_path"
        ],
        "doc": "Recursively scans a directory for .py files and builds the graph."
      }
    },
    "dependencies": [
      "ast",
      "json",
      "microservice_std_lib",
      "os",
      "pathlib",
      "sys",
      "typing"
    ]
  },
  {
    "token_id": "MS_7BE95FCF",
    "name": "CodeJanitorMS",
    "path": "_CodeJanitorMS.py",
    "description": "",
    "methods": {
      "enforce_standards": {
        "args": [
          "dry_run"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "logging",
      "microservice_std_lib",
      "pathlib",
      "re",
      "typing"
    ]
  },
  {
    "token_id": "MS_30C25085",
    "name": "CognitiveMemoryMS",
    "path": "_CognitiveMemoryMS.py",
    "description": "The Hippocampus: Manages Short-Term (Working) Memory and orchestrates \nflushing to Long-Term Memory (Vector Store).",
    "methods": {
      "add_entry": {
        "args": [
          "role",
          "content",
          "metadata"
        ],
        "doc": "Adds an item to working memory and persists it."
      },
      "get_context": {
        "args": [
          "limit"
        ],
        "doc": "Returns the most recent conversation history formatted for an LLM."
      },
      "get_full_history": {
        "args": [],
        "doc": "Returns the raw list of memory objects."
      },
      "commit_turn": {
        "args": [],
        "doc": "Signal that a \"Turn\" (User + AI response) is complete.\nChecks if memory is full and triggers a flush if needed."
      }
    },
    "dependencies": [
      "datetime",
      "importlib",
      "json",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "pydantic",
      "sys",
      "typing",
      "uuid"
    ]
  },
  {
    "token_id": "MS_AC0BAF8F",
    "name": "ContentExtractorMS",
    "path": "_ContentExtractorMS.py",
    "description": "The Decoder.\nA standalone utility microservice that separates the concern of \ndocument parsing from ingestion logic.",
    "methods": {
      "get_health": {
        "args": [],
        "doc": "Returns the operational status and library availability."
      },
      "extract_text": {
        "args": [
          "blob",
          "mime_type"
        ],
        "doc": "Main routing logic for extraction. \n logic is internalized here."
      }
    },
    "dependencies": [
      "bs4",
      "io",
      "microservice_std_lib",
      "pypdf",
      "re",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_97FCA3AF",
    "name": "ContextAggregatorMS",
    "path": "_ContextAggregatorMS.py",
    "description": "The Context Builder: Flattens a project folder into a single readable text file.",
    "methods": {
      "aggregate": {
        "args": [
          "root_path",
          "output_file",
          "extra_exclusions",
          "use_default_exclusions"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "datetime",
      "fnmatch",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "typing"
    ]
  },
  {
    "token_id": "MS_C5235D34",
    "name": "ContextPackerMS",
    "path": "_ContextPackerMS.py",
    "description": "The Packer: Walks a directory and dumps all text-readable files \ninto a single monolithic text file with delimiters.",
    "methods": {
      "pack_directory": {
        "args": [
          "root_path",
          "output_filename",
          "additional_excludes"
        ],
        "doc": "Walks the directory and writes file contents to the output file."
      }
    },
    "dependencies": [
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "typing"
    ]
  },
  {
    "token_id": "MS_CCCC1208",
    "name": "DiffEngineMS",
    "path": "_DiffEngineMS.py",
    "description": "The Timekeeper: Implements a 'Hybrid' versioning architecture.\n1. HEAD: Stores full current content for fast read access (UI/RAG).\n2. HISTORY: Stores diff deltas using difflib for audit trails.",
    "methods": {
      "update_file": {
        "args": [
          "path",
          "new_content",
          "author"
        ],
        "doc": "The Atomic Update Operation:\n1. Checks current state.\n2. Calculates Diff.\n3. Writes Diff to History.\n4. Updates Head to New Content."
      },
      "get_head": {
        "args": [
          "path"
        ],
        "doc": "Fast retrieval of current content."
      },
      "get_history": {
        "args": [
          "path"
        ],
        "doc": "Retrieves the full evolution history of a file."
      }
    },
    "dependencies": [
      "datetime",
      "difflib",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "sqlite3",
      "typing",
      "uuid"
    ]
  },
  {
    "token_id": "MS_2E13D7BF",
    "name": "EnvironmentManagerMS",
    "path": "_EnvironmentManagerMS.py",
    "description": "The Operator.\nFinds the right Python interpreter (System vs Venv) and launches processes.",
    "methods": {
      "resolve_python": {
        "args": [
          "project_path",
          "config_override"
        ],
        "doc": "Priority:\n1. Explicit config override\n2. Local .venv\n3. System default (py or sys.executable)"
      },
      "launch_script": {
        "args": [
          "project_path",
          "script_rel_path",
          "env_vars"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "subprocess",
      "sys",
      "typing"
    ]
  },
  {
    "token_id": "MS_240BD73E",
    "name": "ExplorerWidgetMS",
    "path": "_ExplorerWidgetMS.py",
    "description": "A standalone file system tree viewer.",
    "methods": {
      "refresh_tree": {
        "args": [],
        "doc": ""
      },
      "get_selected_paths": {
        "args": [],
        "doc": ""
      },
      "process_gui_queue": {
        "args": [],
        "doc": ""
      }
    },
    "dependencies": [
      "importlib",
      "microservice_std_lib",
      "os",
      "pathlib",
      "queue",
      "sys",
      "threading",
      "tkinter",
      "typing"
    ]
  },
  {
    "token_id": "MS_B87AB5EB",
    "name": "FingerprintScannerMS",
    "path": "_FingerprintScannerMS.py",
    "description": "The Detective: Scans a directory tree and generates a deterministic\n'Fingerprint' (SHA-256 Merkle Root) representing its exact state.",
    "methods": {
      "scan_project": {
        "args": [
          "root_path"
        ],
        "doc": "Scans the project and returns a comprehensive state object."
      }
    },
    "dependencies": [
      "fnmatch",
      "hashlib",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "shutil",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_8CED7E7B",
    "name": "GitPilotMS",
    "path": "_GitPilotMS.py",
    "description": "",
    "methods": {
      "set_repo": {
        "args": [
          "path"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "dataclasses",
      "microservice_std_lib",
      "os",
      "pathlib",
      "queue",
      "subprocess",
      "threading",
      "time",
      "tkinter",
      "typing"
    ]
  },
  {
    "token_id": "MS_B7BF2C48",
    "name": "HeuristicSumMS",
    "path": "_HeuristicSumMS.py",
    "description": "The Skimmer: Generates quick summaries of code/text files without AI.\nScans for high-value lines (headers, signatures, docstrings) and concatenates them.",
    "methods": {
      "summarize": {
        "args": [
          "text",
          "filename",
          "max_chars"
        ],
        "doc": "Generates a summary string from the provided text."
      }
    },
    "dependencies": [
      "microservice_std_lib",
      "os",
      "re",
      "typing"
    ]
  },
  {
    "token_id": "MS_6D21E917",
    "name": "IngestEngineMS",
    "path": "_IngestEngineMS.py",
    "description": "The Heavy Lifter: Reads files, chunks text, fetches embeddings,\npopulates the Graph Nodes, and weaves Graph Edges.",
    "methods": {
      "abort": {
        "args": [],
        "doc": ""
      },
      "check_ollama_connection": {
        "args": [],
        "doc": ""
      },
      "get_available_models": {
        "args": [],
        "doc": ""
      },
      "process_files": {
        "args": [
          "file_paths",
          "model_name"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "dataclasses",
      "importlib",
      "json",
      "microservice_std_lib",
      "os",
      "re",
      "requests",
      "sqlite3",
      "sys",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_CEB3C2F6",
    "name": "IntakeServiceMS",
    "path": "_IntakeServiceMS.py",
    "description": "The Vacuum. \nNow supports two-phase ingestion:\n1. Scan -> Build Tree (with .gitignore respect)\n2. Ingest -> Process selected paths",
    "methods": {
      "get_health": {
        "args": [],
        "doc": "Returns the operational status of the IntakeServiceMS."
      },
      "ingest_source": {
        "args": [
          "source_path"
        ],
        "doc": "Headless/CLI Entry point: Scans and Ingests in one go."
      },
      "scan_path": {
        "args": [
          "root_path",
          "web_depth"
        ],
        "doc": "Unified Scanner Interface.\nDelegates to ScoutMS for both Web and Local FS to ensure consistent node structure."
      },
      "ingest_selected": {
        "args": [
          "file_list",
          "root_path"
        ],
        "doc": "Ingests only the specific files passed in the list."
      },
      "save_persistence": {
        "args": [
          "root_path",
          "checked_map"
        ],
        "doc": "Saves user selections into the Cartridge Manifest (Portable)."
      }
    },
    "dependencies": [
      "_CartridgeServiceMS",
      "_ScannerMS",
      "base_service",
      "bs4",
      "document_utils",
      "fnmatch",
      "json",
      "microservice_std_lib",
      "mimetypes",
      "os",
      "pathlib",
      "requests",
      "typing"
    ]
  },
  {
    "token_id": "MS_6D977CA1",
    "name": "IsoProcessMS",
    "path": "_IsoProcessMS.py",
    "description": "The Safety Valve: Spawns isolated processes with real-time logging feedback.",
    "methods": {
      "execute": {
        "args": [
          "payload",
          "config"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "logging",
      "microservice_std_lib",
      "multiprocessing",
      "queue",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_B73D91A1",
    "name": "LexicalSearchMS",
    "path": "_LexicalSearchMS.py",
    "description": "The Librarian's Index: A lightweight, AI-free search engine.\n\nUses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).\nIdeal for environments where installing PyTorch/Transformers is impossible\nor overkill.",
    "methods": {
      "add_document": {
        "args": [
          "doc_id",
          "text",
          "metadata"
        ],
        "doc": "Adds or updates a document in the index."
      },
      "search": {
        "args": [
          "query",
          "top_k"
        ],
        "doc": "Performs a BM25 Ranked Search."
      }
    },
    "dependencies": [
      "json",
      "microservice_std_lib",
      "os",
      "pathlib",
      "sqlite3",
      "typing"
    ]
  },
  {
    "token_id": "MS_100A23E3",
    "name": "LibrarianMS",
    "path": "_LibrarianMS.py",
    "description": "The Swarm Librarian.\nSpawns concurrent AI workers to scan the codebase and create a system manifest.\nOptimized for Ryzen CPUs and 32GB RAM.",
    "methods": {
      "generate_catalog": {
        "args": [
          "output_file"
        ],
        "doc": "Main entry point. Uses ThreadPoolExecutor for parallel processing."
      }
    },
    "dependencies": [
      "ast",
      "concurrent",
      "datetime",
      "importlib",
      "json",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "requests",
      "typing"
    ]
  },
  {
    "token_id": "MS_100A23E3",
    "name": "LibrarianMS",
    "path": "_LibrarianServiceMS.py",
    "description": "",
    "methods": {
      "generate_catalog": {
        "args": [
          "output_file"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "ast",
      "concurrent",
      "datetime",
      "importlib",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "requests",
      "typing"
    ]
  },
  {
    "token_id": "MS_8EEDB65C",
    "name": "LogViewMS",
    "path": "_LogViewMS.py",
    "description": "The Console: A professional log viewer widget.\nFeatures:\n- Thread-safe (consumes from a Queue).\n- Message Consolidation (\"Error occurred (x5)\").\n- Level Filtering (Toggle INFO/DEBUG/ERROR).",
    "methods": {
      "clear": {
        "args": [],
        "doc": ""
      },
      "save": {
        "args": [],
        "doc": ""
      }
    },
    "dependencies": [
      "datetime",
      "logging",
      "microservice_std_lib",
      "queue",
      "tkinter",
      "typing"
    ]
  },
  {
    "token_id": "MS_FD93FB4F",
    "name": "MonacoHostMS",
    "path": "_MonacoHostMS.py",
    "description": "Hosts the Monaco Editor.\nThis service spawns a GUI window and cannot be run in headless environments.",
    "methods": {
      "launch": {
        "args": [
          "title",
          "width",
          "height",
          "func"
        ],
        "doc": "Create and launch the window.\n:param func: Optional function to run in a separate thread after launch."
      },
      "set_save_callback": {
        "args": [
          "callback"
        ],
        "doc": "Sets the function to trigger when Ctrl+S is pressed in the editor."
      },
      "open_file": {
        "args": [
          "filepath",
          "content"
        ],
        "doc": "Opens a file in the editor (must be called from a background thread or callback)."
      }
    },
    "dependencies": [
      "importlib",
      "json",
      "logging",
      "microservice_std_lib",
      "sys",
      "threading",
      "typing",
      "webview"
    ]
  },
  {
    "token_id": "MS_1F8493F8",
    "name": "NetworkLayoutMS",
    "path": "_NetworkLayoutMS.py",
    "description": "The Topologist: Calculates visual coordinates for graph nodes using\nserver-side algorithms (NetworkX). \nUseful for generating static map snapshots or pre-calculating positions \nto offload client-side rendering.",
    "methods": {
      "calculate_layout": {
        "args": [
          "nodes",
          "edges",
          "algorithm"
        ],
        "doc": "Computes (x, y) coordinates for the given graph.\n\n:param nodes: List of node IDs.\n:param edges: List of (source, target) tuples.\n:param algorithm: 'spring' (Force-directed) or 'circular'.\n:return: Dictionary {node_id: (x, y)}"
      }
    },
    "dependencies": [
      "importlib",
      "logging",
      "microservice_std_lib",
      "networkx",
      "sys",
      "typing"
    ]
  },
  {
    "token_id": "MS_E7AE2BD6",
    "name": "NeuralGraphEngineMS",
    "path": "_NeuralGraphEngineMS.py",
    "description": "",
    "methods": {
      "get_health": {
        "args": [],
        "doc": "Returns the operational status of the NeuralGraphEngineMS."
      },
      "resize": {
        "args": [
          "width",
          "height"
        ],
        "doc": ""
      },
      "set_data": {
        "args": [
          "nodes",
          "links"
        ],
        "doc": ""
      },
      "screen_to_world": {
        "args": [
          "sx",
          "sy"
        ],
        "doc": ""
      },
      "get_node_at": {
        "args": [
          "sx",
          "sy"
        ],
        "doc": ""
      },
      "handle_mouse_down": {
        "args": [
          "x",
          "y"
        ],
        "doc": ""
      },
      "handle_mouse_move": {
        "args": [
          "x",
          "y",
          "is_dragging"
        ],
        "doc": ""
      },
      "handle_mouse_up": {
        "args": [],
        "doc": ""
      },
      "pan": {
        "args": [
          "dx",
          "dy"
        ],
        "doc": ""
      },
      "zoom_camera": {
        "args": [
          "amount",
          "mouse_x",
          "mouse_y"
        ],
        "doc": ""
      },
      "highlight_nodes": {
        "args": [
          "node_ids"
        ],
        "doc": "Highlights specific nodes by ID."
      },
      "step_physics": {
        "args": [],
        "doc": ""
      },
      "get_image_bytes": {
        "args": [],
        "doc": ""
      }
    },
    "dependencies": [
      "math",
      "microservice_std_lib",
      "pygame",
      "random",
      "time"
    ]
  },
  {
    "token_id": "MS_BC3EA28D",
    "name": "NeuralGraphViewerMS",
    "path": "_NeuralGraphViewerMS.py",
    "description": "",
    "methods": {
      "bind_services": {
        "args": [
          "cartridge",
          "neural"
        ],
        "doc": ""
      },
      "run_search": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "load_from_db": {
        "args": [
          "db_path"
        ],
        "doc": "Loads graph data from SQLite.\nDoes NOT block the UI. The physics engine will settle the nodes frame-by-frame."
      },
      "on_resize": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "on_double_click": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "on_click": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "on_release": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "on_drag": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "on_hover": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "on_zoom": {
        "args": [
          "amount"
        ],
        "doc": ""
      },
      "on_windows_scroll": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "animate": {
        "args": [],
        "doc": "The Heartbeat Loop.\nRuns at ~30 FPS. Handles Physics + Rendering."
      }
    },
    "dependencies": [
      "PIL",
      "_NeuralGraphEngineMS",
      "json",
      "microservice_std_lib",
      "os",
      "sqlite3",
      "tkinter"
    ]
  },
  {
    "token_id": "MS_BE7F8AE6",
    "name": "NeuralServiceMS",
    "path": "_NeuralServiceMS.py",
    "description": "The Brain Interface: Orchestrates local AI operations via Ollama for inference and embeddings.",
    "methods": {
      "update_models": {
        "args": [
          "fast_model",
          "smart_model",
          "embed_model"
        ],
        "doc": "Called by the UI Settings Modal to change models on the fly."
      },
      "get_available_models": {
        "args": [],
        "doc": "Fetches list from Ollama for the UI dropdown."
      },
      "check_connection": {
        "args": [],
        "doc": "Pings Ollama to see if it's alive."
      },
      "get_embedding": {
        "args": [
          "text"
        ],
        "doc": "Generates a vector using the configured embedding model."
      },
      "request_inference": {
        "args": [
          "prompt",
          "tier",
          "format_json"
        ],
        "doc": "Synchronous inference request.\ntier: 'fast', 'smart', or other keys in self.models"
      },
      "process_parallel": {
        "args": [
          "items",
          "worker_func"
        ],
        "doc": "Helper to run a function across many items using a ThreadPool.\nUseful for batch ingestion.\nNote: Not exposed as an endpoint as it takes a function as an argument."
      }
    },
    "dependencies": [
      "concurrent",
      "json",
      "logging",
      "microservice_std_lib",
      "requests",
      "typing"
    ]
  },
  {
    "token_id": "MS_6EDB246A",
    "name": "ProjectForgeMS",
    "path": "_ProjectForgeMS.py",
    "description": "The Blacksmith.\nCreates directory structures, stamps out boilerplate code, and injects dependencies.",
    "methods": {
      "forge_project": {
        "args": [
          "parent_path",
          "project_name",
          "dependencies",
          "project_type"
        ],
        "doc": "Stamps out a new project folder."
      }
    },
    "dependencies": [
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "shutil",
      "typing"
    ]
  },
  {
    "token_id": "MS_42FE6B64",
    "name": "PromptOptimizerMS",
    "path": "_PromptOptimizerMS.py",
    "description": "The Tuner: Uses an LLM to refine prompts or generate variations.\nRequires an 'inference_func' to be passed in the config, which accepts a string\nand returns a string (simulating an LLM call).",
    "methods": {
      "refine_prompt": {
        "args": [
          "draft_prompt",
          "feedback"
        ],
        "doc": "Rewrites a prompt based on feedback."
      },
      "generate_variations": {
        "args": [
          "draft_prompt",
          "num_variations",
          "context_data"
        ],
        "doc": "Generates multiple versions of a prompt for testing."
      }
    },
    "dependencies": [
      "json",
      "logging",
      "microservice_std_lib",
      "typing"
    ]
  },
  {
    "token_id": "MS_8FC21A0E",
    "name": "PromptVaultMS",
    "path": "_PromptVaultMS.py",
    "description": "The Vault: A persistent SQLite store for managing, versioning, \nand rendering AI prompts.",
    "methods": {
      "create_template": {
        "args": [
          "slug",
          "title",
          "content",
          "author",
          "tags"
        ],
        "doc": "Creates a new prompt template with an initial version 1."
      },
      "add_version": {
        "args": [
          "slug",
          "content",
          "author"
        ],
        "doc": "Adds a new version to an existing template."
      },
      "get_template": {
        "args": [
          "slug"
        ],
        "doc": "Retrieves a full template with all history."
      },
      "render": {
        "args": [
          "slug",
          "context"
        ],
        "doc": "Fetches the latest version and renders it with Jinja2."
      },
      "list_slugs": {
        "args": [],
        "doc": ""
      }
    },
    "dependencies": [
      "datetime",
      "importlib",
      "jinja2",
      "json",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "pydantic",
      "sqlite3",
      "sys",
      "typing",
      "uuid"
    ]
  },
  {
    "token_id": "MS_037ABE0D",
    "name": "PythonChunkerMS",
    "path": "_PythonChunkerMS.py",
    "description": "Specialized Python AST Chunker.\nFocuses exclusively on identifying classes and functions to preserve code logic.",
    "methods": {
      "get_health": {
        "args": [],
        "doc": "Returns the operational status of the PythonChunkerMS."
      },
      "chunk": {
        "args": [
          "content"
        ],
        "doc": "Parses Python source into semantic CodeChunks."
      }
    },
    "dependencies": [
      "ast",
      "dataclasses",
      "microservice_std_lib",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_88923AC3",
    "name": "RefineryServiceMS",
    "path": "_RefineryServiceMS.py",
    "description": "The Night Shift.\nPolls the DB for 'RAW' files and processes them into Chunks and Graph Nodes.\n\nGraph Enrichment:\n- Code: function/class nodes, resolved import edges when possible.\n- Docs: section/chapter nodes for long-form text (md/txt/rst).",
    "methods": {
      "get_health": {
        "args": [],
        "doc": "Returns the operational status of the RefineryServiceMS."
      },
      "process_pending": {
        "args": [
          "batch_size"
        ],
        "doc": "Main loop. Returns number of files processed."
      }
    },
    "dependencies": [
      "_CartridgeServiceMS",
      "_ChunkingRouterMS",
      "_NeuralServiceMS",
      "ast",
      "concurrent",
      "json",
      "logging",
      "microservice_std_lib",
      "os",
      "re",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_0694A27B",
    "name": "RegexWeaverMS",
    "path": "_RegexWeaverMS.py",
    "description": "The Weaver: A fault-tolerant dependency extractor.\nUses Regex to find imports, making it faster and more permissive\nthan AST parsers (works on broken code).",
    "methods": {
      "extract_dependencies": {
        "args": [
          "content",
          "language"
        ],
        "doc": "Scans code content for import statements.\n:param language: 'python' or 'javascript' (includes ts/jsx)."
      }
    },
    "dependencies": [
      "logging",
      "microservice_std_lib",
      "re",
      "typing"
    ]
  },
  {
    "token_id": "MS_12D07056",
    "name": "RoleManagerMS",
    "path": "_RoleManagerMS.py",
    "description": "The Casting Director: Manages Agent Personas (Roles).\nPersists configuration for System Prompts, Attached KBs, and Memory Settings.",
    "methods": {
      "create_role": {
        "args": [
          "name",
          "system_prompt",
          "description",
          "kbs"
        ],
        "doc": "Creates a new Agent Persona."
      },
      "get_role": {
        "args": [
          "name_or_id"
        ],
        "doc": "Retrieves a role by Name or ID."
      },
      "list_roles": {
        "args": [],
        "doc": ""
      },
      "delete_role": {
        "args": [
          "name"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "datetime",
      "importlib",
      "json",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "pydantic",
      "sqlite3",
      "sys",
      "typing",
      "uuid"
    ]
  },
  {
    "token_id": "MS_AD63F401",
    "name": "SandboxManagerMS",
    "path": "_SandboxManagerMS.py",
    "description": "The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.\nAllows for safe experimentation, diffing, and atomic promotion of changes.",
    "methods": {
      "init_sandbox": {
        "args": [
          "force"
        ],
        "doc": "Creates or resets the sandbox by mirroring the live project."
      },
      "reset_sandbox": {
        "args": [],
        "doc": "Discards all sandbox changes and re-syncs from live."
      },
      "get_diff": {
        "args": [],
        "doc": "Compares Sandbox vs Live. Returns added, modified, and deleted files."
      },
      "promote_changes": {
        "args": [],
        "doc": "Applies changes from Sandbox to Live.\nReturns (added_count, modified_count, deleted_count)."
      }
    },
    "dependencies": [
      "hashlib",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "shutil",
      "typing"
    ]
  },
  {
    "token_id": "MS_7237CDDC",
    "name": "ScannerMS",
    "path": "_ScannerMS.py",
    "description": "The Scanner: Walks the file system, filters junk, and detects binary files.\nGenerates the tree structure used by the UI.",
    "methods": {
      "is_binary": {
        "args": [
          "file_path"
        ],
        "doc": "Determines if a file is binary using two heuristics:\n1. Extension check (Fast)\n2. Content check for null bytes (Accurate)"
      },
      "scan_directory": {
        "args": [
          "root_path"
        ],
        "doc": "Recursively scans a directory and returns a JSON-compatible tree.\nReturns None if path is invalid."
      },
      "flatten_tree": {
        "args": [
          "tree_node"
        ],
        "doc": "Helper to extract all valid file paths from a tree node \n(e.g., when the user clicks 'Start Ingest')."
      }
    },
    "dependencies": [
      "microservice_std_lib",
      "os",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_DFC1A928",
    "name": "ScoutMS",
    "path": "_ScoutMS.py",
    "description": "The Scanner: Walks file systems OR crawls websites (Depth-Aware).",
    "methods": {
      "is_binary": {
        "args": [
          "file_path"
        ],
        "doc": ""
      },
      "scan_directory": {
        "args": [
          "root_path",
          "web_depth"
        ],
        "doc": "Main Entry Point.\n:param root_path: File path or URL.\n:param web_depth: How many links deep to crawl (0 = single page)."
      },
      "flatten_tree": {
        "args": [
          "tree_node"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "bs4",
      "microservice_std_lib",
      "os",
      "requests",
      "time",
      "typing",
      "urllib"
    ]
  },
  {
    "token_id": "MS_D02A07FF",
    "name": "SearchEngineMS",
    "path": "_SearchEngineMS.py",
    "description": "The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).\n\nArchitecture:\n1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.\n2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.\n3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).",
    "methods": {
      "search": {
        "args": [
          "db_path",
          "query",
          "limit"
        ],
        "doc": "Main entry point. Returns a list of results sorted by relevance."
      }
    },
    "dependencies": [
      "importlib",
      "json",
      "logging",
      "microservice_std_lib",
      "os",
      "requests",
      "sqlite3",
      "sqlite_vec",
      "struct",
      "sys",
      "typing"
    ]
  },
  {
    "token_id": "MS_9F825A66",
    "name": "SemanticChunkerMS",
    "path": "_SemanticChunkerMS.py",
    "description": "Intelligent Code Splitter.\nParses source code into logical units (Classes, Functions) \nrather than arbitrary text windows.",
    "methods": {
      "chunk_file": {
        "args": [
          "content",
          "filename"
        ],
        "doc": "Splits file content into chunks.\nReturns a list of dictionaries suitable for JSON response."
      }
    },
    "dependencies": [
      "ast",
      "dataclasses",
      "microservice_std_lib",
      "typing"
    ]
  },
  {
    "token_id": "MS_0A156562",
    "name": "ServiceRegistryMS",
    "path": "_ServiceRegistryMS.py",
    "description": "The Tokenizer (v2): Scans a library of Python microservices and generates\nstandardized JSON 'Service Tokens'.\nFeature: Hybrid AST/Regex parsing for maximum robustness.",
    "methods": {
      "scan": {
        "args": [
          "save_to"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "ast",
      "json",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "re",
      "typing",
      "uuid"
    ]
  },
  {
    "token_id": "MS_02781C0A",
    "name": "SpinnerThingyMaBobberMS",
    "path": "_SpinnerThingyMaBobberMS.py",
    "description": "The Visualizer: An interactive spinner widget.\nUseful for \"Processing...\" screens or OBS overlays.",
    "methods": {
      "launch": {
        "args": [],
        "doc": "Starts the Tkinter main event loop."
      },
      "handle_keypress": {
        "args": [
          "event"
        ],
        "doc": ""
      },
      "get_neon_color": {
        "args": [
          "offset"
        ],
        "doc": ""
      },
      "draw_arc": {
        "args": [
          "cx",
          "cy",
          "radius",
          "width",
          "start",
          "extent",
          "color"
        ],
        "doc": ""
      },
      "animate": {
        "args": [],
        "doc": ""
      }
    },
    "dependencies": [
      "colorsys",
      "math",
      "microservice_std_lib",
      "time",
      "tkinter",
      "typing"
    ]
  },
  {
    "token_id": "MS_E3D913D6",
    "name": "SysInspectorMS",
    "path": "_SysInspectorMS.py",
    "description": "The Auditor: Gathers hardware and environment statistics.\nSupports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).",
    "methods": {
      "generate_report": {
        "args": [],
        "doc": "Runs the full audit and returns a formatted string report."
      }
    },
    "dependencies": [
      "datetime",
      "logging",
      "microservice_std_lib",
      "platform",
      "subprocess",
      "sys",
      "typing"
    ]
  },
  {
    "token_id": "MS_2668E502",
    "name": "TasklistVaultMS",
    "path": "_TasklistVaultMS.py",
    "description": "The Taskmaster: A persistent SQLite engine for hierarchical task management.\nSupports infinite nesting of sub-tasks and status tracking.",
    "methods": {
      "create_list": {
        "args": [
          "name"
        ],
        "doc": "Creates a new task list and returns its ID."
      },
      "get_lists": {
        "args": [],
        "doc": "Returns metadata for all task lists."
      },
      "add_task": {
        "args": [
          "list_id",
          "content",
          "parent_id"
        ],
        "doc": "Adds a task (or sub-task) to a list."
      },
      "update_task": {
        "args": [
          "task_id",
          "content",
          "status",
          "result"
        ],
        "doc": "Updates a task's details."
      },
      "get_full_tree": {
        "args": [
          "list_id"
        ],
        "doc": "Fetches a list and reconstructs the full hierarchy of tasks."
      },
      "delete_list": {
        "args": [
          "list_id"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "datetime",
      "json",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "sqlite3",
      "typing",
      "uuid"
    ]
  },
  {
    "token_id": "MS_76AAA88C",
    "name": "TelemetryServiceMS",
    "path": "_TelemetryServiceMS.py",
    "description": "The Nervous System.\nWatches the thread-safe LogQueue and updates the GUI Panels.",
    "methods": {
      "get_health": {
        "args": [],
        "doc": "Returns the operational status of the TelemetryServiceMS."
      },
      "start": {
        "args": [],
        "doc": "Begins the GUI update loop."
      },
      "ping": {
        "args": [],
        "doc": "Allows an agent to verify the pulse of the UI loop."
      }
    },
    "dependencies": [
      "logging",
      "microservice_std_lib",
      "queue",
      "time",
      "typing"
    ]
  },
  {
    "token_id": "MS_33756201",
    "name": "TextChunkerMS",
    "path": "_TextChunkerMS.py",
    "description": "The Butcher: A unified service for splitting text into digestible chunks\nfor RAG (Retrieval Augmented Generation).",
    "methods": {
      "chunk_by_chars": {
        "args": [
          "text",
          "chunk_size",
          "chunk_overlap"
        ],
        "doc": "Standard Sliding Window. Best for prose/documentation.\nSplits purely by character count."
      },
      "chunk_by_lines": {
        "args": [
          "text",
          "max_lines",
          "max_chars"
        ],
        "doc": "Line-Preserving Chunker. Best for Code.\nRespects line boundaries and returns metadata about line numbers."
      }
    },
    "dependencies": [
      "logging",
      "microservice_std_lib",
      "typing"
    ]
  },
  {
    "token_id": "MS_7E985AE0",
    "name": "ThoughtStreamMS",
    "path": "_ThoughtStreamMS.py",
    "description": "The Neural Inspector: A UI widget for displaying a stream of AI thoughts/logs\nvisualized as 'bubbles' with sparklines.",
    "methods": {
      "add_thought_bubble": {
        "args": [
          "filename",
          "chunk_id",
          "content",
          "vector_preview",
          "color"
        ],
        "doc": "Mimics the 'InspectorFrame' from your React code."
      }
    },
    "dependencies": [
      "datetime",
      "microservice_std_lib",
      "random",
      "tkinter",
      "typing"
    ]
  },
  {
    "token_id": "MS_42A8CBA5",
    "name": "TkinterAppShellMS",
    "path": "_TkinterAppShellMS.py",
    "description": "The Mother Ship.\nOwns the Tkinter Root. All other UI microservices dock into this.",
    "methods": {
      "launch": {
        "args": [],
        "doc": "Ignition sequence start."
      },
      "get_main_container": {
        "args": [],
        "doc": "Other services call this to know where to .pack() themselves."
      },
      "shutdown": {
        "args": [],
        "doc": ""
      }
    },
    "dependencies": [
      "_TkinterThemeManagerMS",
      "logging",
      "microservice_std_lib",
      "tkinter",
      "typing"
    ]
  },
  {
    "token_id": "MS_BC4CE5A9",
    "name": "TkinterSmartExplorerMS",
    "path": "_TkinterSmartExplorerMS.py",
    "description": "The Navigator.\nA TreeView widget that expects standard 'Node' dictionaries (name, type, children).",
    "methods": {
      "load_data": {
        "args": [
          "data"
        ],
        "doc": "Ingests a dictionary tree (like from _ScoutMS or _TreeMapperMS)."
      }
    },
    "dependencies": [
      "microservice_std_lib",
      "tkinter",
      "typing"
    ]
  },
  {
    "token_id": "MS_A266E0EB",
    "name": "TkinterThemeManagerMS",
    "path": "_TkinterThemeManagerMS.py",
    "description": "The Stylist: Holds the color palette and font settings.\nAll UI components query this service to decide how to draw themselves.",
    "methods": {
      "get_theme": {
        "args": [],
        "doc": ""
      },
      "update_key": {
        "args": [
          "key",
          "value"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "microservice_std_lib",
      "typing"
    ]
  },
  {
    "token_id": "MS_BA633F4B",
    "name": "TkinterUniButtonMS",
    "path": "_TkinterUniButtonMS.py",
    "description": "A generic button group that can merge ANY two actions.\nPass the visual/functional definitions in via the config objects.",
    "methods": {},
    "dependencies": [
      "dataclasses",
      "microservice_std_lib",
      "tkinter",
      "typing"
    ]
  },
  {
    "token_id": "MS_B77D7064",
    "name": "TreeMapperMS",
    "path": "_TreeMapperMS.py",
    "description": "The Cartographer: Generates ASCII-art style directory maps.\nUseful for creating context snapshots for LLMs.",
    "methods": {
      "generate_tree": {
        "args": [
          "root_path",
          "additional_exclusions",
          "use_default_exclusions"
        ],
        "doc": ""
      }
    },
    "dependencies": [
      "datetime",
      "logging",
      "microservice_std_lib",
      "os",
      "pathlib",
      "typing"
    ]
  },
  {
    "token_id": "MS_D493B241",
    "name": "VectorFactoryMS",
    "path": "_VectorFactoryMS.py",
    "description": "The Switchboard: Returns the appropriate VectorStore implementation\nbased on configuration.",
    "methods": {
      "create": {
        "args": [
          "backend",
          "config"
        ],
        "doc": ":param backend: 'faiss' or 'chroma'\n:param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)"
      }
    },
    "dependencies": [
      "chromadb",
      "faiss",
      "importlib",
      "json",
      "logging",
      "microservice_std_lib",
      "numpy",
      "os",
      "pathlib",
      "shutil",
      "sys",
      "typing",
      "uuid"
    ]
  },
  {
    "token_id": "MS_8D71E176",
    "name": "WebScraperMS",
    "path": "_WebScraperMS.py",
    "description": "The Reader: Fetches URLs and extracts the main content using Readability.\nStrips ads, navbars, and boilerplate to return clean text for LLMs.",
    "methods": {
      "scrape": {
        "args": [
          "url"
        ],
        "doc": "Synchronous wrapper for fetching and cleaning a URL.\nReturns: {\n    \"url\": str,\n    \"title\": str,\n    \"content\": str (The main body text),\n    \"html\": str (The raw HTML of the main content area)\n}"
      }
    },
    "dependencies": [
      "asyncio",
      "httpx",
      "importlib",
      "logging",
      "microservice_std_lib",
      "re",
      "readability",
      "sys",
      "typing"
    ]
  }
]
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ArchiveBotMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ArchiveBotMS
ENTRY_POINT: _ArchiveBotMS.py
DEPENDENCIES: None
"""

import datetime
import fnmatch
import logging
import os
import tarfile
from pathlib import Path
from typing import Any, Dict, Optional, Set, Tuple

from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# CONFIGURATION
# ==============================================================================
SERVICE_TITLE = "ArchiveBot"
SERVICE_VERSION = "1.1.0"
LOG_LEVEL = logging.INFO

# Default exclusions (Dev artifacts, caches, system files)
DEFAULT_IGNORE_DIRS: Set[str] = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", "dist",
    "build", "coverage", "target", "out", "bin", "obj"
}

DEFAULT_IGNORE_FILES: Set[str] = {
    ".DS_Store", "Thumbs.db", "*.pyc", "*.pyo", "*.log", "*.tmp"
}

logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(SERVICE_TITLE)

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name=SERVICE_TITLE,
    version=SERVICE_VERSION,
    description="Creates timestamped .tar.gz backups of directory trees.",
    tags=["utility", "backup", "filesystem"],
    capabilities=["filesystem:read", "filesystem:write"],
    dependencies=["tarfile", "pathlib", "datetime"],
    side_effects=["filesystem:write"]
)
class ArchiveBotMS(BaseService):
    
    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        super().__init__(SERVICE_TITLE)
        self.config = config or {}

    @service_endpoint(
        inputs={
            "source_path": "str",
            "output_dir": "str",
            "extra_exclusions": "List[str]",
            "use_default_exclusions": "bool"
        },
        outputs={
            "archive_path": "str",
            "file_count": "int"
        },
        description="Compresses a directory into a .tar.gz archive.",
        tags=["action", "backup"],
        side_effects=["filesystem:write"]
    )
    def create_backup(
        self,
        source_path: str,
        output_dir: str,
        extra_exclusions: Optional[Set[str]] = None,
        use_default_exclusions: bool = True,
    ) -> Dict[str, Any]:
        
        src = Path(source_path).resolve()
        out = Path(output_dir).resolve()

        if not src.exists():
            logger.error(f"Source not found: {src}")
            raise FileNotFoundError(f"Source path does not exist: {src}")

        out.mkdir(parents=True, exist_ok=True)

        # Build exclusion set
        exclude_patterns: Set[str] = set()
        if use_default_exclusions:
            exclude_patterns.update(DEFAULT_IGNORE_DIRS)
            exclude_patterns.update(DEFAULT_IGNORE_FILES)
        if extra_exclusions:
            exclude_patterns.update(extra_exclusions)

        # Generate filename: backup_FOLDERNAME_YYYY-MM-DD_HH-MM-SS.tar.gz
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        archive_name = f"backup_{src.name}_{timestamp}.tar.gz"
        archive_path = out / archive_name

        file_count = 0

        try:
            with tarfile.open(archive_path, "w:gz") as tar:
                for root, dirs, files in os.walk(src):
                    # Filter directories in-place to prevent walking into them
                    dirs[:] = [d for d in dirs if not self._is_excluded(d, exclude_patterns)]

                    for file_name in files:
                        if self._is_excluded(file_name, exclude_patterns):
                            continue

                        full_path = Path(root) / file_name
                        
                        # Don't zip the file if we are writing it inside the source folder
                        if full_path == archive_path: 
                            continue

                        rel_path = full_path.relative_to(src)
                        tar.add(full_path, arcname=rel_path)
                        file_count += 1

            logger.info(f"Archive created: {archive_path} ({file_count} files)")
            return {
                "archive_path": str(archive_path),
                "file_count": file_count
            }

        except Exception as exc:
            logger.exception(f"Backup failed: {exc}")
            if archive_path.exists():
                try:
                    archive_path.unlink()
                except Exception: pass
            raise exc

    # --- Helpers ---
    def _is_excluded(self, name: str, patterns: Set[str]) -> bool:
        for pattern in patterns:
            if name == pattern or fnmatch.fnmatch(name, pattern):
                return True
        return False

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    import tempfile
    
    bot = ArchiveBotMS()
    print(f"Service Ready: {bot}")

    with tempfile.TemporaryDirectory() as tmp_source:
        with tempfile.TemporaryDirectory() as tmp_out:
            # Create a test file
            p = Path(tmp_source) / "test_file.txt"
            p.write_text("Hello Archive")
            
            print(f"Backing up {tmp_source} to {tmp_out}...")
            result = bot.create_backup(tmp_source, tmp_out)
            print(f"Result: {result}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_AuthMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _AuthMS
ENTRY_POINT: _AuthMS.py
DEPENDENCIES: None
"""

import base64
import hashlib
import json
import logging
import time
from typing import Any, Dict, Optional

from microservice_std_lib import service_metadata, service_endpoint

"""
SERVICE: Auth
ROLE: Manage user authentication and signed session tokens.
INPUTS:
  - username: Login identifier.
  - password: Secret credential.
  - token: Serialized session token string.
OUTPUTS:
  - token: Signed session token (str) or None on failure.
  - is_valid: Boolean indicating whether a token is valid and not expired.
NOTES:
  This is a simplified in-memory auth system intended for local tools and
  pipelines, not production-grade security.
"""

logger = logging.getLogger(__name__)

# ==============================================================================
# CONFIGURATION
# ==============================================================================

DEFAULT_SECRET_KEY = "super_secret_cortex_key"
DEFAULT_SALT = "cortex_salt"


# ==============================================================================


@service_metadata(
    name="Auth",
    version="1.0.0",
    description="Manages user authentication and session tokens.",
    tags=["auth", "security", "crypto"],
    capabilities=["crypto"],
)
class AuthMS:
    """
    ROLE: Simple authentication microservice providing username/password login
          and signed session tokens.

    INPUTS:
      - config: Optional configuration dict. Recognized keys:
          - 'secret_key': Secret used to sign tokens.

    OUTPUTS:
      - Exposes `login` and `validate_session` endpoints for use in pipelines.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}
        self.secret_key: str = self.config.get("secret_key", DEFAULT_SECRET_KEY)

        # In a real scenario, this might load from a secure config file or DB.
        # For now, we keep a minimal in-memory user database.
        self.users_db: Dict[str, str] = {
            "admin": self._hash_password("admin123"),
        }

    # --------------------------------------------------------------------- #
    # Public endpoints
    # --------------------------------------------------------------------- #

    @service_endpoint(
        inputs={"username": "str", "password": "str"},
        outputs={"token": "Optional[str]"},
        description="Attempts to log in and returns a signed session token.",
        tags=["auth", "security", "session"],
    )
    def login(self, username: str, password: str) -> Optional[str]:
        """
        Attempt to log in with the provided username and password.

        :param username: Login identifier.
        :param password: Plain-text password.
        :returns: Signed session token if successful, None otherwise.
        """
        if username not in self.users_db:
            return None

        stored_hash = self.users_db[username]
        if self._verify_password(password, stored_hash):
            return self._create_token(username)

        return None

    @service_endpoint(
        inputs={"token": "str"},
        outputs={"is_valid": "bool"},
        description="Checks whether a token is valid and not expired.",
        tags=["auth", "security"],
    )
    def validate_session(self, token: str) -> bool:
        """
        Check if a serialized token is valid and not expired.

        :param token: Session token string.
        :returns: True if token is valid and not expired, False otherwise.
        """
        payload = self._decode_token(token)
        return payload is not None

    # --------------------------------------------------------------------- #
    # Internal helpers
    # --------------------------------------------------------------------- #

    def _hash_password(self, password: str) -> str:
        """
        Securely hashes a password using SHA-256 with a static salt.
        """
        return hashlib.sha256((password + DEFAULT_SALT).encode("utf-8")).hexdigest()

    def _verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verifies a provided password against the stored hash.
        """
        return self._hash_password(plain_password) == hashed_password

    def _create_token(self, user_id: str, expires_in: int = 3600) -> str:
        """
        Generates a signed session token.

        Payload includes:
          - 'sub' (subject)
          - 'exp' (expiration time)
          - 'iat' (issued-at time)
          - 'scope' (authorization scope)
        """
        now = int(time.time())
        payload = {
            "sub": user_id,
            "exp": now + expires_in,
            "iat": now,
            "scope": "admin",
        }

        json_payload = json.dumps(payload).encode("utf-8")
        token_part = base64.b64encode(json_payload).decode("utf-8")

        signature = hashlib.sha256((token_part + self.secret_key).encode("utf-8")).hexdigest()
        return f"{token_part}.{signature}"

    def _decode_token(self, token: str) -> Optional[Dict[str, Any]]:
        """
        Parses and validates the incoming token.

        Returns the payload if valid, None otherwise.
        """
        try:
            if not token or "." not in token:
                return None

            token_part, signature = token.split(".", 1)

            # Recalculate signature to verify integrity
            recalc_signature = hashlib.sha256(
                (token_part + self.secret_key).encode("utf-8")
            ).hexdigest()

            if signature != recalc_signature:
                return None  # Invalid signature

            # Decode payload
            payload_json = base64.b64decode(token_part).decode("utf-8")
            payload: Dict[str, Any] = json.loads(payload_json)

            # Check expiration
            if payload.get("exp", 0) < time.time():
                return None  # Expired

            return payload

        except Exception:
            # Intentionally swallow details here and just treat token as invalid.
            logger.exception("Failed to decode or validate auth token.")
            return None


if __name__ == "__main__":
    svc = AuthMS()
    print("Service ready:", svc)
    # Example (manual) usage:
    # token = svc.login("admin", "admin123")
    # print("Token:", token)
    # print("Valid:", svc.validate_session(token) if token else False)

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_CartridgeServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CartridgeServiceMS
ENTRY_POINT: _CartridgeServiceMS.py
DEPENDENCIES: None
"""

import datetime
import json
import os
import sqlite3
import struct
import time
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional

# Try to import sqlite-vec (pip install sqlite-vec)
try:
    import sqlite_vec
except ImportError:
    sqlite_vec = None

from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="CartridgeServiceMS",
    version="1.1.0",
    description="The Source of Truth. Manages the Unified Neural Cartridge Format (UNCF v1.0).",
    tags=["storage", "database", "RAG"],
    capabilities=["sqlite", "vector-search", "graph-storage"],
    dependencies=["sqlite3", "json", "uuid"],
    side_effects=["filesystem:read", "filesystem:write"]
)
class CartridgeServiceMS(BaseService):
    """
    The Source of Truth.
    Manages the Unified Neural Cartridge Format (UNCF v1.0).
    """
    
    SCHEMA_VERSION = "uncf_v1.0"

    def __init__(self, db_path: str):
        super().__init__("CartridgeServiceMS")
        self.db_path = Path(db_path)
        self._init_db()

    def _get_conn(self):
        # Set generous timeout (60s) for multi-threaded Ingest/Refinery contention
        conn = sqlite3.connect(self.db_path, timeout=60.0)
        if sqlite_vec:
            try:
                conn.enable_load_extension(True)
                sqlite_vec.load(conn)
                conn.enable_load_extension(False)
            except Exception as e:
                self.log_error(f"Failed to load sqlite-vec: {e}")
        return conn

    def get_vector_dim(self) -> int:
        """Retrieves the expected vector dimension from the manifest spec."""
        spec = self.get_manifest("embedding_spec") or {}
        if isinstance(spec, str):
            try: spec = json.loads(spec)
            except: spec = {}
        return int(spec.get("dim", 0))

    def _init_db(self):
        """Initializes the standard Schema."""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = self._get_conn()
        cursor = conn.cursor()
        
        # Enable WAL Mode: Allows concurrent Readers (Refinery) & Writers (Ingest)
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL")
        
        # 1. Manifest (The Boot Sector)
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")
        
        # 1.5 Directories (The VFS Index)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS directories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT UNIQUE NOT NULL,
                parent_path TEXT,
                metadata TEXT DEFAULT '{}'
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_dir_parent ON directories(parent_path)")

        # 2. Files (The Content Store)
        # Supports Text AND Binary (blob_data)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT NOT NULL,       -- Portable path (e.g. "src/main.py")
                origin_path TEXT,             -- Provenance (e.g. "C:/Users/...")
                origin_type TEXT,             -- 'filesystem', 'web', 'github'
                content TEXT,                 -- Text content (UTF-8)
                blob_data BLOB,               -- Binary content (Images, PDFs)
                mime_type TEXT,
                status TEXT DEFAULT 'RAW',    -- RAW, REFINED, ERROR, SKIPPED
                metadata TEXT DEFAULT '{}',   -- JSON tags, summaries
                last_updated TIMESTAMP
            )
        """)
        # Index for fast lookups by VFS path
        cursor.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_vfs ON files(vfs_path)")

        # 3. Chunks (The Vector Store)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_id INTEGER,
                chunk_index INTEGER,
                content TEXT,
                embedding BLOB,
                name TEXT,
                type TEXT,
                start_line INTEGER,
                end_line INTEGER,
                FOREIGN KEY(file_id) REFERENCES files(id)
            )
        """)

        # 3.5 Vector Index (sqlite-vec)
        if sqlite_vec:
            dim = self.get_vector_dim()
            if dim > 0:
                try:
                    cursor.execute(f"CREATE VIRTUAL TABLE IF NOT EXISTS vec_items USING vec0(embedding float[{dim}])")
                except Exception as e:
                    self.log_error(f"Vector Table Init Error: {e}")
            else:
                self.log_info("Vector table creation deferred: No dimensions found in manifest yet.")

        # 4. Graph Topology (The Neural Wiring)
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, relation TEXT, weight REAL)")

        # 5. Validation Logs
        cursor.execute("CREATE TABLE IF NOT EXISTS logs (timestamp REAL, level TEXT, message TEXT, context TEXT)")
        
        conn.commit()
        conn.close()
        
        # Initialize standard keys if new
        self.initialize_manifest()

    def initialize_manifest(self):
        """Populates the boot sector with strict RagFORGE Cartridge Schema (UNCF) v1.1."""
        if not self.get_manifest("cartridge_id"):
            now = datetime.datetime.utcnow().isoformat()

            # 1. Identity & Versioning
            self.set_manifest("schema_name", "ragforge_cartridge")
            self.set_manifest("schema_version", "1.1.0")
            self.set_manifest("cartridge_id", str(uuid.uuid4()))
            self.set_manifest("created_at_utc", now)
            self.set_manifest("created_by_app", "RagFORGE")

            # 2. Provenance / Sources
            self.set_manifest("sources", [])
            self.set_manifest("source_policies", {
                "binary_policy": "Extract Text",
                "web_depth": 0
            })

            # 3. Specs (Defaults - updated by RefineryService._stamp_specs)
            self.set_manifest("embedding_spec", {
                "provider": "unknown",
                "model": "pending_init",
                "dim": 0,
                "dtype": "unknown",
                "distance": "unknown"
            })
            self.set_manifest("chunking_spec", {
                "strategy": "semantic_hybrid",
                "python_ast": True,
                "generic_window": 1500
            })

            # 4. VFS + Content Stats (populated/updated over time)
            self.set_manifest("vfs", {
                "root_label": "",
                "directories": {"count": 0},
                "files": {
                    "count": 0,
                    "by_origin_type": {},
                    "by_mime": {}
                },
                "index_built": False
            })
            self.set_manifest("content_stats", {
                "chunks": {"count": 0},
                "vector_index": {
                    "enabled": False,
                    "table": "vec_items",
                    "backend": "sqlite-vec",
                    "dims": 0,
                    "status": "unknown"
                },
                "graph": {
                    "nodes": 0,
                    "edges": 0
                }
            })

            # 5. Capabilities Contract (what an agent can assume exists / how to navigate)
            self.set_manifest("capabilities", {
                "tables": {
                    "manifest": True,
                    "directories": True,
                    "files": True,
                    "chunks": True,
                    "vec_items": True,
                    "graph_nodes": True,
                    "graph_edges": True,
                    "logs": True
                },
                "navigation": {
                    "vfs_path": "files.vfs_path",
                    "directory_index": "directories.vfs_path",
                    "list_files_query": "SELECT vfs_path, mime_type, origin_type, status FROM files ORDER BY vfs_path",
                    "list_directories_query": "SELECT vfs_path, parent_path FROM directories ORDER BY vfs_path"
                },
                "retrieval": {
                    "raw_file_content_query": "SELECT content, blob_data, mime_type FROM files WHERE vfs_path=?",
                    "chunks_by_file_query": "SELECT chunk_index, name, type, start_line, end_line, content FROM chunks WHERE file_id=? ORDER BY chunk_index",
                    "vector_search": "sqlite-vec on vec_items if available"
                },
                "python_helper_api": {
                    "note": "Optional convenience layer for agents running inside Python. For non-Python consumers, use the SQL queries above.",
                    "methods": [
                        "CartridgeServiceMS.get_status_flags",
                        "CartridgeServiceMS.list_files",
                        "CartridgeServiceMS.list_directories",
                        "CartridgeServiceMS.get_file_record",
                        "CartridgeServiceMS.get_directory_tree",
                        "CartridgeServiceMS.get_status_summary",
                        "CartridgeServiceMS.add_node",
                        "CartridgeServiceMS.add_edge",
                        "CartridgeServiceMS.search_embeddings"
                    ]
                }
            })

            # 6. Status & Health
            self.set_manifest("cartridge_health", "FRESH")
            self.set_manifest("ingest_complete", False)
            self.set_manifest("refine_complete", False)
            self.set_manifest("last_ingest_at_utc", "")
            self.set_manifest("last_refine_at_utc", "")
            self.set_manifest("last_error", "")
            self.set_manifest("locks", {
                "write_lock_expected": False,
                "notes": "If DB locks occur, consider batching writes and shorter-lived connections."
            })

    def set_manifest(self, key: str, value: Any):
        """Upsert metadata key."""
        conn = self._get_conn()
        val_str = json.dumps(value) if isinstance(value, (dict, list)) else str(value)
        conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", (key, val_str))
        conn.commit()
        conn.close()

    def get_manifest(self, key: str) -> Optional[str]:
        """Retrieve metadata key."""
        conn = self._get_conn()
        row = conn.execute("SELECT value FROM manifest WHERE key=?", (key,)).fetchone()
        conn.close()
        return row[0] if row else None

    def validate_cartridge(self) -> Dict[str, Any]:
        """Quality Control: Checks if the cartridge is Agent-Safe."""
        report = {"valid": True, "health": "OK", "errors": []}
        
        # 1. Check Required Keys
        required = [
            "schema_name", "schema_version", "cartridge_id",
            "created_at_utc", "created_by_app", "embedding_spec",
            "chunking_spec", "capabilities"
        ]
        for key in required:
            if not self.get_manifest(key):
                report["valid"] = False
                report["errors"].append(f"Missing Manifest Key: {key}")
        
        # 2. Check Vector Index Presence
        conn = self._get_conn()
        vec_enabled = False
        vec_status = "unknown"
        try:
            conn.execute("SELECT count(*) FROM vec_items").fetchone()
            vec_enabled = True
            vec_status = "available"
        except Exception:
            vec_enabled = False
            vec_status = "unavailable"
            report["errors"].append("Vector Index (vec_items) missing or not loaded.")
            report["health"] = "WARN_NO_VECTORS"
        finally:
            conn.close()

        # Update manifest content_stats.vector_index to reflect truth
        try:
            content_stats = self.get_manifest("content_stats") or {}
            if isinstance(content_stats, str):
                 try: content_stats = json.loads(content_stats)
                 except: content_stats = {}
            
            vec = content_stats.get("vector_index", {}) if isinstance(content_stats, dict) else {}

            embed_spec = self.get_manifest("embedding_spec") or {}
            if isinstance(embed_spec, str):
                try: embed_spec = json.loads(embed_spec)
                except: embed_spec = {}
            
            spec_dim = 0
            if isinstance(embed_spec, dict):
                spec_dim = int(embed_spec.get("dim", 0) or 0)

            vec["enabled"] = bool(vec_enabled)
            vec["status"] = vec_status
            if spec_dim > 0:
                vec["dims"] = spec_dim

            if "table" not in vec:
                vec["table"] = "vec_items"
            if "backend" not in vec:
                vec["backend"] = "sqlite-vec"

            content_stats["vector_index"] = vec
            self.set_manifest("content_stats", content_stats)
        except Exception as e:
            report["errors"].append(f"Failed to stamp vector_index status into manifest: {e}")
            report["health"] = "WARN_MANIFEST_STAMP_FAIL"
            
        return report

    def store_file(self, vfs_path: str, origin_path: str, content: str = None, blob: bytes = None, mime_type: str = "text/plain", origin_type: str = "filesystem"):
        """
        The Universal Input Method. 
        Stores raw data. If file exists, updates it and resets status to 'RAW' for re-refining.
        """
        conn = self._get_conn()
        try:
            conn.execute("""
                INSERT OR REPLACE INTO files 
                (vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, 'RAW', ?)
            """, (vfs_path, origin_path, origin_type, content, blob, mime_type, time.time()))
            conn.commit()
            return True
        except Exception as e:
            self.log_error(f"DB Store Error ({vfs_path}): {e}")
            return False
        finally:
            conn.close()

    def get_pending_files(self, limit: int = 10) -> List[Dict]:
        """Fetches files waiting for the Refinery."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        rows = conn.execute("SELECT * FROM files WHERE status = 'RAW' LIMIT ?", (limit,)).fetchall()
        conn.close()
        return [dict(row) for row in rows]

    def update_status(self, file_id: int, status: str, metadata: dict = None):
        conn = self._get_conn()
        if metadata:
            conn.execute("UPDATE files SET status = ?, metadata = ? WHERE id = ?", 
                         (status, json.dumps(metadata), file_id))
        else:
            conn.execute("UPDATE files SET status = ? WHERE id = ?", (status, file_id))
        conn.commit()
        conn.close()

    def ensure_directory(self, vfs_path: str):
        """Idempotent insert for VFS directories."""
        if not vfs_path: return
        parent = os.path.dirname(vfs_path).replace("\\", "/")
        if parent == vfs_path: parent = "" # Root case
        
        conn = self._get_conn()
        try:
            conn.execute("INSERT OR IGNORE INTO directories (vfs_path, parent_path) VALUES (?, ?)", (vfs_path, parent))
            conn.commit()
        except: pass
        finally:
            conn.close()

    # --- Agent-Friendly Helpers (No raw SQL required) ---
    def _coerce_bool(self, v: Any) -> bool:
        """Best-effort conversion for manifest values stored as strings."""
        if v is None:
            return False
        if isinstance(v, bool):
            return v
        s = str(v).strip().lower()
        return s in ("1", "true", "yes", "y", "on")

    @service_endpoint(
        inputs={},
        outputs={"ingest_complete": "bool", "refine_complete": "bool", "cartridge_health": "str"},
        description="Returns key manifest status flags (ingest/refine status and health) in a single call.",
        tags=["status", "health"]
    )
    def get_status_flags(self) -> Dict[str, Any]:
        """Returns key manifest status flags in a single call."""
        ingest_complete = self._coerce_bool(self.get_manifest("ingest_complete"))
        refine_complete = self._coerce_bool(self.get_manifest("refine_complete"))
        health = self.get_manifest("cartridge_health") or "UNKNOWN"
        return {
            "ingest_complete": ingest_complete,
            "refine_complete": refine_complete,
            "cartridge_health": health,
            "schema_name": self.get_manifest("schema_name") or "",
            "schema_version": self.get_manifest("schema_version") or "",
            "cartridge_id": self.get_manifest("cartridge_id") or ""
        }

    def list_files(self, prefix: str = "", status: Optional[str] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Enumerate files in the cartridge (optionally filtered by VFS prefix and/or status)."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            sql = "SELECT id, vfs_path, origin_path, origin_type, mime_type, status, last_updated, metadata FROM files"
            clauses = []
            params = []

            if prefix:
                clauses.append("vfs_path LIKE ?")
                params.append(prefix.rstrip("/") + "/%")

            if status:
                clauses.append("status = ?")
                params.append(status)

            if clauses:
                sql += " WHERE " + " AND ".join(clauses)

            sql += " ORDER BY vfs_path"

            if limit is not None:
                sql += " LIMIT ?"
                params.append(int(limit))

            rows = conn.execute(sql, tuple(params)).fetchall()
            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    def get_file_record(self, vfs_path: str) -> Optional[Dict[str, Any]]:
        """Fetch a single file record by VFS path."""
        if not vfs_path:
            return None
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            row = conn.execute(
                "SELECT id, vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, metadata, last_updated FROM files WHERE vfs_path = ?",
                (vfs_path,)
            ).fetchone()
            if not row:
                return None
            d = dict(row)
            try:
                d["metadata"] = json.loads(d.get("metadata") or "{}")
            except Exception:
                d["metadata"] = {}
            return d
        finally:
            conn.close()

    def list_directories(self, prefix: str = "") -> List[Dict[str, Any]]:
        """Enumerate directories in the cartridge VFS."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            if prefix:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories WHERE vfs_path LIKE ? ORDER BY vfs_path",
                    (prefix.rstrip("/") + "/%",)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories ORDER BY vfs_path"
                ).fetchall()

            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    @service_endpoint(
        inputs={"root": "str"},
        outputs={"tree": "dict"},
        description="Builds a nested directory tree structure for UI navigation or context mapping.",
        tags=["vfs", "navigation"]
    )
    def get_directory_tree(self, root: str = "") -> Dict[str, Any]:
        """Builds a nested directory tree starting at `root` ("" for full tree)."""
        dirs = self.list_directories(prefix=root) if root else self.list_directories()
        files = self.list_files(prefix=root) if root else self.list_files()

        # Tree nodes are dicts: {"_dirs": {name: node}, "_files": [file_records...]}
        def new_node():
            return {"_dirs": {}, "_files": []}

        tree = new_node()

        # Insert directories
        for d in dirs:
            path = (d.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            cur = tree
            for p in parts:
                cur = cur["_dirs"].setdefault(p, new_node())

        # Insert files
        for f in files:
            path = (f.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            fname = parts[-1]
            
            cur = tree
            for p in parts[:-1]:
                cur = cur["_dirs"].setdefault(p, new_node())
            # Store a light file record for tree browsing
            cur["_files"].append({
                "name": fname,
                "vfs_path": f.get("vfs_path"),
                "mime_type": f.get("mime_type"),
                "origin_type": f.get("origin_type"),
                "status": f.get("status")
            })

        return tree

    def get_status_summary(self) -> Dict[str, Any]:
        """Counts files by status and provides a quick cartridge overview."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            rows = conn.execute("SELECT status, COUNT(*) as n FROM files GROUP BY status").fetchall()
            by_status = {r["status"]: r["n"] for r in rows}

            dcnt = conn.execute("SELECT COUNT(*) FROM directories").fetchone()[0]
            fcnt = conn.execute("SELECT COUNT(*) FROM files").fetchone()[0]
            ccnt = conn.execute("SELECT COUNT(*) FROM chunks").fetchone()[0]
            ncnt = conn.execute("SELECT COUNT(*) FROM graph_nodes").fetchone()[0]
            ecnt = conn.execute("SELECT COUNT(*) FROM graph_edges").fetchone()[0]

            return {
                "directories": int(dcnt),
                "files": int(fcnt),
                "chunks": int(ccnt),
                "graph_nodes": int(ncnt),
                "graph_edges": int(ecnt),
                "files_by_status": by_status,
                "flags": self.get_status_flags()
            }
        finally:
            conn.close()

    # --- Graph Helpers ---
    def add_node(self, node_id: str, node_type: str, label: str, data: dict = None):
        conn = self._get_conn()
        conn.execute("INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json) VALUES (?, ?, ?, ?)",
                     (node_id, node_type, label, json.dumps(data or {})))
        conn.commit()
        conn.close()

    def add_edge(self, source: str, target: str, relation: str = "related", weight: float = 1.0):
        conn = self._get_conn()
        conn.execute("INSERT OR IGNORE INTO graph_edges (source, target, relation, weight) VALUES (?, ?, ?, ?)",
                     (source, target, relation, weight))
        conn.commit()
        conn.close()

    # --- Vector Search ---
    @service_endpoint(
        inputs={"query_vector": "list", "limit": "int"},
        outputs={"results": "list"},
        description="Performs semantic vector search using sqlite-vec against the cartridge chunks.",
        tags=["search", "vector"]
    )
    def search_embeddings(self, query_vector: List[float], limit: int = 5) -> List[Dict]:
        """Performs semantic search using sqlite-vec."""
        if not sqlite_vec or not query_vector:
            return []

        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        results = []
        
        try:
            # Pack vector to binary if needed, but sqlite-vec usually handles raw lists in parameterized queries
            # dependent on the binding. We'll pass binary for safety if using standard bindings,
            # but typically raw list works with the extension's adapters.
            # For now, we assume the extension handles the list->vector conversion.
            rows = conn.execute("""
                SELECT
                    rowid,
                    distance
                FROM vec_items
                WHERE embedding MATCH ?
                ORDER BY distance
                LIMIT ?
            """, (json.dumps(query_vector), limit)).fetchall()
            
            # Resolve back to chunks with VFS context
            for r in rows:
                chunk_id = r['rowid']
                # Join with files to get vfs_path
                query = """
                    SELECT c.*, f.vfs_path 
                    FROM chunks c 
                    JOIN files f ON c.file_id = f.id 
                    WHERE c.id=?
                """
                chunk = conn.execute(query, (chunk_id,)).fetchone()
                
                if chunk:
                    res = dict(chunk)
                    res['score'] = r['distance']
                    results.append(res)
                    
        except Exception as e:
            self.log_error(f"Vector Search Error: {e}")
        finally:
            conn.close()
            
        return results

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    import tempfile
    
    with tempfile.TemporaryDirectory() as tmp_dir:
        db_file = os.path.join(tmp_dir, "test_cartridge.db")
        print(f"Initializing service at: {db_file}")
        
        svc = CartridgeServiceMS(db_file)
        print(f"Service Ready: {svc}")
        
        # Simple test: check manifest
        status = svc.get_status_flags()
        print(f"Initial Status: {status}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ChalkBoardMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ChalkBoardMS
ENTRY_POINT: _ChalkBoardMS.py
DEPENDENCIES: None
"""

import json
import os
import webview

from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# CONFIGURATION & ASSETS
# ==============================================================================
HTML_CONTENT = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>OBS Signboard</title>
    <link href="https://fonts.googleapis.com/css2?family=Neonderthaw&family=Press+Start+2P&family=Fredericka+the+Great&family=Orbitron:wght@700&family=Special+Elite&display=swap" rel="stylesheet">
    <style>
        body, html { margin: 0; padding: 0; height: 100%; overflow: hidden; display: flex; justify-content: center; align-items: center; transition: all 0.5s ease; }
        #sign-container { width: 90%; text-align: center; outline: none; cursor: text; transition: transform 0.2s; }
        
        /* --- THEME 1: NEON NIGHTS --- */
        body.neon { background-color: #050505; font-family: 'Neonderthaw', cursive; }
        body.neon #sign-container { color: #fff; font-size: 8rem; text-shadow: 0 0 7px #fff, 0 0 42px #bc13fe, 0 0 102px #bc13fe; animation: flicker 1.5s infinite alternate; }

        /* --- THEME 2: 8-BIT HACKER --- */
        body.terminal { background-color: #000; font-family: 'Press Start 2P', cursive; }
        body.terminal #sign-container { color: #00ff41; font-size: 3.5rem; text-shadow: 0 0 10px #00ff41; text-transform: uppercase; }
        body.terminal #sign-container::after { content: '_'; animation: blink 1s step-end infinite; }

        /* --- THEME 3: CHALKBOARD --- */
        body.chalk { background-color: #2b3a28; font-family: 'Fredericka the Great', cursive; background-image: radial-gradient(circle, rgba(255,255,255,0.05) 1px, transparent 1px); background-size: 20px 20px; }
        body.chalk #sign-container { color: rgba(255,255,255,0.9); font-size: 6rem; transform: rotate(-1deg); }

        /* --- NEW THEME 4: BLUEPRINT (Technical) --- */
        body.blueprint { background-color: #003366; font-family: 'Orbitron', sans-serif; background-image: linear-gradient(#004080 1px, transparent 1px), linear-gradient(90deg, #004080 1px, transparent 1px); background-size: 50px 50px; }
        body.blueprint #sign-container { color: #00d9ff; font-size: 5rem; text-transform: uppercase; border: 2px solid #00d9ff; padding: 20px; box-shadow: 0 0 15px #00d9ff; }

        /* --- NEW THEME 5: RETRO WOOD --- */
        body.retro { background-color: #3d2b1f; font-family: 'Special Elite', serif; background-image: repeating-linear-gradient(90deg, transparent, transparent 40px, rgba(0,0,0,0.1) 41px); }
        body.retro #sign-container { color: #e6b450; font-size: 5.5rem; text-shadow: 2px 2px 0px #20150d; }

        /* --- NEW THEME 6: CYBERPUNK (Yellow/Black) --- */
        body.cyber { background-color: #fcee0a; font-family: 'Orbitron', sans-serif; }
        body.cyber #sign-container { color: #000; font-size: 5rem; font-weight: 900; text-transform: uppercase; font-style: italic; background: #000; color: #fcee0a; padding: 10px 40px; clip-path: polygon(0% 0%, 100% 0%, 95% 100%, 5% 100%); }

        /* ANIMATIONS & EFFECTS */
        @keyframes flicker { 0%, 19%, 21%, 100% { opacity: 1; } 20% { opacity: 0.5; } }
        @keyframes blink { 0%, 100% { opacity: 1; } 50% { opacity: 0; } }
        .shake { animation: shake 0.5s cubic-bezier(.36,.07,.19,.97) both; }
        @keyframes shake { 10%, 90% { transform: translate3d(-1px, 0, 0); } 20%, 80% { transform: translate3d(2px, 0, 0); } 30%, 50%, 70% { transform: translate3d(-4px, 0, 0); } 40%, 60% { transform: translate3d(4px, 0, 0); } }
    </style>
</head>
<body class="neon">
    <div id="sign-container" contenteditable="true" spellcheck="false">ON AIR</div>

    <script>
        const container = document.getElementById('sign-container');
        
        function updateDisplay(text, theme) {
            container.innerText = text;
            document.body.className = theme;
        }

        function triggerEffect(effect) {
            if (effect === 'shake') {
                container.classList.add('shake');
                setTimeout(() => container.classList.remove('shake'), 500);
            }
        }

        // Notify Python on load
        window.addEventListener('pywebviewready', () => {
            window.pywebview.api.loaded().then(state => {
                updateDisplay(state.text, state.theme);
            });
        });

        document.addEventListener('keydown', (e) => {
            const themes = { 'F1': 'neon', 'F2': 'terminal', 'F3': 'chalk', 'F4': 'blueprint', 'F5': 'retro', 'F6': 'cyber' };
            if (themes[e.key]) {
                document.body.className = themes[e.key];
                window.pywebview.api.log_action('switch_theme_' + themes[e.key]);
            }
        });
    </script>
</body>
</html>
"""

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="ChalkboardWeb",
    version="2.0.1",
    description="Integrated HTML5/CSS3 Digital Signage Engine",
    tags=["ui", "webview", "obs"],
    capabilities=["ui:gui"],
    dependencies=["webview", "json"],
    side_effects=["ui:update"]
)
class ChalkBoardMS(BaseService):
    def __init__(self):
        super().__init__("ChalkboardWeb")
        self._window = None
        self.state = {"text": "ON AIR", "theme": "neon"}

    # --- Internal/JS Callbacks ---
    def loaded(self):
        """Called by JS when the page is ready."""
        print("Frontend handshake complete.")
        return self.state

    def log_action(self, action_name):
        """Called by JS when user interacts."""
        print(f"Webview Event: {action_name}")

    # --- Public Endpoints ---
    @service_endpoint(
        inputs={"text": "str", "theme": "str"}, 
        outputs={},
        description="Updates the embedded HTML via JS injection.",
        tags=["ui", "display"]
    )
    def update_sign(self, text: str, theme: str = "neon"):
        """Updates the embedded HTML via JS injection."""
        self.state["text"] = text
        self.state["theme"] = theme
        if self._window:
            sanitized_text = json.dumps(text)
            self._window.evaluate_js(f"updateDisplay({sanitized_text}, '{theme}')")

    @service_endpoint(
        inputs={"effect": "str"}, 
        outputs={},
        description="Triggers CSS animations like 'shake'.",
        tags=["ui", "animation"]
    )
    def trigger_effect(self, effect: str):
        """Triggers CSS animations like 'shake'."""
        if self._window:
            self._window.evaluate_js(f"triggerEffect('{effect}')")

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    api = ChalkBoardMS()
    print(f"Service Ready: {api}")
    
    window = webview.create_window(
        'OBS Signboard v2', 
        html=HTML_CONTENT,
        js_api=api,
        width=1000, 
        height=700,
        background_color='#000000'
    )
    api._window = window
    webview.start(debug=True)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ChunkingRouterMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ChunkingRouterMS
ENTRY_POINT: _ChunkingRouterMS.py
DEPENDENCIES: None
"""

import re
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint, BaseService

# Attempt to import the specialist. If missing, we will fallback or warn.
try:
    from _PythonChunkerMS import PythonChunkerMS, CodeChunk
except ImportError:
    # Fallback mock for standalone testing if dependency is missing
    class CodeChunk:
        def __init__(self, name, type, content, start_line, end_line):
            self.name = name; self.type = type; self.content = content
            self.start_line = start_line; self.end_line = end_line
        def __repr__(self): return f"<CodeChunk {self.name}>"
    
    class PythonChunkerMS:
        def chunk(self, text): return [CodeChunk("mock", "text", text, 0, 0)]

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="ChunkingRouterMS",
    version="1.1.0",
    description="The Dispatcher: Routes files to specialized chunkers based on extension (AST for Python, Recursive for Prose).",
    tags=["orchestration", "chunking", "nlp"],
    capabilities=["routing", "text-processing"],
    dependencies=["re"],
    side_effects=[]
)
class ChunkingRouterMS(BaseService):
    """
    The Editor: A 'Recursive' text splitter.
    It respects the natural structure of text (Paragraphs -> Sentences -> Words)
    rather than just hacking it apart by character count.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("ChunkingRouterMS")
        self.config = config or {}
        self.python_specialist = PythonChunkerMS()
        # Separators for the Prose Specialist logic
        self.separators = ["\n\n", "\n", "(?<=[.?!])\s+", " ", ""]

    @service_endpoint(
        inputs={"text": "str", "filename": "str", "max_size": "int", "overlap": "int"},
        outputs={"chunks": "list"},
        description="Routes text to the appropriate specialist. Returns a list of CodeChunk objects or raw strings.",
        tags=["routing", "chunking"]
    )
    def chunk_file(self, text: str, filename: str, max_size: int = 1000, overlap: int = 100) -> List[Any]:
        """
        Extension-aware router.
        """
        if filename.endswith(".py"):
            return self.python_specialist.chunk(text)
        
        # Fallback to the internal Prose Specialist (Recursive Splitter)
        raw_chunks = self._recursive_split(text, self.separators, max_size, overlap)
        
        # Standardize output for the Refinery: Wrap prose in CodeChunk objects
        return [
            CodeChunk(
                name=f"prose_chunk_{i}", 
                type="text", 
                content=c, 
                start_line=0, 
                end_line=0
            ) for i, c in enumerate(raw_chunks)
        ]

    # ==========================================================================
    # INTERNAL LOGIC (The Prose Specialist)
    # ==========================================================================
    def _recursive_split(self, text: str, separators: List[str], max_size: int, overlap: int) -> List[str]:
        final_chunks = []
        
        # 1. Base Case: If the text fits, return it
        if len(text) <= max_size:
            return [text]
        
        # 2. Edge Case: No more separators, forced hard split
        if not separators:
            return self._hard_split(text, max_size, overlap)

        # 3. Recursive Step: Try to split by the current separator
        current_sep = separators[0]
        next_separators = separators[1:]
        
        # Regex split to keep delimiters if possible (logic varies by regex complexity)
        # For simple string splits like \n\n, we just split.
        if len(current_sep) > 1 and "(" in current_sep: 
            # It's a regex lookbehind (sentence splitter), use re.split
            splits = re.split(current_sep, text)
        else:
            splits = text.split(current_sep)

        # Now we have a list of smaller pieces. We need to merge them back together
        # until they fill the 'max_size' bucket, then start a new bucket.
        current_doc = []
        current_length = 0
        
        for split in splits:
            if not split: continue
            
            # If a single split is STILL too big, recurse deeper on it
            if len(split) > max_size:
                # If we have stuff in the buffer, flush it first
                if current_doc:
                    final_chunks.append(current_sep.join(current_doc))
                    current_doc = []
                    current_length = 0
                
                # Recurse on the big chunk using the NEXT separator
                sub_chunks = self._recursive_split(split, next_separators, max_size, overlap)
                final_chunks.extend(sub_chunks)
                continue

            # Check if adding this split would overflow
            if current_length + len(split) + len(current_sep) > max_size:
                # Flush the current buffer
                doc_text = current_sep.join(current_doc)
                final_chunks.append(doc_text)
                
                # For simplicity in recursion, we start fresh with the current split.
                current_doc = [split]
                current_length = len(split)
            else:
                # Add to buffer
                current_doc.append(split)
                current_length += len(split) + len(current_sep)

        # Flush remaining
        if current_doc:
            final_chunks.append(current_sep.join(current_doc))

        return final_chunks

    def _hard_split(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Last resort: naive character sliding window."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    chunker = ChunkingRouterMS()
    print(f"Service ready: {chunker}")
    
    # Example: A technical document with structure
    doc = """
    # Intro to AI
    Artificial Intelligence is great. It helps us code.
    
    ## How it works
    1. Ingestion: Reading data.
    2. Processing: Thinking about data.
    
    This is a very long paragraph that effectively serves as a stress test for the sentence splitter. It should hopefully not break in the middle of a thought! We want to keep sentences whole.
    """
    
    print("--- Testing Smart Chunking (Max 60 chars) ---")
    # We set max_size very small to force it to use the sentence/word splitters
    chunks = chunker.chunk_file(doc, "test.md", max_size=60, overlap=0)
    
    for i, c in enumerate(chunks):
        print(f"[{i}] {repr(c)}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_CodeChunkerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeChunkerMS
ENTRY_POINT: _CodeChunkerMS.py
DEPENDENCIES: None
"""

import os
import re
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional

from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="CodeChunker",
    version="1.0.0",
    description="Splits code into semantic blocks (Classes, Functions) using indentation and regex heuristics.",
    tags=["parsing", "chunking", "code"],
    capabilities=["filesystem:read"],
    dependencies=["re", "pathlib"],
    side_effects=["filesystem:read"]
)
class CodeChunkerMS(BaseService):
    """
    The Surgeon (Pure Python Edition): Splits code into semantic blocks
    (Classes, Functions) using indentation and regex heuristics.
    
    Advantages: Zero dependencies. Works on any machine.
    Disadvantages: Slightly less precise than Tree-Sitter for messy code.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("CodeChunker")
        self.config = config or {}
        # Regex to find definitions. Capture group 1 is the indentation.
        # Supports Python, JS, TS, Go signatures loosely.
        self.def_pattern = re.compile(
            r'^(\s*)(?:async\s+)?(?:class|def|function|func|var|const)\s+([a-zA-Z0-9_]+)', 
            re.MULTILINE
        )

    @service_endpoint(
        inputs={"file_path": "str", "max_chars": "int"},
        outputs={"chunks": "List[Dict]"},
        description="Reads a file and breaks it into logical blocks based on indentation.",
        tags=["parsing", "chunking"],
        side_effects=["filesystem:read"]
    )
    def chunk_file(self, file_path: str, max_chars: int = 1500) -> List[Dict[str, Any]]:
        """
        Reads a file and breaks it into logical blocks based on indentation.
        """
        path = Path(file_path)
        try:
            code = path.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return []

        return self._chunk_by_indentation(code, max_chars)

    # ==========================================================================
    # INTERNAL LOGIC
    # ==========================================================================
    def _chunk_by_indentation(self, code: str, max_chars: int) -> List[Dict]:
        lines = code.splitlines()
        chunks = []
        
        current_chunk_lines = []
        current_start_line = 0
        current_indent = 0
        in_block = False
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # 1. Skip empty lines if we aren't in a block
            if not stripped and not in_block:
                continue

            # 2. Calculate Indentation
            indent_match = re.match(r'^(\s*)', line)
            indent_level = len(indent_match.group(1)) if indent_match else 0

            # 3. Check for Block Start (def/class at root level or low indent)
            # We allow indent < 4 spaces to catch top-level stuff or slight nesting
            match = self.def_pattern.match(line)
            is_def = match is not None and indent_level <= 4
            
            # IF we hit a new definition AND we have a chunk pending:
            if is_def and current_chunk_lines:
                # Save previous chunk
                self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                # Reset
                current_chunk_lines = []
                current_start_line = i + 1
                in_block = True
                current_indent = indent_level

            # IF we hit a line with LESS indentation than the current block start,
            # the block has ended. (Python/Yaml logic, mostly holds for C-style too if formatted)
            if in_block and stripped and indent_level <= current_indent and not is_def:
                # Special case: Closing braces '}' often have same indent as start
                if not stripped.startswith('}'):
                    self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                    current_chunk_lines = []
                    current_start_line = i + 1
                    in_block = False

            current_chunk_lines.append(line)

        # Flush remaining
        if current_chunk_lines:
            self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)

        return chunks

    def _finalize_chunk(self, chunks, lines, start_line, max_chars):
        """Recursively splits huge chunks if they exceed max_chars."""
        full_text = "\n".join(lines)
        if not full_text.strip(): return

        # If chunk is too big, split it by lines (naive fallback for massive functions)
        if len(full_text) > max_chars:
            self._split_large_block(chunks, lines, start_line, max_chars)
        else:
            chunks.append({
                "type": "block", # Generic type since we aren't parsing AST
                "text": full_text,
                "start_line": start_line,
                "end_line": start_line + len(lines)
            })

    def _split_large_block(self, chunks, lines, start_line, max_chars):
        """Force split a large block while keeping line boundaries."""
        current_sub = []
        current_len = 0
        sub_start = start_line
        
        for i, line in enumerate(lines):
            if current_len + len(line) > max_chars:
                if current_sub:
                    chunks.append({
                        "type": "fragment",
                        "text": "\n".join(current_sub),
                        "start_line": sub_start,
                        "end_line": sub_start + len(current_sub)
                    })
                current_sub = []
                current_len = 0
                sub_start = start_line + i
            
            current_sub.append(line)
            current_len += len(line)
            
        if current_sub:
            chunks.append({
                "type": "fragment",
                "text": "\n".join(current_sub),
                "start_line": sub_start,
                "end_line": sub_start + len(current_sub)
            })

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    chunker = CodeChunkerMS()
    print(f"Service ready: {chunker}")
    
    # Test Python Code
    py_code = """
import os

def small_helper():
    return True

class DataProcessor:
    def __init__(self):
        self.data = []

    def process(self, raw_input):
        # This is a comment inside the function
        if raw_input:
            self.data.append(raw_input)
        return True
    """
    
    # Write temp file
    with tempfile.NamedTemporaryFile(suffix=".py", mode="w+", delete=False) as tmp:
        tmp.write(py_code)
        tmp_path = tmp.name
        
    try:
        print(f"--- Chunking {tmp_path} (Pure Python) ---")
        chunks = chunker.chunk_file(tmp_path)
        
        for i, c in enumerate(chunks):
            print(f"\n[Chunk {i}] Lines {c['start_line']}-{c['end_line']}")
            print(f"{'-'*20}\n{c['text'].strip()}\n{'-'*20}")
    finally:
        os.remove(tmp_path)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_CodeFormatterMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeFormatterMS
ENTRY_POINT: _CodeFormatterMS.py
DEPENDENCIES: None
"""

import re
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("CodeFormatter")

# ==============================================================================
# LOGIC ENGINE (Extracted from IndentArchitect)
# ==============================================================================

class WhitespaceEngine:
    """
    Parses code into a granular map of (Indent + Content + Trailing).
    Can Normalize structure and generate 'Hunk' patches.
    """
    def __init__(self):
        self.raw_lines = []
        self.nodes = []
        self.normalized_text = ""
        self.patch_data = {"hunks": []}

    def load_source(self, text):
        self.raw_lines = text.splitlines()
        self.nodes = []
        
        # State trackers
        indent_stack = [0] # Stack of indentation widths
        last_line_was_block_starter = False # Did previous line end with ':'?
        
        for i, line in enumerate(self.raw_lines):
            # 1. Parse content
            match = re.match(r"^([ \t]*)(.*?)([ \t]*)$", line)
            if not match:
                self.nodes.append({"id": i, "indent": "", "content": line, "depth": 0, "is_empty": True})
                continue
            
            indent, content, trailing = match.groups()
            is_empty = (len(content) == 0)
            
            # 2. Calculate Raw Width (Tab = 4 spaces)
            current_width = 0
            for char in indent:
                current_width += 4 if char == '\t' else 1

            # 3. Determine Depth
            if is_empty:
                # Empty lines preserve the current context
                depth = len(indent_stack) - 1
            else:
                # --- THE COLON GUARD ---
                # Logic: Should we indent deeper?
                if current_width > indent_stack[-1]:
                    if last_line_was_block_starter:
                        # Legitimate block entry
                        indent_stack.append(current_width)
                    else:
                        # "False Nesting" detected (Staircase Effect). 
                        pass 

                # Logic: Should we dedent?
                while len(indent_stack) > 1 and current_width < indent_stack[-1]:
                    indent_stack.pop()
                
                depth = len(indent_stack) - 1
                
                # Update tracker for NEXT line
                clean_content = content.split("#")[0].strip()
                last_line_was_block_starter = clean_content.endswith(":")

            self.nodes.append({
                "id": i,
                "raw_indent": indent,
                "depth": depth, 
                "content": content,
                "trailing": trailing,
                "is_empty": is_empty
            })

    def normalize(self, use_tabs=False, space_count=4):
        """Reconstructs the code with strict indentation rules."""
        char = "\t" if use_tabs else (" " * space_count)
        clean_lines = []
        
        for node in self.nodes:
            if node["is_empty"]:
                clean_lines.append("") # Strip whitespace on empty lines
            else:
                new_indent = char * node["depth"]
                clean_lines.append(f"{new_indent}{node['content']}")
        
        self.normalized_text = "\n".join(clean_lines)
        return self.normalized_text

    def generate_patch(self):
        """Compares Raw vs Normalized and generates JSON Schema Hunks."""
        clean_lines = self.normalized_text.splitlines()
        if not clean_lines: 
            return {"hunks": []}

        hunks = []
        current_hunk = None
        
        for i, (raw, clean) in enumerate(zip(self.raw_lines, clean_lines)):
            if raw != clean:
                if current_hunk is None:
                    current_hunk = {
                        "start_line": i,
                        "raw_block": [raw],
                        "clean_block": [clean]
                    }
                else:
                    # Check continuity
                    if i == current_hunk["start_line"] + len(current_hunk["raw_block"]):
                        current_hunk["raw_block"].append(raw)
                        current_hunk["clean_block"].append(clean)
                    else:
                        self._finalize_hunk(hunks, current_hunk)
                        current_hunk = {
                            "start_line": i,
                            "raw_block": [raw],
                            "clean_block": [clean]
                        }
            else:
                if current_hunk:
                    self._finalize_hunk(hunks, current_hunk)
                    current_hunk = None

        if current_hunk:
            self._finalize_hunk(hunks, current_hunk)

        self.patch_data = {"hunks": hunks}
        return self.patch_data

    def _finalize_hunk(self, hunks_list, hunk_data):
        search_txt = "\n".join(hunk_data["raw_block"])
        replace_txt = "\n".join(hunk_data["clean_block"])
        schema_hunk = {
            "description": f"Normalize indentation (Lines {hunk_data['start_line']}-{hunk_data['start_line'] + len(hunk_data['raw_block'])})",
            "search_block": search_txt,
            "replace_block": replace_txt
        }
        hunks_list.append(schema_hunk)


# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="CodeFormatter",
    version="1.0.0",
    description="The Architect: Intelligent whitespace normalization and structural repair engine.",
    tags=["formatting", "code", "utility"],
    capabilities=["compute", "filesystem:write"]
)
class CodeFormatterMS:
    """
    The Architect.
    Uses the WhitespaceEngine to enforce strict indentation rules, 
    fixing 'staircase' formatting and mixed tabs/spaces.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"content": "str", "use_tabs": "bool", "spaces": "int"},
        outputs={"normalized": "str", "patch": "Dict"},
        description="Takes raw code and returns the normalized version plus a JSON patch of changes.",
        tags=["formatting", "compute"],
        side_effects=[]
    )
    def normalize_code(self, content: str, use_tabs: bool = False, spaces: int = 4) -> Dict[str, Any]:
        """
        Pure logic endpoint: Takes string, returns string + patch.
        Does not touch the filesystem.
        """
        engine = WhitespaceEngine()
        engine.load_source(content)
        normalized = engine.normalize(use_tabs=use_tabs, space_count=spaces)
        patch = engine.generate_patch()
        
        return {
            "normalized": normalized,
            "patch": patch
        }

    @service_endpoint(
        inputs={"file_path": "str", "use_tabs": "bool", "spaces": "int"},
        outputs={"status": "str", "changes": "int"},
        description="Reads a file, normalizes it, and overwrites it if changes are needed.",
        tags=["formatting", "filesystem"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def format_file(self, file_path: str, use_tabs: bool = False, spaces: int = 4) -> Dict[str, Any]:
        """
        Filesystem endpoint: In-place repair of a file.
        """
        path = Path(file_path).resolve()
        if not path.exists():
            return {"status": "error", "message": "File not found"}
            
        try:
            content = path.read_text(encoding="utf-8")
            
            engine = WhitespaceEngine()
            engine.load_source(content)
            normalized = engine.normalize(use_tabs=use_tabs, space_count=spaces)
            patch = engine.generate_patch()
            
            changes = len(patch["hunks"])
            
            if changes > 0:
                path.write_text(normalized, encoding="utf-8")
                logger.info(f"Formatted {path.name}: {changes} hunks applied.")
                return {"status": "modified", "changes": changes}
            else:
                return {"status": "clean", "changes": 0}
                
        except Exception as e:
            logger.error(f"Formatting failed for {path}: {e}")
            return {"status": "error", "message": str(e)}


# --- Independent Test Block ---
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    svc = CodeFormatterMS()
    print("Service ready:", svc)
    
    # Test with broken indentation
    broken_code = """
def hello():
  print("Indented with 2 spaces")
      print("Suddenly 6 spaces!")
    """
    print("\n--- Processing Broken Code ---")
    result = svc.normalize_code(broken_code, spaces=4)
    
    print(f"Hunks Detected: {len(result['patch']['hunks'])}")
    print("\n--- Normalized Output ---")
    print(result['normalized'])

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_CodeGrapherMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeGrapherMS
ENTRY_POINT: _CodeGrapherMS.py
DEPENDENCIES: None
"""

import ast
import json
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# HELPER: AST VISITOR
# ==============================================================================
class SurgicalVisitor(ast.NodeVisitor):
    """
    Extracts function definitions, calls, and class structures from AST.
    """
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.symbols = []

    def visit_FunctionDef(self, node):
        self._handle_func(node, "function")

    def visit_AsyncFunctionDef(self, node):
        self._handle_func(node, "async_function")

    def visit_ClassDef(self, node):
        # Record the class
        class_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": class_id,
            "file": self.file_path,
            "name": node.name,
            "type": "class",
            "line": node.lineno,
            "calls": [] # Classes don't 'call' things directly usually, their methods do
        })
        # Visit children (methods)
        self.generic_visit(node)

    def _handle_func(self, node, type_name):
        # Extract outgoing calls from the function body
        calls = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.append(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.append(child.func.attr)
        
        unique_calls = list(set(calls))
        
        node_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": node_id,
            "file": self.file_path,
            "name": node.name,
            "type": type_name,
            "line": node.lineno,
            "calls": unique_calls
        })

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="CodeGrapher",
    version="1.0.0",
    description="Parses Python code to extract symbols (nodes) and call relationships (edges).",
    tags=["parsing", "graph", "analysis"],
    capabilities=["filesystem:read"],
    dependencies=["ast", "pathlib"],
    side_effects=["filesystem:read"]
)
class CodeGrapherMS(BaseService):
    """
    The Cartographer of Logic: Parses Python code to extract high-level 
    symbols (classes, functions) and maps their 'Call' relationships.
    
    Output: A graph structure (Nodes + Edges) suitable for visualization 
    or dependency analysis.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("CodeGrapher")
        self.config = config or {}
        self.nodes = [] # List of symbols (functions, classes)
        self.edges = [] # List of relationships (source -> target)

    @service_endpoint(
        inputs={"root_path": "str"},
        outputs={"graph_data": "Dict[str, Any]"},
        description="Recursively scans a directory for .py files and builds the graph.",
        tags=["parsing", "graph"],
        side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Dict[str, Any]:
        """
        Recursively scans a directory for .py files and builds the graph.
        """
        root = Path(root_path).resolve()
        self.nodes = []
        self.edges = []
        
        if not root.exists():
            return {"error": f"Path {root} does not exist"}

        # 1. Parsing Pass (Create Nodes)
        for path in root.rglob("*.py"):
            try:
                # Skip hidden/venv folders
                if any(p.startswith('.') for p in path.parts) or "venv" in path.parts:
                    continue
                    
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    source = f.read()
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_symbols = self._parse_source(source, rel_path)
                self.nodes.extend(file_symbols)
                
            except Exception as e:
                print(f"Failed to parse {path.name}: {e}")

        # 2. Linking Pass (Create Edges)
        self._build_edges()

        return {
            "root": str(root),
            "node_count": len(self.nodes),
            "edge_count": len(self.edges),
            "nodes": self.nodes,
            "edges": self.edges
        }

    # ==========================================================================
    # INTERNAL HELPERS
    # ==========================================================================

    def _parse_source(self, source: str, file_path: str) -> List[Dict]:
        """
        Uses Python's AST to extract surgical symbol info.
        """
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return []

        visitor = SurgicalVisitor(file_path)
        visitor.visit(tree)
        return visitor.symbols

    def _build_edges(self):
        """
        Resolves 'calls' strings into explicit graph edges.
        """
        # Create a quick lookup map: "my_function" -> NodeID
        # Note: This is a naive lookup (name collision possible). 
        # A robust version would use "module.class.method" fully qualified names.
        name_map = {n['name']: n['id'] for n in self.nodes}

        for node in self.nodes:
            source_id = node['id']
            calls = node.get('calls', [])
            
            for target_name in calls:
                if target_name in name_map:
                    target_id = name_map[target_name]
                    
                    # Avoid self-loops for cleanliness
                    if source_id != target_id:
                        self.edges.append({
                            "source": source_id,
                            "target": target_id,
                            "type": "calls"
                        })

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    # Defaults to current directory
    target_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    
    print(f"Mapping Logic in: {target_dir}")
    grapher = CodeGrapherMS()
    print(f"Service Ready: {grapher}")
    
    graph_data = grapher.scan_directory(target_dir)
    
    print(f"\n--- Scan Complete ---")
    print(f"Nodes Found: {graph_data.get('node_count', 0)}")
    print(f"Edges Built: {graph_data.get('edge_count', 0)}")
    
    # Save to JSON for inspection
    out_file = "code_graph_dump.json"
    with open(out_file, "w") as f:
        json.dump(graph_data, f, indent=2)
    print(f"Graph saved to {out_file}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_CodeJanitorMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeJanitorMS
ENTRY_POINT: _CodeJanitorMS.py
DEPENDENCIES: None
"""
import re
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("CodeJanitor")

RENAME_MAP = {
    "__AppShellMS": "_TkinterAppShellMS",
    "_AppShellMS": "_TkinterAppShellMS",
    "__ThemeManagerMS": "_TkinterThemeManagerMS",
    "_ThemeManagerMS": "_TkinterThemeManagerMS",
    "__SmartExplorerMS": "_TkinterSmartExplorerMS",
    "_SmartExplorerMS": "_TkinterSmartExplorerMS",
    "__UniButtonMS": "_TkinterUniButtonMS",
    "_UniButtonMS": "_TkinterUniButtonMS",
}

@service_metadata(
    name="CodeJanitor",
    version="2.2.0",
    description="Fast version: Skips backup, high verbosity.",
    tags=["maintenance"],
    capabilities=["filesystem:write"]
)
class CodeJanitorMS:
    def __init__(self):
        self.root = Path(".").resolve()

    def enforce_standards(self, dry_run: bool = True):
        print(f"--- üßπ JANITOR STARTED in {'DRY RUN' if dry_run else 'LIVE'} MODE ---")
        
        files = list(self.root.glob("*.py"))
        print(f"Found {len(files)} Python files to scan.\n")

        # Regex Patterns
        generic_import = re.compile(r'(from|import)\s+__([A-Z])')
        generic_string = re.compile(r'["\']__([A-Z]\w+MS)["\']')
        entry_point = re.compile(r'ENTRY_POINT:\s*__([A-Z]\w+MS\.py)')

        for file_path in files:
            if file_path.name == "_CodeJanitorMS.py": continue
            
            try:
                original = file_path.read_text(encoding="utf-8")
                new = original
                
                # 1. Fix ENTRY_POINT
                new = entry_point.sub(r'ENTRY_POINT: _\1', new)

                # 2. Specific Renames
                for old_name, new_name in RENAME_MAP.items():
                    pattern = re.compile(rf'\b{old_name}\b')
                    if pattern.search(new):
                        new = pattern.sub(new_name, new)

                # 3. Generic Fixes
                new = generic_import.sub(r'\1 _\2', new)
                new = generic_string.sub(r'"_\1"', new)

                if new != original:
                    print(f"üõ†Ô∏è  PATCHING: {file_path.name}")
                    if not dry_run:
                        file_path.write_text(new, encoding="utf-8")
                else:
                    # Verbose check to prove it's running
                    # print(f"    OK: {file_path.name}") 
                    pass

            except Exception as e:
                print(f"‚ùå ERROR {file_path.name}: {e}")

        print("\n--- üèÅ JANITOR FINISHED ---")

if __name__ == "__main__":
    janitor = CodeJanitorMS()
    # RUN LIVE IMMEDIATELY
    janitor.enforce_standards(dry_run=False)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_CodeJanitorMS_OG.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeJanitorMS
ENTRY_POINT: _CodeJanitorMS.py
DEPENDENCIES: None
"""

import os
import re
import shutil
import datetime
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logger = logging.getLogger("CodeJanitor")

# Files to ignore during mass operations
IGNORE_PATTERNS = {
    r"^\.",                # Hidden files
    r"^__pycache__",       # Python cache
    r"^venv",              # Virtual env
    r".*\.git.*",          # Git
    r".*\.db$",            # Databases
    r"_CodeJanitorMS\.py"  # Don't let the janitor scrub himself while running
}

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="CodeJanitor",
    version="1.0.0",
    description="Automated maintenance service: Enforces file naming conventions, patches imports, and manages backups.",
    tags=["maintenance", "refactoring", "utility"],
    capabilities=["filesystem:write", "filesystem:read"]
)
class CodeJanitorMS:
    """
    The Custodian: Keeps the microservice ecosystem clean and standardized.
    Can rename files, patch source code, and create emergency backups.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.root = Path(self.config.get("root_path", ".")).resolve()

    @service_endpoint(
        inputs={"backup_name": "str"},
        outputs={"archive_path": "str"},
        description="Creates a timestamped ZIP archive of the entire project.",
        tags=["maintenance", "backup"],
        side_effects=["filesystem:write"]
    )
    def create_snapshot(self, backup_name: str = "auto_backup") -> str:
        """Creates a safety snapshot of the codebase."""
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_name = f"{backup_name}_{timestamp}"
        
        # Create a _backups folder if it doesn't exist
        backup_dir = self.root / "_backups"
        backup_dir.mkdir(exist_ok=True)
        
        output_path = backup_dir / archive_name
        
        # We use shutil.make_archive
        # Note: We exclude _backups to prevent recursion if running from root
        def filter_func(path_str):
            p = Path(path_str)
            if "_backups" in p.parts: return False
            if "__pycache__" in p.parts: return False
            if ".git" in p.parts: return False
            return True

        # shutil doesn't have a native filter in older python versions, 
        # so we might just zip the root. 
        # For a robust implementation, we'd manually zip. 
        # Here we use the simple approach:
        
        archive = shutil.make_archive(
            str(output_path), 
            'zip', 
            root_dir=self.root
        )
        
        logger.info(f"‚úÖ Snapshot created: {archive}")
        return str(archive)

    @service_endpoint(
        inputs={"dry_run": "bool"},
        outputs={"renamed": "List[str]", "patched": "List[str]"},
        description="Scans the folder and enforces the '_NameMS.py' single-underscore convention.",
        tags=["maintenance", "refactoring"],
        side_effects=["filesystem:write"]
    )
    def enforce_standards(self, dry_run: bool = True) -> Dict[str, List[str]]:
        """
        The Migration Logic.
        1. Renames __Name.py -> _Name.py
        2. Patches imports (from _Name import) -> (from _Name import)
        """
        renamed_files = []
        patched_files = []
        
        files = [f for f in self.root.glob("*.py")]
        
        # Regex setup
        import_pattern = re.compile(r'(from|import)\s+__([A-Z])')
        string_pattern = re.compile(r'["\']__([A-Z]\w+MS)["\']')

        # 1. Patch Content First
        for file_path in files:
            if file_path.name == "_CodeJanitorMS.py": continue
            
            try:
                original_content = file_path.read_text(encoding="utf-8")
                new_content = original_content
                
                # Fix imports: "from _Auth" -> "from _Auth"
                new_content = import_pattern.sub(r'\1 _\2', new_content)
                # Fix strings: "_AuthMS" -> "_AuthMS"
                new_content = string_pattern.sub(r'"_\1"', new_content)
                
                # Special fix for Registry logic if it exists
                if "ServiceRegistry" in file_path.name:
                    new_content = new_content.replace('item.name.startswith("__")', 'item.name.startswith("_")')

                if new_content != original_content:
                    patched_files.append(file_path.name)
                    if not dry_run:
                        file_path.write_text(new_content, encoding="utf-8")
                        
            except Exception as e:
                logger.error(f"Failed to read {file_path}: {e}")

        # 2. Rename Files
        for file_path in files:
            name = file_path.name
            if name.startswith("__") and len(name) > 2 and name[2].isupper():
                new_name = "_" + name[2:]
                renamed_files.append(f"{name} -> {new_name}")
                
                if not dry_run:
                    try:
                        file_path.rename(self.root / new_name)
                    except OSError as e:
                        logger.error(f"Rename failed for {name}: {e}")

        status = "[DRY RUN] " if dry_run else "[LIVE] "
        logger.info(f"{status}Standards Enforcement Complete.")
        return {
            "renamed": renamed_files,
            "patched": patched_files
        }

    @service_endpoint(
        inputs={"find_pattern": "str", "replace_pattern": "str", "dry_run": "bool"},
        outputs={"affected_files": "List[str]"},
        description="Performs a regex Find & Replace across all Python files in the directory.",
        tags=["refactoring", "utility"],
        side_effects=["filesystem:write"]
    )
    def global_replace(self, find_pattern: str, replace_pattern: str, dry_run: bool = True) -> List[str]:
        """
        Global Search & Replace. 
        Useful if you rename a dependency or change a config key everywhere.
        """
        affected = []
        regex = re.compile(find_pattern)
        
        for file_path in self.root.glob("*.py"):
            if file_path.name == "_CodeJanitorMS.py": continue
            
            try:
                content = file_path.read_text(encoding="utf-8")
                if regex.search(content):
                    affected.append(file_path.name)
                    if not dry_run:
                        new_content = regex.sub(replace_pattern, content)
                        file_path.write_text(new_content, encoding="utf-8")
            except Exception:
                pass
                
        return affected

# --- Independent Test Block ---
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    # We run in Dry Run mode by default to be safe
    janitor = CodeJanitorMS()
    print("Service ready:", janitor)
    
    print("\n--- Running Standards Check (Dry Run) ---")
    report = janitor.enforce_standards(dry_run=True)
    
    print(f"Files to Rename: {len(report['renamed'])}")
    for f in report['renamed']: print(f"  {f}")
    
    print(f"Files to Patch:  {len(report['patched'])}")
    for f in report['patched']: print(f"  {f}")

    # Uncomment to actually create a backup
    # janitor.create_snapshot("pre_migration_backup")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_CognitiveMemoryMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CognitiveMemoryMS
ENTRY_POINT: _CognitiveMemoryMS.py
DEPENDENCIES: pip install pydantic
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util
import sys

REQUIRED = ["pydantic"]
MISSING = []
for lib in REQUIRED:
    if importlib.util.find_spec(lib) is None:
        MISSING.append(lib)

if MISSING:
    print(f"MISSING DEPENDENCIES: {' '.join(MISSING)}")
    print("Please run: pip install pydantic")

import datetime
import json
import logging
import uuid
import os
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional

from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_MEMORY_FILE = Path("working_memory.jsonl")
FLUSH_THRESHOLD = 5  # Number of turns before summarizing to Long Term Memory

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("CognitiveMem")

# ==============================================================================
# DATA MODELS
# ==============================================================================
class MemoryEntry(BaseModel):
    """Atomic unit of memory."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)
    role: str  # 'user', 'assistant', 'system', 'tool'
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="CognitiveMemory",
    version="1.0.0",
    description="Manages Short-Term (Working) Memory and orchestrates flushing to Long-Term Memory.",
    tags=["memory", "history", "context"],
    capabilities=["filesystem:read", "filesystem:write"],
    dependencies=["pydantic"],
    side_effects=["filesystem:write"]
)
class CognitiveMemoryMS(BaseService):
    """
    The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
    flushing to Long-Term Memory (Vector Store).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("CognitiveMemory")
        self.config = config or {}
        self.file_path = Path(self.config.get("persistence_path", DEFAULT_MEMORY_FILE))
        self.summarizer = self.config.get("summarizer_func")
        self.ingestor = self.config.get("long_term_ingest_func")
        
        self.working_memory: List[MemoryEntry] = []
        self._load_working_memory()

    # ==========================================================================
    # WORKING MEMORY OPERATIONS
    # ==========================================================================

    @service_endpoint(
        inputs={"role": "str", "content": "str", "metadata": "Dict"},
        outputs={"entry": "MemoryEntry"},
        description="Adds an item to working memory and persists it.",
        tags=["memory", "write"],
        side_effects=["filesystem:write"]
    )
    def add_entry(self, role: str, content: str, metadata: Dict = None) -> MemoryEntry:
        """Adds an item to working memory and persists it."""
        entry = MemoryEntry(role=role, content=content, metadata=metadata or {})
        self.working_memory.append(entry)
        self._append_to_file(entry)
        log.info(f"Added memory: [{role}] {content[:30]}...")
        return entry

    @service_endpoint(
        inputs={"limit": "int"},
        outputs={"context": "str"},
        description="Returns the most recent conversation history formatted for an LLM.",
        tags=["memory", "read", "llm"],
        side_effects=["filesystem:read"]
    )
    def get_context(self, limit: int = 10) -> str:
        """
        Returns the most recent conversation history formatted for an LLM.
        """
        recent = self.working_memory[-limit:]
        return "\n".join([f"{e.role.upper()}: {e.content}" for e in recent])

    def get_full_history(self) -> List[Dict]:
        """Returns the raw list of memory objects."""
        return [e.dict() for e in self.working_memory]

    # ==========================================================================
    # CONSOLIDATION (The "Sleep" Cycle)
    # ==========================================================================

    @service_endpoint(
        inputs={},
        outputs={},
        description="Signals that a turn is complete; checks if memory flush is needed.",
        tags=["memory", "maintenance"],
        side_effects=["filesystem:write"]
    )
    def commit_turn(self):
        """
        Signal that a "Turn" (User + AI response) is complete.
        Checks if memory is full and triggers a flush if needed.
        """
        if len(self.working_memory) >= FLUSH_THRESHOLD:
            self._flush_to_long_term()

    def _flush_to_long_term(self):
        """
        Compresses working memory into a summary and moves it to Long-Term storage.
        """
        if not self.summarizer or not self.ingestor:
            log.warning("Flush triggered but Summarizer/Ingestor not configured. Skipping.")
            return

        log.info("üåÄ Flushing Working Memory to Long-Term Storage...")
        
        # 1. Combine Text
        full_text = "\n".join([f"{e.role}: {e.content}" for e in self.working_memory])
        
        # 2. Summarize
        try:
            summary = self.summarizer(full_text)
            log.info(f"Summary generated: {summary[:50]}...")
        except Exception as e:
            log.error(f"Summarization failed: {e}")
            return

        # 3. Ingest into Vector DB
        try:
            meta = {
                "source": "cognitive_memory_flush", 
                "date": datetime.datetime.utcnow().isoformat(),
                "original_entry_count": len(self.working_memory)
            }
            self.ingestor(summary, meta)
            log.info("‚úÖ Saved to Long-Term Memory.")
        except Exception as e:
            log.error(f"Ingestion failed: {e}")
            return

        # 4. Clear Working Memory
        # For this pattern, we clear the 'Active' RAM, and rotate the log file.
        self.working_memory.clear()
        self._rotate_log_file()

    # ==========================================================================
    # PERSISTENCE HELPERS
    # ==========================================================================

    def _load_working_memory(self):
        """Rehydrates memory from the JSONL file."""
        if not self.file_path.exists():
            return
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.working_memory.append(MemoryEntry.parse_raw(line))
            log.info(f"Loaded {len(self.working_memory)} items from {self.file_path}")
        except Exception as e:
            log.error(f"Corrupt memory file: {e}")

    def _append_to_file(self, entry: MemoryEntry):
        """Appends a single entry to the JSONL log."""
        with open(self.file_path, 'a', encoding='utf-8') as f:
            f.write(entry.json() + "\n")

    def _rotate_log_file(self):
        """Renames the current log to an archive timestamp."""
        if self.file_path.exists():
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = self.file_path.with_name(f"memory_archive_{timestamp}.jsonl")
            self.file_path.rename(archive_name)
            log.info(f"Rotated memory log to {archive_name}")


# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    # 1. Setup Mock Dependencies
    def mock_summarizer(text):
        return f"SUMMARY OF {len(text)} CHARS: The user and AI discussed AI architecture."

    def mock_ingest(text, metadata):
        print(f"\n[VectorDB] Indexing: '{text}'\n[VectorDB] Meta: {metadata}")

    # 2. Initialize
    print("--- Initializing Cognitive Memory ---")
    mem = CognitiveMemoryMS({
        "summarizer_func": mock_summarizer,
        "long_term_ingest_func": mock_ingest
    })
    print(f"Service ready: {mem}")

    # 3. Simulate Conversation
    print("\n--- Simulating Conversation ---")
    mem.add_entry("user", "Hello, who are you?")
    mem.add_entry("assistant", "I am a Cognitive Agent.")
    mem.add_entry("user", "What is your memory capacity?")
    mem.add_entry("assistant", "I have a tiered memory system.")
    mem.add_entry("user", "That sounds complex.")

    print(f"\nCurrent Context:\n{mem.get_context()}")

    # 4. Trigger Flush (Threshold is 5)
    print("\n--- Triggering Memory Flush ---")
    mem.commit_turn() # Should trigger flush because count is 5

    print(f"\nWorking Memory after flush: {len(mem.working_memory)} items")

    # Cleanup
    if Path("working_memory.jsonl").exists():
        os.remove("working_memory.jsonl")
    # Clean up archives if any were made
    for p in Path(".").glob("memory_archive_*.jsonl"):
        os.remove(p)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ContentExtractorMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ContentExtractorMS
ENTRY_POINT: _ContentExtractorMS.py
DEPENDENCIES: None
"""

import io
import re
import time
from typing import Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# Configuration for the Graph Mapper
@service_metadata(
    name="ContentExtractorMS",
    version="1.0.0",
    description="The Decoder: A specialist service for extracting clean text from complex formats like PDF and HTML.",
    tags=["utility", "extraction", "nlp"],
    capabilities=["pdf-to-text", "html-cleaning"],
    dependencies=["pypdf", "beautifulsoup4"],
    side_effects=["filesystem:read"]
)
class ContentExtractorMS:
    """
    The Decoder.
    A standalone utility microservice that separates the concern of 
    document parsing from ingestion logic.
    """
    
    def __init__(self):
        self.start_time = time.time()
        
        # Lazy load imports to prevent service crash if dependencies are missing
        self._pdf_ready = False
        try:
            from pypdf import PdfReader
            self._pdf_ready = True
        except ImportError:
            pass
            
        self._html_ready = False
        try:
            from bs4 import BeautifulSoup
            self._html_ready = True
        except ImportError:
            pass

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "pdf_support": "bool", "html_support": "bool"},
        description="Health check to verify which extraction backends are installed.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status and library availability."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "pdf_support": self._pdf_ready,
            "html_support": self._html_ready
        }

    @service_endpoint(
        inputs={"blob": "bytes", "mime_type": "str"},
        outputs={"text": "str"},
        description="Unified entry point for text extraction. Routes to the correct parser based on mime_type.",
        tags=["processing", "extraction"]
    )
    def extract_text(self, blob: bytes, mime_type: str) -> str:
        """
        Main routing logic for extraction. 
         logic is internalized here.
        """
        if "pdf" in mime_type.lower():
            return self._extract_pdf(blob)
        elif "html" in mime_type.lower():
            # Decode bytes to string for HTML parser
            try:
                html_content = blob.decode('utf-8', errors='ignore')
                return self._extract_html(html_content)
            except:
                return ""
        return ""

    def _extract_pdf(self, file_bytes: bytes) -> str:
        """Extracts text from a PDF blob using pypdf. [cite: 96-97]"""
        if not self._pdf_ready:
            return ""
        
        from pypdf import PdfReader
        text_content = []
        try:
            stream = io.BytesIO(file_bytes)
            reader = PdfReader(stream)
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text_content.append(extracted)
            return "\n".join(text_content)
        except Exception as e:
            return f"PDF Extraction Error: {e}"

    def _extract_html(self, html_content: str) -> str:
        """Cleans HTML to raw text using BeautifulSoup. [cite: 98-99]"""
        if not self._html_ready:
            return self._strip_tags_regex(html_content)
        
        from bs4 import BeautifulSoup
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for script in soup(["script", "style", "meta", "noscript"]):
                script.decompose()
                
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            return '\n'.join(chunk for chunk in chunks if chunk)
        except Exception:
            return self._strip_tags_regex(html_content)

    def _strip_tags_regex(self, html: str) -> str:
        """Fallback if BS4 is missing. [cite: 100]"""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', html)

if __name__ == "__main__":
    svc = ContentExtractorMS()
    print("Service ready:", svc._service_info["name"])
    print("Health:", svc.get_health())

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ContextAggregatorMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ContextAggregatorMS
ENTRY_POINT: _ContextAggregatorMS.py
DEPENDENCIES: None
"""

import os
import fnmatch
import datetime
import logging
from pathlib import Path
from typing import Set, Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
# Extensions known to be binary/non-text (Images, Archives, Executables)
DEFAULT_BINARY_EXTENSIONS = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".mp4", ".mkv", ".avi", ".mov", ".webm",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".exe", ".dll", ".so",
    ".db", ".sqlite", ".mdb", ".pyc", ".pyo", ".class", ".jar", ".wasm"
}

# Folders to ignore by default
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", ".env", 
    "dist", "build", "coverage", ".idea", ".vscode"
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("ContextAggregator")
# ==============================================================================

@service_metadata(
    name="ContextAggregator",
    version="1.0.0",
    description="Flattens a project folder into a single readable text file.",
    tags=["filesystem", "context", "compilation"],
    capabilities=["filesystem:read", "filesystem:write"],
    dependencies=["os", "fnmatch", "datetime"],
    side_effects=["filesystem:read", "filesystem:write"]
)
class ContextAggregatorMS:
    """
    The Context Builder: Flattens a project folder into a single readable text file.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        max_file_size_mb = self.config.get("max_file_size_mb", 1)
        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024

    @service_endpoint(
        inputs={"root_path": "str", "output_file": "str", "extra_exclusions": "Set[str]", "use_default_exclusions": "bool"},
        outputs={"file_count": "int"},
        description="Aggregates project files into a single text dump.",
        tags=["filesystem", "dump"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def aggregate(self, 
                  root_path: str, 
                  output_file: str, 
                  extra_exclusions: Optional[Set[str]] = None,
                  use_default_exclusions: bool = True) -> int:
        
        project_root = Path(root_path).resolve()
        out_path = Path(output_file).resolve()
        
        # Build Exclusions
        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_IGNORE_DIRS)
        if extra_exclusions:
            exclusions.update(extra_exclusions)

        # Build Binary List
        binary_exts = DEFAULT_BINARY_EXTENSIONS.copy()
        
        file_count = 0
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.write(f"File Dump from Project: {project_root.name}\nGenerated: {timestamp}\n{'='*60}\n\n")

                for root, dirs, files in os.walk(project_root):
                    # In-place filtering of directories
                    dirs[:] = [d for d in dirs if d not in exclusions]
                    
                    for filename in files:
                        if self._should_exclude(filename, exclusions): continue

                        file_path = Path(root) / filename
                        if file_path.resolve() == out_path: continue

                        if self._is_safe_to_dump(file_path, binary_exts):
                            self._write_file_content(out_f, file_path, project_root)
                            file_count += 1                            
        except IOError as e:
            log.error(f"Error writing dump: {e}")
            
        return file_count

    def _should_exclude(self, filename: str, exclusions: Set[str]) -> bool:
        return any(fnmatch.fnmatch(filename, pattern) for pattern in exclusions)

    def _is_safe_to_dump(self, file_path: Path, binary_exts: Set[str]) -> bool:
        if "".join(file_path.suffixes).lower() in binary_exts: return False
        try:
            if file_path.stat().st_size > self.max_file_size_bytes: return False
            with open(file_path, 'rb') as f:
                if b'\0' in f.read(1024): return False
        except (IOError, OSError): return False
        return True

    def _write_file_content(self, out_f, file_path: Path, project_root: Path):
        relative_path = file_path.relative_to(project_root)
        header = f"\n{'-'*20} FILE: {relative_path} {'-'*20}\n"
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as in_f:
                out_f.write(header + in_f.read() + f"\n{'-'*60}\n")
        except Exception as e:
            out_f.write(f"\n[Error reading file: {e}]\n")

if __name__ == "__main__":
    svc = ContextAggregatorMS()
    print("Service ready:", svc)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ContextPackerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ContextPackerMS
ENTRY_POINT: _ContextPackerMS.py
DEPENDENCIES: None
"""

import os
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Set

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    'venv', '.venv', 'dist', 'build', '.DS_Store', 'file-dump.txt'
}

logger = logging.getLogger("ContextPacker")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="ContextPacker",
    version="1.0.0",
    description="Flattens a directory of source code into a single text file (useful for LLM context stuffing).",
    tags=["filesystem", "export", "utility"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ContextPackerMS:
    """
    The Packer: Walks a directory and dumps all text-readable files 
    into a single monolithic text file with delimiters.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"root_path": "str", "output_filename": "str", "additional_excludes": "Set[str]"},
        outputs={"output_path": "str", "file_count": "int"},
        description="Packs directory contents into a single text file.",
        tags=["export", "dump"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def pack_directory(self, 
                       root_path: str, 
                       output_filename: str = "context_dump.txt", 
                       additional_excludes: Optional[Set[str]] = None) -> Dict[str, Any]:
        """
        Walks the directory and writes file contents to the output file.
        """
        root = Path(root_path).resolve()
        output_file = root / output_filename
        
        # Merge excludes
        excludes = DEFAULT_EXCLUDES.copy()
        if additional_excludes:
            excludes.update(additional_excludes)
            
        # Ensure we don't pack the output file itself if it already exists
        excludes.add(output_filename)

        count = 0
        logger.info(f"Packing context from {root} into {output_filename}...")

        try:
            with open(output_file, 'w', encoding='utf-8') as out_f:
                # Add Header
                out_f.write(f"CONTEXT PACKER DUMP\n")
                out_f.write(f"SOURCE: {root}\n")
                out_f.write("="*60 + "\n\n")

                for current_dir, dirs, files in os.walk(root):
                    # In-place modification of dirs to skip excluded folders during walk
                    dirs[:] = [d for d in dirs if d not in excludes and not d.startswith('.')]
                    
                    for file in files:
                        if file in excludes or file.startswith('.'):
                            continue
                            
                        file_path = Path(current_dir) / file
                        
                        # Process the file
                        self._append_file(file_path, root, out_f)
                        count += 1
                        
            return {
                "output_path": str(output_file),
                "file_count": count
            }
            
        except Exception as e:
            logger.error(f"Packing failed: {e}")
            raise

    def _append_file(self, file_path: Path, root: Path, out_f):
        """Helper to append a single file's content to the dump."""
        rel_path = file_path.relative_to(root)
        
        try:
            # Try reading as text
            content = file_path.read_text(encoding='utf-8')
            
            out_f.write(f"==================================================\n")
            out_f.write(f"FILE: {rel_path}\n")
            out_f.write(f"==================================================\n")
            out_f.write(content + "\n\n")
            
        except UnicodeDecodeError:
            # It's a binary file (image, pyc, etc.)
            out_f.write(f"==================================================\n")
            out_f.write(f"FILE: {rel_path} [SKIPPED - BINARY]\n")
            out_f.write(f"==================================================\n\n")
        except Exception as e:
            logger.warning(f"Could not read {rel_path}: {e}")


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    packer = ContextPackerMS()
    print("Service ready:", packer)
    
    # Run a test pack on the current folder
    print("\n--- Packing Current Directory ---")
    result = packer.pack_directory(".", "test_dump.txt")
    print(f"Packed {result['file_count']} files to: {result['output_path']}")

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_DiffEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _DiffEngineMS
ENTRY_POINT: _DiffEngineMS.py
DEPENDENCIES: None
"""

import sqlite3
import difflib
import datetime
import uuid
import logging
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "diff_engine.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("DiffEngine")
# ==============================================================================

@service_metadata(
    name="DiffEngineMS",
    version="1.0.0",
    description="Implements hybrid versioning (Head + Diff History) for file content.",
    tags=["version-control", "diff", "db"],
    capabilities=["db:sqlite", "filesystem:write"],
    dependencies=["sqlite3", "difflib", "uuid", "datetime"],
    side_effects=["db:read", "db:write"]
)
class DiffEngineMS:
    """
    The Timekeeper: Implements a 'Hybrid' versioning architecture.
    1. HEAD: Stores full current content for fast read access (UI/RAG).
    2. HISTORY: Stores diff deltas using difflib for audit trails.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.db_path = Path(self.config.get("db_path", DB_PATH))
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. The Head (Fast Access Cache)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id TEXT PRIMARY KEY,
                    path TEXT UNIQUE NOT NULL,
                    content TEXT,
                    last_updated TIMESTAMP
                )
            """)
            
            # 2. The Rising Edge (Diff History)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS diff_log (
                    id TEXT PRIMARY KEY,
                    file_id TEXT NOT NULL,
                    timestamp TIMESTAMP,
                    change_type TEXT,  -- 'CREATE', 'EDIT', 'DELETE'
                    diff_blob TEXT,    -- The text output of difflib
                    author TEXT,
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)

    # --- Core Workflow ---

    @service_endpoint(
        inputs={"path": "str", "new_content": "str", "author": "str"},
        outputs={"status": "str", "file_id": "str"},
        description="Updates a file, creating a diff history entry and updating the head state.",
        tags=["version-control", "write"],
        side_effects=["db:write"]
    )
    def update_file(self, path: str, new_content: str, author: str = "agent") -> Dict[str, Any]:
        """
        The Atomic Update Operation:
        1. Checks current state.
        2. Calculates Diff.
        3. Writes Diff to History.
        4. Updates Head to New Content.
        """
        path = str(Path(path).as_posix()) # Normalize path
        now = datetime.datetime.utcnow()
        
        with self._get_conn() as conn:
            # 1. Fetch Head
            row = conn.execute("SELECT id, content FROM files WHERE path = ?", (path,)).fetchone()
            
            if not row:
                # --- CASE: NEW FILE ---
                file_id = str(uuid.uuid4())
                conn.execute(
                    "INSERT INTO files (id, path, content, last_updated) VALUES (?, ?, ?, ?)",
                    (file_id, path, new_content, now)
                )
                self._log_diff(conn, file_id, "CREATE", "[New File Created]", author, now)
                log.info(f"Created new file: {path}")
                return {"status": "created", "file_id": file_id}

            # --- CASE: EXISTING FILE ---
            file_id = row['id']
            old_content = row['content'] or ""

            # 2. Calculate Diff
            old_lines = old_content.splitlines(keepends=True)
            new_lines = new_content.splitlines(keepends=True)

            diff_gen = difflib.unified_diff(
                old_lines, new_lines, 
                fromfile=f"a/{path}", tofile=f"b/{path}",
                lineterm=''
            )
            diff_text = "".join(diff_gen)

            if not diff_text:
                return {"status": "unchanged", "file_id": file_id}

            # 3. Write History
            self._log_diff(conn, file_id, "EDIT", diff_text, author, now)

            # 4. Update Head
            conn.execute(
                "UPDATE files SET content = ?, last_updated = ? WHERE id = ?",
                (new_content, now, file_id)
            )
            log.info(f"Updated file: {path}")
            return {"status": "updated", "file_id": file_id, "diff_size": len(diff_text)}

    def _log_diff(self, conn, file_id, change_type, diff_text, author, timestamp):
        diff_id = str(uuid.uuid4())
        conn.execute(
            "INSERT INTO diff_log (id, file_id, timestamp, change_type, diff_blob, author) VALUES (?, ?, ?, ?, ?, ?)",
            (diff_id, file_id, timestamp, change_type, diff_text, author)
        )

    # --- Retrieval ---

    @service_endpoint(
        inputs={"path": "str"},
        outputs={"content": "Optional[str]"},
        description="Fast retrieval of current content.",
        tags=["version-control", "read"],
        side_effects=["db:read"]
    )
    def get_head(self, path: str) -> Optional[str]:
        """Fast retrieval of current content."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT content FROM files WHERE path = ?", (path,)).fetchone()
            return row['content'] if row else None

    @service_endpoint(
        inputs={"path": "str"},
        outputs={"history": "List[Dict]"},
        description="Retrieves the full evolution history of a file.",
        tags=["version-control", "read"],
        side_effects=["db:read"]
    )
    def get_history(self, path: str) -> List[Dict]:
        """Retrieves the full evolution history of a file."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT id FROM files WHERE path = ?", (path,)).fetchone()
            if not row: return []
            
            rows = conn.execute(
                "SELECT timestamp, change_type, diff_blob, author FROM diff_log WHERE file_id = ? ORDER BY timestamp DESC",
                (row['id'],)
            ).fetchall()
            
            return [dict(r) for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    engine = DiffEngineMS()
    print("Service ready:", engine)
    
    print("--- 1. Creating File ---")
    engine.update_file("notes.txt", "Todo List:\n1. Buy Milk\n")
    
    print("\n--- 2. Updating File (The Rising Edge) ---")
    new_text = "Todo List:\n1. Buy Eggs\n2. Code Python\n"
    res = engine.update_file("notes.txt", new_text, author="Jacob")
    
    print(f"Update Result: {res['status']}")
    
    print("\n--- 3. Inspecting History ---")
    history = engine.get_history("notes.txt")
    for event in history:
        print(f"\n[{event['timestamp']}] {event['change_type']} by {event['author']}")
        print(f"Diff Preview:\n{event['diff_blob'].strip()}")

    print("\n--- 4. Inspecting Head (Cache) ---")
    print(engine.get_head("notes.txt"))
    
    if DB_PATH.exists(): os.remove(DB_PATH)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_EnvironmentManagerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _EnvironmentManagerMS
ENTRY_POINT: _EnvironmentManagerMS.py
DEPENDENCIES: None
"""
import os
import sys
import subprocess
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Union

from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("EnvManager")

@service_metadata(
    name="EnvironmentManager",
    version="1.0.0",
    description="Manages Python runtime resolution and process execution.",
    tags=["runtime", "python", "venv", "process"],
    capabilities=["os:shell", "os:process"]
)
class EnvironmentManagerMS:
    """
    The Operator.
    Finds the right Python interpreter (System vs Venv) and launches processes.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"project_path": "str", "config_override": "str"},
        outputs={"interpreter": "str", "source": "str"},
        description="Determines the absolute path to the Python interpreter for a given project.",
        tags=["runtime", "resolve"]
    )
    def resolve_python(self, project_path: str, config_override: Optional[str] = None) -> Dict[str, str]:
        """
        Priority:
        1. Explicit config override
        2. Local .venv
        3. System default (py or sys.executable)
        """
        path = Path(project_path).resolve()

        # 1. Explicit
        if config_override:
            # If relative, resolve against project
            if os.path.sep in config_override or "/" in config_override:
                return {"interpreter": str((path / config_override).resolve()), "source": "explicit"}
            return {"interpreter": config_override, "source": "command"}

        # 2. Local Venv
        # Windows
        win_venv = path / ".venv" / "Scripts" / "python.exe"
        if win_venv.exists(): return {"interpreter": str(win_venv), "source": "venv"}
        
        # Unix
        nix_venv = path / ".venv" / "bin" / "python"
        if nix_venv.exists(): return {"interpreter": str(nix_venv), "source": "venv"}

        # 3. System Fallback
        if os.name == "nt":
            return {"interpreter": "py", "source": "system_launcher"}
        return {"interpreter": sys.executable, "source": "system_default"}

    @service_endpoint(
        inputs={"project_path": "str", "script_rel_path": "str", "env_vars": "Dict"},
        outputs={"pid": "int"},
        description="Launches a python script in a subprocess using the resolved environment.",
        tags=["runtime", "execute"],
        side_effects=["os:process"]
    )
    def launch_script(self, 
                      project_path: str, 
                      script_rel_path: str = "src/app.py", 
                      env_vars: Dict[str, str] = None) -> int:
        
        root = Path(project_path).resolve()
        script = root / script_rel_path
        
        if not script.exists():
            raise FileNotFoundError(f"Script not found: {script}")

        # Resolve Python
        python_info = self.resolve_python(str(root))
        cmd = [python_info["interpreter"], str(script)]
        
        # Prepare Env
        proc_env = os.environ.copy()
        if env_vars:
            proc_env.update(env_vars)

        logger.info(f"Launching {cmd} in {root} via {python_info['source']}")

        # Launch
        if os.name == "nt":
            # New console window on Windows
            proc = subprocess.Popen(
                cmd, 
                cwd=str(root), 
                env=proc_env,
                creationflags=subprocess.CREATE_NEW_CONSOLE
            )
        else:
            proc = subprocess.Popen(cmd, cwd=str(root), env=proc_env)
            
        return proc.pid

if __name__ == "__main__":
    mgr = EnvironmentManagerMS()
    # Self-test: Resolve own environment
    print("Resolved Self:", mgr.resolve_python("."))

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ExplorerWidgetMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ExplorerWidgetMS
ENTRY_POINT: _ExplorerWidgetMS.py
DEPENDENCIES: microservice-std-lib>=1.0.0
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["microservice-std-lib>=1.0.0"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _ExplorerWidgetMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import os
import queue
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional

import tkinter as tk
from tkinter import ttk

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
DEFAULT_EXCLUDED_FOLDERS = {
    "node_modules",
    ".git",
    "__pycache__",
    ".venv",
    ".mypy_cache",
    "_logs",
    "dist",
    "build",
    ".vscode",
    ".idea",
    "target",
    "out",
    "bin",
    "obj",
    "Debug",
    "Release",
    "logs",
}
# ==============================================================================


@service_metadata(
    name="ExplorerWidgetMS",
    version="1.0.0",
    description="A standalone file system tree viewer widget.",
    tags=["ui", "filesystem", "widget"],
    capabilities=["ui:gui", "filesystem:read"],
    dependencies=["tkinter", "ttk"],
    side_effects=["ui:update", "ui:read", "filesystem:read"]
)
class ExplorerWidgetMS(BaseService):
    """
    A standalone file system tree viewer.
    """

    GLYPH_CHECKED = "[X]"
    GLYPH_UNCHECKED = "[ ]"

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("ExplorerWidgetMS")
        self.config_data: Dict[str, Any] = config or {}
        parent = self.config_data.get("parent")
        # Note: UI parent handling remains, but BaseService provides logging/identity.

        self.root_path: Path = Path(
            self.config_data.get("root_path", ".")
        ).resolve()
        self.use_defaults: bool = self.config_data.get(
            "use_default_exclusions", True
        )

        # GUI coordination
        self.gui_queue: queue.Queue = queue.Queue()
        self.folder_item_states: Dict[str, str] = {}
        self.state_lock = threading.RLock()

        self._setup_styles()
        self._build_ui()
        self.process_gui_queue()
        self.refresh_tree()

    # ------------------------------------------------------------------ UI SETUP

    def _setup_styles(self) -> None:
        style = ttk.Style()
        if "clam" in style.theme_names():
            style.theme_use("clam")

        style.configure(
            "Explorer.Treeview",
            background="#252526",
            foreground="lightgray",
            fieldbackground="#252526",
            borderwidth=0,
            font=("Consolas", 10),
        )
        style.map(
            "Explorer.Treeview",
            background=[("selected", "#007ACC")],
            foreground=[("selected", "white")],
        )

    def _build_ui(self) -> None:
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)

        self.tree = ttk.Treeview(
            self,
            show="tree",
            columns=("size",),
            selectmode="none",
            style="Explorer.Treeview",
        )
        self.tree.column("size", width=80, anchor="e")

        ysb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        xsb = ttk.Scrollbar(self, orient="horizontal", command=self.tree.xview)

        self.tree.configure(yscrollcommand=ysb.set, xscrollcommand=xsb.set)

        self.tree.grid(row=0, column=0, sticky="nsew")
        ysb.grid(row=0, column=1, sticky="ns")
        xsb.grid(row=1, column=0, sticky="ew")

        self.tree.bind("<ButtonRelease-1>", self._on_click)

    # ----------------------------------------------------------------- SERVICE API

    @service_endpoint(
        inputs={},
        outputs={},
        description="Rescans the directory and refreshes the tree view.",
        tags=["ui", "refresh"],
        side_effects=["filesystem:read", "ui:update"],
    )
    def refresh_tree(self) -> None:
        # Clear the tree view
        for item in self.tree.get_children():
            self.tree.delete(item)

        # Reset state
        with self.state_lock:
            self.folder_item_states.clear()
            self.folder_item_states[str(self.root_path)] = "checked"

        # Build a flat list of tree items for insertion
        root_id = str(self.root_path)
        tree_data: List[Dict[str, Any]] = [
            {
                "parent": "",
                "iid": root_id,
                "text": f" {self.root_path.name} (Root)",
                "open": True,
            }
        ]

        self._scan_recursive(self.root_path, root_id, tree_data)

        # Insert into Treeview
        for item in tree_data:
            self.tree.insert(
                item["parent"],
                "end",
                iid=item["iid"],
                text=item["text"],
                open=item.get("open", False),
            )
            self.tree.set(item["iid"], "size", "...")

        # Update glyphs
        self._refresh_visuals(root_id)

        # Kick off background size calculation (currently stubbed)
        threading.Thread(
            target=self._calc_sizes_thread,
            args=(root_id,),
            daemon=True,
        ).start()

    # ----------------------------------------------------------------- INTERNALS

    def _scan_recursive(
        self, current_path: Path, parent_id: str, data_list: List[Dict[str, Any]]
    ) -> None:
        try:
            items = sorted(
                current_path.iterdir(),
                key=lambda x: (not x.is_dir(), x.name.lower()),
            )
            for item in items:
                if not item.is_dir():
                    continue

                path_str = str(item.resolve())

                state = "checked"
                if self.use_defaults and item.name in DEFAULT_EXCLUDED_FOLDERS:
                    state = "unchecked"

                with self.state_lock:
                    self.folder_item_states[path_str] = state

                data_list.append(
                    {"parent": parent_id, "iid": path_str, "text": f" {item.name}"}
                )
                self._scan_recursive(item, path_str, data_list)
        except (PermissionError, OSError):
            # Skip directories we can't read
            pass

    def _on_click(self, event: tk.Event) -> None:
        item_id = self.tree.identify_row(event.y)
        if not item_id:
            return

        with self.state_lock:
            curr = self.folder_item_states.get(item_id, "unchecked")
            self.folder_item_states[item_id] = (
                "checked" if curr == "unchecked" else "unchecked"
            )

        self._refresh_visuals(str(self.root_path))

    def _refresh_visuals(self, start_node: str) -> None:
        def _update(node_id: str) -> None:
            if not self.tree.exists(node_id):
                return

            with self.state_lock:
                state = self.folder_item_states.get(node_id, "unchecked")

            glyph = (
                self.GLYPH_CHECKED if state == "checked" else self.GLYPH_UNCHECKED
            )

            name = Path(node_id).name
            if node_id == str(self.root_path):
                name += " (Root)"

            self.tree.item(node_id, text=f"{glyph} {name}")

            for child in self.tree.get_children(node_id):
                _update(child)

        _update(start_node)

    def _calc_sizes_thread(self, root_id: str) -> None:
        """
        Background worker for calculating folder sizes.

        Currently a stub so that the thread exits cleanly without errors.
        You can later extend this to walk the filesystem and push
        size updates via self.gui_queue.
        """
        return

    # ----------------------------------------------------------------- SERVICE API

    @service_endpoint(
        inputs={},
        outputs={"selected_paths": "List[str]"},
        description="Returns a list of currently checked folder paths.",
        tags=["ui", "read"],
        side_effects=["ui:read"],
    )
    def get_selected_paths(self) -> List[str]:
        selected: List[str] = []
        with self.state_lock:
            for path, state in self.folder_item_states.items():
                if state == "checked":
                    selected.append(path)
        return selected

    # ------------------------------------------------------------------ GUI QUEUE

    def process_gui_queue(self) -> None:
        while not self.gui_queue.empty():
            try:
                callback = self.gui_queue.get_nowait()
            except queue.Empty:
                break
            else:
                try:
                    callback()
                except Exception:
                    # In production you might want logging here.
                    pass

        # Schedule next pump
        self.after(100, self.process_gui_queue)


if __name__ == "__main__":
    # Simple harness for manual testing.
    root = tk.Tk()
    root.title("ExplorerWidgetMS Test Harness")

    widget = ExplorerWidgetMS({"parent": root, "root_path": os.getcwd()})
    widget.pack(fill="both", expand=True)

    root.mainloop()


--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_FingerprintScannerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _FingerprintScannerMS
ENTRY_POINT: _FingerprintScannerMS.py
DEPENDENCIES: None
"""

import hashlib
import os
import logging
from pathlib import Path
from typing import Any, Dict, List, Set, Optional, Tuple
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", 
    "dist", "build", "coverage", "target", "out", "bin", "obj",
    "_project_library", "_sandbox", "_logs"
}

DEFAULT_IGNORE_FILES = {
    ".DS_Store", "Thumbs.db", "*.log", "*.tmp", "*.lock"
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("Fingerprint")
# ==============================================================================

@service_metadata(
    name="FingerprintScannerMS",
    version="1.0.0",
    description="Scans a directory tree and generates a deterministic SHA-256 fingerprint.",
    tags=["scanning", "integrity", "hashing"],
    capabilities=["filesystem:read"],
    dependencies=["hashlib", "os"],
    side_effects=["filesystem:read"]
)
class FingerprintScannerMS:
    """
    The Detective: Scans a directory tree and generates a deterministic
    'Fingerprint' (SHA-256 Merkle Root) representing its exact state.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"root_path": "str"},
        outputs={"state": "Dict[str, Any]"},
        description="Scans the project and returns a comprehensive state object (hashes + Merkle root).",
        tags=["scanning", "read"],
        side_effects=["filesystem:read"]
    )
    def scan_project(self, root_path: str) -> Dict[str, Any]:
        """
        Scans the project and returns a comprehensive state object.
        """
        root = Path(root_path).resolve()
        if not root.exists():
            raise FileNotFoundError(f"Path not found: {root}")

        file_map = {}
        
        # 1. Walk and Hash
        for path in sorted(root.rglob("*")):
            if path.is_file():
                if self._should_ignore(path, root):
                    continue
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_hash = self._hash_file(path)
                
                if file_hash:
                    file_map[rel_path] = file_hash

        # 2. Calculate Merkle Root
        sorted_hashes = [file_map[p] for p in sorted(file_map.keys())]
        combined_data = "".join(sorted_hashes).encode('utf-8')
        project_fingerprint = hashlib.sha256(combined_data).hexdigest()

        log.info(f"Scanned {len(file_map)} files. Fingerprint: {project_fingerprint[:8]}...")

        return {
            "root": str(root),
            "project_fingerprint": project_fingerprint,
            "file_hashes": file_map,
            "file_count": len(file_map)
        }

    def _should_ignore(self, path: Path, root: Path) -> bool:
        """Checks path against exclusion lists."""
        try:
            rel_parts = path.relative_to(root).parts
            for part in rel_parts[:-1]: 
                if part in DEFAULT_IGNORE_DIRS: return True
            
            import fnmatch
            name = path.name
            if name in DEFAULT_IGNORE_FILES: return True
            if any(fnmatch.fnmatch(name, pat) for pat in DEFAULT_IGNORE_FILES): return True
                
            return False
        except ValueError:
            return True

    def _hash_file(self, path: Path) -> Optional[str]:
        try:
            content = path.read_bytes()
            return hashlib.sha256(content).hexdigest()
        except (PermissionError, OSError):
            log.warning(f"Could not read/hash: {path}")
            return None

# --- Independent Test Block ---
if __name__ == "__main__":
    import time
    
    test_dir = Path("test_fingerprint_proj")
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
    test_dir.mkdir()
    
    (test_dir / "main.py").write_text("print('hello')")
    
    scanner = FingerprintScannerMS()
    print("Service ready:", scanner)
    
    print("--- Scan 1 (Initial) ---")
    state_1 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 1: {state_1['project_fingerprint']}")
    
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_GitPilotMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _GitPilotMS
ENTRY_POINT: _GitPilotMS.py
DEPENDENCIES: None
"""

import os
import subprocess
import threading
import queue
import time
import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple, Any, Callable, Dict
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
def which(cmd: str) -> Optional[str]:
    for p in os.environ.get("PATH", "").split(os.pathsep):
        f = Path(p) / cmd
        if os.name == 'nt':
            for ext in (".exe", ".cmd", ".bat"): 
                if (f.with_suffix(ext)).exists(): return str(f.with_suffix(ext))
        if f.exists() and os.access(f, os.X_OK): return str(f)
    return None

USE_GH = which("gh") is not None
# ==============================================================================

@dataclass
class GitStatusEntry:
    path: str
    index: str
    workdir: str

@dataclass
class GitStatus:
    repo_path: str
    branch: Optional[str]
    ahead: int
    behind: int
    entries: List[GitStatusEntry]

# --- Backend: The Git Wrapper ---
class GitCLI:
    """
    A robust wrapper around the git command line executable.
    """
    def __init__(self, repo_path: Path):
        self.root = self._resolve_repo_root(repo_path)

    def _run(self, args: List[str], *, cwd: Optional[Path] = None) -> Tuple[str, str]:
        cmd = ["git", *args]
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        proc = subprocess.run(
            cmd,
            cwd=str(cwd or self.root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
            startupinfo=startupinfo
        )
        if proc.returncode != 0:
            raise RuntimeError(proc.stderr.strip() or f"git {' '.join(args)} failed")
        return proc.stdout, proc.stderr

    @staticmethod
    def _resolve_repo_root(path: Path) -> Path:
        path = path.resolve()
        if (path / ".git").exists(): return path
        p = path
        while True:
            if (p / ".git").exists(): return p
            if p.parent == p: break
            p = p.parent
        return path

    def init(self) -> None:
        self._run(["init"])

    def status(self) -> GitStatus:
        try:
            out, _ = self._run(["rev-parse", "--abbrev-ref", "HEAD"])
            branch = out.strip()
        except Exception: branch = None
        
        ahead = behind = 0
        try:
            out, _ = self._run(["rev-list", "--left-right", "--count", "@{upstream}...HEAD"])
            left, right = out.strip().split()
            behind, ahead = int(left), int(right)
        except Exception: pass
        
        out, _ = self._run(["status", "--porcelain=v1"])
        entries = []
        for line in out.splitlines():
            if not line.strip(): continue
            xy = line[:2]
            path = line[3:]
            index, work = xy[0], xy[1]
            entries.append(GitStatusEntry(path=path, index=index, workdir=work))
        return GitStatus(str(self.root), branch, ahead, behind, entries)

    def stage(self, paths: List[str]) -> None:
        if paths: self._run(["add", "--"] + paths)

    def unstage(self, paths: List[str]) -> None:
        if paths: self._run(["reset", "HEAD", "--"] + paths)

    def diff(self, file: Optional[str] = None) -> str:
        args = ["diff"]
        if file: args += ["--", file]
        out, _ = self._run(args)
        return out

    def commit(self, message: str, author_name: str, author_email: str) -> str:
        env = os.environ.copy()
        if author_name: 
            env["GIT_AUTHOR_NAME"] = author_name
            env["GIT_COMMITTER_NAME"] = author_name
        if author_email:
            env["GIT_AUTHOR_EMAIL"] = author_email
            env["GIT_COMMITTER_EMAIL"] = author_email
            
        proc = subprocess.run(
            ["git", "commit", "-m", message], 
            cwd=str(self.root), 
            capture_output=True, 
            text=True, 
            env=env
        )
        if proc.returncode != 0: raise RuntimeError(proc.stderr.strip() or proc.stdout.strip())
        out, _ = self._run(["rev-parse", "HEAD"])
        return out.strip()

    def log(self, limit: int = 100) -> List[Tuple[str, str, str, int]]:
        fmt = "%H%x1f%s%x1f%an%x1f%at"
        try:
            out, _ = self._run(["log", f"-n{limit}", f"--pretty=format:{fmt}"])
            items = []
            for line in out.splitlines():
                commit, summary, author, at = line.split("\x1f")
                items.append((commit, summary, author, int(at)))
            return items
        except Exception: return []

    def branches(self) -> List[Tuple[str, bool]]:
        try:
            out, _ = self._run(["branch"])
            res = []
            for line in out.splitlines():
                is_head = line.strip().startswith("*")
                name = line.replace("*", "", 1).strip()
                res.append((name, is_head))
            return res
        except Exception: return []

    def checkout(self, name: str, create: bool = False) -> None:
        if create: self._run(["checkout", "-B", name])
        else: self._run(["checkout", name])

    def push(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        args = ["push", remote]
        if branch: args.append(branch)
        out, _ = self._run(args)
        return out

    def pull(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        if branch: out, _ = self._run(["pull", remote, branch])
        else: out, _ = self._run(["pull", remote])
        return out

# --- Threading Helper ---
class Worker:
    def __init__(self, ui_callback):
        self.q = queue.Queue()
        self.ui_callback = ui_callback
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()

    def submit(self, op: str, func, *args, **kwargs):
        self.q.put((op, func, args, kwargs))

    def _loop(self):
        while True:
            op, func, args, kwargs = self.q.get()
            try:
                result = op, True, func(*args, **kwargs)
            except Exception as e:
                result = op, False, e
            finally:
                self.ui_callback(result)

# --- Frontend: The GUI Panel ---
@service_metadata(
    name="GitPilotMS",
    version="1.0.0",
    description="A Tkinter GUI panel for Git operations (Stage, Commit, Push, Pull).",
    tags=["ui", "git", "version-control", "widget"],
    capabilities=["ui:gui", "filesystem:read", "filesystem:write", "network:outbound"],
    dependencies=["git", "subprocess", "tkinter"],
    side_effects=["filesystem:read", "filesystem:write", "network:outbound", "ui:update"]
)
class GitPilotMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        ttk.Frame.__init__(self, parent)

        initial_path = self.config.get("initial_path")
        self.repo_path = None
        self.git = None
        self.worker = Worker(self._on_worker_done)

        self._build_ui()
        if initial_path:
            self.set_repo(initial_path)

    @service_endpoint(
        inputs={"path": "Path"},
        outputs={},
        description="Sets the active repository path and refreshes status.",
        tags=["git", "config"],
        side_effects=["filesystem:read", "ui:update"]
    )
    def set_repo(self, path: Path):
        try:
            self.git = GitCLI(path)
            self.repo_path = self.git.root
            self.path_var.set(f"Repo: {self.repo_path}")
            self._refresh()
        except Exception as e:
            self.path_var.set(f"Error: {e}")

    def _build_ui(self):
        self.columnconfigure(0, weight=1)
        self.rowconfigure(1, weight=1)

        # Status Bar
        bar = ttk.Frame(self)
        bar.grid(row=0, column=0, sticky="ew")
        self.path_var = tk.StringVar(value="No Repo Selected")
        self.busy_var = tk.StringVar()
        ttk.Label(bar, textvariable=self.path_var).pack(side="left", padx=5)
        ttk.Label(bar, textvariable=self.busy_var, foreground="blue").pack(side="right", padx=5)

        # Tabs
        self.nb = ttk.Notebook(self)
        self.nb.grid(row=1, column=0, sticky="nsew")
        
        self.tab_changes = self._build_changes_tab(self.nb)
        self.tab_log = self._build_log_tab(self.nb)
        
        self.nb.add(self.tab_changes, text="Changes")
        self.nb.add(self.tab_log, text="History")

    def _build_changes_tab(self, parent):
        frame = ttk.Frame(parent)
        paned = ttk.PanedWindow(frame, orient=tk.VERTICAL)
        paned.pack(fill="both", expand=True)

        # File List
        top = ttk.Frame(paned)
        top.rowconfigure(1, weight=1)
        top.columnconfigure(0, weight=1)
        
        # Toolbar
        tb = ttk.Frame(top)
        tb.grid(row=0, column=0, sticky="ew")
        ttk.Button(tb, text="Refresh", command=self._refresh).pack(side="left")
        ttk.Button(tb, text="Stage", command=self._stage).pack(side="left")
        ttk.Button(tb, text="Unstage", command=self._unstage).pack(side="left")
        ttk.Button(tb, text="Diff", command=self._show_diff).pack(side="left")
        ttk.Button(tb, text="Push", command=self._push).pack(side="left", padx=10)
        ttk.Button(tb, text="Pull", command=self._pull).pack(side="left")

        # Treeview
        self.tree = ttk.Treeview(top, columns=("path", "idx", "wd"), show="headings", selectmode="extended")
        self.tree.heading("path", text="Path")
        self.tree.heading("idx", text="Index")
        self.tree.heading("wd", text="Workdir")
        self.tree.column("path", width=400)
        self.tree.column("idx", width=50, anchor="center")
        self.tree.column("wd", width=50, anchor="center")
        self.tree.grid(row=1, column=0, sticky="nsew")
        
        paned.add(top, weight=3)

        # Commit Area
        bot = ttk.Frame(paned)
        bot.columnconfigure(1, weight=1)
        ttk.Label(bot, text="Message:").grid(row=0, column=0, sticky="nw")
        self.msg_text = tk.Text(bot, height=4)
        self.msg_text.grid(row=0, column=1, sticky="nsew")
        ttk.Button(bot, text="Commit", command=self._commit).grid(row=1, column=1, sticky="e", pady=5)
        
        paned.add(bot, weight=1)
        return frame

    def _build_log_tab(self, parent):
        frame = ttk.Frame(parent)
        self.log_tree = ttk.Treeview(frame, columns=("sha", "msg", "auth", "time"), show="headings")
        self.log_tree.heading("sha", text="SHA")
        self.log_tree.heading("msg", text="Message")
        self.log_tree.heading("auth", text="Author")
        self.log_tree.heading("time", text="Time")
        self.log_tree.column("sha", width=80)
        self.log_tree.column("msg", width=400)
        self.log_tree.pack(fill="both", expand=True)
        return frame

    # --- Actions ---

    def _submit(self, label, func, *args):
        self.busy_var.set(f"{label}...")
        self.worker.submit(label, func, *args)

    def _on_worker_done(self, result):
        self.after(0, self._handle_result, result)

    def _handle_result(self, result):
        label, ok, data = result
        self.busy_var.set("")
        if not ok:
            messagebox.showerror("Error", str(data))
            return
        
        if label == "refresh":
            status, logs = data
            self.tree.delete(*self.tree.get_children())
            for e in status.entries:
                self.tree.insert("", "end", values=(e.path, e.index, e.workdir))
            
            self.log_tree.delete(*self.log_tree.get_children())
            for sha, msg, auth, ts in logs:
                t_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(ts))
                self.log_tree.insert("", "end", values=(sha[:7], msg, auth, t_str))
        
        if label == "diff":
            top = tk.Toplevel(self)
            top.title("Diff")
            txt = tk.Text(top, font=("Consolas", 10))
            txt.pack(fill="both", expand=True)
            txt.insert("1.0", data)

        if label in ["stage", "unstage", "commit", "push", "pull"]:
            self._refresh()

    def _refresh(self):
        if not self.git: return
        self._submit("refresh", lambda: (self.git.status(), self.git.log()))

    def _get_selection(self):
        return [self.tree.item(i)['values'][0] for i in self.tree.selection()]

    def _stage(self):
        paths = self._get_selection()
        if paths: self._submit("stage", self.git.stage, paths)

    def _unstage(self):
        paths = self._get_selection()
        if paths: self._submit("unstage", self.git.unstage, paths)

    def _commit(self):
        msg = self.msg_text.get("1.0", "end").strip()
        if not msg: return
        self._submit("commit", self.git.commit, msg, "GitPilot", "pilot@local")
        self.msg_text.delete("1.0", "end")

    def _push(self):
        self._submit("push", self.git.push)

    def _pull(self):
        self._submit("pull", self.git.pull)

    def _show_diff(self):
        sel = self._get_selection()
        file = sel[0] if sel else None
        self._submit("diff", self.git.diff, file)

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Git Pilot Test")
    root.geometry("800x600")
    
    # Use current directory
    cwd = Path(os.getcwd())
    
    panel = GitPilotMS({"parent": root, "initial_path": cwd})
    print("Service ready:", panel)
    panel.pack(fill="both", expand=True)
    
    root.mainloop()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_HeuristicSumMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _HeuristicSumMS
ENTRY_POINT: _HeuristicSumMS.py
DEPENDENCIES: None
"""

import os
import re
from typing import Any, Dict, List, Optional

from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# CONFIGURATION: REGEX PATTERNS
# ==============================================================================
# Captures: def my_func, class MyClass, function myFunc, interface MyInterface
SIG_RE = re.compile(r'^\s*(def|class|function|interface|struct|impl|func)\s+([A-Za-z_][A-Za-z0-9_]*)')

# Captures: # Heading, ## Subheading
MD_HDR_RE = re.compile(r'^\s{0,3}(#{1,3})\s+(.+)')

# Captures: """ Docstring """ or ''' Docstring ''' (Start of block)
DOC_RE = re.compile(r'^\s*("{3}|\'{3})(.*)', re.DOTALL)

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="HeuristicSum",
    version="1.0.0",
    description="Generates quick summaries of code/text files using regex heuristics (No AI).",
    tags=["parsing", "summary", "heuristics"],
    capabilities=["compute"],
    dependencies=["re"],
    side_effects=[]
)
class HeuristicSumMS(BaseService):
    """
    The Skimmer: Generates quick summaries of code/text files without AI.
    Scans for high-value lines (headers, signatures, docstrings) and concatenates them.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("HeuristicSum")
        self.config = config or {}

    @service_endpoint(
        inputs={"text": "str", "filename": "str", "max_chars": "int"},
        outputs={"summary": "str"},
        description="Generates a summary string from the provided text.",
        tags=["summary", "parsing"]
    )
    def summarize(self, text: str, filename: str = "", max_chars: int = 480) -> str:
        """
        Generates a summary string from the provided text.
        """
        lines = text.splitlines()
        picks = []

        # 1. Scan top 20 lines for Markdown Headers
        for ln in lines[:20]:
            m = MD_HDR_RE.match(ln)
            if m:
                picks.append(f"Heading: {m.group(2).strip()}")

        # 2. Scan top 40 lines for Code Signatures (Functions/Classes)
        for ln in lines[:40]:
            m = SIG_RE.match(ln)
            if m:
                picks.append(f"{m.group(1)} {m.group(2)}")

        # 3. Check for Docstrings / Preamble
        if lines:
            # Join first 80 lines to check for multi-line docstrings
            joined = "\n".join(lines[:80])
            m = DOC_RE.match(joined)
            if m:
                # Grab the first few lines of the docstring content
                after = joined.splitlines()[1:3]
                if after:
                    clean_doc = " ".join(s.strip() for s in after).strip()
                    picks.append(f"Doc: {clean_doc}")

        # 4. Fallback: First non-empty line if nothing else found
        if not picks:
            head = " ".join(l.strip() for l in lines[:2] if l.strip())
            if head:
                picks.append(head)

        # 5. Add Filename Context
        if filename:
            picks.append(f"[{os.path.basename(filename)}]")

        # 6. Deduplicate and Format
        seen = set()
        uniq = []
        for p in picks:
            if p and p not in seen:
                uniq.append(p)
                seen.add(p)

        summary = " | ".join(uniq)
        
        # 7. Truncate
        if len(summary) > max_chars:
            summary = summary[:max_chars-3] + "..."
            
        return summary.strip() if summary else "[No summary available]"

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    skimmer = HeuristicSumMS()
    print(f"Service ready: {skimmer}")
    
    # Test 1: Python Code
    py_code = """
    class DataProcessor:
        '''
        Handles the transformation of raw input data into structured formats.
        '''
        def process(self, data):
            pass
    """
    print(f"Python Summary: {skimmer.summarize(py_code, 'processor.py')}")

    # Test 2: Markdown
    md_text = """
    # Project Roadmap
    ## Phase 1
    We begin with ingestion.
    """
    print(f"Markdown Summary: {skimmer.summarize(md_text, 'README.md')}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_IngestEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IngestEngineMS
ENTRY_POINT: _IngestEngineMS.py
DEPENDENCIES: requests
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util
import sys

REQUIRED = ["requests"]
MISSING = []
for lib in REQUIRED:
    if importlib.util.find_spec(lib) is None:
        MISSING.append(lib)

if MISSING:
    print(f"MISSING DEPENDENCIES: {' '.join(MISSING)}")
    print("Please run: pip install requests")

import json
import os
import re
import sqlite3
import time
from dataclasses import dataclass
from typing import Any, Dict, Generator, List, Optional

import requests
from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass
class IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

# ==============================================================================
# HELPER: SYNAPSE WEAVER
# ==============================================================================
class SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    Used to generate the 'DEPENDS_ON' edges in the Knowledge Graph.
    """
    def __init__(self):
        # Python: "from x import y", "import x"
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        # JS/TS: "import ... from 'x'", "require('x')"
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        ext = os.path.splitext(file_path)[1].lower()
        
        lines = content.split('\n')
        for line in lines:
            match = None
            if ext == '.py':
                match = self.py_pattern.match(line)
            elif ext in ['.js', '.ts', '.tsx', '.jsx']:
                match = self.js_pattern.search(line)
            
            if match:
                # Clean up the module name (e.g., "backend.database" -> "database")
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        
        return dependencies

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="IngestEngine",
    version="1.0.0",
    description="Reads files, chunks text, fetches embeddings, and weaves graph edges.",
    tags=["ingest", "rag", "parsing", "embedding"],
    capabilities=["filesystem:read", "network:outbound", "db:sqlite"],
    dependencies=["requests", "sqlite3"],
    side_effects=["db:write", "network:outbound"]
)
class IngestEngineMS(BaseService):
    """
    The Heavy Lifter: Reads files, chunks text, fetches embeddings,
    populates the Graph Nodes, and weaves Graph Edges.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("IngestEngine")
        self.config = config or {}
        self.db_path = self.config.get("db_path", "knowledge.db")
        self.stop_signal = False
        self.weaver = SynapseWeaver()
        self._init_db()

    def _init_db(self):
        """Ensures the target database has the required schema."""
        conn = sqlite3.connect(self.db_path)
        conn.execute("CREATE TABLE IF NOT EXISTS files (id INTEGER PRIMARY KEY, path TEXT, last_updated REAL)")
        conn.execute("CREATE TABLE IF NOT EXISTS chunks (id INTEGER PRIMARY KEY, file_id INT, chunk_index INT, content TEXT, embedding BLOB)")
        conn.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
        conn.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)")
        conn.close()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    @service_endpoint(
        inputs={"file_paths": "List[str]", "model_name": "str"},
        outputs={"status": "IngestStatus"},
        description="Processes a list of files, ingesting them into the knowledge graph.",
        tags=["ingest", "processing"],
        mode="generator",
        side_effects=["db:write", "network:outbound"]
    )
    def process_files(self, file_paths: List[str], model_name: str = "none") -> Generator[IngestStatus, None, None]:
        total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Optimization settings
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # Memory for graph weaving (Node Name -> Node ID)
        node_registry = {}
        file_contents = {} # Cache content for the weaving pass

        # --- PHASE 1: INGESTION (Files, Chunks, Nodes) ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)

            # 1. Read
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                file_contents[filename] = content # Cache for Phase 2
            except Exception as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"Error: {e}")
                continue

            # 2. Track File
            try:
                cursor.execute("INSERT OR REPLACE INTO files (path, last_updated) VALUES (?, ?)", 
                              (file_path, time.time()))
                file_id = cursor.lastrowid
            except sqlite3.Error:
                continue

            # 3. Create Graph Node (for Visualization)
            # We use the filename as the unique ID for the graph to make linking easier
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (filename, 'file', filename, json.dumps({"path": file_path})))
            
            node_registry[filename] = filename

            # 4. Chunking & Embedding
            chunks = self._chunk_text(content)
            
            for i, chunk_text in enumerate(chunks):
                if self.stop_signal: break
                
                embedding = None
                if model_name != "none":
                    embedding = self._get_embedding(model_name, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                    INSERT INTO chunks (file_id, chunk_index, content, embedding)
                    VALUES (?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": filename,
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            # Checkpoint per file
            conn.commit()

        # --- PHASE 2: WEAVING (Edges) ---
        yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
        
        edge_count = 0
        for filename, content in file_contents.items():
            if self.stop_signal: break
            
            # Find imports
            deps = self.weaver.extract_dependencies(content, filename)
            
            for dep in deps:
                # Naive matching: if 'database' is imported, look for 'database.py' or 'database.ts'
                # in our registry.
                target_id = None
                for potential_match in node_registry.keys():
                    if potential_match.startswith(dep + '.') or potential_match == dep:
                        target_id = potential_match
                        break
                
                if target_id and target_id != filename:
                    try:
                        cursor.execute("""
                            INSERT OR IGNORE INTO graph_edges (source, target, weight)
                            VALUES (?, ?, 1.0)
                        """, (filename, target_id))
                        edge_count += 1
                    except:
                        pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. Created {edge_count} dependency edges."
        )

    # --- Internal Helpers ---

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        if len(text) < chunk_size: return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += (chunk_size - overlap)
        return chunks

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": model, "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    TEST_DB = "test_ingest_v2.db"
    
    # Init Engine (Schema created automatically in __init__)
    engine = IngestEngineMS({"db_path": TEST_DB})
    print(f"Service Ready: {engine}")

    # Self-ingest to test dependency parsing
    # Note: Ensure this file actually exists in the run directory or change filename
    target_file = "__IngestEngineMS.py"
    if not os.path.exists(target_file):
        # Create a dummy file if running in a temp environment without self
        with open(target_file, "w") as f:
            f.write("import os\nimport json\nprint('Hello World')")

    print(f"Running Ingest on {target_file}...")
    
    files = [target_file] 
    for status in engine.process_files(files, "none"):
        print(f"[{status.progress_percent:.0f}%] {status.log_message}")
    
    # Verify Edges
    conn = sqlite3.connect(TEST_DB)
    edges = conn.execute("SELECT * FROM graph_edges").fetchall()
    nodes = conn.execute("SELECT * FROM graph_nodes").fetchall()
    print(f"\nResult: {len(nodes)} Nodes, {len(edges)} Edges.")
    conn.close()
    
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)
    if os.path.exists(target_file) and "Hello World" in open(target_file).read():
        os.remove(target_file)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_IntakeServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IntakeServiceMS
ENTRY_POINT: _IntakeServiceMS.py
DEPENDENCIES: None
"""

import os
import mimetypes
import requests
import fnmatch
import json
from pathlib import Path
from typing import Dict, Set, List, Any
from base_service import BaseService
from _CartridgeServiceMS import CartridgeServiceMS
from _ScannerMS import ScannerMS
import document_utils
from microservice_std_lib import service_metadata, service_endpoint

# Optional import for Web
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

@service_metadata(
    name="IntakeServiceMS",
    version="1.2.0",
    description="The Vacuum: Handles two-phase ingestion by scanning sources and processing selected paths into the cartridge.",
    tags=["ingestion", "scanner", "vfs"],
    capabilities=["filesystem:read", "web:crawl"],
    dependencies=["bs4", "requests"],
    side_effects=["filesystem:read", "cartridge:write"]
)
class IntakeServiceMS(BaseService):
    """
    The Vacuum. 
    Now supports two-phase ingestion:
    1. Scan -> Build Tree (with .gitignore respect)
    2. Ingest -> Process selected paths
    """

    DEFAULT_IGNORE_DIRS = {
        '.git', '__pycache__', 'node_modules', 'venv', '.venv', 'env', '.env', 
        '.idea', '.vscode', 'dist', 'build', 'target', 'bin', 'obj', 
        '__cartridge__'
    }
    
    DEFAULT_IGNORE_EXTS = {
        '.pyc', '.pyd', '.exe', '.dll', '.so', '.db', '.sqlite', '.sqlite3', 
        '.bin', '.iso', '.img', '.zip', '.tar', '.gz', '.7z', '.jpg', '.png'
    }


    def __init__(self, cartridge: CartridgeService):
        super().__init__("IntakeServiceMS")
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "cartridge_connected": "bool"},
        description="Standardized health check to verify service status and cartridge connectivity.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the IntakeServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "cartridge_connected": self.cartridge is not None
        }
        self.cartridge = cartridge
        self.ignore_patterns: Set[str] = set()

    def ingest_source(self, source_path: str) -> Dict[str, int]:
        """Headless/CLI Entry point: Scans and Ingests in one go."""
        self.cartridge.initialize_manifest()
        
        # Update Manifest source info
        self.cartridge.set_manifest("source_root", source_path)
        
        is_web = source_path.startswith("http")
        self.cartridge.set_manifest("source_type", "web_root" if is_web else "filesystem_dir")

        scanner = ScoutMS()
        tree_node = scanner.scan_directory(source_path, web_depth=1 if is_web else 0)
        
        if not tree_node:
             return {"error": "Source not found"}

        # Flatten tree to list of paths
        files_to_ingest = scanner.flatten_tree(tree_node)
        self.cartridge.set_manifest("ingest_config", {"auto_flattened": True, "count": len(files_to_ingest)})
        
        return self.ingest_selected(files_to_ingest, source_path)

    # --- PHASE 1: SCANNING ---

    @service_endpoint(
        inputs={"root_path": "str", "web_depth": "int"},
        outputs={"tree": "dict"},
        description="Scans a local directory or URL to build a hierarchical tree structure of available files.",
        tags=["scan", "discovery"]
    )
    def scan_path(self, root_path: str, web_depth: int = 0) -> Dict[str, Any]:
        """
        Unified Scanner Interface.
        Delegates to ScoutMS for both Web and Local FS to ensure consistent node structure.
        """
        scanner = ScannerMS()

        # 1. Delegate to Scanner
        tree_root = scanner.scan_directory(root_path, web_depth=web_depth)
        if not tree_root: return None

        # 2. Apply Persistence / Checked State
        # (We only do this for FS usually, but we can try for web if we had it)
        if not root_path.startswith("http"):
            saved_config = self._load_persistence(os.path.abspath(root_path))
            self._apply_persistence(tree_root, saved_config)
    
        return tree_root

    def _apply_persistence(self, node: Dict, saved_config: Dict):
        """Recursively applies checked state from saved config."""
        if 'rel_path' in node and node['rel_path'] in saved_config:
            node['checked'] = saved_config[node['rel_path']]
        elif 'children' in node:
            # Default check all if no config? Or check logic from before?
            pass
    
        if 'children' in node:
            for child in node['children']:
                self._apply_persistence(child, saved_config)

    def _scan_recursive(self, current_path: str, root_path: str, saved_config: Dict) -> Dict:
        name = os.path.basename(current_path)
        is_dir = os.path.isdir(current_path)
        rel_path = os.path.relpath(current_path, root_path).replace("\\", "/")
        
        node = {
            'name': name,
            'path': current_path,
            'rel_path': rel_path,
            'type': 'dir' if is_dir else 'file',
            'children': [],
            'checked': True
        }

        # Determine Check State
        if saved_config and rel_path in saved_config:
            # Respect user persistence
            node['checked'] = saved_config[rel_path]
        elif self._is_ignored(name) or (not is_dir and self._is_binary_ext(name)):
            # Default to unchecked if ignored
            node['checked'] = False

        if is_dir:
            try:
                with os.scandir(current_path) as it:
                    entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                    for entry in entries:
                        child = self._scan_recursive(entry.path, root_path, saved_config)
                        node['children'].append(child)
            except PermissionError:
                pass
        
        return node

    # --- PHASE 2: INGESTION ---

    @service_endpoint(
        inputs={"file_list": "list", "root_path": "str"},
        outputs={"stats": "dict"},
        description="Processes a specific list of files into the cartridge storage, handling text extraction and VFS indexing.",
        tags=["ingest", "write"],
        side_effects=["cartridge:write"]
    )
    def ingest_selected(self, file_list: List[str], root_path: str) -> Dict[str, int]:
        """Ingests only the specific files passed in the list."""
        stats = {"added": 0, "skipped": 0, "errors": 0}
        
        for file_path in file_list:
            try:
                # Calculate VFS Path
                try:
                    vfs_path = os.path.relpath(file_path, root_path).replace("\\", "/")
                except ValueError:
                    vfs_path = os.path.basename(file_path)

                self._read_and_store(Path(file_path), vfs_path, "filesystem", stats)
            except Exception as e:
                self.log_error(f"Error ingesting {file_path}: {e}")
                stats["errors"] += 1
        
        # --- POST-INGESTION: Update Manifest ---
        self._rebuild_directory_index()
        
        return stats

    def _rebuild_directory_index(self):
        """
        Scans 'files' table and populates 'directories' table.
        This creates the navigable VFS structure.
        """
        self.log_info("Rebuilding VFS Directory Index...")
        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            seen_dirs = set()
            
            for r in rows:
                path = r[0]
                # Walk up the path to register all parents
                current = os.path.dirname(path).replace("\\", "/")
                while current and current != "." and current not in seen_dirs:
                    self.cartridge.ensure_directory(current)
                    seen_dirs.add(current)
                    current = os.path.dirname(current).replace("\\", "/")
            
        except Exception as e:
            self.log_error(f"Directory Index Error: {e}")
        finally:
            conn.close()

    # --- HELPERS ---

    def _load_persistence(self, root_path: str) -> Dict[str, bool]:
        """Loads config from DB Manifest (Portable) or fallback to local."""
        # 1. Try DB Manifest
        try:
            conn = self.cartridge._get_conn()
            row = conn.execute("SELECT value FROM manifest WHERE key='ingest_config'").fetchone()
            conn.close()
            if row:
                return json.loads(row[0])
        except: pass
        
        # 2. Fallback to local (Legacy)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        if os.path.exists(cfg_path):
            try:
                with open(cfg_path, 'r') as f: return json.load(f)
            except: pass
        return {}

    def save_persistence(self, root_path: str, checked_map: Dict[str, bool]):
        """Saves user selections into the Cartridge Manifest (Portable)."""
        # 1. Save to DB
        try:
            conn = self.cartridge._get_conn()
            conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", 
                         ("ingest_config", json.dumps(checked_map)))
            conn.commit()
            conn.close()
        except Exception as e:
            self.log_error(f"Failed to save persistence to DB: {e}")

        # 2. Save local backup (Optional, keeps scan state if DB is deleted)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        try:
            with open(cfg_path, 'w') as f: json.dump(checked_map, f, indent=2)
        except: pass

    def _load_gitignore(self, root_path: str):
        gitignore_path = os.path.join(root_path, '.gitignore')
        self.ignore_patterns = self.DEFAULT_IGNORE_DIRS.copy()
        if os.path.exists(gitignore_path):
            try:
                with open(gitignore_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            if line.endswith('/'): line = line[:-1]
                            self.ignore_patterns.add(line)
            except: pass

    def _is_ignored(self, name: str) -> bool:
        if name in self.ignore_patterns: return True
        for pattern in self.ignore_patterns:
            if fnmatch.fnmatch(name, pattern): return True
        return False

    def _is_binary_ext(self, name: str) -> bool:
        _, ext = os.path.splitext(name)
        return ext.lower() in self.DEFAULT_IGNORE_EXTS

    def _read_and_store(self, real_path: Path, vfs_path: str, origin_type: str, stats: Dict):
        mime_type, _ = mimetypes.guess_type(real_path)
        if not mime_type: mime_type = "application/octet-stream"
        
        content = None
        blob = None
        
        # 1. Try Binary Read First (Covers PDF/Images/Safe Read)
        try:
            with open(real_path, 'rb') as f:
                blob = f.read()
        except Exception as e:
            self.log_error(f"Read error {real_path}: {e}")
            stats["errors"] += 1
            return

        # 2. Text Extraction / Decoding Strategy
        lower_path = str(real_path).lower()
        
        if lower_path.endswith(".pdf"):
            # PDF: Extract text, keep blob
            content = document_utils.extract_text_from_pdf(blob)
            if not content: mime_type = "application/pdf" # Fallback if extraction fails
            
        elif lower_path.endswith(".html") or lower_path.endswith(".htm"):
            # HTML: Decode and Clean
            try:
                raw_text = blob.decode('utf-8', errors='ignore')
                content = document_utils.extract_text_from_html(raw_text)
            except: pass
            
        else:
            # Default: Try UTF-8 Decode
            try:
                content = blob.decode('utf-8')
            except UnicodeDecodeError:
                content = None # Leave as binary blob

        # 3. Store in Cartridge
        # If content is set, it will be chunked/indexed. If only blob, it's stored but skipped by refinery.
        success = self.cartridge.store_file(
            vfs_path, 
            str(real_path), 
            content=content, 
            blob=blob, 
            mime_type=mime_type, 
            origin_type=origin_type
        )
        
        if success: stats["added"] += 1
        else: stats["errors"] += 1

        if __name__ == "__main__":
            # Manual test setup requires a CartridgeService instance
            from _CartridgeServiceMS import CartridgeService
            mock_cartridge = CartridgeService(":memory:")
            svc = IntakeServiceMS(mock_cartridge)
            print("Service ready:", svc._service_info["name"])




--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_IsoProcessMS.py
--------------------------------------------------------------------------------
import multiprocessing as mp
import logging
import logging.handlers
import time
import queue
from typing import Any, Dict, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# WORKER LOGIC (Runs in Child Process)
# ==============================================================================

def _isolated_worker(result_queue: mp.Queue, log_queue: mp.Queue, payload: Any, config: Dict[str, Any]):
    """
    Entry point for the child process.
    Configures a logging handler to send records back to the parent.
    Note: Must remain top-level for multiprocessing pickling compatibility.
    """
    # 1. Setup Logging Bridge
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    # Clear default handlers to avoid duplicate prints in child
    for h in root.handlers[:]:
        root.removeHandler(h)
    
    # Send all logs to the parent via the queue
    qh = logging.handlers.QueueHandler(log_queue)
    root.addHandler(qh)
    
    log = logging.getLogger("IsoWorker")

    try:
        log.info(f"Worker PID {mp.current_process().pid} started.")
        
        # --- 2. Heavy Imports (Simulated) ---
        log.info("Loading heavy libraries (Torch/Transformers)...")
        # from transformers import pipeline
        time.sleep(0.2) # Simulate import time

        # --- 3. The Logic ---
        model_name = config.get("model_name", "default-model")
        log.info(f"Initializing model '{model_name}'...")
        
        # Simulate processing steps with progress reporting
        for i in range(1, 4):
            time.sleep(0.3)
            log.info(f"Processing chunk {i}/3...")
        
        processed_data = f"Processed({payload}) via {model_name}"
        
        # --- 4. Return Result ---
        log.info("Work complete. Returning result.")
        result_queue.put({"success": True, "data": processed_data})

    except Exception as e:
        log.exception("Critical failure in worker process.")
        result_queue.put({"success": False, "error": str(e)})


# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="IsoProcess",
    version="1.0.0",
    description="Spawns isolated processes with real-time logging feedback.",
    tags=["process", "isolation", "safety"],
    capabilities=["process:spawn"]
)
class IsoProcessMS:
    """
    The Safety Valve: Spawns isolated processes with real-time logging feedback.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.timeout = self.config.get("timeout_seconds", 60)
        
        # Setup main logger
        self.log = logging.getLogger("IsoParent")
        if not self.log.handlers:
            logging.basicConfig(
                level=logging.INFO, 
                format='%(asctime)s [%(name)s] %(message)s',
                datefmt='%H:%M:%S'
            )

    @service_endpoint(
        inputs={"payload": "Any", "config": "Dict"},
        outputs={"result": "Any"},
        description="Executes a payload in an isolated child process.",
        tags=["process", "execution"],
        side_effects=["process:spawn"]
    )
    def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        config = config or {}
        
        # 1. Setup Queues
        ctx = mp.get_context("spawn")
        result_queue = ctx.Queue()
        log_queue = ctx.Queue()

        # 2. Setup Log Listener (The "Ear" of the parent)
        # This thread pulls logs from the queue and handles them in the main process
        listener = logging.handlers.QueueListener(log_queue, *logging.getLogger().handlers)
        listener.start()

        # 3. Launch Process
        process = ctx.Process(
            target=_isolated_worker,
            args=(result_queue, log_queue, payload, config)
        )
        
        self.log.info("üöÄ Spawning isolated process...")
        process.start()
        
        try:
            # 4. Wait for Result
            result_packet = result_queue.get(timeout=self.timeout)
            process.join()

            if result_packet["success"]:
                return result_packet["data"]
            else:
                raise RuntimeError(f"Worker Error: {result_packet['error']}")

        except queue.Empty:
            self.log.error("‚è≥ Worker timed out! Terminating...")
            process.terminate()
            process.join()
            raise TimeoutError(f"Task exceeded {self.timeout}s limit.")
            
        finally:
            # Clean up the log listener so it doesn't hang
            listener.stop()


if __name__ == "__main__":
    print("--- Testing IsoProcessMS with Live Logging ---")
    iso = IsoProcessMS({"timeout_seconds": 5})
    print("Service ready:", iso)
    
    try:
        result = iso.execute("Sensitive Data", {"model_name": "DeepSeek-V3"})
        print(f"\n[Parent] Final Result: {result}")
    except Exception as e:
        print(f"\n[Parent] Failed: {e}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_LexicalSearchMS.py
--------------------------------------------------------------------------------
import sqlite3
import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="LexicalSearch",
    version="1.0.0",
    description="Lightweight BM25 keyword search using SQLite FTS5 (No AI required).",
    tags=["search", "index", "sqlite"],
    capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class LexicalSearchMS:
    """
    The Librarian's Index: A lightweight, AI-free search engine.
    
    Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
    Ideal for environments where installing PyTorch/Transformers is impossible
    or overkill.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Ensure 'pathlib' is imported for this line to work
        default_db = str(Path(__file__).parent / "lexical_index.db")
        self.db_path = self.config.get("db_path", default_db)
        self._init_db()

    def _init_db(self):
        """
        Sets up the schema. 
        Uses Triggers to automatically keep the FTS index in sync with the main table.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1. Main Content Table (Stores the actual data)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                metadata TEXT  -- JSON blob for extra info (path, author, etc)
            );
        """)
        
        # 2. Virtual FTS Table (The Search Index)
        # content='documents' means it references the table above (saves space)
        cur.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                content,
                content='documents',
                content_rowid='rowid'  -- Internal SQLite mapping
            );
        """)

        # 3. Triggers (The "Magic" - Auto-sync index on Insert/Delete/Update)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_au AFTER UPDATE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        
        conn.commit()
        conn.close()

    @service_endpoint(
        inputs={"doc_id": "str", "text": "str", "metadata": "Dict"},
        outputs={},
        description="Adds or updates a document in the FTS index.",
        tags=["search", "write"],
        side_effects=["db:write"]
    )
    def add_document(self, doc_id: str, text: str, metadata: Optional[Dict[str, Any]] = None):
        """
        Adds or updates a document in the index.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        meta_json = json.dumps(metadata or {})
        
        # Upsert logic (Replace if ID exists)
        cur.execute("""
            INSERT OR REPLACE INTO documents (id, content, metadata)
            VALUES (?, ?, ?)
        """, (doc_id, text, meta_json))
        
        conn.commit()
        conn.close()

    @service_endpoint(
        inputs={"query": "str", "top_k": "int"},
        outputs={"results": "List[Dict]"},
        description="Performs a BM25 ranked keyword search.",
        tags=["search", "read"],
        side_effects=["db:read"]
    )
    def search(self, query: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Performs a BM25 Ranked Search.
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row # Allows dict-like access
        cur = conn.cursor()
        
        try:
            # The SQL Magic: 'bm25(documents_fts)' calculates relevance score
            sql = """
                SELECT 
                    d.id, 
                    d.content, 
                    d.metadata,
                    snippet(documents_fts, 0, '<b>', '</b>', '...', 15) as preview,
                    bm25(documents_fts) as score
                FROM documents_fts 
                JOIN documents d ON d.rowid = documents_fts.rowid
                WHERE documents_fts MATCH ? 
                ORDER BY score ASC
                LIMIT ?
            """
            # FTS5 query syntax: quotes typically help with special chars
            safe_query = f'"{query}"'
            rows = cur.execute(sql, (safe_query, top_k)).fetchall()
            
            results = []
            for r in rows:
                results.append({
                    "id": r['id'],
                    "score": round(r['score'], 4),
                    "preview": r['preview'], # FTS5 auto-generates snippets!
                    "metadata": json.loads(r['metadata']),
                    "full_content": r['content']
                })
            
            return results
            
        except sqlite3.OperationalError as e:
            # Usually happens if query syntax is bad (e.g. unmatched quotes)
            print(f"Search syntax error: {e}")
            return []
        finally:
            conn.close()


# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_name = "test_lexical.db"
    
    # 1. Init
    engine = LexicalSearchMS({"db_path": db_name})
    print("Service ready:", engine)
    
    # 2. Ingest Data
    print("Ingesting test data...")
    engine.add_document("doc1", "Python is a great language for data science.", {"category": "coding"})
    engine.add_document("doc2", "The snake python is a reptile found in jungles.", {"category": "biology"})
    engine.add_document("doc3", "Data science involves python, pandas, and SQL.", {"category": "coding"})
    
    # 3. Search
    query = "python data"
    print(f"\nSearching for: '{query}'")
    hits = engine.search(query)
        
    for hit in hits:
        print(f"[{hit['score']:.4f}] {hit['id']} ({hit['metadata']['category']})")
        print(f"   Preview: {hit['preview']}")
            
    # Cleanup
    if os.path.exists(db_name):
        os.remove(db_name)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_LibrarianMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _LibrarianMS
ENTRY_POINT: _LibrarianMS.py
DEPENDENCIES: pip install requests
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util
if importlib.util.find_spec("requests") is None:
    print("! MISSING DEPENDENCY: pip install requests")

import ast
import os
import datetime
import logging
import json
import requests
import concurrent.futures
from pathlib import Path
from typing import Dict, List, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# ‚öôÔ∏è SWARM CONFIGURATION
# ==============================================================================
ENABLE_AI = True
OLLAMA_URL = "http://localhost:11434/api/generate"

# 1. The Worker Drone (Fast, CPU-Optimized)
#    Used for: Scanning individual files, fixing docstrings, tagging.
#    We use 1.5b-cpu as the sweet spot between speed and coherence.
MODEL_WORKER = "qwen2.5-coder:1.5b-cpu"

# 2. The Architect (Smarter, Slower)
#    Used for: The final "System Architecture Overview".
#    We use 3b-cpu (instead of 7b) to keep it responsive on your Ryzen.
MODEL_ARCHITECT = "qwen2.5-coder:3b-cpu"

# 3. Swarm Size
#    How many files to process at once?
#    Ryzen 3800x has 16 threads. Leaving room for OS/Ollama overhead:
MAX_WORKERS = 4 

# Context Safety
MAX_CONTEXT_CHARS = 16000 
# ==============================================================================

logger = logging.getLogger("Librarian")

@service_metadata(
    name="Librarian",
    version="3.0.0",
    description="Uses a swarm of local AI models to generate a 'Card Catalogue' of the microservice library.",
    tags=["documentation", "ai", "catalog", "swarm"],
    capabilities=["filesystem:read", "filesystem:write", "network:outbound", "compute:parallel"]
)
class LibrarianMS:
    """
    The Swarm Librarian.
    Spawns concurrent AI workers to scan the codebase and create a system manifest.
    Optimized for Ryzen CPUs and 32GB RAM.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.root = Path(self.config.get("root_path", ".")).resolve()

    @service_endpoint(
        inputs={"output_file": "str"},
        outputs={"path": "str", "service_count": "int"},
        description="Unleashes the AI swarm to generate a catalog.",
        tags=["catalog", "generate"],
        side_effects=["filesystem:write"]
    )
    def generate_catalog(self, output_file: str = "LIBRARY_CATALOGUE.md") -> Dict[str, Any]:
        """
        Main entry point. Uses ThreadPoolExecutor for parallel processing.
        """
        services = []
        logger.info(f"üöÄ Launching Swarm (Workers: {MAX_WORKERS}, Model: {MODEL_WORKER})...")

        # 1. Identify Targets
        ms_files = list(self.root.glob("*MS.py"))
        # Filter for valid microservices (convention: starts with underscore, ends with MS.py)
        targets = [f for f in ms_files if f.name.startswith("_")]

        # 2. Parallel Processing
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Map the _inspect_file function to all targets
            future_to_file = {executor.submit(self._inspect_file, f): f for f in targets}
            
            for future in concurrent.futures.as_completed(future_to_file):
                f_path = future_to_file[future]
                try:
                    info = future.result()
                    if info:
                        services.append(info)
                        print(f"  [Worker Completed] {f_path.name}")
                except Exception as e:
                    logger.error(f"  [Worker Failed] {f_path.name}: {e}")

        # 3. Sort by Name (Deterministic output)
        services.sort(key=lambda x: x['name'])

        # 4. Generate Executive Summary (The Architect)
        system_summary = self._generate_system_summary(services)

        # 5. Generate Markdown
        content = self._format_markdown(services, system_summary)

        # 6. Write to Disk
        out_path = self.root / output_file
        out_path.write_text(content, encoding="utf-8")
        
        logger.info(f"‚úÖ Catalog generated: {out_path} ({len(services)} services)")
        
        return {
            "path": str(out_path),
            "service_count": len(services)
        }

    def _inspect_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """
        Worker Task: AST Parse -> AI Enrichment.
        """
        try:
            source = file_path.read_text(encoding="utf-8")
            tree = ast.parse(source)
            
            meta = {
                "filename": file_path.name,
                "name": file_path.stem,
                "description": "",
                "tags": [],
                "endpoints": [],
                "ai_enriched": False
            }

            # Find the Service Class
            target_node = None
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef) and "MS" in node.name:
                    target_node = node
                    break
            
            if not target_node: return None

            # Extract Basic Data
            meta["name"] = target_node.name
            meta["description"] = ast.get_docstring(target_node) or ""
            
            # --- AI ENRICHMENT (Worker Model) ---
            # If description is missing or too short, ask the Worker
            if ENABLE_AI and len(meta["description"]) < 10:
                # We limit prompt size for speed
                prompt = (
                    f"Read this Python class and write a 1-sentence description of its purpose. "
                    f"Be technical and concise.\n\nCode snippet:\n{source[:2000]}"
                )
                ai_desc = self._query_ollama(MODEL_WORKER, prompt)
                if ai_desc:
                    meta["description"] = f"‚ú® {ai_desc}"
                    meta["ai_enriched"] = True

            # Extract Endpoints
            for item in target_node.body:
                if isinstance(item, ast.FunctionDef) and not item.name.startswith("_"):
                    args = [a.arg for a in item.args.args if a.arg != 'self']
                    doc = ast.get_docstring(item) or ""
                    
                    meta["endpoints"].append({
                        "name": item.name,
                        "args": args,
                        "doc": doc.split("\n")[0]
                    })

            return meta

        except Exception as e:
            logger.warning(f"Failed to inspect {file_path.name}: {e}")
            return None

    def _generate_system_summary(self, services: List[Dict]) -> str:
        """
        Architect Task: Analyzes the entire system map using the larger model.
        """
        if not ENABLE_AI:
            return "Auto-generated catalog of available microservices."

        logger.info(f"üß† Architect ({MODEL_ARCHITECT}) is analyzing system structure...")
        
        # Create a compressed list for the context window
        service_list = "\n".join([f"- {s['name']}: {s['description']}" for s in services])
        
        if len(service_list) > MAX_CONTEXT_CHARS:
            service_list = service_list[:MAX_CONTEXT_CHARS] + "\n...(truncated)..."

        prompt = (
            f"You are a System Architect. Analyze this list of microservices. "
            f"Write a brief 'Executive Summary' (max 150 words) that explains what this system is capable of. "
            f"Group capabilities logically (e.g., 'UI Layer', 'Data Ingestion', 'Core Logic').\n\n"
            f"Service List:\n{service_list}"
        )

        summary = self._query_ollama(MODEL_ARCHITECT, prompt)
        return summary or "System analysis failed."

    def _query_ollama(self, model: str, prompt: str) -> str:
        """Helper to hit local Ollama instance."""
        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.2, # Low temp for factual descriptions
                    "num_ctx": 4096
                }
            }
            # Timeout slightly higher for the Architect
            timeout = 60 if model == MODEL_ARCHITECT else 20
            
            res = requests.post(OLLAMA_URL, json=payload, timeout=timeout)
            if res.status_code == 200:
                return res.json().get("response", "").strip()
        except Exception:
            # Fail silently to keep the swarm moving
            pass
        return ""

    def _format_markdown(self, services: List[Dict], summary: str) -> str:
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
        md = [
            f"# üìö Microservice Library Card Catalogue",
            f"> **Generated**: {timestamp}",
            f"> **Total Services**: {len(services)}",
            f"> **Swarm Configuration**: `{MAX_WORKERS}` Workers (`{MODEL_WORKER}`), 1 Architect (`{MODEL_ARCHITECT}`)",
            "",
            "## üß† System Architecture Overview",
            summary,
            "",
            "## üìá Index",
        ]
        
        # Table of Contents
        for s in services:
            desc_short = s['description'].split('\n')[0][:80]
            md.append(f"- **[{s['name']}](#{s['name'].lower()})**: {desc_short}")
        
        md.append("\n---\n")

        # Detail Cards
        for s in services:
            md.append(f"### {s['name']}")
            md.append(f"**File**: `{s['filename']}`")
            md.append(f"**Description**: {s['description']}")
            
            if s['endpoints']:
                md.append("\n| Endpoint | Inputs | Summary |")
                md.append("| :--- | :--- | :--- |")
                for ep in s['endpoints']:
                    args_str = ", ".join(ep['args']) or "None"
                    md.append(f"| `{ep['name']}` | `{args_str}` | {ep['doc']} |")
            
            md.append("\n---\n")

        return "\n".join(md)

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    lib = LibrarianMS()
    print("Service Ready:", lib)
    
    # Generate the catalog for the current directory
    res = lib.generate_catalog()
    print(f"Catalog created at: {res['path']}")

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_LibrarianServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _LibrarianMS
ENTRY_POINT: _LibrarianMS.py
DEPENDENCIES: pip install requests
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util
if importlib.util.find_spec("requests") is None:
    print("! MISSING DEPENDENCY: pip install requests")

import ast
import os
import datetime
import logging
import requests
import concurrent.futures
from pathlib import Path
from typing import Dict, List, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# ‚öôÔ∏è SWARM CONFIGURATION
# ==============================================================================
ENABLE_AI = True
OLLAMA_URL = "http://localhost:11434/api/generate"

# 1. The Worker Drone (Fast, CPU-Optimized)
MODEL_WORKER = "qwen2.5-coder:1.5b-cpu"

# 2. The Architect (Smarter, Slower)
MODEL_ARCHITECT = "qwen2.5-coder:3b-cpu"

# 3. Swarm Size (4 workers for Ryzen 3800x)
MAX_WORKERS = 4 

MAX_CONTEXT_CHARS = 16000 
# ==============================================================================

logger = logging.getLogger("Librarian")
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

@service_metadata(
    name="Librarian",
    version="3.0.0",
    description="Uses a swarm of local AI models to generate a 'Card Catalogue'.",
    tags=["documentation", "ai", "catalog", "swarm"],
    capabilities=["filesystem:read", "filesystem:write", "network:outbound", "compute:parallel"]
)
class LibrarianMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.root = Path(self.config.get("root_path", ".")).resolve()

    @service_endpoint(
        inputs={"output_file": "str"},
        outputs={"path": "str", "service_count": "int"},
        description="Unleashes the AI swarm to generate a catalog.",
        tags=["catalog", "generate"]
    )
    def generate_catalog(self, output_file: str = "LIBRARY_CATALOGUE.md") -> Dict[str, Any]:
        services = []
        print(f"\nüöÄ LAUNCHING SWARM (Workers: {MAX_WORKERS} | Model: {MODEL_WORKER})...")

        # 1. Identify Targets
        ms_files = list(self.root.glob("*MS.py"))
        targets = [f for f in ms_files if f.name.startswith("_")]
        print(f"üéØ Targets acquired: {len(targets)} microservices.\n")

        # 2. Parallel Processing
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            future_to_file = {executor.submit(self._inspect_file, f): f for f in targets}
            
            for future in concurrent.futures.as_completed(future_to_file):
                f_path = future_to_file[future]
                try:
                    info = future.result()
                    if info:
                        services.append(info)
                        print(f"  ‚ú® Indexed: {f_path.name}")
                except Exception as e:
                    print(f"  ‚ùå Failed: {f_path.name} - {e}")

        # 3. Sort
        services.sort(key=lambda x: x['name'])

        # 4. Generate Summary
        system_summary = self._generate_system_summary(services)

        # 5. Write Disk
        content = self._format_markdown(services, system_summary)
        out_path = self.root / output_file
        out_path.write_text(content, encoding="utf-8")
        
        print(f"\n‚úÖ CATALOG GENERATED: {out_path}")
        return {"path": str(out_path), "service_count": len(services)}

    def _inspect_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        try:
            source = file_path.read_text(encoding="utf-8")
            tree = ast.parse(source)
            
            meta = {
                "filename": file_path.name,
                "name": file_path.stem,
                "description": "",
                "endpoints": [],
                "ai_enriched": False
            }

            target_node = None
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef) and "MS" in node.name:
                    target_node = node
                    break
            
            if not target_node: return None

            meta["name"] = target_node.name
            meta["description"] = ast.get_docstring(target_node) or ""
            
            # AI Enrichment
            if ENABLE_AI and len(meta["description"]) < 10:
                prompt = (f"Read this Python class. Write a 1-sentence technical description.\n\nCode:\n{source[:2000]}")
                ai_desc = self._query_ollama(MODEL_WORKER, prompt)
                if ai_desc:
                    meta["description"] = f"‚ú® {ai_desc}"
                    meta["ai_enriched"] = True

            # Endpoints
            for item in target_node.body:
                if isinstance(item, ast.FunctionDef) and not item.name.startswith("_"):
                    args = [a.arg for a in item.args.args if a.arg != 'self']
                    doc = ast.get_docstring(item) or ""
                    meta["endpoints"].append({
                        "name": item.name,
                        "args": args,
                        "doc": doc.split("\n")[0]
                    })
            return meta
        except Exception:
            return None

    def _generate_system_summary(self, services: List[Dict]) -> str:
        if not ENABLE_AI: return "Auto-generated catalog."
        print(f"\nüß† Architect ({MODEL_ARCHITECT}) is analyzing system structure...")
        
        service_list = "\n".join([f"- {s['name']}: {s['description']}" for s in services])
        if len(service_list) > MAX_CONTEXT_CHARS:
            service_list = service_list[:MAX_CONTEXT_CHARS] + "\n...(truncated)..."

        prompt = (
            f"You are a System Architect. Analyze this microservice library. "
            f"Write a brief 'Executive Summary' (max 150 words) grouping capabilities.\n\n"
            f"Services:\n{service_list}"
        )
        return self._query_ollama(MODEL_ARCHITECT, prompt) or "Analysis failed."

    def _query_ollama(self, model: str, prompt: str) -> str:
        try:
            res = requests.post(OLLAMA_URL, json={
                "model": model, "prompt": prompt, "stream": False, 
                "options": {"temperature": 0.2, "num_ctx": 4096}
            }, timeout=60)
            if res.status_code == 200: return res.json().get("response", "").strip()
        except: pass
        return ""

    def _format_markdown(self, services: List[Dict], summary: str) -> str:
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
        md = [
            f"# üìö Microservice Library Codex",
            f"> **Generated**: {timestamp}",
            f"> **Services**: {len(services)}",
            "",
            "## üß† System Architecture",
            summary,
            "",
            "## üìá Index"
        ]
        for s in services:
            md.append(f"- **[{s['name']}](#{s['name'].lower()})**: {s['description'].split(chr(10))[0][:80]}")
        
        md.append("\n---\n")
        for s in services:
            md.append(f"### {s['name']}\n**File**: `{s['filename']}`\n\n{s['description']}\n")
            if s['endpoints']:
                md.append("| Endpoint | Inputs | Summary |\n|---|---|---|")
                for ep in s['endpoints']:
                    md.append(f"| `{ep['name']}` | `{', '.join(ep['args'])}` | {ep['doc']} |")
            md.append("\n---\n")
        return "\n".join(md)

if __name__ == "__main__":
    lib = LibrarianMS()
    lib.generate_catalog()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_LogViewMS.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import scrolledtext, filedialog
import queue
import logging
import datetime
from typing import Any, Dict, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# HELPER CLASS (Logging Handler)
# ==============================================================================

class QueueHandler(logging.Handler):
    """
    Sends log records to a thread-safe queue.
    Used to bridge the gap between Python's logging system and the Tkinter UI.
    """
    def __init__(self, log_queue: queue.Queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.log_queue.put(record)


# ==============================================================================
# MICROSERVICE CLASS (UI Widget)
# ==============================================================================

@service_metadata(
    name="LogView",
    version="1.0.0",
    description="A thread-safe log viewer widget for Tkinter.",
    tags=["ui", "logs", "widget"],
    capabilities=["ui:gui", "filesystem:write"]
)
class LogViewMS(tk.Frame):
    """
    The Console: A professional log viewer widget.
    Features:
    - Thread-safe (consumes from a Queue).
    - Message Consolidation ("Error occurred (x5)").
    - Level Filtering (Toggle INFO/DEBUG/ERROR).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        # Initialize tk.Frame
        super().__init__(parent)
        
        # Ensure we have a queue to pull from
        self.log_queue: queue.Queue = self.config.get("log_queue")
        if self.log_queue is None:
            # Fallback for safe instantiation if no queue provided
            self.log_queue = queue.Queue()

        # State for consolidation
        self.last_msg = None
        self.last_count = 0
        self.last_line_index = None

        self._build_ui()
        self._poll_queue()

    def _build_ui(self):
        # Toolbar
        toolbar = tk.Frame(self, bg="#2d2d2d", height=30)
        toolbar.pack(fill="x", side="top")
        
        # Filters
        self.filters = {
            "INFO": tk.BooleanVar(value=True),
            "DEBUG": tk.BooleanVar(value=True),
            "WARNING": tk.BooleanVar(value=True),
            "ERROR": tk.BooleanVar(value=True)
        }
        
        for level, var in self.filters.items():
            cb = tk.Checkbutton(
                toolbar, text=level, variable=var, 
                bg="#2d2d2d", fg="white", selectcolor="#444",
                activebackground="#2d2d2d", activeforeground="white"
            )
            cb.pack(side="left", padx=5)

        tk.Button(toolbar, text="Clear", command=self.clear, bg="#444", fg="white", relief="flat").pack(side="right", padx=5)
        tk.Button(toolbar, text="Save", command=self.save, bg="#444", fg="white", relief="flat").pack(side="right")

        # Text Area
        self.text = scrolledtext.ScrolledText(
            self, state="disabled", bg="#1e1e1e", fg="#d4d4d4", 
            font=("Consolas", 10), insertbackground="white"
        )
        self.text.pack(fill="both", expand=True)
        
        # Color Tags
        self.text.tag_config("INFO", foreground="#d4d4d4")
        self.text.tag_config("DEBUG", foreground="#569cd6")
        self.text.tag_config("WARNING", foreground="#ce9178")
        self.text.tag_config("ERROR", foreground="#f44747")
        self.text.tag_config("timestamp", foreground="#608b4e")

    def _poll_queue(self):
        """Pulls logs from the queue and updates UI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                self._display(record)
        except queue.Empty:
            pass
        finally:
            # Schedule the next poll in 100ms
            self.after(100, self._poll_queue)

    def _display(self, record):
        level = record.levelname
        # Skip if filter for this level is off
        if not self.filters.get(level, tk.BooleanVar(value=True)).get():
            return

        msg = record.getMessage()
        ts = datetime.datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        
        self.text.config(state="normal")
        
        # Basic display logic (Consolidation placeholder)
        if msg == self.last_msg:
            self.last_count += 1
            # In a full implementation, we would update the previous line here.
            # For this microservice, we append normally to ensure stability.
        else:
            self.last_msg = msg
            self.last_count = 1
        
        self.text.insert("end", f"[{ts}] ", "timestamp")
        self.text.insert("end", f"{msg}\n", level)
        self.text.see("end")
        self.text.config(state="disabled")

    @service_endpoint(
        inputs={},
        outputs={},
        description="Clears the log console.",
        tags=["ui", "logs"],
        side_effects=["ui:update"]
    )
    def clear(self):
        self.text.config(state="normal")
        self.text.delete("1.0", "end")
        self.text.config(state="disabled")

    @service_endpoint(
        inputs={},
        outputs={},
        description="Opens a dialog to save logs to a file.",
        tags=["ui", "filesystem"],
        side_effects=["filesystem:write", "ui:dialog"]
    )
    def save(self):
        path = filedialog.asksaveasfilename(defaultextension=".log", filetypes=[("Log Files", "*.log")])
        if path:
            try:
                with open(path, "w", encoding="utf-8") as f:
                    f.write(self.text.get("1.0", "end"))
            except Exception as e:
                print(f"Save failed: {e}")


# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Log View Test")
    root.geometry("600x400")
    
    # 1. Setup Queue
    q = queue.Queue()
    
    # 2. Setup Logger
    # Use the separated QueueHandler class
    logger = logging.getLogger("TestApp")
    logger.setLevel(logging.DEBUG)
    logger.addHandler(QueueHandler(q))
    
    # 3. Mount View
    # Pass the queue into the config
    log_view = LogViewMS({"parent": root, "log_queue": q})
    print("Service ready:", log_view)
    log_view.pack(fill="both", expand=True)
    
    # 4. Generate Logs
    def generate_noise():
        logger.info("System initializing...")
        logger.debug("Checking sensors...")
        logger.warning("Sensor 4 response slow.")
        logger.error("Connection failed!")
        # Keep generating noise to test the polling
        root.after(2000, generate_noise)
        
    generate_noise()
    root.mainloop()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_MonacoHostMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import threading
import json
import logging
from typing import Any, Dict, Optional, Callable

# --- RUNTIME DEPENDENCY CHECK ---
# We check this early so the service fails gracefully if dependencies are missing.
REQUIRED = ["webview"] # 'pywebview' package import name is 'webview'
MISSING = []

if importlib.util.find_spec("webview") is None:
    MISSING.append("pywebview")

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _MonacoHostMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # We don't exit here to allow the class to load, but launch() will likely fail.

import webview  # type: ignore
from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("MonacoHost")

# ==============================================================================
# EMBEDDED HTML/JS
# ==============================================================================

MONACO_HTML = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Monaco Host</title>
    <style>
        html, body { margin: 0; padding: 0; width: 100%; height: 100%; overflow: hidden; background-color: #1e1e1e; font-family: sans-serif; }
        #container { display: flex; flex-direction: column; height: 100%; }
        #tabs { background: #252526; display: flex; overflow-x: auto; height: 35px; border-bottom: 1px solid #3e3e3e; }
        .tab { 
            padding: 8px 15px; color: #969696; background: #2d2d2d; cursor: pointer; border-right: 1px solid #1e1e1e; font-size: 12px;
            display: flex; align-items: center; white-space: nowrap;
        }
        .tab.active { background: #1e1e1e; color: #fff; border-top: 1px solid #007acc; }
        .tab:hover { background: #323233; color: #fff; }
        #editor { flex-grow: 1; }
    </style>
</head>
<body>
    <div id="container">
        <div id="tabs"></div>
        <div id="editor"></div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs/loader.js"></script>
    <script>
        require.config({ paths: { 'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs' }});
        let editor;
        let models = {}; 
        let currentPath = null;

        require(['vs/editor/editor.main'], function() {
            editor = monaco.editor.create(document.getElementById('editor'), {
                value: "# Monaco Editor Ready\\n",
                language: 'python',
                theme: 'vs-dark',
                automaticLayout: true,
                fontSize: 14
            });

            if (window.pywebview) window.pywebview.api.signal_editor_ready();

            editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.KeyS, function() {
                if (currentPath) {
                    window.pywebview.api.save_file(currentPath, editor.getValue());
                }
            });
        });

        window.pywebview = window.pywebview || {};
        window.pywebview.api = window.pywebview.api || {};

        window.pywebview.api.open_in_tab = function(filepath, content) {
            let ext = filepath.split('.').pop();
            let langMap = { 'py': 'python', 'js': 'javascript', 'html': 'html', 'json': 'json', 'css': 'css' };
            let lang = langMap[ext] || 'plaintext';

            if (!models[filepath]) {
                models[filepath] = monaco.editor.createModel(content, lang, monaco.Uri.file(filepath));
                const tab = document.createElement('div');
                tab.className = 'tab';
                tab.innerText = filepath.split(/[\\\\/]/).pop();
                tab.title = filepath;
                tab.onclick = () => switchTo(filepath);
                tab.dataset.path = filepath;
                document.getElementById('tabs').appendChild(tab);
            }
            switchTo(filepath);
        };

        window.pywebview.api.reveal_range = function(filepath, startLine, endLine) {
            if (filepath !== currentPath) switchTo(filepath);
            editor.revealLineInCenter(startLine);
            editor.setSelection({ startLineNumber: startLine, startColumn: 1, endLineNumber: endLine, endColumn: 1000 });
        };

        function switchTo(filepath) {
            if (!models[filepath]) return;
            editor.setModel(models[filepath]);
            currentPath = filepath;
            document.querySelectorAll('.tab').forEach(t => t.classList.toggle('active', t.dataset.path === filepath));
        }
    </script>
</body>
</html>
"""

# ==============================================================================
# HELPER CLASS (JS API Bridge)
# ==============================================================================

class MonacoApiBridge:
    """
    Acts as the bridge between Python and the JavaScript running inside the webview.
    Methods here are callable from JS via `window.pywebview.api.methodName()`.
    """
    def __init__(self):
        self._window = None
        self._ready_event = threading.Event()
        self.on_save_callback: Optional[Callable[[str, str], None]] = None

    def set_window(self, window):
        self._window = window

    def signal_editor_ready(self):
        """Called by JS when Monaco is fully loaded."""
        self._ready_event.set()
        logger.info("Monaco Editor reported ready.")

    def save_file(self, filepath: str, content: str):
        """Called by JS when Ctrl+S is pressed."""
        if self.on_save_callback:
            self.on_save_callback(filepath, content)
        else:
            logger.warning(f"Saved {filepath} (No callback registered)")

    def open_file_in_js(self, filepath: str, content: str):
        """Python helper to push data to JS."""
        self._ready_event.wait(timeout=10)
        if not self._window: 
            return
        # Using json.dumps ensures strings are properly escaped for JS
        js = f"window.pywebview.api.open_in_tab({json.dumps(filepath)}, {json.dumps(content)})"
        self._window.evaluate_js(js)


# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="MonacoHost",
    version="1.1.0",
    description="Hosts an embedded Monaco Editor instance using PyWebview.",
    tags=["ui", "editor", "webview"],
    capabilities=["ui:gui"]
)
class MonacoHostMS:
    """
    Hosts the Monaco Editor.
    This service spawns a GUI window and cannot be run in headless environments.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Instantiate the Bridge class, NOT this service class (avoids recursion/error)
        self.api = MonacoApiBridge()
        self.window = None

    @service_endpoint(
        inputs={"title": "str", "width": "int", "height": "int"},
        outputs={},
        description="Launches the editor window. Blocking call.",
        tags=["ui", "launch"],
        side_effects=["ui:window"]
    )
    def launch(self, title="Monaco Editor", width=1000, height=700, func=None):
        """
        Create and launch the window.
        :param func: Optional function to run in a separate thread after launch.
        """
        self.window = webview.create_window(
            title, 
            html=MONACO_HTML, 
            js_api=self.api,
            width=width, 
            height=height
        )
        self.api.set_window(self.window)
        
        # Start the GUI loop
        webview.start(func, debug=True) if func else webview.start(debug=True)

    def set_save_callback(self, callback: Callable[[str, str], None]):
        """Sets the function to trigger when Ctrl+S is pressed in the editor."""
        self.api.on_save_callback = callback

    def open_file(self, filepath: str, content: str):
        """Opens a file in the editor (must be called from a background thread or callback)."""
        self.api.open_file_in_js(filepath, content)


# --- Independent Test Block ---
if __name__ == "__main__":
    host = MonacoHostMS()
    
    def background_actions():
        # Wait for the JS to signal it's ready
        host.api._ready_event.wait()
        
        # Open a demo file
        print("Opening demo file...")
        host.open_file("demo.py", "print('Hello World')\n# Try Ctrl+S to save!")
        
        # Register what happens when user saves
        host.set_save_callback(lambda p, c: print(f"File: {p} was saved with {len(c)} chars."))

    print("Launching Monaco Host...")
    host.launch(func=background_actions)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_NetworkLayoutMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import logging
from typing import List, Dict, Any, Tuple, Optional

# --- RUNTIME DEPENDENCY CHECK ---
REQUIRED = ["networkx"]
MISSING = []

for lib in REQUIRED:
    if importlib.util.find_spec(lib) is None:
        MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _NetworkLayoutMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # Allow execution to proceed so the class definition loads, 
    # but actual usage will fail if import is missing.

# Import conditionally to avoid crashing immediately if missing
try:
    import networkx as nx
except ImportError:
    nx = None

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logger = logging.getLogger("NetLayout")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="NetworkLayout",
    version="1.0.0",
    description="Calculates visual (x,y) coordinates for graph nodes using NetworkX.",
    tags=["graph", "layout", "visualization"],
    capabilities=["compute"]
)
class NetworkLayoutMS:
    """
    The Topologist: Calculates visual coordinates for graph nodes using
    server-side algorithms (NetworkX). 
    Useful for generating static map snapshots or pre-calculating positions 
    to offload client-side rendering.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        if nx is None:
            logger.error("NetworkX is not installed. Layout calculations will fail.")

    @service_endpoint(
        inputs={"nodes": "List[str]", "edges": "List[Tuple]", "algorithm": "str"},
        outputs={"positions": "Dict[str, Tuple]"},
        description="Computes (x, y) coordinates for the given graph nodes and edges.",
        tags=["graph", "compute"],
        side_effects=[]
    )
    def calculate_layout(self, nodes: List[str], edges: List[Tuple[str, str]], 
                         algorithm: str = "spring", **kwargs) -> Dict[str, Tuple[float, float]]:
        """
        Computes (x, y) coordinates for the given graph.
        
        :param nodes: List of node IDs.
        :param edges: List of (source, target) tuples.
        :param algorithm: 'spring' (Force-directed) or 'circular'.
        :return: Dictionary {node_id: (x, y)}
        """
        if nx is None:
            return {}

        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        G.add_edges_from(edges)
        
        logger.info(f"Computing layout for {len(nodes)} nodes, {len(edges)} edges using '{algorithm}'...")
        
        try:
            if algorithm == "circular":
                pos = nx.circular_layout(G)
            else:
                # Spring layout (Fruchterman-Reingold) is standard for knowledge graphs
                k_val = kwargs.get('k', 0.15) # Optimal distance between nodes
                iter_val = kwargs.get('iterations', 50)
                pos = nx.spring_layout(G, k=k_val, iterations=iter_val, seed=42)
                
            # Convert numpy arrays to simple lists/tuples for JSON serialization
            return {n: (float(p[0]), float(p[1])) for n, p in pos.items()}
            
        except Exception as e:
            logger.error(f"Layout calculation failed: {e}")
            return {}


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup basic logging for standalone test
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    
    layout = NetworkLayoutMS()
    print("Service ready:", layout)
    
    if nx:
        # 1. Define a simple graph
        test_nodes = ["Main", "Utils", "Config", "DB", "Auth"]
        test_edges = [
            ("Main", "Utils"),
            ("Main", "Config"),
            ("Main", "DB"),
            ("Main", "Auth"),
            ("DB", "Config"),
            ("Auth", "DB")
        ]
        
        # 2. Compute Layout
        positions = layout.calculate_layout(test_nodes, test_edges, k=0.5)
        
        print("--- Calculated Positions ---")
        for node, (x, y) in positions.items():
            print(f"{node:<10}: ({x: .4f}, {y: .4f})")
    else:
        print("Skipping test: NetworkX not found.")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_NeuralGraphEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NeuralGraphEngineMS
ENTRY_POINT: _NeuralGraphEngineMS.py
DEPENDENCIES: None
"""

import pygame
import math
import random
import time

# Initialize font module globally once
pygame.font.init()

from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="NeuralGraphEngineMS",
    version="1.1.0",
    description="The Cartographer: A physics-driven rendering engine for visualizing complex neural relationships in a 2D force-directed graph.",
    tags=["visualization", "graph", "pygame"],
    capabilities=["force-directed-layout", "real-time-rendering"],
    dependencies=["pygame", "math", "random"],
    side_effects=["ui:update", "render:write"]
)
class NeuralGraphEngineMS(BaseService):
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        super().__init__("NeuralGraphEngineMS")
        self.width = width
        self.start_time = time.time()
        self.height = height
        self.bg_color = bg_color
        
        self.surface = pygame.Surface((width, height))
        
        # Camera
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction
        self.dragged_node_idx = None
        self.hovered_node_idx = None
        
        # Physics State
        self.settled = False

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "nodes": "int", "settled": "bool"},
        description="Standardized health check to verify the operational state of the graph renderer.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the NeuralGraphEngineMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "nodes": len(self.nodes),
            "settled": self.settled
        }

    def resize(self, width, height):
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    @service_endpoint(
        inputs={"nodes": "list", "links": "list"},
        outputs={},
        description="Injects new node and edge data into the engine and wakes up the physics simulation.",
        tags=["data", "update"],
        side_effects=["graph:write"]
    )
    def set_data(self, nodes, links):
        self.nodes = nodes
        self.links = links
        self.settled = False # Wake up physics on new data
        
        # 1. Build an ID map so we can find parents
        node_map = {node['id']: node for node in self.nodes}

        for n in self.nodes:
            # GNN Injection: Use pre-calculated layout if available
            if 'gnn_x' in n and 'gnn_y' in n:
                n['x'] = n['gnn_x'] * self.width
                n['y'] = n['gnn_y'] * self.height

            elif 'x' not in n:
                # SMART SPAWN: If I am a satellite, spawn near my planet
                parent_id = n.get('meta', {}).get('parent')
                if parent_id and parent_id in node_map and 'x' in node_map[parent_id]:
                    p = node_map[parent_id]
                    angle = random.random() * 6.28
                    dist = 30
                    n['x'] = p['x'] + math.cos(angle) * dist
                    n['y'] = p['y'] + math.sin(angle) * dist
                else:
                    # Random spawn for Files
                    n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                    n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Semantic Coloring
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # Blue
                n['_radius'] = 6
            elif n.get('type') == 'web':
                n['_color'] = (204, 0, 122) # Purple/Pink
                n['_radius'] = 7
            elif n.get('type') == 'chunk':
                n['_color'] = (100, 200, 100) # Satellite Green
                n['_radius'] = 3
            else:
                n['_color'] = (160, 32, 240) # Default
                n['_radius'] = 6

    # --- INPUT HANDLING ---
    
    def screen_to_world(self, sx, sy):
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def get_node_at(self, sx, sy):
        wx, wy = self.screen_to_world(sx, sy)
        for n in self.nodes:
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                return n
        return None

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                self.dragged_node_idx = i
                self.settled = False # Wake up physics
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
            self.settled = False
        else:
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            return prev_hover != self.hovered_node_idx

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    def highlight_nodes(self, node_ids):
        """Highlights specific nodes by ID."""
        for n in self.nodes:
            # 1. Reset to defaults
            if n.get('type') == 'file': 
                n['_color'] = (0, 122, 204)
                n['_radius'] = 6
            elif n.get('type') == 'web': 
                n['_color'] = (204, 0, 122)
                n['_radius'] = 7
            elif n.get('type') == 'chunk': 
                n['_color'] = (100, 200, 100)
                n['_radius'] = 3
            else: 
                n['_color'] = (160, 32, 240)
                n['_radius'] = 6
                
            # 2. Apply Highlight
            if n['id'] in node_ids:
                n['_color'] = (255, 255, 0) # Bright Yellow
                n['_radius'] = 12
                
        self.settled = False # Wake up physics

    # --- PHYSICS (Damped) ---

    @service_endpoint(
        inputs={},
        outputs={"settled": "bool"},
        description="Performs one iteration of the force-directed physics calculation.",
        tags=["physics", "lifecycle"],
        mode="async",
        side_effects=["graph:update"]
    )
    def step_physics(self):
        if not self.nodes or self.settled: return

        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.85 # Increased damping to settle faster
        
        cx, cy = self.width / 2, self.height / 2
        total_kinetic_energy = 0

        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue

            # LOD: Freeze satellites if zoomed out
            if self.zoom < 1.2 and a.get('type') == 'chunk':
                a['vx'] = 0
                a['vy'] = 0
                continue
            
            fx, fy = 0, 0
            
            # 1. Gravity (Center pull)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # 2. Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Performance opt: Ignore far away nodes
                if dist_sq > 25000: continue 

                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 3. Attraction (Links)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 4. Apply & Measure Energy
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']
            total_kinetic_energy += (abs(n['vx']) + abs(n['vy']))

        # 5. Sleep Threshold
        if total_kinetic_energy < 0.5:
            self.settled = True

    # --- RENDERING ---

    @service_endpoint(
        inputs={},
        outputs={"raw_data": "bytes"},
        description="Renders the current frame to a byte buffer for display in UI components.",
        tags=["render", "output"],
        side_effects=["ui:update", "render:write"]
    )
    def get_image_bytes(self):
        self.surface.fill(self.bg_color)
        
        cx, cy = self.width / 2, self.height / 2
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # Links
        for u, v in self.links:
            if self.zoom < 1.2:
                if self.nodes[u].get('type') == 'chunk' or self.nodes[v].get('type') == 'chunk':
                    continue

            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # Nodes
        for i, n in enumerate(self.nodes):
            # LOD: Hide chunks if zoomed out
            if self.zoom < 1.2 and n.get('type') == 'chunk':
                continue

            sx, sy = to_screen(n['x'], n['y'])
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20: continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')

        if __name__ == "__main__":
            # Manual test setup
            engine = NeuralGraphEngineMS(400, 300)
            print("Service ready:", engine._service_info['name'])
            test_nodes = [{'id': 'A', 'type': 'file', 'label': 'Node A'}, {'id': 'B', 'type': 'chunk', 'label': 'Node B'}]
            test_links = [(0, 1)]
            engine.set_data(test_nodes, test_links)
            engine.step_physics()
            print("Physics step completed.")






--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_NeuralGraphViewerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NeuralGraphViewerMS
ENTRY_POINT: _NeuralGraphViewerMS.py
DEPENDENCIES: None
"""

import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import json
import os
from _NeuralGraphEngineMS import GraphRenderer
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="NeuralGraphViewerMS",
    version="1.0.0",
    description="The Lens: A Tkinter-based UI component that hosts the neural graph engine and provides search/highlighting overlays.",
    tags=["ui", "visualization", "tkinter"],
    capabilities=["graph-rendering", "search-highlighting"],
    dependencies=["tkinter", "PIL", "sqlite3", "json"],
    side_effects=["ui:update", "filesystem:read"]
)
class NeuralGraphViewerMS(BaseService, ttk.Frame):
    def __init__(self, parent):
        BaseService.__init__(self, "NeuralGraphViewerMS")
        ttk.Frame.__init__(self, parent)
        self.pack(fill="both", expand=True)
        
        # Search Overlay
        self.controls = tk.Frame(self, bg="#101018")
        self.controls.pack(fill="x", side="top", padx=5, pady=5)
        
        self.entry_search = tk.Entry(self.controls, bg="#252526", fg="white", insertbackground="white", font=("Consolas", 10))
        self.entry_search.pack(side="left", fill="x", expand=True, padx=(0, 5))
        self.entry_search.bind("<Return>", self.run_search)
        
        btn = tk.Button(self.controls, text="NEURAL TEST", command=self.run_search, bg="#007ACC", fg="white", relief="flat")
        btn.pack(side="right")

        # UI Container
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Services
        self.cartridge = None
        self.neural = None
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None 
        
        # Input State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        self.is_panning = False

        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<Double-Button-1>', self.on_double_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Start the Heartbeat
        self.animate()

    def bind_services(self, cartridge, neural):
        self.cartridge = cartridge
        self.neural = neural

    @service_endpoint(
        inputs={"event": "any"},
        outputs={},
        description="Triggers a neural search based on the entry field and highlights resulting nodes in the viewer.",
        tags=["ui-action", "search"],
        side_effects=["ui:update", "graph:highlight"]
    )
    def run_search(self, event=None):
        if not self.cartridge or not self.neural:
            return
            
        query = self.entry_search.get().strip()
        if not query: return
        
        # 1. Embed
        vec = self.neural.get_embedding(query)
        if not vec: return
        
        # 2. Search
        results = self.cartridge.search_embeddings(vec, limit=5)
        
        # 3. Resolve IDs for Graph
        # Graph Node ID format: "{vfs_path}::{chunk_name}"
        ids = set()
        for r in results:
            if 'vfs_path' in r and 'name' in r:
                ids.add(f"{r['vfs_path']}::{r['name']}")
                
        # 4. Highlight
        self.engine.highlight_nodes(ids)

    @service_endpoint(
        inputs={"db_path": "str"},
        outputs={},
        description="Loads graph nodes and edges from a Cartridge database and triggers the physics engine.",
        tags=["data-load", "sqlite"],
        side_effects=["filesystem:read"]
    )
    def load_from_db(self, db_path):
        """
        Loads graph data from SQLite.
        Does NOT block the UI. The physics engine will settle the nodes frame-by-frame.
        """
        if not os.path.exists(db_path): return
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # [cite_start]Fetch Nodes [cite: 198]
            db_nodes = cursor.execute("SELECT id, type, label, data_json FROM graph_nodes").fetchall()
            
            # [cite_start]Fetch Edges [cite: 198]
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
            
            conn.close()
        except Exception as e:
            print(f"Graph Load Error: {e}")
            return

        # Format for Engine
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label, raw_json = row
            meta = {}
            try:
                if raw_json: meta = json.loads(raw_json)
            except: pass
            
            id_to_index[node_id] = idx
            formatted_nodes.append({'id': node_id, 'type': n_type, 'label': label, 'meta': meta})

        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                formatted_links.append((id_to_index[src], id_to_index[tgt]))

        # Inject Data - The Physics Engine handles the "Explosion" logic internally
        self.engine.set_data(formatted_nodes, formatted_links)

    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_double_click(self, event):
        # Zoom in on the node we clicked
        hit_node = self.engine.get_node_at(event.x, event.y)
        if hit_node:
            # Center camera on node and zoom in
            self.engine.cam_x = hit_node['x']
            self.engine.cam_y = hit_node['y']
            self.engine.zoom = 2.0
            self.engine.settled = False

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        if hit:
            self.is_dragging_node = True
        else:
            self.is_panning = True

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False
        self.is_panning = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        elif self.is_panning:
            # Camera Pan
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)
        self.engine.settled = False # Wake up physics on zoom

    def on_windows_scroll(self, event):
        if event.delta > 0: self.on_zoom(1.1)
        else: self.on_zoom(0.9)

    @service_endpoint(
        inputs={},
        outputs={},
        description="The primary heartbeat loop that orchestrates frame-by-frame physics steps and UI blitting.",
        tags=["lifecycle", "rendering"],
        mode="async",
        side_effects=["ui:update", "render:write"]
    )
    def animate(self):
        """
        The Heartbeat Loop.
        Runs at ~30 FPS. Handles Physics + Rendering.
        """
        # 1. Step Physics (Micro-calculations)
        self.engine.step_physics()
        
        # 2. Render to Buffer
        raw_data = self.engine.get_image_bytes()
        
        # 3. Blit to Screen
        if raw_data:
            img = Image.frombytes('RGB', (self.engine.width, self.engine.height), raw_data)
            self.photo = ImageTk.PhotoImage(img)
            self.canvas_lbl.configure(image=self.photo)
        
        # 4. Loop
        self.after(30, self.animate)

        if __name__ == "__main__":
            root = tk.Tk()
            root.title("NeuralGraphViewerMS Test")
            view = NeuralGraphViewerMS(root)
            print("Service ready:", view._service_info['name'])
            # Note: Requires a valid DB and Pygame environment to fully render
            root.mainloop()



--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_NeuralServiceMS.py
--------------------------------------------------------------------------------
import requests
import json
import concurrent.futures
import logging
from typing import Optional, Dict, Any, List

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================

OLLAMA_API_URL = "http://localhost:11434/api"
logger = logging.getLogger("NeuralService")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="NeuralService",
    version="1.0.0",
    description="The Brain Interface: Orchestrates local AI operations via Ollama.",
    tags=["ai", "neural", "inference", "ollama"],
    capabilities=["text-generation", "embeddings", "parallel-processing"]
)
class NeuralServiceMS:
    """
    The Brain Interface: Orchestrates local AI operations via Ollama for inference and embeddings.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.max_workers = self.config.get("max_workers", 4)
        
        # Default internal model config
        self.models = {
            "fast": "qwen2.5-coder:1.5b-cpu",
            "smart": "qwen2.5:3b-cpu",
            "embed": "mxbai-embed-large:latest-cpu"
        }
        # Override defaults if provided in initial config
        if "models" in self.config:
            self.models.update(self.config["models"])

    @service_endpoint(
        inputs={"fast_model": "str", "smart_model": "str", "embed_model": "str"},
        outputs={"status": "str"},
        description="Updates the active model configurations on the fly.",
        tags=["config", "write"],
        side_effects=["config:update"]
    )
    def update_models(self, fast_model: str, smart_model: str, embed_model: str) -> Dict[str, str]:
        """Called by the UI Settings Modal to change models on the fly."""
        self.models["fast"] = fast_model
        self.models["smart"] = smart_model
        self.models["embed"] = embed_model
        logger.info(f"Models Updated: Fast={fast_model}, Smart={smart_model}")
        return {"status": "success", "config": str(self.models)}

    @service_endpoint(
        inputs={},
        outputs={"models": "List[str]"},
        description="Fetches a list of available models from the local Ollama instance.",
        tags=["ai", "read"],
        side_effects=["network:read"]
    )
    def get_available_models(self) -> List[str]:
        """Fetches list from Ollama for the UI dropdown."""
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            if res.status_code == 200:
                return [m['name'] for m in res.json().get('models', [])]
        except Exception as e:
            logger.error(f"Failed to fetch models: {e}")
            return []
        return []

    @service_endpoint(
        inputs={},
        outputs={"is_alive": "bool"},
        description="Pings Ollama to verify connectivity.",
        tags=["health", "read"],
        side_effects=["network:read"]
    )
    def check_connection(self) -> bool:
        """Pings Ollama to see if it's alive."""
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except requests.RequestException:
            logger.error("Ollama connection failed. Is 'ollama serve' running?")
            return False

    @service_endpoint(
        inputs={"text": "str"},
        outputs={"embedding": "list"},
        description="Generates a vector embedding for the provided text.",
        tags=["nlp", "vector", "ai"],
        side_effects=["network:read"]
    )
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Generates a vector using the configured embedding model."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.models["embed"], "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
        return None

    @service_endpoint(
        inputs={"prompt": "str", "tier": "str", "format_json": "bool"},
        outputs={"response": "str"},
        description="Requests synchronous text generation from a local LLM.",
        tags=["llm", "inference"],
        side_effects=["network:read"]
    )
    def request_inference(self, prompt: str, tier: str = "fast", format_json: bool = False) -> str:
        """
        Synchronous inference request.
        tier: 'fast', 'smart', or other keys in self.models
        """
        model = self.models.get(tier, self.models["fast"])
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        if format_json:
            payload["format"] = "json"

        try:
            res = requests.post(f"{OLLAMA_API_URL}/generate", json=payload, timeout=60)
            if res.status_code == 200:
                return res.json().get("response", "").strip()
        except Exception as e:
            logger.error(f"Inference ({tier}) failed: {e}")
        return ""

    def process_parallel(self, items: List[Any], worker_func) -> List[Any]:
        """
        Helper to run a function across many items using a ThreadPool.
        Useful for batch ingestion.
        Note: Not exposed as an endpoint as it takes a function as an argument.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # worker_func should take a single item and return a result
            futures = {executor.submit(worker_func, item): item for item in items}
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    logger.error(f"Worker task failed: {e}")
        
        return results


# --- Independent Test Block ---
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    svc = NeuralServiceMS()
    print("Service ready:", svc)
    
    if svc.check_connection():
        print("Ollama Connection: OK")
        print(f"Models available: {svc.get_available_models()}")
        
        # Simple Inference Test
        print("Testing Inference (Fast Tier)...")
        response = svc.request_inference("Why is the sky blue? Answer in 1 sentence.")
        print(f"Response: {response}")
    else:
        print("Ollama Connection: FAILED (Is Ollama running?)")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ProjectForgeMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ProjectForgeMS
ENTRY_POINT: _ProjectForgeMS.py
DEPENDENCIES: None
"""
import os
import shutil
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("ProjectForge")

@service_metadata(
    name="ProjectForge",
    version="1.0.0",
    description="Scaffolding engine for creating new microservice projects or Python apps.",
    tags=["scaffolding", "generator", "filesystem"],
    capabilities=["filesystem:write"]
)
class ProjectForgeMS:
    """
    The Blacksmith.
    Creates directory structures, stamps out boilerplate code, and injects dependencies.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Optional: Define a central template library path
        self.template_root = self.config.get("template_root")

    @service_endpoint(
        inputs={"parent_path": "str", "project_name": "str", "dependencies": "List[str]", "type": "str"},
        outputs={"path": "str", "status": "str"},
        description="Creates a new standardized Python project with the given specifications.",
        tags=["create", "scaffold"],
        side_effects=["filesystem:write"]
    )
    def forge_project(self, 
                      parent_path: str, 
                      project_name: str, 
                      dependencies: List[str] = None, 
                      project_type: str = "microservice") -> Dict[str, Any]:
        """
        Stamps out a new project folder.
        """
        root = Path(parent_path).resolve()
        project_dir = root / project_name
        
        if project_dir.exists():
            raise FileExistsError(f"Directory already exists: {project_dir}")

        logger.info(f"Forging new project '{project_name}' at {project_dir}...")
        
        try:
            # 1. Create Skeleton
            project_dir.mkdir(parents=True)
            (project_dir / "src").mkdir()
            (project_dir / "tests").mkdir()
            (project_dir / "config").mkdir()
            
            # 2. Create Files
            self._create_readme(project_dir, project_name)
            self._create_requirements(project_dir, dependencies or [])
            self._create_gitignore(project_dir)
            
            # 3. Create Entry Point (based on type)
            if project_type == "microservice":
                self._create_microservice_boilerplate(project_dir, project_name)
            else:
                self._create_standard_app_boilerplate(project_dir)

            return {
                "path": str(project_dir),
                "status": "created",
                "name": project_name
            }
            
        except Exception as e:
            # Rollback on failure? Or just log.
            logger.error(f"Forge failed: {e}")
            raise e

    def _create_readme(self, root: Path, name: str):
        content = f"# {name}\n\nGenerated by ProjectForgeMS.\n\n## Overview\nTODO: Add description.\n"
        (root / "README.md").write_text(content, encoding="utf-8")

    def _create_requirements(self, root: Path, deps: List[str]):
        # Always add standard libs if needed
        base_deps = ["requests", "logging"]
        all_deps = sorted(list(set(base_deps + deps)))
        content = "\n".join(all_deps)
        (root / "requirements.txt").write_text(content, encoding="utf-8")

    def _create_gitignore(self, root: Path):
        content = "__pycache__/\n*.pyc\n.env\n.venv/\n.vscode/\n"
        (root / ".gitignore").write_text(content, encoding="utf-8")

    def _create_microservice_boilerplate(self, root: Path, name: str):
        # Create a standard microservice file
        content = f'''"""
SERVICE_NAME: _{name}MS
ENTRY_POINT: _{name}MS.py
DEPENDENCIES: None
"""
from typing import Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="{name}",
    version="1.0.0",
    description="Auto-generated microservice.",
    tags=["new"],
    capabilities=[]
)
class {name}MS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {{}}

    @service_endpoint(
        inputs={{}},
        outputs={{"message": "str"}},
        description="Default endpoint.",
        tags=["hello"]
    )
    def hello(self):
        return {{"message": "Hello from {name}!"}}

if __name__ == "__main__":
    svc = {name}MS()
    print("Service ready:", svc)
'''
        (root / "src" / f"_{name}MS.py").write_text(content, encoding="utf-8")

    def _create_standard_app_boilerplate(self, root: Path):
        content = "def main():\n    print('Hello World')\n\nif __name__ == '__main__':\n    main()"
        (root / "src" / "app.py").write_text(content, encoding="utf-8")

if __name__ == "__main__":
    forge = ProjectForgeMS()
    # Test creating a dummy project
    try:
        forge.forge_project(".", "TestForgedApp", ["pandas", "numpy"])
        print("Test Project Created.")
        shutil.rmtree("TestForgedApp") # Cleanup
        print("Test Project Cleaned up.")
    except Exception as e:
        print(f"Test failed: {e}")

--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_PromptOptimizerMS.py
--------------------------------------------------------------------------------
import json
import logging
from typing import List, Dict, Any, Callable, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: META-PROMPTS
# ==============================================================================

# The system prompt used to turn the LLM into a Prompt Engineer
REFINE_SYSTEM_PROMPT = (
    "You are a world-class prompt engineer. "
    "Given an original prompt and specific feedback, "
    "provide an improved, refined version of the prompt that incorporates the feedback. "
    "Return ONLY the refined prompt text, no preamble."
)

# The system prompt used to generate A/B test variations
VARIATION_SYSTEM_PROMPT = (
    "You are a creative AI assistant. "
    "Generate {num} innovative and diverse variations of the following prompt. "
    "Return the result as a valid JSON array of strings. "
    "Example: [\"variation 1\", \"variation 2\"]"
)

logger = logging.getLogger("PromptOpt")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="PromptOptimizer",
    version="1.0.0",
    description="Uses an LLM to refine prompts or generate variations.",
    tags=["llm", "prompt-engineering", "optimization"],
    capabilities=["network:outbound"]
)
class PromptOptimizerMS:
    """
    The Tuner: Uses an LLM to refine prompts or generate variations.
    Requires an 'inference_func' to be passed in the config, which accepts a string
    and returns a string (simulating an LLM call).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Default to a lambda that raises an error if no inference engine is provided
        self.infer: Callable[[str], str] = self.config.get(
            "inference_func", 
            lambda x: "Error: No inference function configured."
        )

    @service_endpoint(
        inputs={"draft_prompt": "str", "feedback": "str"},
        outputs={"refined_prompt": "str"},
        description="Rewrites a prompt based on feedback.",
        tags=["llm", "refine"],
        side_effects=["network:outbound"]
    )
    def refine_prompt(self, draft_prompt: str, feedback: str) -> str:
        """
        Rewrites a prompt based on feedback.
        """
        full_prompt = (
            f"{REFINE_SYSTEM_PROMPT}\n\n"
            f"[Original Prompt]:\n{draft_prompt}\n\n"
            f"[Feedback]:\n{feedback}\n\n"
            f"[Refined Prompt]:"
        )
        
        logger.info("Refining prompt...")
        try:
            result = self.infer(full_prompt)
            return result.strip()
        except Exception as e:
            logger.error(f"Refinement failed: {e}")
            return draft_prompt # Fallback to original

    @service_endpoint(
        inputs={"draft_prompt": "str", "num_variations": "int", "context_data": "Dict"},
        outputs={"variations": "List[str]"},
        description="Generates multiple versions of a prompt for testing.",
        tags=["llm", "variations"],
        side_effects=["network:outbound"]
    )
    def generate_variations(self, draft_prompt: str, num_variations: int = 3, context_data: Optional[Dict[str, Any]] = None) -> List[str]:
        """
        Generates multiple versions of a prompt for testing.
        """
        meta_prompt = VARIATION_SYSTEM_PROMPT.format(num=num_variations)
        
        prompt_content = draft_prompt
        if context_data:
            prompt_content += f"\n\n--- Context ---\n{json.dumps(context_data, indent=2)}"

        full_prompt = (
            f"{meta_prompt}\n\n"
            f"[Original Prompt]:\n{prompt_content}\n\n"
            f"[JSON Array of Variations]:"
        )

        logger.info(f"Generating {num_variations} variations...")
        try:
            # We explicitly ask for JSON, but LLMs are chatty, so we might need cleaning logic here
            raw_response = self.infer(full_prompt)
            
            # Simple cleanup to find the JSON array if the LLM added text around it
            start = raw_response.find('[')
            end = raw_response.rfind(']') + 1
            if start == -1 or end == 0:
                # If pure JSON wasn't found, just return the raw text as a single item list
                logger.warning("No JSON array found in response. Returning raw response.")
                return [raw_response]
                
            clean_json = raw_response[start:end]
            variations = json.loads(clean_json)
            
            if isinstance(variations, list):
                return [str(v) for v in variations]
            return []
            
        except Exception as e:
            logger.error(f"Variation generation failed: {e}")
            return []


# --- Independent Test Block ---
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    # 1. Mock Inference Engine (Simulating an LLM)
    def mock_llm(prompt: str) -> str:
        if "[Refined Prompt]" in prompt:
            return "You are a helpful assistant who speaks like a pirate. How may I help ye?"
        if "[JSON Array]" in prompt:
            return '["Variation A: Pirate Mode", "Variation B: Formal Mode", "Variation C: Concise Mode"]'
        return "Error"

    # Inject the mock engine
    optimizer = PromptOptimizerMS({"inference_func": mock_llm})
    print("Service ready:", optimizer)

    # 2. Test Refine
    print("--- Test: Refine ---")
    draft = "Help me."
    feedback = "Make it sound like a pirate."
    refined = optimizer.refine_prompt(draft, feedback)
    print(f"Original: {draft}")
    print(f"Refined:  {refined}")

    # 3. Test Variations
    print("\n--- Test: Variations ---")
    vars = optimizer.generate_variations(draft, num_variations=3)
    for i, v in enumerate(vars):
        print(f" {i+1}. {v}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_PromptVaultMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any

# --- RUNTIME DEPENDENCY CHECK ---
REQUIRED = ["pydantic", "jinja2"]
MISSING = []

for lib in REQUIRED:
    if importlib.util.find_spec(lib) is None:
        MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _PromptVaultMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # We proceed so the class loads, but methods will likely fail if deps are missing.

from pydantic import BaseModel
from jinja2 import Environment, BaseLoader

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION & MODELS
# ==============================================================================

DB_PATH = Path(__file__).parent / "prompt_vault.db"
logger = logging.getLogger("PromptVault")

class PromptVersion(BaseModel):
    """A specific historical version of a prompt."""
    version_num: int
    content: str
    author: str
    timestamp: datetime.datetime
    embedding: Optional[List[float]] = None

class PromptTemplate(BaseModel):
    """The master record for a prompt."""
    id: str
    slug: str
    title: str
    description: Optional[str] = ""
    tags: List[str] = []
    latest_version_num: int
    versions: List[PromptVersion] = []
    
    @property
    def latest(self) -> PromptVersion:
        """Helper to get the most recent content."""
        if not self.versions:
            raise ValueError("No versions found.")
        # Sort by version number to be safe
        return sorted(self.versions, key=lambda v: v.version_num)[-1]


# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="PromptVault",
    version="1.0.0",
    description="A persistent SQLite store for managing, versioning, and rendering AI prompts.",
    tags=["prompt", "database", "versioning", "jinja"],
    capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class PromptVaultMS:
    """
    The Vault: A persistent SQLite store for managing, versioning, 
    and rendering AI prompts.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.db_path = Path(self.config.get("db_path", DB_PATH))
        self._init_db()
        self.jinja_env = Environment(loader=BaseLoader())

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        """Bootstraps the schema."""
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    slug TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    tags_json TEXT,
                    latest_version INTEGER DEFAULT 1,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS versions (
                    id TEXT PRIMARY KEY,
                    template_id TEXT,
                    version_num INTEGER,
                    content TEXT,
                    author TEXT,
                    timestamp TIMESTAMP,
                    embedding_json TEXT,
                    FOREIGN KEY(template_id) REFERENCES templates(id)
                )
            """)

    @service_endpoint(
        inputs={"slug": "str", "title": "str", "content": "str", "author": "str", "tags": "List[str]"},
        outputs={"template": "Dict"},
        description="Creates a new prompt template with an initial version.",
        tags=["prompt", "create"],
        side_effects=["db:write"]
    )
    def create_template(self, slug: str, title: str, content: str, author: str = "system", tags: List[str] = None) -> Dict[str, Any]:
        """Creates a new prompt template with an initial version 1."""
        tags = tags or []
        now = datetime.datetime.utcnow()
        t_id = str(uuid.uuid4())
        v_id = str(uuid.uuid4())

        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO templates (id, slug, title, description, tags_json, latest_version, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                    (t_id, slug, title, "", json.dumps(tags), 1, now, now)
                )
                conn.execute(
                    "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                    (v_id, t_id, 1, content, author, now)
                )
            logger.info(f"Created template: {slug}")
            # Return dict representation
            tpl = self.get_template(slug)
            return tpl.dict() if tpl else {}
        except sqlite3.IntegrityError:
            raise ValueError(f"Template '{slug}' already exists.")

    @service_endpoint(
        inputs={"slug": "str", "content": "str", "author": "str"},
        outputs={"template": "Dict"},
        description="Adds a new version to an existing template.",
        tags=["prompt", "update"],
        side_effects=["db:write"]
    )
    def add_version(self, slug: str, content: str, author: str = "user") -> Dict[str, Any]:
        """Adds a new version to an existing template."""
        current = self.get_template(slug)
        if not current:
            raise ValueError(f"Template '{slug}' not found.")

        new_ver = current.latest_version_num + 1
        now = datetime.datetime.utcnow()
        v_id = str(uuid.uuid4())

        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                (v_id, current.id, new_ver, content, author, now)
            )
            conn.execute(
                "UPDATE templates SET latest_version = ?, updated_at = ? WHERE id = ?",
                (new_ver, now, current.id)
            )
        logger.info(f"Updated {slug} to v{new_ver}")
        
        tpl = self.get_template(slug)
        return tpl.dict() if tpl else {}

    @service_endpoint(
        inputs={"slug": "str"},
        outputs={"template": "Optional[PromptTemplate]"},
        description="Retrieves a full template with all history.",
        tags=["prompt", "read"],
        side_effects=["db:read"]
    )
    def get_template(self, slug: str) -> Optional[PromptTemplate]:
        """Retrieves a full template with all history."""
        with self._get_conn() as conn:
            # 1. Fetch Template
            row = conn.execute("SELECT * FROM templates WHERE slug = ?", (slug,)).fetchone()
            if not row: return None

            # 2. Fetch Versions
            v_rows = conn.execute("SELECT * FROM versions WHERE template_id = ? ORDER BY version_num ASC", (row['id'],)).fetchall()

            versions = []
            for v in v_rows:
                versions.append(PromptVersion(
                    version_num=v['version_num'],
                    content=v['content'],
                    author=v['author'],
                    timestamp=v['timestamp']
                    # embedding logic skipped for brevity
                ))

            return PromptTemplate(
                id=row['id'],
                slug=row['slug'],
                title=row['title'],
                description=row['description'],
                tags=json.loads(row['tags_json']),
                latest_version_num=row['latest_version'],
                versions=versions
            )

    @service_endpoint(
        inputs={"slug": "str", "context": "Dict"},
        outputs={"rendered_text": "str"},
        description="Fetches the latest version and renders it with Jinja2.",
        tags=["prompt", "render"],
        side_effects=["db:read"]
    )
    def render(self, slug: str, context: Optional[Dict[str, Any]] = None) -> str:
        """Fetches the latest version and renders it with Jinja2."""
        template = self.get_template(slug)
        if not template:
            raise ValueError(f"Template '{slug}' not found.")
        
        raw_text = template.latest.content
        jinja_template = self.jinja_env.from_string(raw_text)
        return jinja_template.render(**(context or {}))

    @service_endpoint(
        inputs={},
        outputs={"slugs": "List[str]"},
        description="Lists all available prompt slugs.",
        tags=["prompt", "list"],
        side_effects=["db:read"]
    )
    def list_slugs(self) -> List[str]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT slug FROM templates").fetchall()
            return [r[0] for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_file = Path("test_prompt_vault.db")
    
    # 1. Setup
    if db_file.exists(): os.remove(db_file)
    vault = PromptVaultMS({"db_path": db_file})
    print("Service ready:", vault)
    
    # 2. Create
    print("--- Creating Prompt ---")
    vault.create_template(
        slug="greet_user",
        title="Greeting Protocol",
        content="Hello {{ name }}, welcome to the {{ system_name }}!",
        tags=["ui", "onboarding"]
    )
    
    # 3. Versioning
    print("--- Updating Prompt ---")
    vault.add_version("greet_user", "Greetings, {{ name }}. System {{ system_name }} is online.")
    
    # 4. Retrieval & Rendering
    print("--- Rendering ---")
    final_text = vault.render("greet_user", {"name": "Alice", "system_name": "Nexus"})
    print(f"Rendered Output: {final_text}")
    
    # 5. Inspection
    tpl = vault.get_template("greet_user")
    if tpl:
        print(f"Current Version: v{tpl.latest_version_num}")
        print(f"History: {[v.content for v in tpl.versions]}")
        
    # Cleanup
    if db_file.exists(): os.remove(db_file)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_PythonChunkerMS.py
--------------------------------------------------------------------------------
import ast
import time
from dataclasses import dataclass, asdict
from typing import List, Dict, Any

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# HELPER DATA MODEL
# ==============================================================================

@dataclass
class CodeChunk:
    name: str          # e.g., "class AuthMS" or "def login"
    type: str          # "class", "function", "text"
    content: str       # The raw source code of the chunk
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="PythonChunker",
    version="1.2.0",
    description="The Python Surgeon: Specialist in Abstract Syntax Tree (AST) parsing for Python source code.",
    tags=["chunking", "python", "ast"],
    capabilities=["python-ast"]
)
class PythonChunkerMS:
    """
    Specialized Python AST Chunker.
    Focuses exclusively on identifying classes and functions to preserve code logic.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "specialty": "str"},
        description="Standardized health check for the Python specialist service.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the PythonChunkerMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "specialty": "python_ast"
        }

    @service_endpoint(
        inputs={"content": "str"},
        outputs={"chunks": "List[Dict]"},
        description="Primary entry point for high-fidelity Python-specific AST chunking.",
        tags=["processing", "python"]
    )
    def chunk(self, content: str) -> List[Dict[str, Any]]:
        """Parses Python source into semantic CodeChunks."""
        # Convert dataclasses to dicts for JSON serialization safety
        chunks = self._chunk_python(content)
        return [asdict(c) for c in chunks]

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                # end_lineno is available in Python 3.8+
                end = node.end_lineno if hasattr(node, "end_lineno") and node.end_lineno else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", 
                        type="function", 
                        content=text, 
                        start_line=s, 
                        end_line=e, 
                        docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"class {node.name}", 
                        type="class", 
                        content=text, 
                        start_line=s, 
                        end_line=e, 
                        docstring=doc
                    ))

            # Fallback: Return as single chunk if no structures found (e.g., flat script)
            if not chunks and source.strip():
                chunks.append(CodeChunk(
                    name="module_level", 
                    type="text", 
                    content=source, 
                    start_line=1, 
                    end_line=len(lines)
                ))
                
        except SyntaxError:
            # If the code is unparseable, return the whole block as a raw chunk
            chunks.append(CodeChunk(
                name="syntax_error_fallback", 
                type="text", 
                content=source, 
                start_line=1, 
                end_line=source.count('\n') + 1
            ))
            
        return chunks


if __name__ == "__main__":
    svc = PythonChunkerMS()
    print("Service ready:", svc)
    
    # Test on a Python snippet
    test_code = "class Test:\n    def run(self):\n        pass"
    results = svc.chunk(test_code)
    
    for c in results:
        print(f"[{c['type']}] {c['name']} (Lines {c['start_line']}-{c['end_line']})")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_RefineryServiceMS.py
--------------------------------------------------------------------------------
import json
import re
import os
import time
import ast
import concurrent.futures
import logging
from typing import Dict, List, Any, Optional, Tuple

# Assume these are available in the local environment
try:
    from _CartridgeServiceMS import CartridgeServiceMS
    from _NeuralServiceMS import NeuralServiceMS
    from _ChunkingRouterMS import ChunkingRouterMS
except ImportError:
    # Fallbacks for static analysis or isolation
    CartridgeServiceMS = Any
    NeuralServiceMS = Any
    ChunkingRouterMS = Any

from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("RefineryService")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="RefineryService",
    version="1.1.0",
    description="The Night Shift: Processes 'RAW' files into semantic chunks and weaves them into a knowledge graph.",
    tags=["processing", "refinery", "graph", "RAG"],
    capabilities=["smart-chunking", "graph-weaving", "parallel-embedding"]
)
class RefineryServiceMS:
    """
    The Night Shift.
    Polls the DB for 'RAW' files and processes them into Chunks and Graph Nodes.

    Graph Enrichment:
    - Code: function/class nodes, resolved import edges when possible.
    - Docs: section/chapter nodes for long-form text (md/txt/rst).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        
        # Dependencies must be injected via config in this architecture
        self.cartridge = self.config.get("cartridge")
        self.neural = self.config.get("neural")
        
        # Instantiate internal router
        self.chunker = ChunkingRouterMS() if ChunkingRouterMS != Any else None
        
        self.start_time = time.time()
        
        # Import parsing / resolution
        self.import_pattern = re.compile(r"""(?:from|import)\s+([\w\.]+)|require\(['"]([\w\.\-/]+)['"]\)""")

        # Lightweight module/path index cache for resolving imports to VFS files
        self._module_index: Dict[str, str] = {}
        self._path_index: Dict[str, str] = {}
        self._index_built: bool = False

        # Simple section/chapter detection
        self._md_heading = re.compile(r"^(#{1,6})\s+(.+?)\s*$")
        self._chapter_heading = re.compile(r"^\s*(chapter|CHAPTER)\s+([0-9]+|[IVXLC]+)\b\s*[:\-]?\s*(.*)$")
        
        # Initial setup if dependencies exist
        if self.cartridge and self.neural:
            self._stamp_specs()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "cartridge_health": "str"},
        description="Standardized health check to verify the operational state of the Refinery service.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the RefineryServiceMS."""
        cart_status = "UNKNOWN"
        if self.cartridge:
            # Assuming cartridge has a status method or dict
            cart_status = "CONNECTED" 
        
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "cartridge_health": cart_status
        }

    def _stamp_specs(self):
        """Writes the active Neural/Chunker configuration to the Manifest."""
        try:
            # 1. Embedding Spec
            # We assume 1024 dim for mxbai-large, but ideally we'd probe it.
            # Access config from the neural service instance
            embed_model = getattr(self.neural, "models", {}).get("embed", "default")
            
            spec = {
                "provider": "ollama",
                "model": embed_model,
                "dim": 1024,  # Hardcoded for now based on mxbai-embed-large
                "dtype": "float32",
                "distance": "cosine"
            }
            if self.cartridge:
                self.cartridge.set_manifest("embedding_spec", spec)

        except Exception as e:
            logger.error(f"Failed to stamp specs: {e}")

    def _build_import_index(self):
        """Builds caches for resolving imports to VFS targets."""
        if self._index_built or not self.cartridge:
            return

        path_index: Dict[str, str] = {}
        module_index: Dict[str, str] = {}

        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            for (vp,) in rows:
                if not vp:
                    continue
                vfs_path = str(vp).replace("\\", "/")
                path_index[vfs_path] = vfs_path

                # Python module mapping
                if vfs_path.endswith(".py"):
                    mod = vfs_path[:-3].strip("/")
                    mod = mod.replace("/", ".")
                    if mod:
                        module_index[mod] = vfs_path

                    # If it's a package __init__.py, map the package name too
                    if vfs_path.endswith("/__init__.py"):
                        pkg = vfs_path[:-len("/__init__.py")].strip("/").replace("/", ".")
                        if pkg:
                            module_index[pkg] = vfs_path

        finally:
            conn.close()

        self._path_index = path_index
        self._module_index = module_index
        self._index_built = True

    @service_endpoint(
        inputs={"batch_size": "int"},
        outputs={"processed_count": "int"},
        description="Polls the database for files with 'RAW' status and processes them into chunks and graph nodes.",
        tags=["pipeline", "batch"],
        side_effects=["cartridge:write", "neural:inference"]
    )
    def process_pending(self, batch_size: int = 5) -> int:
        """Main loop. Returns number of files processed."""
        if not self.cartridge or not self.neural:
            logger.error("Refinery missing dependencies (Cartridge or Neural).")
            return 0

        pending = self.cartridge.get_pending_files(limit=batch_size)
        if not pending:
            return 0

        logger.info(f"Refining batch of {len(pending)} files...")

        for file_row in pending:
            self._refine_file(file_row)

        return len(pending)

    def _refine_file(self, row: Dict):
        file_id = row['id']
        vfs_path = row['vfs_path']
        content = row['content']

        # Skip binary files for now (unless we add OCR later)
        if not content:
            self.cartridge.update_status(file_id, "SKIPPED_BINARY")
            return

        try:
            # 1. Specialized Chunking via Router
            chunks = self.chunker.chunk_file(content, vfs_path)

            # 2. Vectorization & Storage
            chunk_texts = [c.content for c in chunks]

            # Buffer graph writes while DB transaction is open (prevents nested-writer locks)
            pending_nodes: List[Tuple[str, str, str, Dict[str, Any]]] = []
            pending_edges: List[Tuple[str, str, str, float]] = []

            # Use ThreadPool to embed in parallel (preserve order with map)
            max_workers = getattr(self.neural, "max_workers", 4)
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                vectors = list(executor.map(self.neural.get_embedding, chunk_texts))

            conn = self.cartridge._get_conn()
            try:
                cursor = conn.cursor()

                for i, chunk in enumerate(chunks):
                    vector = vectors[i]
                    vec_blob = json.dumps(vector).encode('utf-8') if vector else None

                    # Store Chunk
                    cursor.execute(
                        """
                        INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (file_id, i, chunk.content, vec_blob, chunk.name, chunk.type, chunk.start_line, chunk.end_line)
                    )

                    chunk_row_id = cursor.lastrowid

                    # Insert into Vector Index (if vector exists)
                    if vector:
                        try:
                            cursor.execute(
                                "INSERT INTO vec_items(rowid, embedding) VALUES (?, ?)",
                                (chunk_row_id, json.dumps(vector))
                            )
                        except Exception as ve:
                            logger.error(f"Vector Index Insert Failed: {ve}")

                    # Graph Node for Chunks (Functions/Classes)
                    if chunk.type in ['class', 'function']:
                        node_id = f"{vfs_path}::{chunk.name}"
                        pending_nodes.append(
                            (
                                node_id,
                                'chunk',
                                chunk.name,
                                {
                                    'parent': vfs_path,
                                    'file_id': file_id,
                                    'chunk_row_id': chunk_row_id,
                                    'chunk_type': chunk.type,
                                    'start_line': chunk.start_line,
                                    'end_line': chunk.end_line
                                }
                            )
                        )
                        pending_edges.append((node_id, vfs_path, "defined_in", 1.0))

                conn.commit()

            finally:
                conn.close()

            # 3. File Level Graph Node (after close)
            pending_nodes.append((vfs_path, 'file', vfs_path.split('/')[-1], {'path': vfs_path, 'file_id': file_id}))

            # 4. Section/Chapter Weaving (docs)
            self._weave_sections(vfs_path, content)

            # 5. Import Weaving (resolved when possible)
            self._weave_imports(vfs_path, content)

            # 6. Flush buffered graph writes
            for nid, ntype, label, data in pending_nodes:
                self.cartridge.add_node(nid, ntype, label, data)
            for src, tgt, rel, w in pending_edges:
                self.cartridge.add_edge(src, tgt, rel, w)

            # 7. Mark file refined
            self.cartridge.update_status(file_id, "REFINED")

        except Exception as e:
            logger.error(f"Refining failed for {vfs_path}: {e}")
            self.cartridge.update_status(file_id, "ERROR", {"error": str(e)})

    def _extract_imports_python(self, source_path: str, content: str) -> List[Tuple[str, int, int]]:
        """Returns list of (module_or_path, level, lineno)."""
        out: List[Tuple[str, int, int]] = []
        try:
            tree = ast.parse(content)
        except Exception:
            return out

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias and alias.name:
                        out.append((alias.name, 0, getattr(node, 'lineno', 0)))
            elif isinstance(node, ast.ImportFrom):
                level = int(getattr(node, 'level', 0) or 0)
                mod = getattr(node, 'module', None) or ""
                if mod:
                    out.append((mod, level, getattr(node, 'lineno', 0)))
                else:
                    # from . import x
                    for alias in node.names:
                        if alias and alias.name:
                            out.append((alias.name, level, getattr(node, 'lineno', 0)))
        return out

    def _resolve_python_import(self, source_path: str, module: str, level: int) -> List[str]:
        """Resolve a python import to possible VFS target paths."""
        self._build_import_index()

        # Absolute: try direct module mapping
        if level <= 0:
            if module in self._module_index:
                return [self._module_index[module]]
            return []

        # Relative: resolve from the source directory
        src_dir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        base_parts = src_dir.split("/") if src_dir else []

        # level=1 means "from .", so pop 0; level=2 means "from .." pop 1, etc.
        pops = max(level - 1, 0)
        if pops > 0 and pops <= len(base_parts):
            base_parts = base_parts[:-pops]

        rel_base = "/".join([p for p in base_parts if p])
        mod_path = module.replace(".", "/").strip("/")

        candidates: List[str] = []
        if rel_base:
            if mod_path:
                candidates.append(f"{rel_base}/{mod_path}.py")
                candidates.append(f"{rel_base}/{mod_path}/__init__.py")
            else:
                candidates.append(f"{rel_base}/__init__.py")
        else:
            if mod_path:
                candidates.append(f"{mod_path}.py")
                candidates.append(f"{mod_path}/__init__.py")

        return [c for c in candidates if c in self._path_index]

    def _resolve_js_like_import(self, source_path: str, imp: str) -> List[str]:
        """Resolve require('./x') / import ... from './x' to VFS candidates."""
        self._build_import_index()

        sdir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        raw = imp.strip().replace("\\", "/")

        # Only try to resolve relative-ish paths
        if not (raw.startswith(".") or raw.startswith("/")):
            return []

        # Normalize
        if raw.startswith("/"):
            rel = raw.lstrip("/")
        else:
            rel = os.path.normpath(os.path.join(sdir, raw)).replace("\\", "/").lstrip("./")

        ext_candidates = [rel]
        # Common extensions
        if not os.path.splitext(rel)[1]:
            ext_candidates.extend([rel + ".js", rel + ".ts", rel + ".json"]) 
            ext_candidates.extend([rel + "/index.js", rel + "/index.ts"]) 

        return [c for c in ext_candidates if c in self._path_index]

    def _weave_imports(self, source_path: str, content: str):
        """Scans content for imports and links them in the graph."""
        targets_resolved: List[str] = []

        # Python: use AST when possible
        if source_path.endswith(".py"):
            for mod, level, lineno in self._extract_imports_python(source_path, content):
                resolved = self._resolve_python_import(source_path, mod, level)
                if resolved:
                    for tgt in resolved:
                        self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                        targets_resolved.append(tgt)
                else:
                    self.cartridge.add_edge(source_path, mod, "imports_unresolved", 0.25)
            return

        # JS / generic: regex fallback
        for line in content.splitlines():
            match = self.import_pattern.search(line)
            if not match:
                continue

            imp = match.group(1) or match.group(2)
            if not imp:
                continue

            resolved = self._resolve_js_like_import(source_path, imp)
            if resolved:
                for tgt in resolved:
                    self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                    targets_resolved.append(tgt)
            else:
                self.cartridge.add_edge(source_path, imp, "imports_unresolved", 0.25)

    def _weave_sections(self, vfs_path: str, content: str):
        """Creates section/chapter nodes for long-form text and links them to the file node."""
        ext = os.path.splitext(vfs_path)[1].lower()
        if ext not in (".md", ".markdown", ".txt", ".rst"):
            return

        lines = content.splitlines()
        for idx, line in enumerate(lines):
            lineno = idx + 1

            m = self._md_heading.match(line)
            if m:
                hashes = m.group(1)
                title = (m.group(2) or "").strip()
                level = len(hashes)
                if title:
                    node_id = f"{vfs_path}::section::{lineno}:{title}"
                    self.cartridge.add_node(node_id, "section", title, {
                        "parent": vfs_path,
                        "level": level,
                        "line": lineno
                    })
                    self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)
                continue

            c = self._chapter_heading.match(line)
            if c:
                chap_num = (c.group(2) or "").strip()
                chap_title = (c.group(3) or "").strip()
                title = f"Chapter {chap_num}" + (f": {chap_title}" if chap_title else "")
                node_id = f"{vfs_path}::chapter::{lineno}:{chap_num}"
                self.cartridge.add_node(node_id, "section", title, {
                    "parent": vfs_path,
                    "level": 1,
                    "line": lineno
                })
                self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)


# --- Independent Test Block ---
if __name__ == "__main__":
    # Requires Cartridge and Neural services for testing
    try:
        from _CartridgeServiceMS import CartridgeServiceMS
        from _NeuralServiceMS import NeuralServiceMS
        
        print("Initializing Dependencies...")
        c = CartridgeServiceMS({"db_path": ":memory:"})
        n = NeuralServiceMS()
        
        # Inject dependencies via config
        svc = RefineryServiceMS({"cartridge": c, "neural": n})
        print("Service ready:", svc)
        print("Health Check:", svc.get_health())
        
    except ImportError:
        print("Dependencies not found. Run in project context.")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_RegexWeaverMS.py
--------------------------------------------------------------------------------
import re
import logging
from typing import Any, Dict, List, Optional, Set

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: PATTERNS
# ==============================================================================
# Python: "import x", "from x import y"
PY_IMPORT = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')

# JS/TS: "import ... from 'x'", "require('x')"
JS_IMPORT = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

logger = logging.getLogger("RegexWeaver")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="RegexWeaver",
    version="1.0.0",
    description="Fault-tolerant dependency extractor using Regex.",
    tags=["parsing", "dependencies", "regex"],
    capabilities=["compute"]
)
class RegexWeaverMS:
    """
    The Weaver: A fault-tolerant dependency extractor.
    Uses Regex to find imports, making it faster and more permissive
    than AST parsers (works on broken code).
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"content": "str", "language": "str"},
        outputs={"dependencies": "List[str]"},
        description="Scans code content for import statements.",
        tags=["parsing", "dependencies"],
        side_effects=[]
    )
    def extract_dependencies(self, content: str, language: str) -> List[str]:
        """
        Scans code content for import statements.
        :param language: 'python' or 'javascript' (includes ts/jsx).
        """
        dependencies: Set[str] = set()
        lines = content.splitlines()
        
        pattern = PY_IMPORT if language == 'python' else JS_IMPORT
        
        for line in lines:
            # Skip comments roughly
            if line.strip().startswith(('#', '//')):
                continue
                
            if language == 'python':
                match = pattern.match(line)
            else:
                match = pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                # Clean up: "backend.database" -> "database"
                # We usually want the leaf name for simple linking
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                dependencies.add(clean_dep)
                
        return sorted(list(dependencies))


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging for test
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    
    weaver = RegexWeaverMS()
    print("Service ready:", weaver)
    
    # 1. Python Test
    py_code = """
    import os
    from backend.utils import helper
    # from commented.out import ignore_me
    import pandas as pd
    """
    print(f"Python Deps: {weaver.extract_dependencies(py_code, 'python')}")
    
    # 2. JS Test
    js_code = """
    import React from 'react';
    const utils = require('./lib/utils');
    // import hidden from 'hidden';
    """

    print(f"JS Deps:     {weaver.extract_dependencies(js_code, 'javascript')}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_RoleManagerMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any

# --- RUNTIME DEPENDENCY CHECK ---
REQUIRED = ["pydantic"]
MISSING = []

for lib in REQUIRED:
    if importlib.util.find_spec(lib) is None:
        MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _RoleManagerMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # Proceeding so the class definition loads, but functionality will break.

from pydantic import BaseModel
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION & MODELS
# ==============================================================================

DB_PATH = Path("roles.db")
logger = logging.getLogger("RoleManager")

class RoleModel(BaseModel):
    """Data model representing an Agent Persona."""
    id: str
    name: str
    description: Optional[str] = ""
    system_prompt: str
    knowledge_bases: List[str] = []
    memory_policy: str = "scratchpad" # or 'auto_commit'
    created_at: datetime.datetime


# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="RoleManager",
    version="1.0.0",
    description="Manages Agent Personas (Roles), including System Prompts and Memory Settings.",
    tags=["roles", "personas", "db"],
    capabilities=["db:sqlite"]
)
class RoleManagerMS:
    """
    The Casting Director: Manages Agent Personas (Roles).
    Persists configuration for System Prompts, Attached KBs, and Memory Settings.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Allow config to override DB path
        self.db_path = Path(self.config.get("db_path", DB_PATH))
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS roles (
                    id TEXT PRIMARY KEY,
                    name TEXT UNIQUE NOT NULL,
                    description TEXT,
                    system_prompt TEXT NOT NULL,
                    knowledge_bases_json TEXT,
                    memory_policy TEXT,
                    created_at TIMESTAMP
                )
            """)

    @service_endpoint(
        inputs={"name": "str", "system_prompt": "str", "description": "str", "kbs": "List[str]"},
        outputs={"role": "Dict"},
        description="Creates a new Agent Persona.",
        tags=["roles", "create"],
        side_effects=["db:write"]
    )
    def create_role(self, name: str, system_prompt: str, description: str = "", kbs: List[str] = None) -> Dict[str, Any]:
        """Creates a new Agent Persona."""
        role_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        kbs_json = json.dumps(kbs or [])
        
        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO roles (id, name, description, system_prompt, knowledge_bases_json, memory_policy, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    (role_id, name, description, system_prompt, kbs_json, "scratchpad", now)
                )
            logger.info(f"Created Role: {name}")
            
            # Return the created object as a dict
            role = self.get_role(name)
            return role.dict() if role else {}
            
        except sqlite3.IntegrityError:
            raise ValueError(f"Role '{name}' already exists.")

    @service_endpoint(
        inputs={"name_or_id": "str"},
        outputs={"role": "Optional[RoleModel]"},
        description="Retrieves a role by Name or ID.",
        tags=["roles", "read"],
        side_effects=["db:read"]
    )
    def get_role(self, name_or_id: str) -> Optional[RoleModel]:
        """Retrieves a role by Name or ID."""
        with self._get_conn() as conn:
            # Try ID first
            row = conn.execute("SELECT * FROM roles WHERE id = ?", (name_or_id,)).fetchone()
            if not row:
                # Try Name
                row = conn.execute("SELECT * FROM roles WHERE name = ?", (name_or_id,)).fetchone()
            
            if not row: return None

            return RoleModel(
                id=row['id'],
                name=row['name'],
                description=row['description'],
                system_prompt=row['system_prompt'],
                knowledge_bases=json.loads(row['knowledge_bases_json']),
                memory_policy=row['memory_policy'],
                # SQLite usually returns ISO string or similar for timestamps
                created_at=row['created_at'] 
            )

    @service_endpoint(
        inputs={},
        outputs={"roles": "List[Dict]"},
        description="Lists all available roles.",
        tags=["roles", "read"],
        side_effects=["db:read"]
    )
    def list_roles(self) -> List[Dict[str, Any]]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT id, name, description FROM roles").fetchall()
            return [dict(r) for r in rows]

    @service_endpoint(
        inputs={"name": "str"},
        outputs={},
        description="Deletes a role by name.",
        tags=["roles", "delete"],
        side_effects=["db:write"]
    )
    def delete_role(self, name: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM roles WHERE name = ?", (name,))
        logger.info(f"Deleted Role: {name}")


# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # Use a test DB file
    test_db = Path("test_roles.db")
    if test_db.exists(): 
        os.remove(test_db)
        
    logging.basicConfig(level=logging.INFO)
    
    mgr = RoleManagerMS({"db_path": test_db})
    print("Service ready:", mgr)
    
    # 1. Create
    mgr.create_role(
        name="SeniorDev", 
        system_prompt="You are a senior Python developer. Prefer Clean Code principles.",
        description="Expert coding assistant",
        kbs=["python_docs", "project_repo"]
    )
    
    # 2. Retrieve
    role = mgr.get_role("SeniorDev")
    if role:
        print(f"Role: {role.name}")
        print(f"Prompt: {role.system_prompt}")
        print(f"KBs: {role.knowledge_bases}")
    
    # Cleanup
    if test_db.exists(): 
        os.remove(test_db)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_SandboxManagerMS.py
--------------------------------------------------------------------------------
import shutil
import hashlib
import os
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple, Any

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Default folders to ignore when syncing or diffing
DEFAULT_EXCLUDES = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "_sandbox", "_project_library"
}

logger = logging.getLogger("SandboxMgr")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="SandboxManager",
    version="1.0.0",
    description="The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project for safe experimentation.",
    tags=["filesystem", "safety", "versioning"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class SandboxManagerMS:
    """
    The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
    Allows for safe experimentation, diffing, and atomic promotion of changes.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # In a real microservice usage, paths might come from config
        self.live_root = Path(self.config.get("live_path", "./project")).resolve()
        self.sandbox_root = Path(self.config.get("sandbox_path", "./_sandbox")).resolve()

    @service_endpoint(
        inputs={"force": "bool"},
        outputs={},
        description="Creates or resets the sandbox by mirroring the live project.",
        tags=["sandbox", "reset"],
        side_effects=["filesystem:write"]
    )
    def init_sandbox(self, force: bool = False):
        """
        Creates or resets the sandbox by mirroring the live project.
        """
        if self.sandbox_root.exists():
            if not force:
                raise FileExistsError(f"Sandbox already exists at {self.sandbox_root}")
            logger.info("Wiping existing sandbox...")
            shutil.rmtree(self.sandbox_root)
        
        logger.info(f"Cloning {self.live_root} -> {self.sandbox_root}...")
        self._mirror_tree(self.live_root, self.sandbox_root)
        logger.info("Sandbox initialized.")

    @service_endpoint(
        inputs={},
        outputs={},
        description="Discards all sandbox changes and re-syncs from live.",
        tags=["sandbox", "reset"],
        side_effects=["filesystem:write"]
    )
    def reset_sandbox(self):
        """
        Discards all sandbox changes and re-syncs from live.
        """
        self.init_sandbox(force=True)

    @service_endpoint(
        inputs={},
        outputs={"diff": "Dict[str, List[str]]"},
        description="Compares Sandbox vs Live. Returns added, modified, and deleted files.",
        tags=["sandbox", "diff"],
        side_effects=["filesystem:read"]
    )
    def get_diff(self) -> Dict[str, List[str]]:
        """
        Compares Sandbox vs Live. Returns added, modified, and deleted files.
        """
        sandbox_files = self._scan_files(self.sandbox_root)
        live_files = self._scan_files(self.live_root)
        
        sandbox_paths = set(sandbox_files.keys())
        live_paths = set(live_files.keys())

        # 1. Added: In sandbox but not in live
        added = sorted(list(sandbox_paths - live_paths))
        
        # 2. Deleted: In live but not in sandbox
        deleted = sorted(list(live_paths - sandbox_paths))
        
        # 3. Modified: In both, but hashes differ
        common = sandbox_paths.intersection(live_paths)
        modified = []
        for rel_path in common:
            if sandbox_files[rel_path] != live_files[rel_path]:
                modified.append(rel_path)
        modified.sort()

        return {
            "added": added,
            "modified": modified,
            "deleted": deleted
        }

    @service_endpoint(
        inputs={},
        outputs={"added": "int", "modified": "int", "deleted": "int"},
        description="Applies changes from Sandbox to Live.",
        tags=["sandbox", "promote"],
        side_effects=["filesystem:write"]
    )
    def promote_changes(self) -> Tuple[int, int, int]:
        """
        Applies changes from Sandbox to Live.
        Returns (added_count, modified_count, deleted_count).
        """
        diff = self.get_diff()
        
        # 1. Additions & Modifications (Copy file -> file)
        for rel_path in diff['added'] + diff['modified']:
            src = self.sandbox_root / rel_path
            dst = self.live_root / rel_path
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
            
        # 2. Deletions (Remove file)
        for rel_path in diff['deleted']:
            target = self.live_root / rel_path
            if target.exists():
                os.remove(target)
                
        logger.info(f"Promoted: {len(diff['added'])} added, {len(diff['modified'])} modified, {len(diff['deleted'])} deleted.")
        return len(diff['added']), len(diff['modified']), len(diff['deleted'])

    # --------------------------------------------------------------------- #
    # Internal Helpers
    # --------------------------------------------------------------------- #

    def _mirror_tree(self, src_root: Path, dst_root: Path):
        """Recursive copy that respects the exclusion list."""
        if not dst_root.exists():
            dst_root.mkdir(parents=True, exist_ok=True)

        # Handle case where live root doesn't exist yet
        if not src_root.exists():
            return

        for item in src_root.iterdir():
            if item.name in DEFAULT_EXCLUDES:
                continue
                
            dst_path = dst_root / item.name
            
            if item.is_dir():
                self._mirror_tree(item, dst_path)
            else:
                shutil.copy2(item, dst_path)

    def _scan_files(self, root: Path) -> Dict[str, str]:
        """
        Scans directory and returns {relative_path: sha256_hash}.
        """
        file_map = {}
        if not root.exists():
            return {}
            
        for path in root.rglob("*"):
            if path.is_file() and not self._is_excluded(path, root):
                rel = str(path.relative_to(root)).replace("\\", "/")
                file_map[rel] = self._get_hash(path)
        return file_map

    def _is_excluded(self, path: Path, root: Path) -> bool:
        """Checks if any part of the path is in the exclusion list."""
        try:
            rel_parts = path.relative_to(root).parts
            return any(p in DEFAULT_EXCLUDES for p in rel_parts)
        except ValueError:
            return False

    def _get_hash(self, path: Path) -> str:
        """Fast SHA-256 for file content."""
        try:
            # Skip binary files if needed, or hash them too (hashing is safe)
            return hashlib.sha256(path.read_bytes()).hexdigest()
        except Exception:
            return "read_error"


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup test environment
    base = Path("test_env")
    live = base / "live_project"
    box = base / "sandbox"
    
    if base.exists(): shutil.rmtree(base)
    live.mkdir(parents=True)
    
    # 1. Create Mock Live Project
    (live / "main.py").write_text("print('v1')")
    (live / "utils.py").write_text("def help(): pass")
    (live / "node_modules").mkdir() # Should be ignored
    (live / "node_modules" / "junk.js").write_text("junk")
    
    print("--- Initializing Sandbox ---")
    mgr = SandboxManagerMS({"live_path": str(live), "sandbox_path": str(box)})
    mgr.init_sandbox()
    
    # 2. Make Changes in Sandbox
    print("\n--- Modifying Sandbox ---")
    (box / "main.py").write_text("print('v2')")  # Modify
    (box / "new_feature.py").write_text("print('new')")  # Add
    os.remove(box / "utils.py")  # Delete

    # 3. Check Diff
    diff = mgr.get_diff()
    print(f"Diff Analysis:\n Added: {diff['added']}\n Modified: {diff['modified']}\n Deleted: {diff['deleted']}")

    # 4. Promote
    print("\n--- Promoting Changes ---")
    mgr.promote_changes()

    # Verify Live
    print(f"Live 'main.py' content: {(live / 'main.py').read_text()}")
    print(f"Live 'utils.py' exists? {(live / 'utils.py').exists()}")

    # Cleanup
    if base.exists(): shutil.rmtree(base)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ScannerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ScannerMS
ENTRY_POINT: _ScannerMS.py
DEPENDENCIES: None
"""

import os
import time
from typing import Dict, List, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint


@service_metadata(
    name="ScannerMS",
    version="1.0.0",
    description="Recursively scans directories, filters junk, and detects binaries.",
    tags=["filesystem", "scanner", "tree"],
    capabilities=["filesystem:read"],
    dependencies=["os", "time"],
    side_effects=["filesystem:read"]
)
class ScannerMS(BaseService):
    """
    The Scanner: Walks the file system, filters junk, and detects binary files.
    Generates the tree structure used by the UI.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("ScannerMS")
        self.config = config or {}

        # Folders to completely ignore (Standard developer noise)
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env',
            '.idea', '.vscode', 'dist', 'build', 'coverage'
        }

        # Extensions that are explicitly binary/junk
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class',
            '.jpg', '.jpeg', '.png', '.gif', '.ico', '.svg',
            '.zip', '.tar', '.gz', '.pdf', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }

    def is_binary(self, file_path: str) -> bool:
        """
        Determines if a file is binary using two heuristics:
        1. Extension check (Fast)
        2. Content check for null bytes (Accurate)
        """
        # 1. Fast Fail on Extension
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS:
            return True

        # 2. Content Inspection (Read first 1KB)
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                # Text files shouldn't contain null bytes
                if b'\x00' in chunk:
                    return True
        except (IOError, OSError):
            # If we can't read it, treat as binary/unsafe
            return True

        return False

    @service_endpoint(
        inputs={"root_path": "str"},
        outputs={"tree": "Optional[Dict]"},
        description="Recursively scans a directory and returns a JSON-compatible tree.",
        tags=["filesystem", "scan"],
        side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Optional[Dict[str, Any]]:
        """
        Recursively scans a directory and returns a JSON-compatible tree.
        Returns None if path is invalid.
        """
        target = os.path.abspath(root_path)

        if not os.path.exists(target):
            return None

        if not os.path.isdir(target):
            # Handle single file case
            return self._create_node(target, is_dir=False)

        return self._scan_recursive(target)

    def _scan_recursive(self, current_path: str) -> Dict[str, Any]:
        """
        Internal recursive worker.
        """
        node = self._create_node(current_path, is_dir=True)
        node['children'] = []

        try:
            # os.scandir is faster than os.listdir as it returns file attributes
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))

                for entry in entries:
                    # Skip ignored directories
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS:
                        continue

                    # Skip hidden files (dotfiles)
                    if entry.name.startswith('.'):
                        continue

                    if entry.is_dir():
                        child_node = self._scan_recursive(entry.path)
                        if child_node:  # Only add if valid
                            node['children'].append(child_node)
                    else:
                        child_node = self._create_node(entry.path, is_dir=False)
                        node['children'].append(child_node)

        except PermissionError:
            node['error'] = "Access Denied"

        return node

    def _create_node(self, path: str, is_dir: bool) -> Dict[str, Any]:
        """
        Standardizes the node structure for the UI.
        """
        name = os.path.basename(path)
        node = {
            'text': name,
            'path': path,
            'type': 'folder' if is_dir else 'file',
            'checked': False,  # UI State
        }

        if not is_dir:
            if self.is_binary(path):
                node['type'] = 'binary'

        return node

    @service_endpoint(
        inputs={"tree_node": "Dict"},
        outputs={"files": "List[str]"},
        description="Flattens a tree node into a list of file paths.",
        tags=["filesystem", "utility"],
        side_effects=[]
    )
    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        """
        Helper to extract all valid file paths from a tree node 
        (e.g., when the user clicks 'Start Ingest').
        """
        files = []

        if tree_node['type'] == 'file':
            files.append(tree_node['path'])

        elif tree_node['type'] == 'folder' and 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))

        return files


# --- Independent Test Block ---
if __name__ == "__main__":
    scanner = ScannerMS()
    print("Service ready:", scanner)

    # Scan the current directory
    cwd = os.getcwd()
    print(f"Scanning: {cwd} ...")

    start_time = time.time()
    tree = scanner.scan_directory(cwd)
    duration = time.time() - start_time

    if tree:
        file_count = len(scanner.flatten_tree(tree))
        print(f"Scan complete in {duration:.4f}s")
        print(f"Found {file_count} files.")
        # Print top level children to verify
        print("Top Level Structure:")
        for child in tree.get('children', [])[:5]:
            print(f" - [{child['type'].upper()}] {child['text']}")
    else:
        print("Scan failed or path invalid.")


--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ScoutMS.py
--------------------------------------------------------------------------------
import os
import time
import requests
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# Try imports for Web/PDF support
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

@service_metadata(
    name="Scout",
    version="1.0.0",
    description="The Scout: A depth-aware utility for recursively walking local file systems or crawling websites.",
    tags=["utility", "scanner", "crawler"],
    capabilities=["filesystem:read", "web:crawl"]
)
class ScoutMS:
    """
    The Scanner: Walks file systems OR crawls websites (Depth-Aware).
    """
    
    def __init__(self):
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage', 'site-packages'
        }
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', 
            '.zip', '.tar', '.gz', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }
        self.visited_urls = set()

    def is_binary(self, file_path: str) -> bool:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS: return True
        return False

    @service_endpoint(
        inputs={"root_path": "str", "web_depth": "int"},
        outputs={"tree": "dict"},
        description="Main entry point to perform a recursive scan of a directory or a web crawl.",
        tags=["discovery", "recursive"],
        side_effects=["filesystem:read", "network:read"]
    )
    def scan_directory(self, root_path: str, web_depth: int = 0) -> Optional[Dict[str, Any]]:
        """
        Main Entry Point.
        :param root_path: File path or URL.
        :param web_depth: How many links deep to crawl (0 = single page).
        """
        # 1. Web Crawl Mode
        if root_path.startswith("http://") or root_path.startswith("https://"):
            self.visited_urls.clear()
            return self._crawl_web_recursive(root_path, depth=web_depth, origin_domain=urlparse(root_path).netloc)

        # 2. Local File System Mode
        target = os.path.abspath(root_path)
        if not os.path.exists(target): return None
        
        if not os.path.isdir(target): 
            return self._create_node(target, is_dir=False)
            
        return self._scan_fs_recursive(target)

    # --- Web Logic ---
    def _crawl_web_recursive(self, url: str, depth: int, origin_domain: str) -> Dict[str, Any]:
        """
        Recursively fetches links.
        """
        # Generate a nice VFS path: web/domain/path
        parsed = urlparse(url)
        clean_path = parsed.path.strip("/")
        if not clean_path: clean_path = "index.html"
        rel_path = f"web/{parsed.netloc}/{clean_path}"

        node = {
            'name': url,
            'path': url,
            'rel_path': rel_path,
            'type': 'web',
            'children': [],
            'checked': True
        }
        
        if depth < 0 or url in self.visited_urls: return node
        self.visited_urls.add(url)

        if depth > 0 and BeautifulSoup:
            try:
                # Polite Delay
                time.sleep(0.1)
                resp = requests.get(url, timeout=5)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.content, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        parsed = urlparse(full_url)
                        
                        # Filter: Only same domain, valid schemes
                        if parsed.netloc == origin_domain and parsed.scheme in ['http', 'https']:
                            if full_url not in self.visited_urls:
                                child_node = self._crawl_web_recursive(full_url, depth - 1, origin_domain)
                                node['children'].append(child_node)
            except Exception as e:
                node['error'] = str(e)
                
        return node

    # --- File System Logic ---
    def _scan_fs_recursive(self, current_path: str, root_path: str = None) -> Dict[str, Any]:
        if root_path is None: root_path = current_path
        
        node = self._create_node(current_path, is_dir=True, root_path=root_path)
        node['children'] = []
        try:
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                for entry in entries:
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS: continue
                    if entry.name.startswith('.'): continue

                    if entry.is_dir():
                        child = self._scan_fs_recursive(entry.path, root_path=root_path)
                        if child: node['children'].append(child)
                    else:
                        node['children'].append(self._create_node(entry.path, is_dir=False, root_path=root_path))
        except PermissionError:
            node['error'] = "Access Denied"
        return node

    def _create_node(self, path: str, is_dir: bool, root_path: str = None) -> Dict[str, Any]:
        name = os.path.basename(path)
        # Calculate relative path for VFS
        rel_path = name
        if root_path:
            try:
                rel_path = os.path.relpath(path, root_path).replace("\\", "/")
            except ValueError:
                pass

        node = {
            'name': name, 
            'path': path, 
            'rel_path': rel_path,
            'type': 'folder' if is_dir else 'file', 
            'children': [],
            'checked': False
        }
        return node

    @service_endpoint(
        inputs={"tree_node": "dict"},
        outputs={"file_list": "list"},
        description="Flattens a hierarchical tree node structure into a simple list of paths.",
        tags=["utility", "processing"]
    )
    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        files = []
        if tree_node['type'] in ['file', 'web']:
            files.append(tree_node['path'])
        elif 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files


if __name__ == "__main__":
    svc = ScoutMS()
    print("Service ready:", svc)
    
    # Basic local test
    current_dir = os.path.dirname(os.path.abspath(__file__))
    tree = svc.scan_directory(current_dir)
    if tree:
        print(f"Scanned {len(svc.flatten_tree(tree))} files in current directory.")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_SearchEngineMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import sqlite3
import json
import struct
import requests
import os
import logging
from typing import List, Dict, Any, Optional

# --- RUNTIME DEPENDENCY CHECK ---
REQUIRED = ["requests", "sqlite_vec"] # 'sqlite-vec' package name is often 'sqlite_vec' in pip/import
MISSING = []

for lib in REQUIRED:
    # Handle hyphenated package names for import check vs pip name
    import_name = lib.replace("-", "_")
    if importlib.util.find_spec(import_name) is None:
        MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _SearchEngineMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # We proceed so the class loads, but methods will likely fail.

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================

DEFAULT_OLLAMA_URL = "http://localhost:11434/api"
logger = logging.getLogger("SearchEngine")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="SearchEngine",
    version="1.0.0",
    description="The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching) on SQLite databases.",
    tags=["search", "vector", "hybrid", "rag"],
    capabilities=["db:sqlite", "network:outbound", "compute"]
)
class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    
    Architecture:
    1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
    2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
    3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.model_name = self.config.get("model_name", "phi3:mini-128k")
        self.ollama_url = self.config.get("ollama_url", DEFAULT_OLLAMA_URL)

    @service_endpoint(
        inputs={"db_path": "str", "query": "str", "limit": "int"},
        outputs={"results": "List[Dict]"},
        description="Main entry point. Returns a list of results sorted by relevance (RRF).",
        tags=["search", "query"],
        side_effects=["db:read", "network:outbound"]
    )
    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Main entry point. Returns a list of results sorted by relevance.
        """
        if not os.path.exists(db_path):
            logger.warning(f"Database not found at: {db_path}")
            return []

        conn = sqlite3.connect(db_path)
        # Enable sqlite-vec extension
        try:
            conn.enable_load_extension(True)
            import sqlite_vec
            sqlite_vec.load(conn)
        except Exception as e:
            logger.warning(f"Warning: sqlite_vec not loaded. Vector search may fail. Error: {e}")

        cursor = conn.cursor()

        # 1. Vectorize the User Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails
            logger.info("Vectorization failed. Falling back to keyword-only search.")
            conn.close()
            # Re-open connection is not strictly necessary for fallback logic, 
            # but we return early. Note: _keyword_search_only expects a cursor.
            # We reopen/reuse properly:
            return self._keyword_search_only(db_path, query, limit)

        # Pack vector for sqlite-vec (Float32 Little Endian)
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)

        # 2. HYBRID QUERY (The "Magic" SQL)
        # Note: This SQL assumes a specific schema ('knowledge_vectors', 'documents_fts', 'knowledge_chunks').
        # Ensure your database setup (e.g. Refinery/Librarian) matches these table names.
        sql = """
        WITH 
        vec_matches AS (
            SELECT rowid, distance,
            row_number() OVER (ORDER BY distance) as rank
            FROM knowledge_vectors
            WHERE embedding MATCH ? 
            AND k = 50
        ),
        fts_matches AS (
            SELECT rowid, rank as fts_score,
            row_number() OVER (ORDER BY rank) as rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT 50
        )
        SELECT 
            kc.file_path,
            kc.content,
            (
                -- RRF Formula: 1 / (k + rank)
                COALESCE(1.0 / (60 + v.rank), 0.0) +
                COALESCE(1.0 / (60 + f.rank), 0.0)
            ) as rrf_score
        FROM knowledge_chunks kc
        LEFT JOIN vec_matches v ON kc.id = v.rowid
        LEFT JOIN fts_matches f ON kc.id = f.rowid
        WHERE v.rowid IS NOT NULL OR f.rowid IS NOT NULL
        ORDER BY rrf_score DESC
        LIMIT ?;
        """

        try:
            # Escape quotes for FTS
            fts_query = f'"{query}"' 
            rows = cursor.execute(sql, (vec_bytes, fts_query, limit)).fetchall()
        except sqlite3.OperationalError as e:
            logger.error(f"Search Error (likely missing schema or sqlite-vec): {e}")
            return []
        finally:
            conn.close()

        results = []
        for r in rows:
            path, content, score = r
            snippet = self._extract_snippet(content, query)
            results.append({
                "path": path,
                "score": round(score, 4),
                "snippet": snippet,
                # "full_content": content # Optional: Uncomment if full content is needed
            })

        return results

    def _keyword_search_only(self, db_path: str, query: str, limit: int) -> List[Dict[str, Any]]:
        """Fallback if embeddings are offline."""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        sql = """
            SELECT file_path, content
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        try:
            rows = cursor.execute(sql, (f'"{query}"', limit)).fetchall()
            return [{
                "path": r[0], 
                "score": 0.0, 
                "snippet": self._extract_snippet(r[1], query)
            } for r in rows]
        except sqlite3.OperationalError as e:
            logger.error(f"Keyword Search Error: {e}")
            return []
        finally:
            conn.close()

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        """Call Ollama to get the vector for the search query."""
        try:
            res = requests.post(
                f"{self.ollama_url}/embeddings",
                json={"model": self.model_name, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            logger.error(f"Embedding request failed: {e}")
            return None
        return None

    def _extract_snippet(self, content: str, query: str) -> str:
        """Finds the best window of text around the keyword."""
        if not content:
            return ""
            
        lower_content = content.lower()
        parts = query.lower().split()
        lower_query = parts[0] if parts else "" 
        
        idx = lower_content.find(lower_query)
        if idx == -1:
            return content[:200].replace('\n', ' ') + "..."
            
        start = max(0, idx - 60)
        end = min(len(content), idx + 140)
        snippet = content[start:end].replace('\n', ' ')
        return f"...{snippet}..."


# --- Independent Test Block ---
if __name__ == "__main__":
    # Note: Requires a real DB path to work effectively
    print("Initializing Search Engine...")
    engine = SearchEngineMS({"model_name": "phi3:mini-128k"})
    print("Service ready:", engine)
    
    # Example usage:
    # results = engine.search("my_knowledge.db", "python error handling")
    # print(results)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_SemanticChunkerMS.py
--------------------------------------------------------------------------------
import ast
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# HELPER DATA MODEL
# ==============================================================================

@dataclass
class CodeChunk:
    name: str          # e.g., "class AuthMS" or "def login"
    type: str          # "class", "function", "text_block"
    content: str       # The raw source
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="SemanticChunker",
    version="1.0.0",
    description="The Surgeon: Intelligent Code Splitter that parses source code into logical semantic units (Classes, Functions) using AST.",
    tags=["utility", "nlp", "parser"],
    capabilities=["python-ast", "semantic-chunking"]
)
class SemanticChunkerMS:
    """
    Intelligent Code Splitter.
    Parses source code into logical units (Classes, Functions) 
    rather than arbitrary text windows.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"content": "str", "filename": "str"},
        outputs={"chunks": "List[Dict]"},
        description="Main entry point to split a file into semantic chunks based on its extension and content.",
        tags=["processing", "chunking"],
        side_effects=[]
    )
    def chunk_file(self, content: str, filename: str) -> List[Dict[str, Any]]:
        """
        Splits file content into chunks.
        Returns a list of dictionaries suitable for JSON response.
        """
        chunks: List[CodeChunk] = []
        
        # 1. Python Code
        if filename.endswith(".py"):
            chunks = self._chunk_python(content)
        # 2. Text / Prose Documents (Smaller semantic windows)
        elif filename.lower().endswith(('.md', '.txt', '.pdf', '.html', '.htm', '.rst')):
            chunks = self._chunk_generic(content, window_size=800)
        # 3. Fallback (Generic Code/Binary)
        else:
            chunks = self._chunk_generic(content, window_size=1500)
            
        # Convert dataclasses to dicts for serialization
        return [asdict(c) for c in chunks]

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                # end_lineno is available in Python 3.8+
                end = node.end_lineno if hasattr(node, 'end_lineno') and node.end_lineno else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", 
                        type="function", 
                        content=text, 
                        start_line=s, 
                        end_line=e, 
                        docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"class {node.name}", 
                        type="class", 
                        content=text, 
                        start_line=s, 
                        end_line=e, 
                        docstring=doc
                    ))

            # Fallback: If no classes/functions found (e.g., flat script), treat as generic
            if not chunks:
                return self._chunk_generic(source)
                
        except SyntaxError:
            return self._chunk_generic(source)
            
        return chunks

    def _chunk_generic(self, text: str, window_size: int = 1500) -> List[CodeChunk]:
        """Sliding window for non-code files."""
        chunks = []
        # normalize newlines to avoid massive single-line blobs
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        lines = text.splitlines(keepends=True)
        
        current_chunk = []
        current_size = 0
        chunk_idx = 1
        start_line = 1
        
        for i, line in enumerate(lines):
            current_chunk.append(line)
            current_size += len(line)
            
            if current_size >= window_size:
                chunks.append(CodeChunk(
                    name=f"Chunk {chunk_idx}", 
                    type="text_block",
                    content="".join(current_chunk), 
                    start_line=start_line, 
                    end_line=i + 1
                ))
                current_chunk = []
                current_size = 0
                chunk_idx += 1
                start_line = i + 2
                
        if current_chunk:
            chunks.append(CodeChunk(
                name=f"Chunk {chunk_idx}", 
                type="text_block",
                content="".join(current_chunk), 
                start_line=start_line, 
                end_line=len(lines)
            ))
            
        return chunks


# --- Independent Test Block ---
if __name__ == "__main__":
    svc = SemanticChunkerMS()
    print("Service ready:", svc)
    
    # Basic test on a snippet of Python code
    test_code = "def hello():\n    print('world')\n\nclass Test:\n    pass"
    results = svc.chunk_file(test_code, "test.py")
    
    print(f"Extracted {len(results)} semantic chunks.")
    for c in results:
        print(f" - [{c['type']}] {c['name']} ({c['start_line']}-{c['end_line']})")
        print(f" - [{c['type']}] {c['name']} ({c['start_line']}-{c['end_line']})")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ServiceRegistryMS.py
--------------------------------------------------------------------------------
import ast
import json
import uuid
import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logger = logging.getLogger("ServiceRegistry")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="ServiceRegistry",
    version="1.0.0",
    description="Scans a library of Python microservices and generates standardized JSON Service Tokens.",
    tags=["introspection", "registry", "parsing"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ServiceRegistryMS:
    """
    The Tokenizer (v2): Scans a library of Python microservices and generates
    standardized JSON 'Service Tokens'.
    Feature: Hybrid AST/Regex parsing for maximum robustness.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Default to current directory if not specified
        self.root = Path(self.config.get("root_path", ".")).resolve()
        self.registry = []

    @service_endpoint(
        inputs={"save_to": "str"},
        outputs={"registry": "List[Dict]"},
        description="Scans the file system for microservices and builds a registry.",
        tags=["introspection", "scan"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict[str, Any]]:
        logger.info(f"Scanning for microservices in: {self.root}")
        self.registry = [] # Reset registry
        
        # 1. Walk directories/files
        if self.root.exists():
            for item in self.root.iterdir():
                # Check for Service Folders (e.g. _AuthMS)
                if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                    self._process_folder(item)
                # Check for Service Files (e.g. __AuthMS.py)
                elif item.is_file() and item.name.startswith("_") and item.name.endswith("MS.py"):
                    token = self._tokenize_file(item)
                    if token:
                        self.registry.append(token)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            logger.info(f"‚úÖ Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            logger.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_folder(self, folder: Path):
        # Find the main .py file (usually matches folder name, or is the only .py file)
        candidates = list(folder.glob("*.py"))
        for file in candidates:
            # Usually entry points start with __ inside the folder
            if file.name.startswith("__") or len(candidates) == 1:
                token = self._tokenize_file(file)
                if token:
                    self.registry.append(token)
                    logger.info(f"  + Tokenized: {token['name']}")
                    break 

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source = f.read()
            
            # Attempt 1: Strict AST Parsing (The "Right" Way)
            try:
                return self._ast_parse(source, file_path)
            except Exception:
                # Attempt 2: Regex Fallback (The "Survival" Way)
                return self._regex_parse(source, file_path)
                
        except Exception as e:
            logger.warning(f"  - Failed to read {file_path.name}: {e}")
            return None

    def _ast_parse(self, source: str, file_path: Path):
        tree = ast.parse(source)
        target_class = None
        
        # Find class ending in 'MS'
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name.endswith("MS"):
                target_class = node
                break
        
        if not target_class: return None

        # Extract Metadata
        return self._build_token(
            name=target_class.name,
            doc=ast.get_docstring(target_class) or "",
            methods=[
                (n.name, [a.arg for a in n.args.args if a.arg != 'self'], ast.get_docstring(n) or "")
                for n in target_class.body if isinstance(n, ast.FunctionDef) and not n.name.startswith("_")
            ],
            deps=self._extract_ast_imports(tree),
            file_path=file_path
        )

    def _regex_parse(self, source: str, file_path: Path):
        # Find class definition
        class_match = re.search(r'class\s+(\w+MS)', source)
        if not class_match: return None
        name = class_match.group(1)
        
        # Find methods (def name(args):)
        methods = []
        for match in re.finditer(r'def\s+(\w+)\s*\((.*?)\):', source):
            m_name = match.group(1)
            if not m_name.startswith("_"):
                # Rough args parsing
                args = [a.strip().split(':')[0] for a in match.group(2).split(',') if a.strip() != 'self']
                methods.append((m_name, args, "Regex extracted"))
                
        return self._build_token(name, "Parsed via Regex", methods, [], file_path)

    def _build_token(self, name, doc, methods, deps, file_path):
        # Generate deterministic ID
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, name).hex[:8].upper()}"
        
        method_dict = {
            m_name: {"args": m_args, "doc": m_doc.strip()} 
            for m_name, m_args, m_doc in methods
        }
        
        try:
            rel_path = str(file_path.relative_to(self.root)).replace('\\', '/')
        except ValueError:
            rel_path = file_path.name

        return {
            "token_id": token_id,
            "name": name,
            "path": rel_path,
            "description": doc.strip(),
            "methods": method_dict,
            "dependencies": sorted(deps)
        }

    def _extract_ast_imports(self, tree):
        deps = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: deps.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: deps.add(node.module.split('.')[0])
        return list(deps)


if __name__ == "__main__":
    # Setup logging for independent test
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    
    svc = ServiceRegistryMS()
    print("Service ready:", svc)
    # Perform a test scan of the current directory
    svc.scan()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_SpinnerThingyMaBobberMS.py
--------------------------------------------------------------------------------
import tkinter as tk
import math
import colorsys
import time
from typing import Optional, Dict, Any

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="SpinnerTHINGYMABOBBER",
    version="1.0.0",
    description="Interactive visual spinner widget for OBS/UI overlays.",
    tags=["ui", "widget", "visuals"],
    capabilities=["ui:gui"]
)
class SpinnerThingyMaBobberMS:
    """
    The Visualizer: An interactive spinner widget.
    Useful for "Processing..." screens or OBS overlays.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.root = tk.Tk()
        self.root.title("OBS Interactive Spinner")
        self.root.configure(bg="black")
        
        # Default size
        self.root.geometry("600x600")
        
        # Canvas for drawing
        self.canvas = tk.Canvas(
            self.root, 
            bg="black", 
            highlightthickness=0
        )
        self.canvas.pack(fill="both", expand=True)

        # --- STATE VARIABLES ---
        self.angle_1 = 0
        self.angle_2 = 0
        self.angle_3 = 0
        self.hue = 0
        
        # Text Input State
        self.user_text = "PROCESSING"
        self.cursor_visible = True
        self.last_cursor_toggle = time.time()
        
        # Bind keyboard events to the window
        self.root.bind("<Key>", self.handle_keypress)
        
        # Start animation immediately
        self.animate()

    @service_endpoint(
        inputs={},
        outputs={},
        description="Launches the GUI main loop.",
        tags=["ui", "execution"],
        mode="sync",
        side_effects=["ui:block"]
    )
    def launch(self):
        """Starts the Tkinter main event loop."""
        self.root.mainloop()

    def handle_keypress(self, event):
        # Handle Backspace
        if event.keysym == "BackSpace":
            self.user_text = self.user_text[:-1]
        # Handle Escape (Reset to default)
        elif event.keysym == "Escape":
            self.user_text = "PROCESSING"
        # Ignore special keys (Shift, Ctrl, Alt, F-keys, etc.)
        elif len(event.char) == 1 and ord(event.char) >= 32:
            # Limit length to prevent chaos (optional, but 20 is a safe max)
            if len(self.user_text) < 25: 
                self.user_text += event.char.upper()

    def get_neon_color(self, offset=0):
        h = (self.hue + offset) % 1.0
        r, g, b = colorsys.hsv_to_rgb(h, 1.0, 1.0)
        return f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}'

    def draw_arc(self, cx, cy, radius, width, start, extent, color):
        x0 = cx - radius
        y0 = cy - radius
        x1 = cx + radius
        y1 = cy + radius
        
        self.canvas.create_arc(
            x0, y0, x1, y1,
            start=start, extent=extent,
            outline=color, width=width, style="arc"
        )

    def animate(self):
        self.canvas.delete("all")
        
        # Window Dimensions
        w = self.canvas.winfo_width()
        h = self.canvas.winfo_height()
        
        if w < 10 or h < 10:
            self.root.after(50, self.animate)
            return

        cx, cy = w / 2, h / 2
        base_size = min(w, h) / 2
        
        # Update Hue
        self.hue += 0.005
        if self.hue > 1: self.hue = 0
        c1 = self.get_neon_color(0.0)
        c2 = self.get_neon_color(0.3)
        c3 = self.get_neon_color(0.6)

        # --- RINGS ---
            
        # Ring 1
        r1 = base_size * 0.85
        self.angle_1 -= 3
        for i in range(3):
            self.draw_arc(cx, cy, r1, base_size*0.08, self.angle_1 + (i*120), 80, c1)

        # Ring 2
        r2 = base_size * 0.65
        self.angle_2 += 5
        self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2, 160, c2)
        self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2 + 180, 160, c2)

        # Ring 3
        r3 = base_size * 0.45
        self.angle_3 -= 8
        self.draw_arc(cx, cy, r3, base_size*0.04, self.angle_3, 300, c3)
        
        # --- TEXT LOGIC ---
        
        # Toggle cursor every 0.5 seconds
        if time.time() - self.last_cursor_toggle > 0.5:
            self.cursor_visible = not self.cursor_visible
            self.last_cursor_toggle = time.time()
            
        display_text = self.user_text + ("_" if self.cursor_visible else " ")

        # Dynamic Font Scaling
        # We start with a base size (0.15 of window).
        # If text is long (> 8 chars), we shrink it proportionally so it fits.
        text_len = max(len(self.user_text), 1)
        scaling_factor = 1.0
        if text_len > 8:
            scaling_factor = 8 / text_len
            
        font_size = int(base_size * 0.15 * scaling_factor)
        # Ensure font doesn't vanish
        font_size = max(font_size, 10) 

        self.canvas.create_text(
            cx, cy, 
            text=display_text, 
            fill="white", 
            font=("Courier", font_size, "bold")
        )

        self.root.after(30, self.animate)


# --- Independent Test Block ---
if __name__ == "__main__":
    print("Launching Spinner ThingyMaBobber...")
    svc = SpinnerThingyMaBobberMS()
    svc.launch()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_SysInspectorMS.py
--------------------------------------------------------------------------------
import platform
import subprocess
import sys
import datetime
import logging
from typing import Any, Dict, List, Optional

from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("SysInspector")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="SysInspector",
    version="1.0.0",
    description="Gathers hardware and environment statistics via shell commands.",
    tags=["system", "audit", "hardware"],
    capabilities=["os:shell", "compute"]
)
class SysInspectorMS:
    """
    The Auditor: Gathers hardware and environment statistics.
    Supports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={},
        outputs={"report": "str"},
        description="Runs the full audit and returns a formatted string report.",
        tags=["system", "report"],
        side_effects=["os:read"]
    )
    def generate_report(self) -> str:
        """
        Runs the full audit and returns a formatted string report.
        """
        system_os = platform.system()
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = [
            f"System Audit Report",
            f"Generated: {timestamp}",
            f"OS: {system_os} {platform.release()} ({platform.machine()})",
            "-" * 40,
            ""
        ]

        # 1. Hardware Section
        report.append("--- Hardware Information ---")
        if system_os == "Windows":
            report.extend(self._audit_windows())
        elif system_os == "Linux":
            report.extend(self._audit_linux())
        elif system_os == "Darwin":
            report.extend(self._audit_mac())
        else:
            report.append("Unsupported Operating System for detailed hardware audit.")

        # 2. Software Section
        report.append("\n--- Software Environment ---")
        report.append(f"Python Version: {platform.python_version()}")
        report.append(f"Python Executable: {sys.executable}")
        
        return "\n".join(report)

    def _run_cmd(self, cmd: str) -> str:
        """Helper to run shell commands safely."""
        try:
            # shell=True is often required for piped commands, specifically on Windows/Linux
            result = subprocess.run(
                cmd, 
                text=True, 
                capture_output=True, 
                check=False, 
                shell=True, 
                timeout=5
            )
            if result.returncode == 0 and result.stdout:
                return result.stdout.strip()
            elif result.stderr:
                return f"[Cmd Error]: {result.stderr.strip()}"
            return "[No Output]"
        except Exception as e:
            return f"[Execution Error]: {e}"

    # --- OS Specific Implementations ---

    def _audit_windows(self) -> List[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("wmic cpu get name"))
        # GPU
        data.append("GPU: " + self._run_cmd("wmic path win32_videocontroller get name"))
        # RAM
        try:
            mem_str = self._run_cmd("wmic computersystem get totalphysicalmemory").splitlines()[-1]
            mem_bytes = int(mem_str)
            data.append(f"Memory: {mem_bytes / (1024**3):.2f} GB")
        except:
            data.append("Memory: Could not retrieve total physical memory.")
        # Disk
        data.append("\nDisks:")
        data.append(self._run_cmd("wmic diskdrive get model,size"))
        return data

    def _audit_linux(self) -> List[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("lscpu | grep 'Model name'"))
        # GPU (Requires lspci, usually in pciutils)
        data.append("GPU: " + self._run_cmd("lspci | grep -i vga"))
        # RAM
        data.append("Memory:\n" + self._run_cmd("free -h"))
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("lsblk -o NAME,SIZE,MODEL"))
        return data

    def _audit_mac(self) -> List[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("sysctl -n machdep.cpu.brand_string"))
        # GPU
        data.append("GPU:\n" + self._run_cmd("system_profiler SPDisplaysDataType | grep -E 'Chipset Model|VRAM'"))
        # RAM
        data.append("Memory Details:\n" + self._run_cmd("system_profiler SPMemoryDataType | grep -E 'Size|Type|Speed'"))
        # RAM Total
        try:
            mem_bytes = int(self._run_cmd('sysctl -n hw.memsize'))
            data.append(f"Total Memory: {mem_bytes / (1024**3):.2f} GB")
        except: 
            pass
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("diskutil list physical"))
        return data


# --- Independent Test Block ---
if __name__ == "__main__":
    inspector = SysInspectorMS()
    print("Service ready:", inspector)
    print("Running System Inspector...")
    print("\n" + inspector.generate_report())
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TasklistVaultMS.py
--------------------------------------------------------------------------------
import sqlite3
import uuid
import logging
import datetime
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "task_vault.db"
logger = logging.getLogger("TaskVault")

TaskStatus = Literal["Pending", "Running", "Complete", "Error", "Awaiting-Approval"]

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="TaskVault",
    version="1.0.0",
    description="Persistent SQLite engine for hierarchical task management.",
    tags=["tasks", "db", "project-management"],
    capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class TasklistVaultMS:
    """
    The Taskmaster: A persistent SQLite engine for hierarchical task management.
    Supports infinite nesting of sub-tasks and status tracking.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.db_path = self.config.get("db_path", DB_PATH)
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. Task Lists (The containers)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_lists (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP
                )
            """)
            # 2. Tasks (The items, supporting hierarchy via parent_id)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id TEXT PRIMARY KEY,
                    list_id TEXT NOT NULL,
                    parent_id TEXT,
                    content TEXT NOT NULL,
                    status TEXT DEFAULT 'Pending',
                    result TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    FOREIGN KEY(list_id) REFERENCES task_lists(id) ON DELETE CASCADE,
                    FOREIGN KEY(parent_id) REFERENCES tasks(id) ON DELETE CASCADE
                )
            """)

    # --- List Management ---

    @service_endpoint(
        inputs={"name": "str"},
        outputs={"list_id": "str"},
        description="Creates a new task list and returns its ID.",
        tags=["tasks", "create"],
        side_effects=["db:write"]
    )
    def create_list(self, name: str) -> str:
        """Creates a new task list and returns its ID."""
        list_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO task_lists (id, name, created_at) VALUES (?, ?, ?)",
                (list_id, name, now)
            )
        logger.info(f"Created Task List: '{name}' ({list_id})")
        return list_id

    @service_endpoint(
        inputs={},
        outputs={"lists": "List[Dict]"},
        description="Returns metadata for all task lists.",
        tags=["tasks", "read"],
        side_effects=["db:read"]
    )
    def get_lists(self) -> List[Dict[str, Any]]:
        """Returns metadata for all task lists."""
        with self._get_conn() as conn:
            rows = conn.execute("SELECT * FROM task_lists ORDER BY created_at DESC").fetchall()
            return [dict(r) for r in rows]

    # --- Task Management ---

    @service_endpoint(
        inputs={"list_id": "str", "content": "str", "parent_id": "Optional[str]"},
        outputs={"task_id": "str"},
        description="Adds a task (or sub-task) to a list.",
        tags=["tasks", "write"],
        side_effects=["db:write"]
    )
    def add_task(self, list_id: str, content: str, parent_id: Optional[str] = None) -> str:
        """Adds a task (or sub-task) to a list."""
        task_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                """INSERT INTO tasks (id, list_id, parent_id, content, status, created_at, updated_at) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (task_id, list_id, parent_id, content, "Pending", now, now)
            )
        return task_id

    @service_endpoint(
        inputs={"task_id": "str", "content": "str", "status": "str", "result": "str"},
        outputs={},
        description="Updates a task's details.",
        tags=["tasks", "update"],
        side_effects=["db:write"]
    )
    def update_task(self, task_id: str, content: str = None, status: TaskStatus = None, result: str = None):
        """Updates a task's details."""
        updates = []
        params = []
        
        if content:
            updates.append("content = ?")
            params.append(content)
        if status:
            updates.append("status = ?")
            params.append(status)
        if result:
            updates.append("result = ?")
            params.append(result)
            
        if not updates: return

        updates.append("updated_at = ?")
        params.append(datetime.datetime.utcnow())
        params.append(task_id)

        sql = f"UPDATE tasks SET {', '.join(updates)} WHERE id = ?"
        
        with self._get_conn() as conn:
            conn.execute(sql, params)
        logger.info(f"Updated task {task_id}")

    # --- Tree Reconstruction ---

    @service_endpoint(
        inputs={"list_id": "str"},
        outputs={"tree": "Dict[str, Any]"},
        description="Fetches a list and reconstructs the full hierarchy of tasks.",
        tags=["tasks", "read"],
        side_effects=["db:read"]
    )
    def get_full_tree(self, list_id: str) -> Dict[str, Any]:
        """
        Fetches a list and reconstructs the full hierarchy of tasks.
        """
        with self._get_conn() as conn:
            # 1. Get List Info
            list_row = conn.execute("SELECT * FROM task_lists WHERE id = ?", (list_id,)).fetchone()
            if not list_row: return {}
            
            # 2. Get All Tasks
            task_rows = conn.execute("SELECT * FROM tasks WHERE list_id = ?", (list_id,)).fetchall()
            
        # 3. Build Adjacency Map
        tasks_by_id = {}
        for r in task_rows:
            t = dict(r)
            t['sub_tasks'] = [] # Prepare children container
            tasks_by_id[t['id']] = t

        # 4. Link Parents and Children
        root_tasks = []
        for t_id, task in tasks_by_id.items():
            parent_id = task['parent_id']
            if parent_id and parent_id in tasks_by_id:
                tasks_by_id[parent_id]['sub_tasks'].append(task)
            else:
                root_tasks.append(task)

        return {
            "id": list_row['id'],
            "name": list_row['name'],
            "tasks": root_tasks
        }

    @service_endpoint(
        inputs={"list_id": "str"},
        outputs={},
        description="Deletes a task list and all its tasks.",
        tags=["tasks", "delete"],
        side_effects=["db:write"]
    )
    def delete_list(self, list_id: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM task_lists WHERE id = ?", (list_id,))
        logger.info(f"Deleted list {list_id}")


# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # Use a test DB
    test_db = Path("test_task_vault.db")
    if test_db.exists(): 
        os.remove(test_db)
    
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    vault = TasklistVaultMS({"db_path": test_db})
    print("Service ready:", vault)
    
    # 1. Create a Plan
    plan_id = vault.create_list("System Upgrade Plan")
    
    # 2. Add Root Tasks
    t1 = vault.add_task(plan_id, "Backup Database")
    t2 = vault.add_task(plan_id, "Update Server")
    
    # 3. Add Sub-Tasks
    t2_1 = vault.add_task(plan_id, "Stop Services", parent_id=t2)
    t2_2 = vault.add_task(plan_id, "Run Installer", parent_id=t2)
    
    # 4. Update Status
    vault.update_task(t1, status="Complete", result="Backup saved to /tmp/bk.tar")
    vault.update_task(t2_1, status="Running")
    
    # 5. Render Tree
    tree = vault.get_full_tree(plan_id)
    print(f"\n--- {tree.get('name')} ---")
    
    def print_node(node, indent=0):
        status_icon = "‚úì" if node['status'] == 'Complete' else "‚óã"
        print(f"{'  '*indent}{status_icon} {node['content']} [{node['status']}]")
        for child in node['sub_tasks']:
            print_node(child, indent + 1)

    if 'tasks' in tree:
        for task in tree['tasks']:
            print_node(task)
        
    # Cleanup
    if test_db.exists(): 
        os.remove(test_db)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TelemetryServiceMS.py
--------------------------------------------------------------------------------
import logging
import queue
import time
from typing import Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("TelemetryService")

# ==============================================================================
# HELPER CLASS
# ==============================================================================

class QueueHandler(logging.Handler):
    """
    Custom logging handler that pushes log records into a thread-safe queue.
    """
    def __init__(self, log_queue: queue.Queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        # We format the record before putting it in the queue so the message field exists
        self.format(record)
        self.log_queue.put(record)

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="TelemetryService",
    version="1.0.0",
    description="The Nervous System: Watches the thread-safe LogQueue and updates GUI components with real-time status.",
    tags=["utility", "logging", "telemetry"],
    capabilities=["log-redirection", "real-time-updates"]
)
class TelemetryServiceMS:
    """
    The Nervous System.
    Watches the thread-safe LogQueue and updates the GUI Panels.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        
        # Dependencies injected via config
        self.root = self.config.get("root")
        self.panels = self.config.get("panels")
        
        self.log_queue = queue.Queue()
        self.start_time = time.time()
        self._heartbeat_count = 0
        
        # We set up the global logging hook HERE, inside the service
        self._setup_logging_hook()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "queue_depth": "int"},
        description="Standardized health check to verify the operational state of the telemetry pipeline.",
        tags=["diagnostic", "health"],
        side_effects=[]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the TelemetryServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "queue_depth": self.log_queue.qsize()
        }

    def _setup_logging_hook(self):
        """Redirects Python's standard logging to our Queue."""
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.INFO)
        
        # Create our custom handler that feeds the queue
        q_handler = QueueHandler(self.log_queue)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
        q_handler.setFormatter(formatter)
        root_logger.addHandler(q_handler)

    @service_endpoint(
        inputs={},
        outputs={},
        description="Initiates the telemetry service and begins the asynchronous GUI log-polling loop.",
        tags=["lifecycle", "event-loop"],
        mode="async",
        side_effects=["ui:update"]
    )
    def start(self):
        """Begins the GUI update loop."""
        logger.info("Telemetry Service starting...")
        self._poll_queue()

    @service_endpoint(
        inputs={},
        outputs={"alive": "bool", "heartbeat": "int"},
        description="Verifies that the GUI polling loop is actively processing the log queue.",
        tags=["diagnostic", "heartbeat"],
        side_effects=[]
    )
    def ping(self) -> Dict[str, Any]:
        """Allows an agent to verify the pulse of the UI loop."""
        return {"alive": True, "heartbeat": self._heartbeat_count}

    def _poll_queue(self):
        """The heartbeat that drains the queue into the GUI."""
        if not self.root or not self.panels:
            return

        self._heartbeat_count += 1
        try:
            while True:
                record = self.log_queue.get_nowait()
                # record.message is populated by QueueHandler calling format()
                msg = f"[{record.levelname}] {record.message}" 
                
                # Update the GUI
                if hasattr(self.panels, 'log'):
                    self.panels.log(msg)
                
        except queue.Empty:
            pass
        finally:
            # Check again in 100ms
            if hasattr(self.root, 'after'):
                self.root.after(100, self._poll_queue)


# --- Independent Test Block ---
if __name__ == "__main__":
    # Mock objects for independent test
    class MockRoot: 
        def after(self, ms, func): 
            # Simulate a loop schedule
            pass

    class MockPanels:
        def log(self, msg): 
            print(f"[UI LOG]: {msg}")
    
    # Initialize
    svc = TelemetryServiceMS({"root": MockRoot(), "panels": MockPanels()})
    print("Service ready:", svc)
    
    # Generate a log
    logger.info("Internal test message")
    
    # Manually trigger one poll cycle to verify it picks up the log
    svc._poll_queue()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TextChunkerMS.py
--------------------------------------------------------------------------------
import logging
from typing import Any, Dict, List, Optional, Tuple

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logger = logging.getLogger("TextChunker")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="TextChunker",
    version="1.0.0",
    description="Splits text into chunks using various strategies (chars, lines).",
    tags=["chunking", "nlp", "rag"],
    capabilities=["compute"]
)
class TextChunkerMS:
    """
    The Butcher: A unified service for splitting text into digestible chunks
    for RAG (Retrieval Augmented Generation).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"text": "str", "chunk_size": "int", "chunk_overlap": "int"},
        outputs={"chunks": "List[str]"},
        description="Standard sliding window split by character count.",
        tags=["chunking", "chars"],
        side_effects=[]
    )
    def chunk_by_chars(self, text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """
        Standard Sliding Window. Best for prose/documentation.
        Splits purely by character count.
        """
        if chunk_size <= 0: 
            raise ValueError("chunk_size must be positive")
        
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            
            # Stop if we've reached the end
            if end >= text_length:
                break
                
            # Advance start, backing up by overlap
            start += chunk_size - chunk_overlap
            
        return chunks

    @service_endpoint(
        inputs={"text": "str", "max_lines": "int", "max_chars": "int"},
        outputs={"chunks": "List[Dict]"},
        description="Line-preserving chunker, best for code.",
        tags=["chunking", "lines", "code"],
        side_effects=[]
    )
    def chunk_by_lines(self, text: str, max_lines: int = 200, max_chars: int = 4000) -> List[Dict[str, Any]]:
        """
        Line-Preserving Chunker. Best for Code.
        Respects line boundaries and returns metadata about line numbers.
        """
        lines = text.splitlines()
        chunks = []
        start = 0
        
        while start < len(lines):
            end = min(start + max_lines, len(lines))
            chunk_str = "\n".join(lines[start:end])
            
            # If too big, shrink window (back off)
            while len(chunk_str) > max_chars and end > start + 1:
                end -= 1
                chunk_str = "\n".join(lines[start:end])
            
            chunks.append({
                "text": chunk_str,
                "start_line": start + 1,
                "end_line": end
            })
            start = end
            
        return chunks


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    
    chunker = TextChunkerMS()
    print("Service ready:", chunker)
    
    # 1. Prose Test
    print("--- Prose Chunking ---")
    lorem = "A" * 100 # 100 chars
    result = chunker.chunk_by_chars(lorem, chunk_size=40, chunk_overlap=10)
    for i, c in enumerate(result):
        print(f"Chunk {i}: len={len(c)}")

    # 2. Code Test
    print("\n--- Code Chunking ---")
    code = "\n".join([f"print('Line {i}')" for i in range(1, 10)])
    # Force splits small for testing
    result_code = chunker.chunk_by_lines(code, max_lines=3, max_chars=100)
    for i, c in enumerate(result_code):
        print(f"Chunk {i}: Lines {c['start_line']}-{c['end_line']}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_ThoughtStreamMS.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import datetime
from typing import Any, Dict, Optional, List

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="ThoughtStream",
    version="1.0.0",
    description="A UI widget for displaying a stream of AI thoughts/logs.",
    tags=["ui", "stream", "logs", "widget"],
    capabilities=["ui:gui"]
)
class ThoughtStreamMS(ttk.Frame):
    """
    The Neural Inspector: A UI widget for displaying a stream of AI thoughts/logs
    visualized as 'bubbles' with sparklines.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        # Initialize ttk.Frame
        super().__init__(parent)
        
        # Header
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        # The Stream Area (Canvas allows for custom drawing like sparklines)
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340) # Fixed width like React
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    @service_endpoint(
        inputs={"filename": "str", "chunk_id": "int", "content": "str", "vector_preview": "List[float]", "color": "str"},
        outputs={},
        description="Adds a new thought bubble to the visual stream.",
        tags=["ui", "update"],
        side_effects=["ui:update"]
    )
    def add_thought_bubble(self, filename: str, chunk_id: int, content: str, vector_preview: List[float], color: str):
        """
        Mimics the 'InspectorFrame' from your React code.
        """
        # Bubble Container
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        # Header: File + Timestamp
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        header_lbl = tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", 
                              fg="#007ACC", bg="#1a1a25", font=("Consolas", 8))
        header_lbl.pack(anchor="w", padx=5, pady=2)
        
        # Content Snippet
        snippet = content[:400] + "..." if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", 
                               font=("Consolas", 8), justify="left", wraplength=300)
        content_lbl.pack(fill="x", padx=5, pady=2)
        
        # Vector Sparkline (The Custom Draw)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector: List[float], color: str):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        
        if not vector:
            return

        bar_w = w / len(vector) if len(vector) > 0 else 0
        
        for i, val in enumerate(vector):
            # Normalize -1..1 to 0..1 for height
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            
            # Draw bar
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")


# --- Independent Test Block ---
if __name__ == "__main__":
    import random
    
    root = tk.Tk()
    root.title("Thought Stream Test")
    root.geometry("400x600")
    
    stream = ThoughtStreamMS({"parent": root})
    print("Service ready:", stream)
    stream.pack(fill="both", expand=True)
    
    # Simulate an incoming "Microservice" event
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("ExplorerView.tsx", 1, "import React from 'react'...", fake_vector, "#FF00FF")
    
    # Add another one for effect
    fake_vector_2 = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("Backend.py", 42, "def process_data(self): pass", fake_vector_2, "#00FF00")

    root.mainloop()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TkinterAppShellMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterAppShellMS
ENTRY_POINT: _TkinterAppShellMS.py
DEPENDENCIES: None
"""
import tkinter as tk
from tkinter import ttk
import logging
from typing import Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# Updated Import: Single Underscore + 'Tkinter' prefix
try:
    from _TkinterThemeManagerMS import TkinterThemeManagerMS
except ImportError:
    TkinterThemeManagerMS = None

logger = logging.getLogger("AppShell")

@service_metadata(
    name="TkinterAppShell",
    version="2.0.0",
    description="The Application Container. Manages the root window, main loop, and global layout.",
    tags=["ui", "core", "lifecycle"],
    capabilities=["ui:root", "ui:gui"]
)
class TkinterAppShellMS:
    """
    The Mother Ship.
    Owns the Tkinter Root. All other UI microservices dock into this.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.root = tk.Tk()
        self.root.withdraw() # Hide until launch
        
        # Load Theme (Inject dependency or create new)
        self.theme_svc = self.config.get("theme_manager")
        if not self.theme_svc and TkinterThemeManagerMS:
            self.theme_svc = TkinterThemeManagerMS()
            
        self.colors = self.theme_svc.get_theme() if self.theme_svc else {}
        self._configure_root()
        
    def _configure_root(self):
        self.root.title(self.config.get("title", "Microservice OS"))
        self.root.geometry(self.config.get("geometry", "1200x800"))
        
        # Apply Base Theme
        bg = self.colors.get('background', '#1e1e1e')
        self.root.configure(bg=bg)
        
        # Configure TTK Styles globally
        style = ttk.Style()
        style.theme_use('clam')
        
        # Standard Frames
        style.configure('TFrame', background=bg)
        style.configure('TLabel', background=bg, foreground=self.colors.get('foreground', '#ccc'))
        style.configure('TButton', background=self.colors.get('panel_bg', '#333'), foreground='white')
        
        # Main Container (Grid or Pack)
        self.main_container = tk.Frame(self.root, bg=bg)
        self.main_container.pack(fill="both", expand=True, padx=5, pady=5)

    @service_endpoint(
        inputs={},
        outputs={},
        description="Starts the GUI Main Loop.",
        tags=["lifecycle", "start"],
        mode="sync",
        side_effects=["ui:block"]
    )
    def launch(self):
        """Ignition sequence start."""
        self.root.deiconify()
        logger.info("AppShell Launched.")
        self.root.mainloop()

    @service_endpoint(
        inputs={},
        outputs={"container": "tk.Frame"},
        description="Returns the main content area for other services to dock into.",
        tags=["ui", "layout"]
    )
    def get_main_container(self):
        """Other services call this to know where to .pack() themselves."""
        return self.main_container

    @service_endpoint(
        inputs={},
        outputs={},
        description="Gracefully shuts down the application.",
        tags=["lifecycle", "stop"],
        side_effects=["ui:close"]
    )
    def shutdown(self):
        self.root.quit()

if __name__ == "__main__":
    shell = TkinterAppShellMS({"title": "Test Shell"})
    shell.launch()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TkinterBootstrap.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterBootstrap
ENTRY_POINT: _TkinterBootstrap.py
DEPENDENCIES: None
"""
# --- 1. Import The UI Fabric ---
from _TkinterThemeManagerMS import TkinterThemeManagerMS
from _TkinterAppShellMS import TkinterAppShellMS
from _TkinterSmartExplorerMS import TkinterSmartExplorerMS

# --- 2. Import Capabilities (The Brains) ---
# Assuming you have these from previous steps
try:
    from _ScoutMS import ScoutMS
except ImportError:
    ScoutMS = None

def main():
    print("--- BOOTING MICROSERVICE UI ---")

    # A. Initialize Theme
    theme_mgr = TkinterThemeManagerMS()
    
    # B. Initialize Shell (Pass the theme manager so it knows the colors)
    app = TkinterAppShellMS({
        "theme_manager": theme_mgr, 
        "title": "Neural Command Center",
        "geometry": "1000x700"
    })
    
    # C. Initialize Logic Services
    scout = ScoutMS() if ScoutMS else None

    # --- D. COMPOSE THE UI ---
    # Get the docking bay
    main_deck = app.get_main_container()

    # 1. Dock the Explorer on the Left
    explorer = TkinterSmartExplorerMS({
        "parent": main_deck, 
        "theme": theme_mgr.get_theme()
    })
    explorer.pack(side="left", fill="y", padx=2, pady=2)

    # 2. Load Data (If Scout is available)
    if scout:
        print("Scanning current directory...")
        data = scout.scan_directory(".") # Scan root
        explorer.load_data(data)
    else:
        # Fallback data if Scout isn't found
        explorer.load_data({"name": "No Scout Found", "type": "error", "children": []})

    # --- E. LAUNCH ---
    app.launch()

if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TkinterSmartExplorerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterSmartExplorerMS
ENTRY_POINT: _TkinterSmartExplorerMS.py
DEPENDENCIES: None
"""
import tkinter as tk
from tkinter import ttk
from typing import Dict, Any, Optional, List

from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="TkinterSmartExplorer",
    version="1.0.0",
    description="A hierarchical tree viewer capable of displaying file systems or JSON data structures.",
    tags=["ui", "widget", "explorer"],
    capabilities=["ui:gui"]
)
class TkinterSmartExplorerMS(tk.Frame):
    """
    The Navigator.
    A TreeView widget that expects standard 'Node' dictionaries (name, type, children).
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        theme = self.config.get("theme", {})
        
        super().__init__(parent, bg=theme.get('panel_bg', '#252526'))
        
        # UI Setup
        self.tree = ttk.Treeview(self, show="tree headings", selectmode="browse")
        self.tree.heading("#0", text="Explorer", anchor="w")
        
        # Scrollbars
        vsb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        hsb = ttk.Scrollbar(self, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        
        self.tree.pack(side="left", fill="both", expand=True)
        vsb.pack(side="right", fill="y")
        
        # Icons (Unicode placeholders for simplicity)
        self.icons = {
            "folder": "üìÅ",
            "file": "üìÑ",
            "web": "üåê",
            "unknown": "‚ùì"
        }

    @service_endpoint(
        inputs={"data": "Dict"},
        outputs={},
        description="Populates the tree view with a nested dictionary structure (Standard 'Node' format).",
        tags=["ui", "update"],
        side_effects=["ui:update"]
    )
    def load_data(self, data: Dict[str, Any]):
        """
        Ingests a dictionary tree (like from _ScoutMS or _TreeMapperMS).
        """
        # Clear existing
        for item in self.tree.get_children():
            self.tree.delete(item)
            
        self._build_node("", data)

    def _build_node(self, parent_id, node_data):
        # Determine Icon
        ntype = node_data.get("type", "unknown")
        icon = self.icons.get(ntype, self.icons["unknown"])
        text = f"{icon} {node_data.get('name', '???')}"
        
        # Insert Item
        item_id = self.tree.insert(parent_id, "end", text=text, open=True)
        
        # Recursion
        for child in node_data.get("children", []):
            self._build_node(item_id, child)

if __name__ == "__main__":
    root = tk.Tk()
    explorer = TkinterSmartExplorerMS({"parent": root})
    explorer.pack(fill="both", expand=True)
    
    dummy_data = {
        "name": "Project Root",
        "type": "folder",
        "children": [
            {"name": "src", "type": "folder", "children": []},
            {"name": "README.md", "type": "file"}
        ]
    }
    
    explorer.load_data(dummy_data)
    root.mainloop()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TkinterThemeManagerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterThemeManagerMS
ENTRY_POINT: _TkinterThemeManagerMS.py
DEPENDENCIES: None
"""
from typing import Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# Default "Dark Modern" Theme
DEFAULT_THEME = {
    'background': '#1e1e1e',
    'foreground': '#d4d4d4',
    'panel_bg':   '#252526',
    'border':     '#3c3c3c',
    'accent':     '#007acc',
    'error':      '#f48771',
    'success':    '#89d185',
    'font_main':  ('Segoe UI', 10),
    'font_mono':  ('Consolas', 10)
}

@service_metadata(
    name="TkinterThemeManager",
    version="1.0.0",
    description="Centralized configuration for UI colors and fonts.",
    tags=["ui", "config", "theme"],
    capabilities=["ui:style"]
)
class TkinterThemeManagerMS:
    """
    The Stylist: Holds the color palette and font settings.
    All UI components query this service to decide how to draw themselves.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.theme = DEFAULT_THEME.copy()
        
        # Allow override from config
        if "overrides" in self.config:
            self.theme.update(self.config["overrides"])

    @service_endpoint(
        inputs={},
        outputs={"theme": "Dict"},
        description="Returns the current active theme dictionary.",
        tags=["ui", "read"]
    )
    def get_theme(self) -> Dict[str, Any]:
        return self.theme

    @service_endpoint(
        inputs={"key": "str", "value": "Any"},
        outputs={},
        description="Updates a specific theme attribute (e.g., changing accent color).",
        tags=["ui", "write"],
        side_effects=["ui:refresh"]
    )
    def update_key(self, key: str, value: Any):
        self.theme[key] = value

if __name__ == "__main__":
    svc = TkinterThemeManagerMS()
    print("Theme Ready:", svc.get_theme()['accent'])
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TkinterUniButtonMS.py
--------------------------------------------------------------------------------
import tkinter as tk
from dataclasses import dataclass
from typing import Any, Dict, Optional, Callable

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION MODELS
# ==============================================================================

@dataclass
class ButtonConfig:
    text: str
    command: Callable[[], None]
    bg_color: str
    active_bg_color: str
    fg_color: str = "#FFFFFF"

@dataclass
class LinkConfig:
    """Configuration for the 'Linked' state (The Trap)"""
    trap_bg: str = "#7C3AED"    # Deep Purple
    btn_bg: str = "#8B5CF6"     # Lighter Purple
    text_color: str = "#FFFFFF"

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="LockingDualBtn",
    version="1.0.0",
    description="A unified button group (Left/Right/Link) where linking merges the actions.",
    tags=["ui", "widget", "button"],
    capabilities=["ui:gui"]
)
class TkinterUniButtonMS(tk.Frame):
    """
    A generic button group that can merge ANY two actions.
    Pass the visual/functional definitions in via the config objects.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        # Initialize tk.Frame
        super().__init__(parent)
        
        # Extract Button Configurations
        self.left_cfg: Optional[ButtonConfig] = self.config.get("left_btn")
        self.right_cfg: Optional[ButtonConfig] = self.config.get("right_btn")
        self.link_cfg: LinkConfig = self.config.get("link_config") or LinkConfig()
        
        self.is_linked = False
        
        # Attempt to get parent background for seamless look
        try:
            self.default_bg = parent.cget("bg")
        except AttributeError:
            self.default_bg = "#f0f0f0"

        if not self.left_cfg or not self.right_cfg:
            # Fallback for safe init if configs are missing
            print("Warning: TkinterUniButtonMS initialized without button configs.")
            return

        self._setup_ui()
        self._update_state()

    def _setup_ui(self):
        self.configure(padx=4, pady=4)
        
        common_style = {"relief": "flat", "font": ("Segoe UI", 10, "bold"), "bd": 0, "cursor": "hand2"}

        # 1. Left Button (Generic)
        self.btn_left = tk.Button(self, command=lambda: self._execute("left"), **common_style)
        self.btn_left.pack(side="left", fill="y", padx=(0, 2))

        # 2. Link Toggle (The Chain)
        self.btn_link = tk.Button(self, text="&", width=3, command=self._toggle_link, **common_style)
        self.btn_link.pack(side="left", fill="y", padx=(0, 2))

        # 3. Right Button (Generic)
        self.btn_right = tk.Button(self, command=lambda: self._execute("right"), **common_style)
        self.btn_right.pack(side="left", fill="y")

    def _toggle_link(self):
        self.is_linked = not self.is_linked
        self._update_state()

    def _update_state(self):
        if self.is_linked:
            # --- LINKED STATE (The Trap) ---
            self.configure(bg=self.link_cfg.trap_bg)
            
            # Both buttons look identical in the "Trap"
            for btn in (self.btn_left, self.btn_right, self.btn_link):
                btn.configure(bg=self.link_cfg.btn_bg, fg=self.link_cfg.text_color, activebackground=self.link_cfg.trap_bg)
            
            # Keep original text
            self.btn_left.configure(text=self.left_cfg.text)
            self.btn_right.configure(text=self.right_cfg.text)

        else:
            # --- INDEPENDENT STATE ---
            try: 
                self.configure(bg=self.default_bg)
            except: 
                self.configure(bg="#f0f0f0") 

            # Restore Left Button
            self.btn_left.configure(
                text=self.left_cfg.text, 
                bg=self.left_cfg.bg_color, 
                fg=self.left_cfg.fg_color,
                activebackground=self.left_cfg.active_bg_color
            )

            # Restore Right Button
            self.btn_right.configure(
                text=self.right_cfg.text, 
                bg=self.right_cfg.bg_color, 
                fg=self.right_cfg.fg_color,
                activebackground=self.right_cfg.active_bg_color
            )

            # Restore Link Button (Neutral Gray)
            self.btn_link.configure(bg="#E5E7EB", fg="#374151", activebackground="#D1D5DB")

    def _execute(self, source):
        if self.is_linked:
            # Chain them: Left then Right
            self.left_cfg.command()
            self.right_cfg.command()
        else:
            if source == "left": 
                self.left_cfg.command()
            elif source == "right": 
                self.right_cfg.command()


# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("UniButton Test")
    root.geometry("300x100")
    
    # Define actions
    def on_validate(): print("Validating Data...")
    def on_apply(): print("Applying Changes...")
    
    # Create Configs
    btn1 = ButtonConfig("Validate", on_validate, "#3b82f6", "#2563eb") # Blue
    btn2 = ButtonConfig("Apply", on_apply, "#10b981", "#059669")       # Green
    
    # Init Service
    svc = TkinterUniButtonMS({
        "parent": root, 
        "left_btn": btn1, 
        "right_btn": btn2
    })
    print("Service ready:", svc)
    svc.pack(pady=20)
    
    root.mainloop()
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_TreeMapperMS.py
--------------------------------------------------------------------------------
import os
import datetime
import logging
from pathlib import Path
from typing import Any, Dict, List, Set, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    '.venv', 'env', 'venv', 'dist', 'build', '.DS_Store'
}
logger = logging.getLogger("TreeMapper")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="TreeMapper",
    version="1.0.0",
    description="Generates ASCII-art style directory maps of the file system.",
    tags=["filesystem", "map", "visualization"],
    capabilities=["filesystem:read"]
)
class TreeMapperMS:
    """
    The Cartographer: Generates ASCII-art style directory maps.
    Useful for creating context snapshots for LLMs.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"root_path": "str", "additional_exclusions": "Set[str]", "use_default_exclusions": "bool"},
        outputs={"tree_map": "str"},
        description="Generates an ASCII tree map of the directory.",
        tags=["filesystem", "visualization"],
        side_effects=["filesystem:read"]
    )
    def generate_tree(self, 
                      root_path: str, 
                      additional_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> str:
        
        start_path = Path(root_path).resolve()
        if not start_path.exists(): 
            return f"Error: Path '{root_path}' does not exist."

        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_EXCLUDES)
        if additional_exclusions:
            exclusions.update(additional_exclusions)

        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lines = [
            f"Project Map: {start_path.name}",
            f"Generated: {timestamp}",
            "-" * 40,
            f"üìÅ {start_path.name}/"
        ]

        logger.info(f"Mapping directory: {start_path}")
        self._walk(start_path, "", lines, exclusions)
        return "\n".join(lines)

    def _walk(self, directory: Path, prefix: str, lines: List[str], exclusions: Set[str]):
        try:
            # Sort: Directories first, then files (alphabetical)
            children = sorted(
                [p for p in directory.iterdir() if p.name not in exclusions],
                key=lambda x: (not x.is_dir(), x.name.lower())
            )
        except PermissionError:
            lines.append(f"{prefix}‚îî‚îÄ‚îÄ üö´ [Permission Denied]")
            return

        count = len(children)
        for index, path in enumerate(children):
            is_last = (index == count - 1)
            connector = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
            
            if path.is_dir():
                lines.append(f"{prefix}{connector}üìÅ {path.name}/")
                extension = "    " if is_last else "‚îÇ   "
                self._walk(path, prefix + extension, lines, exclusions)
            else:
                lines.append(f"{prefix}{connector}üìÑ {path.name}")


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    svc = TreeMapperMS()
    print("Service ready:", svc)
    
    # Map the current directory
    print("\n--- Map of Current Dir ---")
    tree = svc.generate_tree(".", additional_exclusions={"__pycache__"})
    print(tree)
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_VectorFactoryMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import os
import uuid
import logging
import shutil
from typing import List, Dict, Any, Optional, Protocol, Union
from pathlib import Path

# --- RUNTIME DEPENDENCY CHECK ---
REQUIRED = ["chromadb", "faiss-cpu", "numpy"]
MISSING = []

for lib in REQUIRED:
    # Clean version numbers/aliases for check
    clean_lib = lib.split('>=')[0].replace('-', '_')
    if clean_lib == "faiss_cpu": clean_lib = "faiss"
    
    if importlib.util.find_spec(clean_lib) is None:
        MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _VectorFactoryMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # We proceed so the class definition loads, but runtime methods will fail.

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logger = logging.getLogger("VectorFactory")

# ==============================================================================
# PROTOCOL DEFINITION
# ==============================================================================

class VectorStore(Protocol):
    """The contract that all vector backends must fulfill."""
    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]) -> None:
        ...
    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        ...
    def count(self) -> int:
        ...
    def clear(self) -> None:
        ...

# ==============================================================================
# IMPLEMENTATIONS
# ==============================================================================

class FaissVectorStore:
    """Local, RAM-heavy, fast vector store using FAISS."""
    
    def __init__(self, index_path: str, dimension: int):
        import numpy as np
        import faiss # type: ignore
        self.np = np
        self.faiss = faiss
        
        self.index_path = index_path
        self.dim = dimension
        self.metadata_store = []
        
        # Load or Create
        if os.path.exists(index_path):
            try:
                self.index = faiss.read_index(index_path)
                # Load metadata (simple JSON sidecar)
                meta_path = index_path + ".meta.json"
                if os.path.exists(meta_path):
                    import json
                    with open(meta_path, 'r') as f:
                        self.metadata_store = json.load(f)
            except Exception as e:
                logger.error(f"Failed to load FAISS index: {e}")
                self.index = faiss.IndexFlatL2(dimension)
        else:
            self.index = faiss.IndexFlatL2(dimension)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        
        vecs = self.np.array(embeddings).astype("float32")
        self.index.add(vecs)
        self.metadata_store.extend(metadatas)
        self._save()

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        if self.index.ntotal == 0: return []
        
        q_vec = self.np.array([query_vector]).astype("float32")
        distances, indices = self.index.search(q_vec, k)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and idx < len(self.metadata_store):
                entry = self.metadata_store[idx].copy()
                entry['score'] = float(dist) # FAISS returns L2 distance (lower is better)
                results.append(entry)
        return results

    def count(self) -> int:
        return self.index.ntotal

    def clear(self):
        self.index.reset()
        self.metadata_store = []
        self._save()

    def _save(self):
        self.faiss.write_index(self.index, self.index_path)
        import json
        with open(self.index_path + ".meta.json", 'w') as f:
            json.dump(self.metadata_store, f)


class ChromaVectorStore:
    """Persistent, feature-rich vector store using ChromaDB."""
    
    def __init__(self, persist_dir: str, collection_name: str):
        import chromadb # type: ignore
        # Suppress Chroma telemetry noise
        logging.getLogger("chromadb").setLevel(logging.ERROR)
        
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(collection_name)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        # Chroma requires unique IDs
        ids = [str(uuid.uuid4()) for _ in embeddings]
        
        # Ensure metadata is flat (Chroma limitation on nested dicts)
        clean_metas = [{k: str(v) if isinstance(v, (list, dict)) else v for k, v in m.items()} for m in metadatas]
        
        # Chroma expects 'documents' usually. 
        docs = [m.get("content", "") for m in metadatas]

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            metadatas=clean_metas,
            documents=docs
        )

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        results = self.collection.query(
            query_embeddings=[query_vector],
            n_results=k
        )
        
        output = []
        if not results['ids']: return []

        # Unpack Chroma's columnar response format
        for i in range(len(results['ids'][0])):
            meta = results['metadatas'][0][i]
            if meta:
                entry = meta.copy()
                entry['score'] = results['distances'][0][i] if results['distances'] else 0.0
                entry['id'] = results['ids'][0][i]
                output.append(entry)
        return output

    def count(self) -> int:
        return self.collection.count()

    def clear(self):
        # Chroma doesn't have a truncate command, so we delete the collection
        name = self.collection.name
        self.client.delete_collection(name)
        self.collection = self.client.get_or_create_collection(name)


# ==============================================================================
# MICROSERVICE CLASS (FACTORY)
# ==============================================================================

@service_metadata(
    name="VectorFactory",
    version="1.0.0",
    description="Factory for creating VectorStore instances (FAISS, Chroma).",
    tags=["vector", "factory", "db"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class VectorFactoryMS:
    """
    The Switchboard: Returns the appropriate VectorStore implementation
    based on configuration.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"backend": "str", "config": "Dict"},
        outputs={"store": "VectorStore"},
        description="Creates and returns a configured VectorStore instance.",
        tags=["vector", "create"],
        side_effects=[]
    )
    def create(self, backend: str, config: Dict[str, Any]) -> VectorStore:
        """
        :param backend: 'faiss' or 'chroma'
        :param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)
        """
        logger.info(f"Initializing Vector Store: {backend.upper()}")
        
        if backend == "faiss":
            path = config.get("path", "vector_index.bin")
            dim = config.get("dim", 384)
            return FaissVectorStore(path, dim)
            
        elif backend == "chroma":
            path = config.get("path", "./chroma_db")
            name = config.get("collection", "default_collection")
            return ChromaVectorStore(path, name)
            
        else:
            raise ValueError(f"Unknown backend: {backend}")


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(level=logging.INFO)
    print("--- Testing VectorFactoryMS ---")
    
    # 1. Mock Data (dim=4 for simplicity)
    mock_vec = [0.1, 0.2, 0.3, 0.4]
    mock_meta = {"text": "Hello World", "source": "test"}
    
    factory = VectorFactoryMS()
    print("Service ready:", factory)

    # 2. Test FAISS
    print("\n[Testing FAISS]")
    try:
        faiss_store = factory.create("faiss", {"path": "test_faiss.index", "dim": 4})
        faiss_store.add([mock_vec], [mock_meta])
        print(f"Count: {faiss_store.count()}")
        res = faiss_store.search(mock_vec, 1)
        if res:
            print(f"Search Result: {res[0]['text']}")
        
        # Cleanup
        if os.path.exists("test_faiss.index"): os.remove("test_faiss.index")
        if os.path.exists("test_faiss.index.meta.json"): os.remove("test_faiss.index.meta.json")
    except ImportError:
        print("Skipping FAISS test (library not installed)")
    except Exception as e:
        print(f"FAISS Test Failed: {e}")

    # 3. Test Chroma
    print("\n[Testing Chroma]")
    try:
        chroma_store = factory.create("chroma", {"path": "./test_chroma_db", "collection": "test_col"})
        chroma_store.add([mock_vec], [mock_meta])
        print(f"Count: {chroma_store.count()}")
        res = chroma_store.search(mock_vec, 1)
        if res:
            print(f"Search Result: {res[0]['text']}")
        
        # Cleanup
        if os.path.exists("./test_chroma_db"): 
            shutil.rmtree("./test_chroma_db")
    except ImportError:
        print("Skipping Chroma test (library not installed)")
    except Exception as e:
        print(f"Chroma Test Failed: {e}")
--------------------------------------------------------------------------------
FILE: _MicroserviceLIBRARY\_WebScraperMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import httpx
import logging
import asyncio
import re
from typing import Optional, Dict, Any

# --- RUNTIME DEPENDENCY CHECK ---
REQUIRED = ["httpx", "readability-lxml"] # readability package is often 'readability' in code
MISSING = []

for lib in REQUIRED:
    # Clean version numbers for check
    clean_lib = lib.split('>=')[0].replace('-', '_')
    if clean_lib == "readability_lxml": clean_lib = "readability"
    
    if importlib.util.find_spec(clean_lib) is None:
        MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _WebScraperMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # We proceed so the class loads, but functionality will break.

# Conditional import to prevent crash during class loading if missing
try:
    from readability import Document
except ImportError:
    Document = None

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
TIMEOUT_SECONDS = 15.0

logger = logging.getLogger("WebScraper")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="WebScraper",
    version="1.0.0",
    description="Fetches URLs and extracts main content using Readability (stripping ads/nav).",
    tags=["scraper", "web", "readability"],
    capabilities=["network:outbound", "compute"]
)
class WebScraperMS:
    """
    The Reader: Fetches URLs and extracts the main content using Readability.
    Strips ads, navbars, and boilerplate to return clean text for LLMs.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.headers = {"User-Agent": USER_AGENT}

    @service_endpoint(
        inputs={"url": "str"},
        outputs={"data": "Dict[str, Any]"},
        description="Fetches and cleans a URL.",
        tags=["scraper", "read"],
        side_effects=["network:outbound"]
    )
    def scrape(self, url: str) -> Dict[str, Any]:
        """
        Synchronous wrapper for fetching and cleaning a URL.
        Returns: {
            "url": str,
            "title": str,
            "content": str (The main body text),
            "html": str (The raw HTML of the main content area)
        }
        """
        return asyncio.run(self._scrape_async(url))

    async def _scrape_async(self, url: str) -> Dict[str, Any]:
        if Document is None:
            raise ImportError("readability-lxml is missing.")

        logger.info(f"Fetching: {url}")
        
        async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=TIMEOUT_SECONDS) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                logger.error(f"HTTP Error {e.response.status_code}: {e}")
                raise
            except httpx.RequestError as e:
                logger.error(f"Request failed: {e}")
                raise

        # Parse with Readability
        try:
            doc = Document(response.text)
            title = doc.title()
            # Summary() returns the HTML of the main content area
            clean_html = doc.summary() 
            
            # Convert HTML content to plain text for the LLM
            clean_text = self._strip_tags(clean_html)
            
            logger.info(f"Successfully scraped '{title}' ({len(clean_text)} chars)")
            
            return {
                "url": url,
                "title": title,
                "content": clean_text,
                "html": clean_html
            }
        except Exception as e:
            logger.error(f"Parsing failed: {e}")
            raise

    def _strip_tags(self, html: str) -> str:
        """
        Removes HTML tags to leave only the readable text.
        """
        # Remove scripts and styles
        html = re.sub(r'<(script|style).*?>.*?</\1>', '', html, flags=re.DOTALL)
        # Remove tags
        text = re.sub(r'<[^>]+>', ' ', html)
        # Collapse whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    
    scraper = WebScraperMS()
    print("Service ready:", scraper)
    
    # Test URL (Example: Python's PEP 8)
    target_url = "https://peps.python.org/pep-0008/"
    
    print(f"--- Scraping {target_url} ---")
    try:
        data = scraper.scrape(target_url)
        print(f"\nTitle: {data['title']}")
        print(f"Content Preview:\n{data['content'][:500]}...")
        print(f"\nTotal Length: {len(data['content'])} characters")
    except Exception as e:
        print(f"Scrape failed: {e}")
--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\README.md
--------------------------------------------------------------------------------
# **Monaco Viewer**

A lightweight, cross-platform desktop editor and command-line utility powered by Python. It uses the same Monaco engine that drives VS Code and is designed to be used as a standalone code viewer, a surgical replacement tool for AI agents, and a headless script for fast regex manipulations.

## **Core Features**

* **Modern Editing Experience**: Leverages the Monaco Editor for a fluid, familiar interface with rich syntax highlighting.  
* **Cross-Platform**: Runs consistently on Windows, macOS, and Linux.  
* **Tabbed Interface**: Manage multiple files in a clean, tabbed layout with unsaved-changes indicators.  
* **Hybrid Functionality**: Use it as a quick-launch GUI app or as a powerful command-line tool.  
* **Surgical & Headless Modes**: Perform complex, UI-assisted replacements or lightning-fast, headless regex substitutions from your scripts.

## **Installation**

The project uses Conda to manage its environment and dependencies.

1. **Prerequisites**: Ensure you have [Miniconda](https://docs.conda.io/en/latest/miniconda.html) or [Anaconda](https://www.anaconda.com/products/distribution) installed.  
2. **Clone the Repository**:  
   git clone \[https://github.com/jacobmentalconstruct/\_MonacoVIEWER.git\](https://github.com/jacobmentalconstruct/\_MonacoVIEWER.git)  
   cd \_MonacoVIEWER

3. **Create the Conda Environment**:  
   conda env create \-f environment.yml

4. **Activate the Environment**:  
   conda activate monaco-viewer-env

## **Usage**

### **As a Standalone App (UI Mode)**

To launch the editor, simply run the launcher script:  
python start\_app.py \--file /path/to/your/file.js

### **Programmatic & Command-Line Integration**

#### **1\. Headless Regex Replacement (Fast & Scriptable)**

For simple find-and-replace operations in scripts, you can run the app in a truly headless mode that does not launch the UI.  
**Example:** To replace all occurrences of old\_api\_key in a configuration file:  
python start\_app.py \--file "config.ini" \\  
\--regex-find "old\_api\_key" \\  
\--regex-replace "new\_super\_secret\_key"

#### **2\. Surgical Text Replacement (Precise & UI-Assisted)**

For complex edits from AI agents or scripts that require precise line/column accuracy, use the surgical replacement flags. This will briefly launch the UI to perform the operation.  
**Example:** To replace lines 10-12 of config.txt and save automatically:  
python start\_app.py \--file "config.txt" \\  
\--sline 10 \--eline 12 \\  
\--replace-text "\#\# NEW CONFIGURATION \#\#\\nkey \= value" \\  
\--autosave

#### **All Command-Line Options**

| Argument | Description |
| :---- | :---- |
| \--file | **Required.** The path to the file. |
| \--regex-find | **\[HEADLESS\]** A regex pattern to find. |
| \--regex-replace | **\[HEADLESS\]** The replacement string for the regex pattern. |
| \--sline | **\[UI\]** The starting line number for selection/replacement. |
| \--eline | **\[UI\]** The ending line number for selection/replacement. |
| \--scol | **\[UI\]** The starting column number for replacement. |
| \--ecol | **\[UI\]** The ending column number for replacement. |
| \--replace-text | **\[UI\]** The text to insert into the specified range. |
| \--autosave | **\[UI\]** Automatically save after a surgical replacement. |
| \--theme | **\[UI\]** Sets the editor theme. Options: vs, vs-dark. |
| \--lang | **\[UI\]** Forces a specific syntax highlighting language. |
| \--read-only | **\[UI\]** Opens the file in read-only mode. |

## **Contributing**

Contributions are welcome\! If you have ideas for new features or have found a bug, please feel free to open an issue or submit a pull request.

## **License**

This project is licensed under the MIT License \- see the [LICENSE.md](http://docs.google.com/LICENSE.md) file for details.
--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\requirements.txt
--------------------------------------------------------------------------------
pywebview==6.0
qtpy
PySide6
bottle
proxy_tools
typing_extensions
--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
setlocal EnableDelayedExpansion

echo [SYSTEM] Initializing Systems Thinker environment...

:: --- 1. Python Version Check ---
:: We need a stable version (3.10, 3.11, or 3.12) because 3.14 breaks pythonnet.
:: We check for specific versions in order of preference.

set "TARGET_PY="

:: Check for 3.11 (Ideal)
py -3.11 --version >nul 2>&1
if %ERRORLEVEL% EQU 0 (
    set "TARGET_PY=py -3.11"
    goto :FOUND_PYTHON
)

:: Check for 3.12 (Backup)
py -3.12 --version >nul 2>&1
if %ERRORLEVEL% EQU 0 (
    set "TARGET_PY=py -3.12"
    goto :FOUND_PYTHON
)

:: Check for 3.10 (Backup)
py -3.10 --version >nul 2>&1
if %ERRORLEVEL% EQU 0 (
    set "TARGET_PY=py -3.10"
    goto :FOUND_PYTHON
)

:: Fallback warning if only 3.14/other exists
echo [WARNING] Could not find Python 3.10, 3.11, or 3.12.
echo [WARNING] Attempting with default 'py' (This may fail if version is 3.14+)...
set "TARGET_PY=py"

:FOUND_PYTHON
echo [SYSTEM] Using Python interpreter: %TARGET_PY%

:: --- 2. Clean/Create Venv ---
:: If .venv exists but might be broken (wrong version), prompt to rebuild? 
:: For now, we will trust the existing one OR you can delete it manually to force rebuild.
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    %TARGET_PY% -m venv .venv
    if %ERRORLEVEL% NEQ 0 (
        echo [ERROR] Failed to create venv. Ensure you have the requested Python version installed.
        pause
        exit /b 1
    )
) else (
    echo [SYSTEM] Found existing .venv.
)

:: --- 3. Install Dependencies ---
echo [SYSTEM] Upgrading pip and installing requirements...
.venv\Scripts\python.exe -m pip install --upgrade pip

if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
    if %ERRORLEVEL% NEQ 0 (
        echo.
        echo [ERROR] Dependency installation failed!
        echo [TIP] If this is a 'pythonnet' error, please install Python 3.11 specifically.
        pause
        exit /b 1
    )
)

echo.
echo [SUCCESS] Environment ready!
echo [INFO] Launch pattern: .venv\Scripts\python -m src.app
pause
--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\assets\index.html
--------------------------------------------------------------------------------
<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <meta http-equiv="Content-Security-Policy"
        content="default-src 'self' https://cdn.jsdelivr.net https://unpkg.com;
                 style-src   'self' 'unsafe-inline' https://cdn.jsdelivr.net https://unpkg.com;
                 script-src  'self' 'unsafe-eval' 'unsafe-inline' https://cdn.jsdelivr.net https://unpkg.com;
                 font-src    https://cdn.jsdelivr.net https://unpkg.com;
                 img-src     'self' data:;
                 worker-src  blob:">
  <title>Monaco Seed</title>
  <style>
    %CSS%
  </style>
  <script>
    %JS%
  </script>
</head>
<body>
  <div id="tabs-container"></div>
  <div id="editor"></div>
  
  <div id="status-bar">
    <span id="status-filepath"></span>
    <span id="status-cursor">Ln 1, Col 1</span>
  </div>

  <div id="find-modal-overlay" class="hidden">
    <div id="find-modal">
      <h2>Find & Replace</h2>
      
      <div class="input-group">
        <label>Find:</label>
        <input type="text" id="find-input" placeholder="Search..." />
      </div>

      <div class="input-group">
        <label>Replace:</label>
        <input type="text" id="replace-input" placeholder="Replacement..." />
      </div>

      <div class="modal-buttons">
        <button id="btn-find-next">Find Next</button>
        <button id="btn-replace">Replace</button>
        <button id="btn-replace-all">Replace All</button>
        <button id="btn-find-close" style="background-color: #4a4a4a;">Close</button>
      </div>
      <div id="find-status"></div>
    </div>
  </div>

  <div id="surgical-modal-overlay" class="hidden">
    <div id="surgical-modal">
      <h2>Agent Surgical Replace</h2>
      <p>Paste a JSON object from an agent into the text area below.</p>
      
      <textarea id="surg-text" rows="10" placeholder=""></textarea>

      <div class="modal-buttons">
        <button id="surg-copy-schema">Copy Schema</button>
        <button id="surg-cancel">Cancel</button>
        <button id="surg-apply">Apply</button>
      </div>
    </div>
  </div>

</body>
</html>
--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\assets\index.js
--------------------------------------------------------------------------------
const BOOT = JSON.parse(atob('%BOOT%'));
// --- Global State ---
let editor;
let monaco;
let tabs = [];
let nextTabId = 0;
let activeTabId = -1;

// --- Find/Replace State ---
let currentMatches = [];
let currentMatchIndex = -1;

const SURGICAL_SCHEMA = {
  "file_path": "string (optional, for context)",
  "start_line": "number",
  "end_line": "number",
  "start_column": "number (optional)",
  "end_column": "number (optional)",
  "replacement_code": "string"
};

// --- FIND & REPLACE LOGIC (New) ---
function showFindReplace() {
    const modal = document.getElementById('find-modal-overlay');
    const input = document.getElementById('find-input');
    if (!modal) return;
    modal.classList.remove('hidden');
    input.focus();
    // Pre-fill with selected text if any
    const selection = editor.getSelection();
    if (selection && !selection.isEmpty()) {
        const text = editor.getModel().getValueInRange(selection);
        input.value = text;
    }
}

function closeFindReplace() {
    document.getElementById('find-modal-overlay').classList.add('hidden');
    editor.focus();
}

function doFindNext() {
    const findText = document.getElementById('find-input').value;
    if (!findText) return;

    const model = editor.getModel();
    // Find all matches
    currentMatches = model.findMatches(findText, false, false, false, null, true);
    
    if (currentMatches.length === 0) {
        document.getElementById('find-status').textContent = "No matches found.";
        return;
    }

    // Cycle index
    currentMatchIndex++;
    if (currentMatchIndex >= currentMatches.length) {
        currentMatchIndex = 0;
    }

    const match = currentMatches[currentMatchIndex];
    editor.setSelection(match.range);
    editor.revealRangeInCenter(match.range);
    document.getElementById('find-status').textContent = `Match ${currentMatchIndex + 1} of ${currentMatches.length}`;
}

function doReplace() {
    const replaceText = document.getElementById('replace-input').value;
    const selection = editor.getSelection();
    
    // Ensure the current selection matches the find text (safety check)
    // For simplicity in this faux-popup, we just replace the current selection
    if (!selection.isEmpty()) {
        editor.executeEdits('source', [{ range: selection, text: replaceText }]);
        doFindNext(); // Move to next
    } else {
        doFindNext(); // Nothing selected, find first
    }
}

function doReplaceAll() {
    const findText = document.getElementById('find-input').value;
    const replaceText = document.getElementById('replace-input').value;
    if (!findText) return;

    const model = editor.getModel();
    const matches = model.findMatches(findText, false, false, false, null, true);
    
    if (matches.length === 0) {
        document.getElementById('find-status').textContent = "Nothing to replace.";
        return;
    }

    const edits = matches.map(m => ({ range: m.range, text: replaceText }));
    editor.executeEdits('source', edits);
    document.getElementById('find-status').textContent = `Replaced ${matches.length} occurrences.`;
}

// --- SURGICAL REPLACE LOGIC (Existing) ---
function showSurgicalReplace() {
    const modal = document.getElementById('surgical-modal-overlay');
    const textArea = document.getElementById('surg-text');
    if (!modal || !textArea) return;

    const selection = editor.getSelection();
    let sampleSchema = { ...SURGICAL_SCHEMA };
    if (selection && !selection.isEmpty()) {
        sampleSchema.start_line = selection.startLineNumber;
        sampleSchema.end_line = selection.endLineNumber;
        sampleSchema.start_column = selection.startColumn;
        sampleSchema.end_column = selection.endColumn;
        sampleSchema.replacement_code = "Your replacement text here...";
    } else {
        sampleSchema.start_line = 1;
        sampleSchema.end_line = 1;
        sampleSchema.replacement_code = "Your replacement text here...";
    }
    
    textArea.placeholder = JSON.stringify(sampleSchema, null, 2);
    textArea.value = '';

    modal.classList.remove('hidden');
    textArea.focus();
}

function applySurgicalReplace() {
    const textEl = document.getElementById('surg-text');
    if (!textEl.value) {
        cancelSurgicalReplace();
        return;
    }

    try {
        const data = JSON.parse(textEl.value);
        const rawStartLine = data.start_line;
        const rawEndLine = data.end_line;
        const rawStartCol = data.start_column;
        const rawEndCol = data.end_column;
        const text = data.replacement_code;

        if (rawStartLine == null || rawEndLine == null || text == null) {
            throw new Error('Missing required fields: start_line, end_line, replacement_code');
        }

        const sline = Number(rawStartLine);
        const eline = Number(rawEndLine);
        const tab = getActiveTab();
        
        if (!tab || !tab.model) throw new Error('No active tab.');

        const maxLine = tab.model.getLineCount();
        const startLine = Math.min(Math.max(sline, 1), maxLine);
        const endLine = Math.min(Math.max(eline, startLine), maxLine);

        const numericStartCol = rawStartCol != null ? Number(rawStartCol) : 1;
        const numericEndCol = rawEndCol != null ? Number(rawEndCol) : tab.model.getLineMaxColumn(endLine);
        
        const maxStartCol = tab.model.getLineMaxColumn(startLine);
        const maxEndCol = tab.model.getLineMaxColumn(endLine);

        const startColumn = Math.min(Math.max(numericStartCol || 1, 1), maxStartCol);
        const endColumn = Math.min(Math.max(numericEndCol || maxEndCol, 1), maxEndCol);

        const range = new monaco.Range(startLine, startColumn, endLine, endColumn);
        tab.model.pushEditOperations([], [{ range: range, text: text }], () => null);
        editor.revealRangeInCenter(range, monaco.editor.ScrollType.Smooth);
        cancelSurgicalReplace();
    } catch (e) {
        if (window.pywebview && window.pywebview.api && window.pywebview.api.create_alert) {
            window.pywebview.api.create_alert('JSON Parse Error', e.message);
        } else {
            alert('JSON Parse Error: ' + e.message);
        }
    }
}

function cancelSurgicalReplace() {
    const modal = document.getElementById('surgical-modal-overlay');
    if (modal) modal.classList.add('hidden');
    if (editor && typeof editor.focus === 'function') {
        editor.focus();
    }
}

function copySurgicalSchema() {
    const textArea = document.getElementById('surg-text');
    const schemaText = textArea.placeholder;
    navigator.clipboard.writeText(schemaText)
        .then(() => {
            const btn = document.getElementById('surg-copy-schema');
            const originalText = btn.textContent;
            btn.textContent = 'Copied!';
            setTimeout(() => { btn.textContent = originalText; }, 1500);
        })
        .catch(err => {
            console.error('Failed to copy schema: ', err);
        });
}

// --- MONACO BOOT ---
function bootMonaco() {
  if (!window.require) { console.error('[monaco] AMD loader not present'); return; }
  window.require.config({ paths: { 'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.45.0/min/vs' } });
  window.require(['vs/editor/editor.main'], function (m) {
    monaco = m;
    const el = document.getElementById('editor');
    editor = monaco.editor.create(el, {
      model: null,
      automaticLayout: true,
      readOnly: !!BOOT.readOnly,
      minimap: { enabled: true },
      lineNumbers: 'on'
    });

    editor.onDidChangeCursorPosition(e => {
        const statusCursor = document.getElementById('status-cursor');
        if (statusCursor) {
            const { lineNumber, column } = e.position;
            statusCursor.textContent = `Ln ${lineNumber}, Col ${column}`;
        }
    });

    if (BOOT.theme) monaco.editor.setTheme(BOOT.theme);

    addTab(BOOT.path, BOOT.text);

    // Initial surgical replacement if passed via CLI args
    const tab = getActiveTab();
    if (tab) {
        if (BOOT.replaceText != null && BOOT.sline && BOOT.eline) {
            const startColumn = BOOT.scol || 1;
            const endColumn = BOOT.ecol || tab.model.getLineMaxColumn(BOOT.eline);
            const range = new monaco.Range(BOOT.sline, startColumn, BOOT.eline, endColumn);
            tab.model.pushEditOperations([], [{ range: range, text: BOOT.replaceText }], () => null);
            editor.setSelection(new monaco.Range(0,0,0,0));
            editor.revealRangeInCenter(range, monaco.editor.ScrollType.Smooth);
            if (BOOT.autosave) {
                setTimeout(doSave, 100);
            }
        } else if (BOOT.sline && BOOT.eline) {
            const range = new monaco.Range(BOOT.sline, 1, BOOT.eline, 1);
            editor.revealRangeInCenter(range, monaco.editor.ScrollType.Smooth);
            editor.setSelection(range);
        }
    }

    document.addEventListener('keydown', (e) => {
        const ctrl = e.ctrlKey || e.metaKey;
        if (ctrl && (e.key === 's' || e.key === 'S')) { e.preventDefault(); doSave(); }
        if (ctrl && (e.key === 'o' || e.key === 'O')) { e.preventDefault(); doOpen(); }
        // Bind Ctrl+F to our custom modal
        if (ctrl && (e.key === 'f' || e.key === 'F')) { e.preventDefault(); showFindReplace(); }
    }, true);

    // Expose Global Hooks
    window.__doNew = () => addTab(null, '');
    window.__doOpen = doOpen;
    window.__doSave = doSave;
    window.__doSaveAs = doSaveAs;
    window.__doUndo = () => getActiveTab()?.model.undo();
    window.__doRedo = () => getActiveTab()?.model.redo();
    window.__doCut = () => editor.getAction('editor.action.clipboardCutAction').run();
    window.__doCopy = () => editor.getAction('editor.action.clipboardCopyAction').run();
    window.__doPaste = () => editor.getAction('editor.action.clipboardPasteAction').run();
    
    // THE FIX: Point to our new function!
    window.__showFindReplace = showFindReplace;
    window.__showSurgicalReplace = showSurgicalReplace;

    // Event Listeners for Surgical
    document.getElementById('surg-apply').addEventListener('click', applySurgicalReplace);
    document.getElementById('surg-cancel').addEventListener('click', cancelSurgicalReplace);
    document.getElementById('surg-copy-schema').addEventListener('click', copySurgicalSchema);
    document.getElementById('surgical-modal-overlay').addEventListener('click', (e) => {
        if (e.target.id === 'surgical-modal-overlay') cancelSurgicalReplace();
    });

    // Event Listeners for Find/Replace
    document.getElementById('btn-find-next').addEventListener('click', doFindNext);
    document.getElementById('btn-replace').addEventListener('click', doReplace);
    document.getElementById('btn-replace-all').addEventListener('click', doReplaceAll);
    document.getElementById('btn-find-close').addEventListener('click', closeFindReplace);
    document.getElementById('find-modal-overlay').addEventListener('click', (e) => {
        if (e.target.id === 'find-modal-overlay') closeFindReplace();
    });

    // Escape Key Handler
    document.addEventListener('keydown', (e) => {
        if (e.key === 'Escape') {
            if (!document.getElementById('surgical-modal-overlay').classList.contains('hidden')) {
                cancelSurgicalReplace();
            }
            if (!document.getElementById('find-modal-overlay').classList.contains('hidden')) {
                closeFindReplace();
            }
        }
    });
  });
}

// --- Tab Helper Functions ---
const getTab = (tabId) => tabs.find(t => t.id === tabId);
const getActiveTab = () => getTab(activeTabId);
const languageFromPath = (p) => {
    if (!p || !monaco) return 'plaintext';
    const ext = '.' + p.split('.').pop();
    const langs = monaco.languages.getLanguages();
    const hit = langs.find(l => Array.isArray(l.extensions) && l.extensions.includes(ext));
    return hit ? hit.id : 'plaintext';
}

function renderTabs() {
    const container = document.getElementById('tabs-container');
    container.innerHTML = '';
    tabs.forEach(tab => {
        const tabEl = document.createElement('div');
        tabEl.className = 'tab' + (tab.id === activeTabId ? ' active' : '');
        tabEl.onclick = () => switchTab(tab.id);
        const name = tab.path ? tab.path.split(/[\\/]/).pop() : 'Untitled';
        const closeIcon = tab.isDirty ? '‚óè' : '&times;';
        tabEl.innerHTML = `<span>${name}</span><span class="tab-close" onclick="closeTab(event, ${tab.id})">${closeIcon}</span>`;
        container.appendChild(tabEl);
    });
    const activeTab = getActiveTab();
    if (window.pywebview && window.pywebview.api && window.pywebview.api.set_active_tab) {
         window.pywebview.api.set_active_tab(activeTab?.path || null, activeTab?.isDirty || false);
    }
    const statusFilepath = document.getElementById('status-filepath');
    if (statusFilepath) {
        statusFilepath.textContent = activeTab?.path || '[Untitled]';
    }
}

function switchTab(tabId) {
    if (activeTabId === tabId) return;
    const currentTab = getActiveTab();
    if (currentTab) {
        currentTab.viewState = editor.saveViewState();
    }
    activeTabId = tabId;
    const newTab = getActiveTab();
    editor.setModel(newTab ? newTab.model : null);
    if (newTab && newTab.viewState) {
        editor.restoreViewState(newTab.viewState);
    }
    editor.focus();
    renderTabs();
}

function addTab(path, text) {
    const existing = path ? tabs.find(t => t.path === path) : null;
    if (existing) {
        switchTab(existing.id);
        return;
    }
    const newTab = {
        id: nextTabId++,
        path: path,
        model: monaco.editor.createModel(text, languageFromPath(path)),
        viewState: null,
        isDirty: false
    };
    newTab.model.onDidChangeContent(() => {
        if (!newTab.isDirty) {
            newTab.isDirty = true;
            renderTabs();
        }
    });
    tabs.push(newTab);
    switchTab(newTab.id);
}

async function closeTab(event, tabId) {
    event.stopPropagation();
    const tabIdx = tabs.findIndex(t => t.id === tabId);
    if (tabIdx === -1) return;

    const tabToClose = getTab(tabId);
    if (tabToClose.isDirty) {
        const confirmed = await window.pywebview.api.confirm_dialog(
            'Unsaved Changes',
            'You have unsaved changes. Are you sure you want to close this tab?'
        );
        if (!confirmed) {
            return;
        }
    }

    const [removedTab] = tabs.splice(tabIdx, 1);
    removedTab.model.dispose();
    if (activeTabId === tabId) {
        const newActiveIdx = Math.max(0, tabIdx - 1);
        const newActiveTab = tabs.length > 0 ? tabs[newActiveIdx] : null;
        switchTab(newActiveTab ? newActiveTab.id : -1);
    }

    if (tabs.length === 0) {
        addTab(null, '');
    } else {
        renderTabs();
    }
}

// --- File Operations ---
async function doOpen() {
    if (!(window.pywebview && window.pywebview.api && window.pywebview.api.open_dialog)) return;
    const res = await window.pywebview.api.open_dialog();
    if (res && res.path != null && typeof res.text === 'string') {
        addTab(res.path, res.text);
    }
}

async function doSave() {
    const tab = getActiveTab();
    if (!tab) return;
    const res = await window.pywebview.api.save_dialog(tab.model.getValue(), tab.path);
    if (res && res.saved && res.path) {
       tab.path = res.path;
       tab.isDirty = false;
       monaco.editor.setModelLanguage(tab.model, languageFromPath(tab.path));
       renderTabs();
    }
}

async function doSaveAs() {
    const tab = getActiveTab();
    if (!tab) return;
    const res = await window.pywebview.api.save_as_dialog(tab.model.getValue(), tab.path);
    if (res && res.saved && res.path) {
        tab.path = res.path;
        tab.isDirty = false;
        monaco.editor.setModelLanguage(tab.model, languageFromPath(tab.path));
        renderTabs();
    }
}

// --- Boot sequence ---
(function(){
  var s = document.createElement('script');
  s.src = 'https://cdn.jsdelivr.net/npm/monaco-editor@0.45.0/min/vs/loader.js';
  s.onload = () => document.readyState === 'loading' ? document.addEventListener('DOMContentLoaded', bootMonaco) : bootMonaco();
  document.head.appendChild(s);
})();
--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\assets\style.css
--------------------------------------------------------------------------------
html, body {
  height: 100%; width: 100%; margin: 0; padding: 0; overflow: hidden;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
  display: flex; flex-direction: column; background: #1e1e1e; color: #ddd;
}
#tabs-container {
  flex: 0 0 auto; display: flex; background-color: #252526;
  padding: 5px 5px 0 5px; overflow-x: auto;
}
.tab {
  display: flex; align-items: center; padding: 8px 12px;
  background-color: #2D2D2D; color: #9e9e9e;
  border-top-left-radius: 4px; border-top-right-radius: 4px;
  margin-right: 2px;
  cursor: pointer; font-size: 14px; white-space: nowrap;
}
.tab.active { background-color: #1E1E1E; color: #ffffff; }
.tab-close {
  margin-left: 10px; width: 16px;
  height: 16px; border-radius: 50%;
  display: flex; align-items: center; justify-content: center; font-weight: bold;
}
.tab:hover .tab-close { background-color: #3f3f3f; }
.tab-close:hover { background-color: #5f5f5f; }

#editor {
    flex: 1 1 auto;
    min-height: 0;
}

#status-bar {
    flex: 0 0 auto;
    display: flex;
    justify-content: space-between;
    align-items: center;
    background-color: #007acc;
    color: white;
    padding: 2px 10px;
    font-size: 13px;
    min-height: 22px;
}

/* --- Common Modal Overlays --- */
#surgical-modal-overlay, #find-modal-overlay {
    position: fixed;
    top: 0; left: 0; width: 100%; height: 100%;
    background-color: rgba(0, 0, 0, 0.6);
    display: flex; align-items: center; justify-content: center;
    z-index: 1000;
}

/* --- Surgical Modal --- */
#surgical-modal {
    background-color: #252526; color: #cccccc;
    padding: 20px; border-radius: 8px;
    width: 90%; max-width: 600px;
    display: flex; flex-direction: column; gap: 15px;
    border: 1px solid #3c3c3c;
}
#surgical-modal h2 { margin: 0; color: white; }
#surgical-modal p { margin: 0; font-size: 14px; color: #9e9e9e; }
#surgical-modal textarea {
    background-color: #3c3c3c; color: #cccccc;
    border: 1px solid #555; border-radius: 4px; padding: 8px;
    font-family: "Courier New", Courier, monospace;
    font-size: 14px; resize: vertical; min-height: 150px;
}
#surgical-modal textarea::placeholder { color: #6a6a6a; opacity: 1; }

/* --- Find/Replace Modal (New) --- */
#find-modal {
    background-color: #252526; color: #cccccc;
    padding: 20px; border-radius: 8px;
    width: 400px; /* Smaller than surgical */
    display: flex; flex-direction: column; gap: 15px;
    border: 1px solid #3c3c3c;
}
#find-modal h2 { margin: 0; color: white; }

.input-group {
    display: flex; flex-direction: column; gap: 5px;
}
.input-group label { font-size: 12px; color: #9e9e9e; font-weight: bold; }
.input-group input {
    background-color: #3c3c3c; color: white;
    border: 1px solid #555; border-radius: 4px;
    padding: 6px; font-size: 14px;
}
.input-group input:focus {
    border-color: #007acc; outline: none;
}
#find-status {
    font-size: 12px; color: #aaa; text-align: center; height: 14px;
}

/* --- Buttons --- */
.modal-buttons {
    display: flex; justify-content: flex-end; align-items: center; gap: 10px;
}
.modal-buttons button {
    border: 1px solid #555; background-color: #3c3c3c;
    color: #cccccc; padding: 8px 16px; border-radius: 4px;
    cursor: pointer; font-weight: 500;
}
.modal-buttons button:hover { background-color: #4a4a4a; }
.modal-buttons button#surg-copy-schema { margin-right: auto; background-color: #4a4a4a; }
.modal-buttons button#surg-apply, 
.modal-buttons button#btn-find-next,
.modal-buttons button#btn-replace,
.modal-buttons button#btn-replace-all {
    background-color: #007acc; color: white; border-color: #007acc;
}
.modal-buttons button#surg-apply:hover, 
.modal-buttons button#btn-find-next:hover,
.modal-buttons button#btn-replace:hover,
.modal-buttons button#btn-replace-all:hover {
    background-color: #0098ff;
}

.hidden { display: none !important; }
--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\src\app.py
--------------------------------------------------------------------------------
import sys
import os
import argparse
import json
import base64
import html
import contextlib
import re
import tkinter as tk # Kept for contract compliance, though primary GUI is pywebview

# --- DEPENDENCY CHECKS ---
# Force Qt backend for stability as per original source 
os.environ['PYWEBVIEW_GUI'] = 'qt'
os.environ.setdefault('PYWEBVIEW_LOG', 'info')

try:
    import qtpy
    from PySide6 import QtCore
    from PySide6.QtGui import QIcon
except ImportError as e:
    print(f"[fatal] Qt backend not available. Install: pip install qtpy PySide6\n{e}", file=sys.stderr)
    sys.exit(1)

try:
    import webview
    from webview import FileDialog
    from webview.menu import Menu, MenuAction, MenuSeparator
except ImportError as e:
    print(f"[fatal] pywebview not available. Install: pip install pywebview\n{e}", file=sys.stderr)
    sys.exit(1)

# --- LOG FILTERING (From legacy start_app.py) ---
def apply_log_filter():
    """Suppress harmless Mesa/Qt warnings often seen on Linux."""
    error_patterns = [
        re.compile(r"MESA-LOADER: failed to open i965"),
        re.compile(r"failed to load driver: i965"),
        re.compile(r"Buffer handle is null"),
        re.compile(r"Creation of StagingBuffer's SharedImage failed"),
        re.compile(r"shared_image_interface_proxy.cc"),
        re.compile(r"one_copy_raster_buffer_provider.cc"),
    ]

    class LogFilter:
        def __init__(self, stream):
            self.stream = stream
        def write(self, data):
            modified = data
            for pattern in error_patterns:
                if pattern.search(data):
                    modified = data.replace("ERROR", "WARNING (safe to ignore)")
                    modified = modified.replace("failed", "note: failed")
                    break
            self.stream.write(modified)
        def flush(self):
            self.stream.flush()

    sys.stdout = LogFilter(sys.stdout)
    sys.stderr = LogFilter(sys.stderr)

# --- CORE LOGIC & HELPERS ---

def b64(s: str) -> str:
    """Encodes a string into Base64 for safe embedding in HTML."""
    return base64.b64encode(s.encode('utf-8')).decode('ascii')

def load_text(path: str | None) -> str:
    """Safely loads text from a file path."""
    if not path:
        return ''
    try:
        with open(path, 'r', encoding='utf-8', errors='replace') as f:
            return f.read()
    except Exception as e:
        print(f"[error] Failed to read file: {path}\n{e}", file=sys.stderr)
        return f"<unable to read {html.escape(str(path))}>"

def get_asset_path(filename: str) -> str:
    """Resolves path to the assets directory relative to this script."""
    # src/app.py -> project_root/assets/filename
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    return os.path.join(base_dir, 'assets', filename)

def load_and_combine_ui() -> str:
    """Reads UI files from assets/ and combines them."""
    try:
        with open(get_asset_path('index.html'), 'r', encoding='utf-8') as f:
            html_template = f.read()
        with open(get_asset_path('style.css'), 'r', encoding='utf-8') as f:
            css_text = f.read()
        with open(get_asset_path('index.js'), 'r', encoding='utf-8') as f:
            js_text = f.read()
    except FileNotFoundError as e:
        print(f"[fatal] UI file not found: {e}. Ensure assets are in the 'assets/' directory.", file=sys.stderr)
        sys.exit(1)
    return html_template.replace('%CSS%', css_text).replace('%JS%', js_text)

class Api:
    """The API class exposed to the JavaScript frontend."""
    def __init__(self):
        self.window: webview.Window | None = None
        self._active_path: str | None = None
        self._active_is_dirty: bool = False
        self._boot: dict | None = None

    def get_boot_data(self) -> dict:
        return self._boot or {}

    def create_alert(self, title: str, message: str):
        if self.window:
            self.window.create_alert(title, message)

    def confirm_dialog(self, title: str, message:str) -> bool:
        if self.window:
            return self.window.create_confirmation_dialog(title, message)
        return False

    def set_active_tab(self, path: str | None, is_dirty: bool):
        self._active_path = path
        self._active_is_dirty = is_dirty
        self._update_title()

    def open_dialog(self) -> dict:
        assert self.window is not None
        result = self.window.create_file_dialog(FileDialog.OPEN, allow_multiple=False, file_types=("All files (*.*)",))
        if not result or not isinstance(result, (list, tuple)) or not result[0]:
            return {'cancelled': True}
        path = result[0]
        try:
            with open(path, 'r', encoding='utf-8', errors='replace') as f:
                text = f.read()
            return {'cancelled': False, 'path': path, 'text': text}
        except Exception as e:
            self.window.create_alert('File Open Error', f'Failed to read file:\n{path}\n\n{e}')
            return {'cancelled': True}

    def save_dialog(self, content: str, path: str | None) -> dict:
        return self._save_logic(content, path, force_dialog=False)

    def save_as_dialog(self, content: str, path: str | None) -> dict:
        return self._save_logic(content, path, force_dialog=True)

    def _save_logic(self, content: str, path: str | None, force_dialog: bool) -> dict:
        assert self.window is not None
        if not path or force_dialog:
            result = self.window.create_file_dialog(
                FileDialog.SAVE,
                directory=os.path.dirname(path) if path else '',
                save_filename=os.path.basename(path) if path else 'untitled.txt',
                file_types=("All files (*.*)",)
            )
            if not result:
                return {'saved': False}
            path = result[0] if isinstance(result, (tuple, list)) and len(result) > 0 else result

        if not path:
             return {'saved': False}

        try:
            with open(path, 'w', encoding='utf-8') as f:
                f.write(content)
            self.set_active_tab(path, is_dirty=False)
            return {'saved': True, 'path': path}
        except Exception as e:
            self.window.create_alert('Save Error', f'Failed to save to {path}\n{e}')
            return {'saved': False, 'error': str(e)}

    def quit(self):
        if self.window:
            self.window.destroy()

    def _update_title(self):
        if not self.window:
            return
        base = os.path.basename(self._active_path) if self._active_path else 'Untitled'
        # Hide NamedTemporaryFile suffixes
        if base.lower().startswith("untitled-") and base.lower().endswith(".txt"):
            base = "Untitled"
        dirty_indicator = '‚óè' if self._active_is_dirty else ''
        self.window.set_title(f"{base}{dirty_indicator} - Monaco Viewer")

# --- GUI MODE (Default / Showcase) ---

def run_gui(file=None, sline=None, eline=None, scol=None, ecol=None,
            replace_text=None, autosave=False, theme='vs-dark',
            lang=None, read_only=False):
    """Launches the PyWebView GUI."""
    
    # Path handling
    path = os.path.abspath(file) if file else None
    text = load_text(path)
    base = os.path.basename(path) if path else ""
    is_untitled = (not base) or (base.lower().startswith("untitled-") and base.lower().endswith(".txt"))
    display_name = "Untitled" if is_untitled else base

    # Prepare Boot Data
    boot = {
        'text': text, 'path': path, 'sline': sline, 'eline': eline, 'scol': scol, 'ecol': ecol,
        'replaceText': replace_text, 'autosave': autosave, 'theme': theme, 'lang': lang,
        'readOnly': read_only, 'displayName': display_name, 'isUntitled': is_untitled,
    }
    
    api = Api()
    api._boot = boot
    
    # Inject Boot Data into HTML
    try:
        final_html = load_and_combine_ui().replace('%BOOT%', b64(json.dumps(boot)))
    except Exception as e:
        print(f"Error preparing UI: {e}")
        return

    # Native Menus
    menu_items = [
        Menu('File', [
            MenuAction('New', lambda: api.window.evaluate_js('window.__doNew()')),
            MenuAction('Open', lambda: api.window.evaluate_js('window.__doOpen()')),
            MenuAction('Save', lambda: api.window.evaluate_js('window.__doSave()')),
            MenuAction('Save As...', lambda: api.window.evaluate_js('window.__doSaveAs()')),
            MenuSeparator(),
            MenuAction('Quit', api.quit)
        ]),
        Menu('Edit', [
            MenuAction('Undo', lambda: api.window.evaluate_js('window.__doUndo()')),
            MenuAction('Redo', lambda: api.window.evaluate_js('window.__doRedo()')),
            MenuSeparator(),
            MenuAction('Cut', lambda: api.window.evaluate_js('window.__doCut()')),
            MenuAction('Copy', lambda: api.window.evaluate_js('window.__doCopy()')),
            MenuAction('Paste', lambda: api.window.evaluate_js('window.__doPaste()')),
            MenuSeparator(),
            MenuAction('Find / Replace', lambda: api.window.evaluate_js('window.__showFindReplace()')),
            MenuSeparator(),
            MenuAction('Agent Surgical Replace...', lambda: api.window.evaluate_js('window.__showSurgicalReplace()'))
        ])
    ]

    win = webview.create_window(
        title="Monaco Viewer", html=final_html, width=1100, height=750,
        js_api=api, confirm_close=True, menu=menu_items
    )
    api.window = win

    def set_icon():
        icon_path = get_asset_path('monaco-viewer-icon.png')
        if webview.windows and hasattr(webview.windows[0], 'gui_window'):
            native_win = webview.windows[0].gui_window
            if native_win and os.path.exists(icon_path):
                try:
                    native_win.setWindowIcon(QIcon(icon_path))
                except Exception:
                    pass

    # Suppress output during launch
    with open(os.devnull, 'w') as f, contextlib.redirect_stderr(f):
        webview.start(set_icon, gui='qt', debug=False)

# --- CLI MODE (Utility) ---

def run_cli():
    """Handles command-line arguments for headless tasks or configured GUI launch."""
    ap = argparse.ArgumentParser(description='Monaco Viewer - Code Editor & Utility')
    
    # GUI Arguments
    ap.add_argument('--file', nargs='?', default=None, help='Path to file to open.')
    ap.add_argument('--untitled', action='store_true', help='Start a new Untitled buffer.')
    ap.add_argument('--sline', type=int, help='Start line.')
    ap.add_argument('--eline', type=int, help='End line.')
    ap.add_argument('--scol', type=int, help='Start column.')
    ap.add_argument('--ecol', type=int, help='End column.')
    ap.add_argument('--replace-text', type=str, help='Text to insert.')
    ap.add_argument('--autosave', action='store_true', help='Autosave after replace.')
    ap.add_argument('--theme', type=str, default='vs-dark', help='vs, vs-dark.')
    ap.add_argument('--lang', type=str, help='Force language.')
    ap.add_argument('--read-only', action='store_true', help='Read-only mode.')

    # Headless Arguments
    ap.add_argument('--regex-find', type=str, help='[HEADLESS] Regex pattern.')
    ap.add_argument('--regex-replace', type=str, help='[HEADLESS] Replacement string.')

    args = ap.parse_args()

    # --- Headless Logic ---
    if args.regex_find and args.regex_replace:
        if not args.file:
            print("[error] --file is required for headless regex mode.", file=sys.stderr)
            sys.exit(2)
        if not os.path.exists(args.file):        
            print(f"[error] File not found: {args.file}", file=sys.stderr)
            sys.exit(1)
        try:
            with open(args.file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            new_content, count = re.subn(args.regex_find, args.regex_replace, content)

            if content != new_content:
                with open(args.file, 'w', encoding='utf-8') as f:
                    f.write(new_content)
                print(f"Successfully made {count} replacement(s) in {args.file}")
            else:
                print("No matches found. File was not changed.")
        except Exception as e:
            print(f"[error] Headless error: {e}", file=sys.stderr)
            sys.exit(1)
        sys.exit(0)

    # --- Configured GUI Launch ---
    # Handle temp file for "Untitled" logic
    if args.untitled or args.file is None:
        from tempfile import NamedTemporaryFile
        tmp = NamedTemporaryFile(mode="w+", suffix=".txt", prefix="Untitled-", delete=False)
        tmp.close()
        args.file = tmp.name

    # Infer language
    if not args.lang and args.file:
        ext = os.path.splitext(args.file)[1].lower()
        args.lang = {
            '.py':'python','.js':'javascript','.ts':'typescript','.json':'json',
            '.md':'markdown','.html':'html','.css':'css','.txt':'plaintext',
            '.c':'c','.cpp':'cpp','.h':'c','.hpp':'cpp','.sh':'shell','.ini':'ini',
        }.get(ext, 'plaintext')

    run_gui(
        file=args.file, sline=args.sline, eline=args.eline, scol=args.scol, ecol=args.ecol,
        replace_text=args.replace_text, autosave=args.autosave, theme=args.theme,
        lang=args.lang, read_only=args.read_only
    )

def main():
    apply_log_filter()
    if len(sys.argv) > 1:
        run_cli()
    else:
        # Default Showcase / Empty Launch
        from tempfile import NamedTemporaryFile
        tmp = NamedTemporaryFile(mode="w+", suffix=".txt", prefix="Untitled-", delete=False)
        tmp.close()
        run_gui(file=tmp.name, theme='vs-dark', lang='plaintext')

if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: _MonacoVIEWER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\requirements.txt
--------------------------------------------------------------------------------
tk>=0.1.0
--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\src\app - Copy (2).py
--------------------------------------------------------------------------------
import sys
import argparse
import tkinter as tk
from tkinter import filedialog, scrolledtext, ttk, messagebox
import tkinter.font as tkFont
from pathlib import Path
from datetime import datetime
import subprocess
import platform
import threading
import queue
import traceback
import fnmatch
import os
import json
import tarfile

# ==============================================================================
# 0. PYTHONW SAFETY CHECK
# ==============================================================================
# Fixes issues where pythonw crashes because it has no stdout/stderr attached
if sys.stdout is None:
    sys.stdout = open(os.devnull, "w")
if sys.stderr is None:
    sys.stderr = open(os.devnull, "w")

# ==============================================================================
# 1. CORE CONFIGURATION & CONSTANTS
# ==============================================================================

APP_DIR = Path(__file__).resolve().parent
DEFAULT_ROOT_DIR = APP_DIR

# --- Exclusions ---
EXCLUDED_FOLDERS = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "target", "out",
    "bin", "obj", "Debug", "Release", "logs", "venv"
}
PREDEFINED_EXCLUDED_FILENAMES = {
    "package-lock.json", "yarn.lock", ".DS_Store", "Thumbs.db",
    "*.pyc", "*.pyo", "*.swp", "*.swo"
}

# --- Binary Extensions (for skipping in dump) ---
FORCE_BINARY_EXTENSIONS_FOR_DUMP = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".aac", ".m4a",
    ".mp4", ".mkv", ".avi", ".mov", ".webm", ".flv", ".wmv",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".odt", ".ods",
    ".exe", ".dll", ".so", ".o", ".a", ".lib", ".app", ".dmg", ".deb", ".rpm",
    ".db", ".sqlite", ".mdb", ".accdb", ".dat", ".idx", ".pickle", ".joblib",
    ".pyc", ".pyo", ".class", ".jar", ".wasm",
    ".ttf", ".otf", ".woff", ".woff2",
    ".iso", ".img", ".bin", ".bak", ".data", ".asset", ".pak"
}

# --- Log Configuration ---
LOG_ROOT_NAME = "_logs"
PROJECT_CONFIG_FILENAME = "_project_mapper_config.json"

# --- State Constants ---
S_CHECKED = "checked"
S_UNCHECKED = "unchecked"

# ==============================================================================
# 2. HELPER FUNCTIONS (Pure Logic / Stateless)
# ==============================================================================

def is_binary(file_path: Path) -> bool:
    """Check if a file is binary by reading the first chunk."""
    try:
        with open(file_path, 'rb') as f:
            return b'\0' in f.read(1024)
    except (IOError, PermissionError):
        return True
    except Exception:
        return True

def get_folder_size_bytes(folder_path: Path) -> int:
    """Recursively calculate folder size."""
    total_size = 0
    try:
        for entry in os.scandir(folder_path):
            if entry.is_file(follow_symlinks=False):
                try: total_size += entry.stat(follow_symlinks=False).st_size
                except OSError: pass
            elif entry.is_dir(follow_symlinks=False):
                try: total_size += get_folder_size_bytes(Path(entry.path))
                except OSError: pass
    except OSError: pass
    return total_size

def format_display_size(size_bytes: int) -> str:
    """Format bytes into readable string."""
    if size_bytes < 1024: return f"{size_bytes} B"
    size_kb = size_bytes / 1024
    if size_kb < 1024: return f"{size_kb:.1f} KB"
    size_mb = size_kb / 1024
    if size_mb < 1024: return f"{size_mb:.1f} MB"
    size_gb = size_mb / 1024
    return f"{size_gb:.2f} GB"

# ==============================================================================
# 3. GUI COMPONENTS & PROGRESS POPUP
# ==============================================================================

class ProgressPopup:
    """A popup window that streams activity and allows cancellation."""
    def __init__(self, parent, title="Processing", on_cancel=None):
        self.top = tk.Toplevel(parent)
        self.top.title(title)
        self.top.geometry("500x300")
        self.top.configure(bg="#252526")
        self.top.transient(parent)
        self.top.grab_set()
        
        self.top.protocol("WM_DELETE_WINDOW", self._on_close_attempt)

        self.on_cancel = on_cancel
        self.is_cancelled = False

        # UI Elements
        lbl = tk.Label(self.top, text=f"{title}...", fg="white", bg="#252526", font=("Arial", 12, "bold"))
        lbl.pack(pady=10)

        self.log_display = scrolledtext.ScrolledText(self.top, height=10, bg="#1e1e1e", fg="#00ff00", font=("Consolas", 9))
        self.log_display.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        btn_frame = tk.Frame(self.top, bg="#252526")
        btn_frame.pack(fill=tk.X, pady=10)

        self.cancel_btn = tk.Button(btn_frame, text="CANCEL OPERATION", bg="#c23621", fg="white", 
                                    font=("Arial", 10, "bold"), command=self.trigger_cancel)
        self.cancel_btn.pack()

    def update_text(self, text):
        self.log_display.insert(tk.END, text + "\n")
        self.log_display.see(tk.END)

    def trigger_cancel(self):
        self.is_cancelled = True
        self.log_display.insert(tk.END, "\n!!! CANCELLATION REQUESTED - STOPPING !!!\n")
        self.log_display.see(tk.END)
        self.cancel_btn.config(state=tk.DISABLED, text="Stopping...")
        if self.on_cancel:
            self.on_cancel()

    def _on_close_attempt(self):
        if not self.is_cancelled:
            self.trigger_cancel()
        
    def close(self):
        self.top.destroy()


class ProjectMapperApp:
    def __init__(self, root: tk.Tk):
        self.root = root
        self.gui_queue = queue.Queue()

        # Application State
        self.folder_item_states = {}
        self.dynamic_global_excluded_filenames = set()

        # .gitignore support (best-effort, simple patterns)
        self.gitignore_dirnames = set()
        self.gitignore_file_patterns = set()
        self.gitignore_path_patterns = set()

        self.running_tasks = set()
        self._tree_is_ready = False
        
        # Threading Safety
        self.state_lock = threading.RLock()
        self.stop_event = threading.Event()

        # References
        self.widgets = {}
        self.current_progress_popup = None

        # --- GENERATE ICONS PROGRAMMATICALLY (Robust/No Base64) ---
        self.icon_imgs = {}

        # 1. Unchecked Icon (Gray Border, Transparent Center)
        img_u = tk.PhotoImage(width=14, height=14)
        img_u.put(("#808080",), to=(0, 0, 14, 1))    # Top border
        img_u.put(("#808080",), to=(0, 13, 14, 14))  # Bottom border
        img_u.put(("#808080",), to=(0, 0, 1, 14))    # Left border
        img_u.put(("#808080",), to=(13, 0, 14, 14))  # Right border
        self.icon_imgs[S_UNCHECKED] = img_u

        # 2. Checked Icon (Blue Fill, White Checkmarkish shape)
        img_c = tk.PhotoImage(width=14, height=14)
        img_c.put(("#007ACC",), to=(0, 0, 14, 14))   # Blue Background
        # Simple white "check" pixels
        img_c.put(("#FFFFFF",), to=(3, 7, 6, 10))    # Short leg
        img_c.put(("#FFFFFF",), to=(6, 5, 11, 8))    # Long leg
        self.icon_imgs[S_CHECKED] = img_c
        # ----------------------------------------------------------

        self._setup_styles()
        self._setup_ui()
        self.process_gui_queue()
        
        self._activity_blinker()

        # Initial Actions
        self.root.after(100, lambda: self.run_threaded_action(self._load_conda_info_impl, task_id='load_conda'))
        self.root.after(200, self._rescan_project_tree)

        # File Icon (Simple text document shape)
        img_f = tk.PhotoImage(width=14, height=14)
        # Outline
        img_f.put(("#FFFFFF",), to=(2, 1, 12, 2))   # Top
        img_f.put(("#FFFFFF",), to=(2, 1, 3, 13))   # Left
        img_f.put(("#FFFFFF",), to=(11, 1, 12, 13)) # Right
        img_f.put(("#FFFFFF",), to=(2, 12, 12, 13)) # Bottom
        # Lines representing text
        img_f.put(("#808080",), to=(4, 4, 10, 5))
        img_f.put(("#808080",), to=(4, 7, 10, 8))
        img_f.put(("#808080",), to=(4, 10, 8, 11))
        self.icon_imgs["file"] = img_f

    # --- UI Setup ---
    def _setup_styles(self):
        style = ttk.Style()
        if "clam" in style.theme_names(): style.theme_use("clam")
        
        self.default_ui_font = "Arial"
        if "DejaVu Sans" in tkFont.families(): self.default_ui_font = "DejaVu Sans"

        tree_font = tkFont.Font(family=self.default_ui_font, size=11)
        
        self.widgets['tree_bg_normal'] = "#252526"
        self.widgets['tree_bg_disabled'] = "#3a3a3a"
        
        style.configure("Treeview", background=self.widgets['tree_bg_normal'], 
                        foreground="lightgray", fieldbackground=self.widgets['tree_bg_normal'],
                        borderwidth=0, font=tree_font, rowheight=24)
        style.map("Treeview", background=[('selected', '#007ACC')], foreground=[('selected', 'white')])
        style.configure("Treeview.Heading", background="#333333", foreground="white", relief=tk.FLAT)
        
        style.configure('TCombobox', fieldbackground='#2a2a3f', background='#4a4a5a', foreground='white')

    def _setup_ui(self):
        self.root.title("Project Mapper - Systems Thinker Edition")
        self.root.configure(bg="#1e1e2f")
        self.root.geometry("1200x850")

        # 1. Top Bar
        top_frame = tk.Frame(self.root, bg="#1e1e2f")
        top_frame.pack(fill=tk.X, padx=10, pady=8)

        tk.Label(top_frame, text="Project Root:", bg="#1e1e2f", fg="white").pack(side=tk.LEFT)
        
        self.widgets['selected_root_var'] = tk.StringVar(value=str(DEFAULT_ROOT_DIR))
        self.widgets['project_path_entry'] = tk.Entry(top_frame, textvariable=self.widgets['selected_root_var'], 
                                                      bg="#2a2a3f", fg="lightblue", width=60)
        self.widgets['project_path_entry'].pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        self.widgets['project_path_entry'].bind("<Return>", self._on_project_root_commit)

        tk.Button(top_frame, text="Choose...", command=self._on_choose_project_directory, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT)
        tk.Button(top_frame, text="‚Üë", command=self._on_click_up_dir, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT, padx=5)

        # 2. Main Split (Changed to VERTICAL for pythonw layout stability)
        paned = ttk.PanedWindow(self.root, orient=tk.VERTICAL)
        paned.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        # Top Pane: Tree
        left_frame = tk.Frame(paned, bg="#1e1e2f")
        self.widgets['folder_tree'] = ttk.Treeview(left_frame, show="tree", columns=("size"), selectmode="none")
        self.widgets['folder_tree'].column("#0", width=800)
        self.widgets['folder_tree'].column("size", width=100, anchor="e")
        self.widgets['folder_tree'].heading("size", text="Size")
        
        vsb = ttk.Scrollbar(left_frame, orient="vertical", command=self.widgets['folder_tree'].yview)
        self.widgets['folder_tree'].configure(yscrollcommand=vsb.set)
        
        self.widgets['folder_tree'].pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)
        self.widgets['folder_tree'].bind("<ButtonRelease-1>", self.on_tree_item_click)
        
        paned.add(left_frame, weight=3) # Give tree more initial weight

        # Bottom Pane: Actions & Logs
        right_frame = tk.Frame(paned, bg="#1e1e2f")
        
        # Action Buttons Grid
        btn_grid = tk.Frame(right_frame, bg="#1e1e2f")
        btn_grid.pack(fill=tk.X, pady=5)
        
        self.widgets['buttons'] = {}
        actions = [
            ("Map Project Tree", self.build_folder_tree_impl, True),
            ("Dump Source Files", self.dump_files_impl, True),
            ("Backup Project (Zip)", self.backup_project_impl, True),
            ("Audit System Info", self.audit_system_impl, False)
        ]

        for idx, (lbl, func, save) in enumerate(actions):
            r, c = divmod(idx, 4) # Spread buttons horizontally
            b = tk.Button(btn_grid, text=lbl, bg="#007ACC", fg="white", font=("Arial", 11, "bold"), pady=8)
            task_id = lbl.split()[0].lower()
            b.config(command=lambda f=func, t=task_id, s=save: self.run_threaded_action(f, task_id=t, save_config_after=s, use_popup=True))
            b.grid(row=r, column=c, sticky="ew", padx=5, pady=5)
            btn_grid.columnconfigure(c, weight=1)
            self.widgets['buttons'][task_id] = b

        # Controls & Utility Section
        util_frame = tk.Frame(right_frame, bg="#1e1e2f")
        util_frame.pack(fill=tk.X, pady=5)

        # -- Timestamp Checkbox --
        self.widgets['use_timestamps'] = tk.BooleanVar(value=False)
        ts_chk = tk.Checkbutton(util_frame, text="Append Timestamps to Filenames", variable=self.widgets['use_timestamps'],
                                bg="#1e1e2f", fg="white", selectcolor="#252526", activebackground="#1e1e2f")
        ts_chk.pack(side=tk.LEFT, padx=10)

        # -- Conda --
        tk.Label(util_frame, text="| Env:", bg="#1e1e2f", fg="gray").pack(side=tk.LEFT)
        self.widgets['conda_env_var'] = tk.StringVar()
        self.widgets['conda_env_combo'] = ttk.Combobox(util_frame, textvariable=self.widgets['conda_env_var'], state="readonly", width=15)
        self.widgets['conda_env_combo'].pack(side=tk.LEFT, padx=5)
        tk.Button(util_frame, text="Audit", bg="#4a4a5a", fg="white", font=("Arial", 8),
                  command=lambda: self.run_threaded_action(self.audit_conda_impl, task_id='audit_conda', use_popup=True)).pack(side=tk.LEFT)

        # -- Utility --
        tk.Button(util_frame, text="Open Logs", command=self.open_main_log_directory, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT, padx=5)
        tk.Button(util_frame, text="Exclusions", command=self.manage_dynamic_exclusions_popup, bg="#007a7a", fg="white").pack(side=tk.RIGHT, padx=5)
        tk.Button(util_frame, text="All", command=lambda: self.set_global_selection(S_CHECKED), bg="#4a4a5a", fg="white", width=4).pack(side=tk.RIGHT, padx=2)
        tk.Button(util_frame, text="None", command=lambda: self.set_global_selection(S_UNCHECKED), bg="#4a4a5a", fg="white", width=4).pack(side=tk.RIGHT, padx=2)
        
        # -- Quick Add Exclusion --
        tk.Button(util_frame, text="Add", command=lambda: self.add_excluded_filename(self.exc_entry), bg="#007ACC", fg="white", font=("Arial", 8)).pack(side=tk.RIGHT, padx=5)
        self.exc_entry = tk.Entry(util_frame, bg="#3a3a4a", fg="white", width=15)
        self.exc_entry.pack(side=tk.RIGHT, padx=5)
        tk.Label(util_frame, text="Excl. Pattern:", bg="#1e1e2f", fg="gray").pack(side=tk.RIGHT)

        # Log Box
        self.widgets['log_box'] = scrolledtext.ScrolledText(right_frame, bg="#151521", fg="#E0E0E0", font=("Consolas", 9), state=tk.DISABLED, height=10)
        self.widgets['log_box'].pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        paned.add(right_frame, weight=1)

        # Status Bar
        self.widgets['status_var'] = tk.StringVar(value="Ready.")
        self.widgets['status_bar'] = tk.Label(self.root, textvariable=self.widgets['status_var'], bg="#111111", fg="#90EE90", anchor="w")
        self.widgets['status_bar'].pack(fill=tk.X, side=tk.BOTTOM)

    # --- Threading & Activity Logic ---
    def _activity_blinker(self):
        if self.running_tasks:
            current_color = self.widgets['status_bar'].cget("bg")
            next_color = "#333333" if current_color == "#111111" else "#111111"
            self.widgets['status_bar'].config(bg=next_color)
            task_names = ", ".join(self.running_tasks)
            self.widgets['status_var'].set(f"[ACTIVE] Processing: {task_names}")
        else:
            self.widgets['status_bar'].config(bg="#111111")
            
        self.root.after(500, self._activity_blinker)

    def cancel_current_operations(self):
        self.stop_event.set()
        self.log_message("Stop signal sent to background threads.", "WARNING")

    def run_threaded_action(self, target_function_impl, task_id: str, widgets_to_disable=None, save_config_after=False, use_popup=False):
        if task_id in self.running_tasks:
            self.log_message(f"Task '{task_id}' is already running.", "WARNING")
            return

        if use_popup:
            self.current_progress_popup = ProgressPopup(self.root, title=f"Working: {task_id}", on_cancel=self.cancel_current_operations)

        def thread_target_wrapper():
            self.running_tasks.add(task_id)
            self.stop_event.clear()
            
            try:
                target_function_impl()
                if save_config_after:
                    path = self._get_current_project_path()
                    if path: self.save_project_config(path)
            except Exception as e:
                err_msg = f"CRASH in {task_id}: {e}\n{traceback.format_exc()}"
                self.schedule_log_message(err_msg, "CRITICAL")
            finally:
                if task_id in self.running_tasks:
                    self.running_tasks.remove(task_id)
                if use_popup and self.current_progress_popup:
                    self.gui_queue.put(self.current_progress_popup.close)
                    self.current_progress_popup = None
                self.schedule_log_message(f"Task '{task_id}' finished.", "INFO")

        threading.Thread(target=thread_target_wrapper, daemon=True).start()

    def schedule_log_message(self, msg: str, level: str = "INFO"):
        self.gui_queue.put(lambda: self.log_message(msg, level))
        def _update_popup_safely():
            if self.current_progress_popup:
                self.current_progress_popup.update_text(f"[{level}] {msg}")
        self.gui_queue.put(_update_popup_safely)

    def log_message(self, msg: str, level: str = "INFO"):
        ts = datetime.now().strftime("[%H:%M:%S]")
        full_msg = f"{ts} [{level}] {msg}\n"
        lb = self.widgets.get('log_box')
        if lb:
            lb.config(state=tk.NORMAL)
            lb.insert(tk.END, full_msg)
            lb.config(state=tk.DISABLED)
            lb.see(tk.END)
        self.widgets['status_var'].set(f"{ts} {msg}")

    def process_gui_queue(self):
        while not self.gui_queue.empty():
            try:
                cb = self.gui_queue.get_nowait()
                try: cb()
                except Exception: pass
            except queue.Empty: pass
        self.root.after(100, self.process_gui_queue)

    # --- Project Management Logic ---
    def _on_choose_project_directory(self):
        d = filedialog.askdirectory()
        if d:
            self.widgets['selected_root_var'].set(d)
            self._rescan_project_tree()

    def _on_project_root_commit(self, event=None):
        self._rescan_project_tree()

    def _on_click_up_dir(self):
        p = self._get_current_project_path()
        if p:
            self.widgets['selected_root_var'].set(str(p.parent))
            self._rescan_project_tree()

    def _get_current_project_path(self) -> Path | None:
        p_str = self.widgets['selected_root_var'].get()
        if p_str:
            p = Path(p_str)
            if p.is_dir(): return p
        return None

    def _rescan_project_tree(self):
        path = self._get_current_project_path()
        tree = self.widgets['folder_tree']
        for i in tree.get_children(): tree.delete(i)
        
        if not path:
            tree.insert("", "end", text="Invalid Root Path")
            return
            
        tree.insert("", "end", text="Scanning...")
        self.run_threaded_action(lambda: self._initial_tree_load_impl(path), task_id='load_tree')

    def _initial_tree_load_impl(self, root_path: Path):
        with self.state_lock:
            self.folder_item_states.clear()
        
        self.load_project_config(root_path)
        tree_data = []

        def _recurse(current: Path, parent_iid: str):
            if self.stop_event.is_set(): return
            try:
                # LIST ALL ITEMS (Files + Folders)
                # Sort: Folders first, then files (case insensitive)
                items = sorted(list(current.iterdir()), key=lambda x: (not x.is_dir(), x.name.lower()))
                
                for p in items:
                    # 1. SAFETY: Skip Excluded Folders/Files immediately
                    if p.is_dir() and self.should_exclude_dir(p, root_path):
                        continue

                    if p.is_file():
                        relp = self._rel_posix(p, root_path)
                        if self.should_exclude_file(p.name, rel_posix=relp):
                            continue

                    path_str = str(p.resolve())
                    
                    # 2. State Inheritance
                    # If we don't have a specific state saved, inherit from parent
                    if path_str not in self.folder_item_states:
                        parent_state = self.folder_item_states.get(parent_iid, S_CHECKED)
                        with self.state_lock:
                            self.folder_item_states[path_str] = parent_state
                    
                    # 3. Add to Tree Data
                    # We add a visual prefix for files since we are using the image slot for the checkbox
                    display_text = f" {p.name}"
                    
                    tree_data.append({
                        'parent': parent_iid, 
                        'iid': path_str, 
                        'text': display_text
                    })
                    
                    # 4. Recurse only if Directory
                    if p.is_dir():
                        _recurse(p, path_str)
                        
            except PermissionError: pass

        root_str = str(root_path.resolve())
        with self.state_lock: self.folder_item_states[root_str] = S_CHECKED
        tree_data.append({'parent': '', 'iid': root_str, 'text': f" {root_path.name}", 'open': True})
        
        _recurse(root_path, root_str)
        self.gui_queue.put(lambda: self._populate_tree(tree_data))
    def _populate_tree(self, data):
        tree = self.widgets['folder_tree']
        for i in tree.get_children(): tree.delete(i)
        for d in data:
            tree.insert(d['parent'], "end", iid=d['iid'], text=d['text'], open=d.get('open', False))
            tree.set(d['iid'], "size", "...")
        
        self.refresh_tree_visuals()
        root_path = self._get_current_project_path()
        if root_path:
             threading.Thread(target=self._calc_sizes_async, args=(str(root_path),), daemon=True).start()

    def _calc_sizes_async(self, root_iid):
        tree = self.widgets['folder_tree']
        q = [root_iid]
        while q:
            if self.stop_event.is_set(): break
            iid = q.pop(0)
            try:
                if not tree.exists(iid): continue
                sz = get_folder_size_bytes(Path(iid))
                fmt = format_display_size(sz)
                self.gui_queue.put(lambda i=iid, s=fmt: (tree.set(i, "size", s), self.refresh_tree_visuals(i)))
                q.extend(tree.get_children(iid))
            except: pass

    def refresh_tree_visuals(self, start_node=None):
        tree = self.widgets['folder_tree']
        def _refresh(iid):
            if not tree.exists(iid): return
            with self.state_lock:
                st = self.folder_item_states.get(iid, S_UNCHECKED)
            
            # Use Checkbox Icon
            icon = self.icon_imgs.get(st, self.icon_imgs[S_UNCHECKED])
            
            # Add File/Folder distinction to text
            p = Path(iid)
            prefix = "üìÑ " if p.is_file() else "" 
            
            tree.item(iid, text=f" {prefix}{p.name}", image=icon)
            
            # Recursion only needed for folders (files have no children)
            if tree.get_children(iid):
                for child in tree.get_children(iid): _refresh(child)
        
        if start_node: _refresh(start_node)
        else:
            root = self._get_current_project_path()
            if root: _refresh(str(root.resolve()))

    def on_tree_item_click(self, event):
        tree = event.widget
        
        # Identify specific element. 
        # Note: element name varies by theme (e.g., "image", "Treeitem.image", etc.)
        element = tree.identify("element", event.x, event.y)
        iid = tree.identify_row(event.y)
        
        if not iid: return

        # ROBUST FIX: Check if "image" is part of the element name
        if "image" in element:
            with self.state_lock:
                curr = self.folder_item_states.get(iid, S_UNCHECKED)
                new = S_CHECKED if curr != S_CHECKED else S_UNCHECKED
                self.folder_item_states[iid] = new
            self.refresh_tree_visuals(iid)

    def set_global_selection(self, state):
        with self.state_lock:
            for k in self.folder_item_states:
                self.folder_item_states[k] = state
        self.refresh_tree_visuals()

    def is_selected(self, path: Path, project_root: Path) -> bool:
        try: p = path.resolve()
        except: return False
        root = project_root.resolve()
        if p != root and not str(p).startswith(str(root)): return False
        curr = p
        while True:
            st = self.folder_item_states.get(str(curr))
            if st == S_UNCHECKED: return False
            if curr == root: return st != S_UNCHECKED
            if curr.parent == curr: break
            curr = curr.parent
        return True

    def _load_gitignore_patterns(self, root: Path):
        """Best-effort .gitignore parsing.

        Supported (simple):
          - dir ignores via trailing '/'
          - bare patterns like '*.log'
          - path-ish patterns containing '/'

        Not supported (yet):
          - negation rules starting with '!'
          - advanced gitignore semantics (root anchoring, '**' edge cases, etc.)
        """
        gi = root / ".gitignore"
        with self.state_lock:
            self.gitignore_dirnames = set()
            self.gitignore_file_patterns = set()
            self.gitignore_path_patterns = set()

        if not gi.exists():
            return

        try:
            lines = gi.read_text(encoding="utf-8", errors="ignore").splitlines()
        except Exception:
            return

        dirnames = set()
        file_pats = set()
        path_pats = set()

        for raw in lines:
            s = raw.strip()
            if not s or s.startswith("#"):
                continue
            # Skip negation rules for now
            if s.startswith("!"):
                continue

            # Normalize Windows separators to POSIX-ish for matching
            s = s.replace("\\", "/")

            if s.endswith("/"):
                # directory name or dir-pattern
                d = s[:-1].strip("/")
                if d:
                    dirnames.add(d)
                continue

            if "/" in s:
                path_pats.add(s.strip("/"))
            else:
                file_pats.add(s)

        with self.state_lock:
            self.gitignore_dirnames = dirnames
            self.gitignore_file_patterns = file_pats
            self.gitignore_path_patterns = path_pats

    def _rel_posix(self, p: Path, root: Path) -> str:
        """Return a posix-style relative path; fall back to name if relative fails."""
        try:
            return p.relative_to(root).as_posix()
        except Exception:
            return p.name

    def should_exclude_dir(self, dir_path: Path, project_root: Path) -> bool:
        """Directory exclusion check: hard-coded exclusions + .gitignore (best-effort)."""
        name = dir_path.name
        if name in EXCLUDED_FOLDERS:
            return True

        rel = self._rel_posix(dir_path, project_root)

        with self.state_lock:
            if name in self.gitignore_dirnames:
                return True
            # Match path patterns against directory relpath
            for pat in self.gitignore_path_patterns:
                if fnmatch.fnmatch(rel, pat) or fnmatch.fnmatch(rel + "/", pat) or fnmatch.fnmatch(rel + "/", pat + "/"):
                    return True

        return False

    def should_exclude_file(self, filename: str, rel_posix: str | None = None) -> bool:
        """File exclusion check: predefined + dynamic + .gitignore (best-effort)."""
        with self.state_lock:
            pats = PREDEFINED_EXCLUDED_FILENAMES.union(self.dynamic_global_excluded_filenames)
            gi_files = set(self.gitignore_file_patterns)
            gi_paths = set(self.gitignore_path_patterns)

        # filename-based patterns
        if any(fnmatch.fnmatch(filename, p) for p in pats):
            return True
        if any(fnmatch.fnmatch(filename, p) for p in gi_files):
            return True

        # relative-path patterns (if available)
        if rel_posix:
            rel_posix = rel_posix.replace("\\", "/")
            for pat in gi_paths:
                if fnmatch.fnmatch(rel_posix, pat):
                    return True

        return False

    # --- Core Actions ---
    def get_log_dir(self, root: Path) -> Path | None:
        if not root: return None
        # CHANGED: All logs go directly to _logs, no subdirectories
        d = root / LOG_ROOT_NAME
        try: d.mkdir(parents=True, exist_ok=True)
        except: return None
        return d

    def _generate_filename(self, root_name: str, base_suffix: str, extension: str) -> str:
        # CHANGED: Naming convention logic
        # Default: FolderName_suffix.ext
        # If timestamp enabled: FolderName_suffix_timestamp.ext
        name = f"{root_name}_{base_suffix}"
        if self.widgets['use_timestamps'].get():
            ts = datetime.now().strftime('%Y%m%d_%H%M%S')
            name += f"_{ts}"
        name += extension
        return name

    def build_folder_tree_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "project_folder_tree", ".txt")
        out_file = out_dir / fname
        
        lines = [f"Project Tree: {root}\nGenerated: {datetime.now()}\n"]
        
        def _write_recurse(curr, prefix):
            if self.stop_event.is_set(): 
                lines.append(f"{prefix}!!! CANCELLED !!!")
                return

            try: items = sorted(list(curr.iterdir()), key=lambda x: (x.is_file(), x.name.lower()))
            except: return
            
            for i, item in enumerate(items):
                is_last = (i == len(items) - 1)
                conn = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
                
                if item.is_dir():
                    # Respect hard exclusions and .gitignore for directories
                    if self.should_exclude_dir(item, root):
                        continue

                    if self.is_selected(item, root):
                        lines.append(f"{prefix}{conn}üìÅ {item.name}/")
                        _write_recurse(item, prefix + ("    " if is_last else "‚îÇ   "))
                else:
                    if not self.should_exclude_file(item.name) and self.is_selected(item.parent, root):
                         lines.append(f"{prefix}{conn}üìÑ {item.name}")
        
        _write_recurse(root, "")
        
        with open(out_file, "w", encoding="utf-8") as f: f.write("\n".join(lines))
        self.schedule_log_message(f"Tree saved: {fname}")

    def dump_files_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "filedump", ".txt")
        out_file = out_dir / fname
        
        count = 0
        with open(out_file, "w", encoding="utf-8") as f_out:
            f_out.write(f"Dump: {root}\n\n")
            
            for r, d, f in os.walk(root):
                if self.stop_event.is_set(): 
                    f_out.write("\n\n!!! DUMP CANCELLED BY USER !!!")
                    break
    
                curr = Path(r)
                # First remove excluded folders (hard + .gitignore), then apply selection logic
                kept_dirs = []
                for x in d:
                    dp = curr / x
                    if self.should_exclude_dir(dp, root):
                        continue
                    if not self.is_selected(dp, root):
                        continue
                    kept_dirs.append(x)
                d[:] = kept_dirs
                if not self.is_selected(curr, root): continue
                
                for fname_item in f:
                    if self.stop_event.is_set(): break

                    fpath = curr / fname_item
                    relp = self._rel_posix(fpath, root)
                    if self.should_exclude_file(fname_item, rel_posix=relp):
                        continue
                    if fpath.stat().st_size > 1_000_000: continue
                    if is_binary(fpath) or "".join(fpath.suffixes).lower() in FORCE_BINARY_EXTENSIONS_FOR_DUMP: continue
                    
                    rel = fpath.relative_to(root)
                    if count % 5 == 0: self.schedule_log_message(f"Dumping: {rel}", "DEBUG")
                    
                    try:
                        f_out.write(f"\n{'-'*80}\nFILE: {rel}\n{'-'*80}\n")
                        with open(fpath, "r", encoding="utf-8", errors="ignore") as f_in:
                            f_out.write(f_in.read())
                        count += 1
                    except Exception as e:
                        f_out.write(f"\n[ERROR READING FILE: {e}]\n")
        
        self.schedule_log_message(f"Dump saved: {fname} ({count} files)")

    def backup_project_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "backup", ".tar.gz")
        out_file = out_dir / fname
        
        count = 0
        with tarfile.open(out_file, "w:gz") as tar:
            for r, d, f in os.walk(root):
                if self.stop_event.is_set(): break
                curr = Path(r)
                kept_dirs = []
                for x in d:
                    dp = curr / x
                    if self.should_exclude_dir(dp, root):
                        continue
                    if not self.is_selected(dp, root):
                        continue
                    kept_dirs.append(x)
                d[:] = kept_dirs
                if not self.is_selected(curr, root): continue
                for fname_item in f:
                    fpath = curr / fname_item
                    relp = self._rel_posix(fpath, root)
                    if self.should_exclude_file(fname_item, rel_posix=relp):
                        continue
                    tar.add(fpath, arcname=fpath.relative_to(root))
                    count += 1
                    if count % 10 == 0: self.schedule_log_message(f"Archiving: {fname_item}", "DEBUG")

        if self.stop_event.is_set():
            self.schedule_log_message("Backup Cancelled.", "WARNING")
        else:
            self.schedule_log_message(f"Backup saved: {fname}")

    def audit_system_impl(self):
        root = self._get_current_project_path() or DEFAULT_ROOT_DIR
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "system_audit", ".txt")
        out_file = out_dir / fname
        
        lines = [f"System Audit: {datetime.now()}", f"Platform: {platform.platform()}"]
        lines.append(f"Python: {sys.version}")
        lines.append("\nEnvironment Variables (Keys only):")
        for k in os.environ.keys(): lines.append(f"  {k}")
        
        with open(out_file, "w") as f: f.write("\n".join(lines))
        self.schedule_log_message(f"System audit saved: {fname}")

    def audit_conda_impl(self):
        env_name = self.widgets['conda_env_var'].get()
        if not env_name: return
        root = self._get_current_project_path() or DEFAULT_ROOT_DIR
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(f"conda_{env_name}", "audit", ".txt")
        out_file = out_dir / fname
        
        self.schedule_log_message(f"Auditing Conda Env: {env_name}...")
        try:
            res = subprocess.run(["conda", "list", "-n", env_name], capture_output=True, text=True, shell=True)
            with open(out_file, "w") as f: f.write(res.stdout)
            self.schedule_log_message(f"Conda audit saved: {fname}")
        except Exception as e:
            self.schedule_log_message(f"Conda audit failed: {e}", "ERROR")

    def _load_conda_info_impl(self):
        try:
            res = subprocess.run(["conda", "env", "list", "--json"], capture_output=True, text=True, shell=True)
            data = json.loads(res.stdout)
            envs = [Path(p).name for p in data.get('envs', [])]
            self.gui_queue.put(lambda: self.widgets['conda_env_combo'].config(values=envs))
            if envs: self.gui_queue.put(lambda: self.widgets['conda_env_combo'].current(0))
        except: pass

    # --- Persistence ---
    def save_project_config(self, root: Path):
        cfg = self.get_log_dir(root) / PROJECT_CONFIG_FILENAME
        rel_states = {}
        with self.state_lock:
            for k, v in self.folder_item_states.items():
                try: rel_states[str(Path(k).relative_to(root))] = v
                except: pass
            data = {
                "folder_states": rel_states,
                "dynamic_exclusions": list(self.dynamic_global_excluded_filenames)
            }
        with open(cfg, "w") as f: json.dump(data, f, indent=2)

    def load_project_config(self, root: Path):
        # Always (re)load .gitignore for the active root (best-effort)
        self._load_gitignore_patterns(root)

        cfg = self.get_log_dir(root) / PROJECT_CONFIG_FILENAME
        if not cfg.exists(): return
        try:
            with open(cfg, "r") as f: data = json.load(f)
            for k, v in data.get("folder_states", {}).items():
                self.folder_item_states[str((root / k).resolve())] = v
            self.dynamic_global_excluded_filenames.update(data.get("dynamic_exclusions", []))
        except: pass

    # --- Dynamic Exclusions ---
    def add_excluded_filename(self, entry):
        val = entry.get().strip()
        if val:
            self.dynamic_global_excluded_filenames.add(val)
            entry.delete(0, tk.END)
            self.schedule_log_message(f"Added exclusion: {val}")

    def manage_dynamic_exclusions_popup(self):
        top = tk.Toplevel(self.root)
        top.title("Exclusions")
        lb = tk.Listbox(top)
        lb.pack(fill=tk.BOTH, expand=True)
        for x in self.dynamic_global_excluded_filenames: lb.insert(tk.END, x)
        def _rem():
            sel = lb.curselection()
            if not sel: return
            val = lb.get(sel[0])
            self.dynamic_global_excluded_filenames.remove(val)
            top.destroy()
            self.manage_dynamic_exclusions_popup()
        tk.Button(top, text="Remove Selected", command=_rem).pack()

    def open_main_log_directory(self):
        p = self._get_current_project_path()
        if not p: return
        d = self.get_log_dir(p)
        if platform.system() == "Windows": os.startfile(d)
        elif platform.system() == "Darwin": subprocess.run(["open", d])
        else: subprocess.run(["xdg-open", d])


# ==============================================================================
# 4. ENTRY POINTS
# ==============================================================================

def run_gui():
    root = tk.Tk()
    app = ProjectMapperApp(root)
    root.mainloop()

def run_cli():
    parser = argparse.ArgumentParser(description="ProjectMapper CLI")
    parser.add_argument("path", nargs="?", default=".", help="Root path to map")
    args = parser.parse_args()
    
    target = Path(args.path).resolve()
    print(f"--- Project Mapper CLI ---\nMapping: {target}\n")
    
    if not target.is_dir():
        print("Error: Invalid directory.")
        sys.exit(1)
        
    for item in target.rglob("*"):
        depth = len(item.relative_to(target).parts)
        indent = "  " * depth
        print(f"{indent}{item.name}")
        
    print("\nDone. For full features (backup, dumping, config), use GUI mode.")

def main():
    if len(sys.argv) > 1 and sys.argv[1] not in ["-m", "src.app"]:
        run_cli()
    else:
        run_gui()

if __name__ == "__main__":
    main()



--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\src\app - Copy.py
--------------------------------------------------------------------------------
import sys
import argparse
import tkinter as tk
from tkinter import filedialog, scrolledtext, ttk, messagebox
import tkinter.font as tkFont
from pathlib import Path
from datetime import datetime
import subprocess
import platform
import threading
import queue
import traceback
import fnmatch
import os
import json
import tarfile

# ==============================================================================
# 0. PYTHONW SAFETY CHECK
# ==============================================================================
# Fixes issues where pythonw crashes because it has no stdout/stderr attached
if sys.stdout is None:
    sys.stdout = open(os.devnull, "w")
if sys.stderr is None:
    sys.stderr = open(os.devnull, "w")

# ==============================================================================
# 1. CORE CONFIGURATION & CONSTANTS
# ==============================================================================

APP_DIR = Path(__file__).resolve().parent
DEFAULT_ROOT_DIR = APP_DIR

# --- Exclusions ---
EXCLUDED_FOLDERS = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "target", "out",
    "bin", "obj", "Debug", "Release", "logs", "venv"
}
PREDEFINED_EXCLUDED_FILENAMES = {
    "package-lock.json", "yarn.lock", ".DS_Store", "Thumbs.db",
    "*.pyc", "*.pyo", "*.swp", "*.swo"
}

# --- Binary Extensions (for skipping in dump) ---
FORCE_BINARY_EXTENSIONS_FOR_DUMP = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".aac", ".m4a",
    ".mp4", ".mkv", ".avi", ".mov", ".webm", ".flv", ".wmv",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".odt", ".ods",
    ".exe", ".dll", ".so", ".o", ".a", ".lib", ".app", ".dmg", ".deb", ".rpm",
    ".db", ".sqlite", ".mdb", ".accdb", ".dat", ".idx", ".pickle", ".joblib",
    ".pyc", ".pyo", ".class", ".jar", ".wasm",
    ".ttf", ".otf", ".woff", ".woff2",
    ".iso", ".img", ".bin", ".bak", ".data", ".asset", ".pak"
}

# --- Log Configuration ---
LOG_ROOT_NAME = "_logs"
PROJECT_CONFIG_FILENAME = "_project_mapper_config.json"

# --- State Constants ---
S_CHECKED = "checked"
S_UNCHECKED = "unchecked"

# ==============================================================================
# 2. HELPER FUNCTIONS (Pure Logic / Stateless)
# ==============================================================================

def is_binary(file_path: Path) -> bool:
    """Check if a file is binary by reading the first chunk."""
    try:
        with open(file_path, 'rb') as f:
            return b'\0' in f.read(1024)
    except (IOError, PermissionError):
        return True
    except Exception:
        return True

def get_folder_size_bytes(folder_path: Path) -> int:
    """Recursively calculate folder size."""
    total_size = 0
    try:
        for entry in os.scandir(folder_path):
            if entry.is_file(follow_symlinks=False):
                try: total_size += entry.stat(follow_symlinks=False).st_size
                except OSError: pass
            elif entry.is_dir(follow_symlinks=False):
                try: total_size += get_folder_size_bytes(Path(entry.path))
                except OSError: pass
    except OSError: pass
    return total_size

def format_display_size(size_bytes: int) -> str:
    """Format bytes into readable string."""
    if size_bytes < 1024: return f"{size_bytes} B"
    size_kb = size_bytes / 1024
    if size_kb < 1024: return f"{size_kb:.1f} KB"
    size_mb = size_kb / 1024
    if size_mb < 1024: return f"{size_mb:.1f} MB"
    size_gb = size_mb / 1024
    return f"{size_gb:.2f} GB"

# ==============================================================================
# 3. GUI COMPONENTS & PROGRESS POPUP
# ==============================================================================

class ProgressPopup:
    """A popup window that streams activity and allows cancellation."""
    def __init__(self, parent, title="Processing", on_cancel=None):
        self.top = tk.Toplevel(parent)
        self.top.title(title)
        self.top.geometry("500x300")
        self.top.configure(bg="#252526")
        self.top.transient(parent)
        self.top.grab_set()
        
        self.top.protocol("WM_DELETE_WINDOW", self._on_close_attempt)

        self.on_cancel = on_cancel
        self.is_cancelled = False

        # UI Elements
        lbl = tk.Label(self.top, text=f"{title}...", fg="white", bg="#252526", font=("Arial", 12, "bold"))
        lbl.pack(pady=10)

        self.log_display = scrolledtext.ScrolledText(self.top, height=10, bg="#1e1e1e", fg="#00ff00", font=("Consolas", 9))
        self.log_display.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        btn_frame = tk.Frame(self.top, bg="#252526")
        btn_frame.pack(fill=tk.X, pady=10)

        self.cancel_btn = tk.Button(btn_frame, text="CANCEL OPERATION", bg="#c23621", fg="white", 
                                    font=("Arial", 10, "bold"), command=self.trigger_cancel)
        self.cancel_btn.pack()

    def update_text(self, text):
        self.log_display.insert(tk.END, text + "\n")
        self.log_display.see(tk.END)

    def trigger_cancel(self):
        self.is_cancelled = True
        self.log_display.insert(tk.END, "\n!!! CANCELLATION REQUESTED - STOPPING !!!\n")
        self.log_display.see(tk.END)
        self.cancel_btn.config(state=tk.DISABLED, text="Stopping...")
        if self.on_cancel:
            self.on_cancel()

    def _on_close_attempt(self):
        if not self.is_cancelled:
            self.trigger_cancel()
        
    def close(self):
        self.top.destroy()


class ProjectMapperApp:
    def __init__(self, root: tk.Tk):
        self.root = root
        self.gui_queue = queue.Queue()

        # Application State
        self.folder_item_states = {}
        self.dynamic_global_excluded_filenames = set()
        self.running_tasks = set()
        self._tree_is_ready = False
        
        # Threading Safety
        self.state_lock = threading.RLock()
        self.stop_event = threading.Event()

        # References
        self.widgets = {}
        self.current_progress_popup = None

        # --- GENERATE ICONS PROGRAMMATICALLY (Robust/No Base64) ---
        self.icon_imgs = {}

        # 1. Unchecked Icon (Gray Border, Transparent Center)
        img_u = tk.PhotoImage(width=14, height=14)
        img_u.put(("#808080",), to=(0, 0, 14, 1))    # Top border
        img_u.put(("#808080",), to=(0, 13, 14, 14))  # Bottom border
        img_u.put(("#808080",), to=(0, 0, 1, 14))    # Left border
        img_u.put(("#808080",), to=(13, 0, 14, 14))  # Right border
        self.icon_imgs[S_UNCHECKED] = img_u

        # 2. Checked Icon (Blue Fill, White Checkmarkish shape)
        img_c = tk.PhotoImage(width=14, height=14)
        img_c.put(("#007ACC",), to=(0, 0, 14, 14))   # Blue Background
        # Simple white "check" pixels
        img_c.put(("#FFFFFF",), to=(3, 7, 6, 10))    # Short leg
        img_c.put(("#FFFFFF",), to=(6, 5, 11, 8))    # Long leg
        self.icon_imgs[S_CHECKED] = img_c
        # ----------------------------------------------------------

        self._setup_styles()
        self._setup_ui()
        self.process_gui_queue()
        
        self._activity_blinker()

        # Initial Actions
        self.root.after(100, lambda: self.run_threaded_action(self._load_conda_info_impl, task_id='load_conda'))
        self.root.after(200, self._rescan_project_tree)

        # File Icon (Simple text document shape)
        img_f = tk.PhotoImage(width=14, height=14)
        # Outline
        img_f.put(("#FFFFFF",), to=(2, 1, 12, 2))   # Top
        img_f.put(("#FFFFFF",), to=(2, 1, 3, 13))   # Left
        img_f.put(("#FFFFFF",), to=(11, 1, 12, 13)) # Right
        img_f.put(("#FFFFFF",), to=(2, 12, 12, 13)) # Bottom
        # Lines representing text
        img_f.put(("#808080",), to=(4, 4, 10, 5))
        img_f.put(("#808080",), to=(4, 7, 10, 8))
        img_f.put(("#808080",), to=(4, 10, 8, 11))
        self.icon_imgs["file"] = img_f

    # --- UI Setup ---
    def _setup_styles(self):
        style = ttk.Style()
        if "clam" in style.theme_names(): style.theme_use("clam")
        
        self.default_ui_font = "Arial"
        if "DejaVu Sans" in tkFont.families(): self.default_ui_font = "DejaVu Sans"

        tree_font = tkFont.Font(family=self.default_ui_font, size=11)
        
        self.widgets['tree_bg_normal'] = "#252526"
        self.widgets['tree_bg_disabled'] = "#3a3a3a"
        
        style.configure("Treeview", background=self.widgets['tree_bg_normal'], 
                        foreground="lightgray", fieldbackground=self.widgets['tree_bg_normal'],
                        borderwidth=0, font=tree_font, rowheight=24)
        style.map("Treeview", background=[('selected', '#007ACC')], foreground=[('selected', 'white')])
        style.configure("Treeview.Heading", background="#333333", foreground="white", relief=tk.FLAT)
        
        style.configure('TCombobox', fieldbackground='#2a2a3f', background='#4a4a5a', foreground='white')

    def _setup_ui(self):
        self.root.title("Project Mapper - Systems Thinker Edition")
        self.root.configure(bg="#1e1e2f")
        self.root.geometry("1200x850")

        # 1. Top Bar
        top_frame = tk.Frame(self.root, bg="#1e1e2f")
        top_frame.pack(fill=tk.X, padx=10, pady=8)

        tk.Label(top_frame, text="Project Root:", bg="#1e1e2f", fg="white").pack(side=tk.LEFT)
        
        self.widgets['selected_root_var'] = tk.StringVar(value=str(DEFAULT_ROOT_DIR))
        self.widgets['project_path_entry'] = tk.Entry(top_frame, textvariable=self.widgets['selected_root_var'], 
                                                      bg="#2a2a3f", fg="lightblue", width=60)
        self.widgets['project_path_entry'].pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        self.widgets['project_path_entry'].bind("<Return>", self._on_project_root_commit)

        tk.Button(top_frame, text="Choose...", command=self._on_choose_project_directory, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT)
        tk.Button(top_frame, text="‚Üë", command=self._on_click_up_dir, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT, padx=5)

        # 2. Main Split (Changed to VERTICAL for pythonw layout stability)
        paned = ttk.PanedWindow(self.root, orient=tk.VERTICAL)
        paned.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        # Top Pane: Tree
        left_frame = tk.Frame(paned, bg="#1e1e2f")
        self.widgets['folder_tree'] = ttk.Treeview(left_frame, show="tree", columns=("size"), selectmode="none")
        self.widgets['folder_tree'].column("#0", width=800)
        self.widgets['folder_tree'].column("size", width=100, anchor="e")
        self.widgets['folder_tree'].heading("size", text="Size")
        
        vsb = ttk.Scrollbar(left_frame, orient="vertical", command=self.widgets['folder_tree'].yview)
        self.widgets['folder_tree'].configure(yscrollcommand=vsb.set)
        
        self.widgets['folder_tree'].pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)
        self.widgets['folder_tree'].bind("<ButtonRelease-1>", self.on_tree_item_click)
        
        paned.add(left_frame, weight=3) # Give tree more initial weight

        # Bottom Pane: Actions & Logs
        right_frame = tk.Frame(paned, bg="#1e1e2f")
        
        # Action Buttons Grid
        btn_grid = tk.Frame(right_frame, bg="#1e1e2f")
        btn_grid.pack(fill=tk.X, pady=5)
        
        self.widgets['buttons'] = {}
        actions = [
            ("Map Project Tree", self.build_folder_tree_impl, True),
            ("Dump Source Files", self.dump_files_impl, True),
            ("Backup Project (Zip)", self.backup_project_impl, True),
            ("Audit System Info", self.audit_system_impl, False)
        ]

        for idx, (lbl, func, save) in enumerate(actions):
            r, c = divmod(idx, 4) # Spread buttons horizontally
            b = tk.Button(btn_grid, text=lbl, bg="#007ACC", fg="white", font=("Arial", 11, "bold"), pady=8)
            task_id = lbl.split()[0].lower()
            b.config(command=lambda f=func, t=task_id, s=save: self.run_threaded_action(f, task_id=t, save_config_after=s, use_popup=True))
            b.grid(row=r, column=c, sticky="ew", padx=5, pady=5)
            btn_grid.columnconfigure(c, weight=1)
            self.widgets['buttons'][task_id] = b

        # Controls & Utility Section
        util_frame = tk.Frame(right_frame, bg="#1e1e2f")
        util_frame.pack(fill=tk.X, pady=5)

        # -- Timestamp Checkbox --
        self.widgets['use_timestamps'] = tk.BooleanVar(value=False)
        ts_chk = tk.Checkbutton(util_frame, text="Append Timestamps to Filenames", variable=self.widgets['use_timestamps'],
                                bg="#1e1e2f", fg="white", selectcolor="#252526", activebackground="#1e1e2f")
        ts_chk.pack(side=tk.LEFT, padx=10)

        # -- Conda --
        tk.Label(util_frame, text="| Env:", bg="#1e1e2f", fg="gray").pack(side=tk.LEFT)
        self.widgets['conda_env_var'] = tk.StringVar()
        self.widgets['conda_env_combo'] = ttk.Combobox(util_frame, textvariable=self.widgets['conda_env_var'], state="readonly", width=15)
        self.widgets['conda_env_combo'].pack(side=tk.LEFT, padx=5)
        tk.Button(util_frame, text="Audit", bg="#4a4a5a", fg="white", font=("Arial", 8),
                  command=lambda: self.run_threaded_action(self.audit_conda_impl, task_id='audit_conda', use_popup=True)).pack(side=tk.LEFT)

        # -- Utility --
        tk.Button(util_frame, text="Open Logs", command=self.open_main_log_directory, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT, padx=5)
        tk.Button(util_frame, text="Exclusions", command=self.manage_dynamic_exclusions_popup, bg="#007a7a", fg="white").pack(side=tk.RIGHT, padx=5)
        tk.Button(util_frame, text="All", command=lambda: self.set_global_selection(S_CHECKED), bg="#4a4a5a", fg="white", width=4).pack(side=tk.RIGHT, padx=2)
        tk.Button(util_frame, text="None", command=lambda: self.set_global_selection(S_UNCHECKED), bg="#4a4a5a", fg="white", width=4).pack(side=tk.RIGHT, padx=2)
        
        # -- Quick Add Exclusion --
        tk.Button(util_frame, text="Add", command=lambda: self.add_excluded_filename(self.exc_entry), bg="#007ACC", fg="white", font=("Arial", 8)).pack(side=tk.RIGHT, padx=5)
        self.exc_entry = tk.Entry(util_frame, bg="#3a3a4a", fg="white", width=15)
        self.exc_entry.pack(side=tk.RIGHT, padx=5)
        tk.Label(util_frame, text="Excl. Pattern:", bg="#1e1e2f", fg="gray").pack(side=tk.RIGHT)

        # Log Box
        self.widgets['log_box'] = scrolledtext.ScrolledText(right_frame, bg="#151521", fg="#E0E0E0", font=("Consolas", 9), state=tk.DISABLED, height=10)
        self.widgets['log_box'].pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        paned.add(right_frame, weight=1)

        # Status Bar
        self.widgets['status_var'] = tk.StringVar(value="Ready.")
        self.widgets['status_bar'] = tk.Label(self.root, textvariable=self.widgets['status_var'], bg="#111111", fg="#90EE90", anchor="w")
        self.widgets['status_bar'].pack(fill=tk.X, side=tk.BOTTOM)

    # --- Threading & Activity Logic ---
    def _activity_blinker(self):
        if self.running_tasks:
            current_color = self.widgets['status_bar'].cget("bg")
            next_color = "#333333" if current_color == "#111111" else "#111111"
            self.widgets['status_bar'].config(bg=next_color)
            task_names = ", ".join(self.running_tasks)
            self.widgets['status_var'].set(f"[ACTIVE] Processing: {task_names}")
        else:
            self.widgets['status_bar'].config(bg="#111111")
            
        self.root.after(500, self._activity_blinker)

    def cancel_current_operations(self):
        self.stop_event.set()
        self.log_message("Stop signal sent to background threads.", "WARNING")

    def run_threaded_action(self, target_function_impl, task_id: str, widgets_to_disable=None, save_config_after=False, use_popup=False):
        if task_id in self.running_tasks:
            self.log_message(f"Task '{task_id}' is already running.", "WARNING")
            return

        if use_popup:
            self.current_progress_popup = ProgressPopup(self.root, title=f"Working: {task_id}", on_cancel=self.cancel_current_operations)

        def thread_target_wrapper():
            self.running_tasks.add(task_id)
            self.stop_event.clear()
            
            try:
                target_function_impl()
                if save_config_after:
                    path = self._get_current_project_path()
                    if path: self.save_project_config(path)
            except Exception as e:
                err_msg = f"CRASH in {task_id}: {e}\n{traceback.format_exc()}"
                self.schedule_log_message(err_msg, "CRITICAL")
            finally:
                if task_id in self.running_tasks:
                    self.running_tasks.remove(task_id)
                if use_popup and self.current_progress_popup:
                    self.gui_queue.put(self.current_progress_popup.close)
                    self.current_progress_popup = None
                self.schedule_log_message(f"Task '{task_id}' finished.", "INFO")

        threading.Thread(target=thread_target_wrapper, daemon=True).start()

    def schedule_log_message(self, msg: str, level: str = "INFO"):
        self.gui_queue.put(lambda: self.log_message(msg, level))
        def _update_popup_safely():
            if self.current_progress_popup:
                self.current_progress_popup.update_text(f"[{level}] {msg}")
        self.gui_queue.put(_update_popup_safely)

    def log_message(self, msg: str, level: str = "INFO"):
        ts = datetime.now().strftime("[%H:%M:%S]")
        full_msg = f"{ts} [{level}] {msg}\n"
        lb = self.widgets.get('log_box')
        if lb:
            lb.config(state=tk.NORMAL)
            lb.insert(tk.END, full_msg)
            lb.config(state=tk.DISABLED)
            lb.see(tk.END)
        self.widgets['status_var'].set(f"{ts} {msg}")

    def process_gui_queue(self):
        while not self.gui_queue.empty():
            try:
                cb = self.gui_queue.get_nowait()
                try: cb()
                except Exception: pass
            except queue.Empty: pass
        self.root.after(100, self.process_gui_queue)

    # --- Project Management Logic ---
    def _on_choose_project_directory(self):
        d = filedialog.askdirectory()
        if d:
            self.widgets['selected_root_var'].set(d)
            self._rescan_project_tree()

    def _on_project_root_commit(self, event=None):
        self._rescan_project_tree()

    def _on_click_up_dir(self):
        p = self._get_current_project_path()
        if p:
            self.widgets['selected_root_var'].set(str(p.parent))
            self._rescan_project_tree()

    def _get_current_project_path(self) -> Path | None:
        p_str = self.widgets['selected_root_var'].get()
        if p_str:
            p = Path(p_str)
            if p.is_dir(): return p
        return None

    def _rescan_project_tree(self):
        path = self._get_current_project_path()
        tree = self.widgets['folder_tree']
        for i in tree.get_children(): tree.delete(i)
        
        if not path:
            tree.insert("", "end", text="Invalid Root Path")
            return
            
        tree.insert("", "end", text="Scanning...")
        self.run_threaded_action(lambda: self._initial_tree_load_impl(path), task_id='load_tree')

    def _initial_tree_load_impl(self, root_path: Path):
        with self.state_lock:
            self.folder_item_states.clear()
        
        self.load_project_config(root_path)
        tree_data = []

        def _recurse(current: Path, parent_iid: str):
            if self.stop_event.is_set(): return
            try:
                # LIST ALL ITEMS (Files + Folders)
                # Sort: Folders first, then files (case insensitive)
                items = sorted(list(current.iterdir()), key=lambda x: (not x.is_dir(), x.name.lower()))
                
                for p in items:
                    # 1. SAFETY: Skip Excluded Folders/Files immediately
                    if p.name in EXCLUDED_FOLDERS: continue
                    if p.is_file() and self.should_exclude_file(p.name): continue

                    path_str = str(p.resolve())
                    
                    # 2. State Inheritance
                    # If we don't have a specific state saved, inherit from parent
                    if path_str not in self.folder_item_states:
                        parent_state = self.folder_item_states.get(parent_iid, S_CHECKED)
                        with self.state_lock:
                            self.folder_item_states[path_str] = parent_state
                    
                    # 3. Add to Tree Data
                    # We add a visual prefix for files since we are using the image slot for the checkbox
                    display_text = f" {p.name}"
                    
                    tree_data.append({
                        'parent': parent_iid, 
                        'iid': path_str, 
                        'text': display_text
                    })
                    
                    # 4. Recurse only if Directory
                    if p.is_dir():
                        _recurse(p, path_str)
                        
            except PermissionError: pass

        root_str = str(root_path.resolve())
        with self.state_lock: self.folder_item_states[root_str] = S_CHECKED
        tree_data.append({'parent': '', 'iid': root_str, 'text': f" {root_path.name}", 'open': True})
        
        _recurse(root_path, root_str)
        self.gui_queue.put(lambda: self._populate_tree(tree_data))
    def _populate_tree(self, data):
        tree = self.widgets['folder_tree']
        for i in tree.get_children(): tree.delete(i)
        for d in data:
            tree.insert(d['parent'], "end", iid=d['iid'], text=d['text'], open=d.get('open', False))
            tree.set(d['iid'], "size", "...")
        
        self.refresh_tree_visuals()
        root_path = self._get_current_project_path()
        if root_path:
             threading.Thread(target=self._calc_sizes_async, args=(str(root_path),), daemon=True).start()

    def _calc_sizes_async(self, root_iid):
        tree = self.widgets['folder_tree']
        q = [root_iid]
        while q:
            if self.stop_event.is_set(): break
            iid = q.pop(0)
            try:
                if not tree.exists(iid): continue
                sz = get_folder_size_bytes(Path(iid))
                fmt = format_display_size(sz)
                self.gui_queue.put(lambda i=iid, s=fmt: (tree.set(i, "size", s), self.refresh_tree_visuals(i)))
                q.extend(tree.get_children(iid))
            except: pass

    def refresh_tree_visuals(self, start_node=None):
        tree = self.widgets['folder_tree']
        def _refresh(iid):
            if not tree.exists(iid): return
            with self.state_lock:
                st = self.folder_item_states.get(iid, S_UNCHECKED)
            
            # Use Checkbox Icon
            icon = self.icon_imgs.get(st, self.icon_imgs[S_UNCHECKED])
            
            # Add File/Folder distinction to text
            p = Path(iid)
            prefix = "üìÑ " if p.is_file() else "" 
            
            tree.item(iid, text=f" {prefix}{p.name}", image=icon)
            
            # Recursion only needed for folders (files have no children)
            if tree.get_children(iid):
                for child in tree.get_children(iid): _refresh(child)
        
        if start_node: _refresh(start_node)
        else:
            root = self._get_current_project_path()
            if root: _refresh(str(root.resolve()))

    def on_tree_item_click(self, event):
        tree = event.widget
        
        # Identify specific element. 
        # Note: element name varies by theme (e.g., "image", "Treeitem.image", etc.)
        element = tree.identify("element", event.x, event.y)
        iid = tree.identify_row(event.y)
        
        if not iid: return

        # ROBUST FIX: Check if "image" is part of the element name
        if "image" in element:
            with self.state_lock:
                curr = self.folder_item_states.get(iid, S_UNCHECKED)
                new = S_CHECKED if curr != S_CHECKED else S_UNCHECKED
                self.folder_item_states[iid] = new
            self.refresh_tree_visuals(iid)

    def set_global_selection(self, state):
        with self.state_lock:
            for k in self.folder_item_states:
                self.folder_item_states[k] = state
        self.refresh_tree_visuals()

    def is_selected(self, path: Path, project_root: Path) -> bool:
        try: p = path.resolve()
        except: return False
        root = project_root.resolve()
        if p != root and not str(p).startswith(str(root)): return False
        curr = p
        while True:
            st = self.folder_item_states.get(str(curr))
            if st == S_UNCHECKED: return False
            if curr == root: return st != S_UNCHECKED
            if curr.parent == curr: break
            curr = curr.parent
        return True

    def should_exclude_file(self, filename: str) -> bool:
        with self.state_lock:
            pats = PREDEFINED_EXCLUDED_FILENAMES.union(self.dynamic_global_excluded_filenames)
        return any(fnmatch.fnmatch(filename, p) for p in pats)

    # --- Core Actions ---
    def get_log_dir(self, root: Path) -> Path | None:
        if not root: return None
        # CHANGED: All logs go directly to _logs, no subdirectories
        d = root / LOG_ROOT_NAME
        try: d.mkdir(parents=True, exist_ok=True)
        except: return None
        return d

    def _generate_filename(self, root_name: str, base_suffix: str, extension: str) -> str:
        # CHANGED: Naming convention logic
        # Default: FolderName_suffix.ext
        # If timestamp enabled: FolderName_suffix_timestamp.ext
        name = f"{root_name}_{base_suffix}"
        if self.widgets['use_timestamps'].get():
            ts = datetime.now().strftime('%Y%m%d_%H%M%S')
            name += f"_{ts}"
        name += extension
        return name

    def build_folder_tree_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "project_folder_tree", ".txt")
        out_file = out_dir / fname
        
        lines = [f"Project Tree: {root}\nGenerated: {datetime.now()}\n"]
        
        def _write_recurse(curr, prefix):
            if self.stop_event.is_set(): 
                lines.append(f"{prefix}!!! CANCELLED !!!")
                return

            try: items = sorted(list(curr.iterdir()), key=lambda x: (x.is_file(), x.name.lower()))
            except: return
            
            for i, item in enumerate(items):
                is_last = (i == len(items) - 1)
                conn = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
                
                if item.is_dir():
                    if self.is_selected(item, root):
                        lines.append(f"{prefix}{conn}üìÅ {item.name}/")
                        _write_recurse(item, prefix + ("    " if is_last else "‚îÇ   "))
                else:
                    if not self.should_exclude_file(item.name) and self.is_selected(item.parent, root):
                         lines.append(f"{prefix}{conn}üìÑ {item.name}")
        
        _write_recurse(root, "")
        
        with open(out_file, "w", encoding="utf-8") as f: f.write("\n".join(lines))
        self.schedule_log_message(f"Tree saved: {fname}")

    def dump_files_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "filedump", ".txt")
        out_file = out_dir / fname
        
        count = 0
        with open(out_file, "w", encoding="utf-8") as f_out:
            f_out.write(f"Dump: {root}\n\n")
            
            for r, d, f in os.walk(root):
                if self.stop_event.is_set(): 
                    f_out.write("\n\n!!! DUMP CANCELLED BY USER !!!")
                    break
    
                curr = Path(r)
                # First remove excluded folders, then apply selection logic
                d[:] = [x for x in d if x not in EXCLUDED_FOLDERS and self.is_selected(curr/x, root)]
                if not self.is_selected(curr, root): continue
                
                for fname_item in f:
                    if self.stop_event.is_set(): break
                    if self.should_exclude_file(fname_item): continue
                    
                    fpath = curr / fname_item
                    if fpath.stat().st_size > 1_000_000: continue
                    if is_binary(fpath) or "".join(fpath.suffixes).lower() in FORCE_BINARY_EXTENSIONS_FOR_DUMP: continue
                    
                    rel = fpath.relative_to(root)
                    if count % 5 == 0: self.schedule_log_message(f"Dumping: {rel}", "DEBUG")
                    
                    try:
                        f_out.write(f"\n{'-'*80}\nFILE: {rel}\n{'-'*80}\n")
                        with open(fpath, "r", encoding="utf-8", errors="ignore") as f_in:
                            f_out.write(f_in.read())
                        count += 1
                    except Exception as e:
                        f_out.write(f"\n[ERROR READING FILE: {e}]\n")
        
        self.schedule_log_message(f"Dump saved: {fname} ({count} files)")

    def backup_project_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "backup", ".tar.gz")
        out_file = out_dir / fname
        
        count = 0
        with tarfile.open(out_file, "w:gz") as tar:
            for r, d, f in os.walk(root):
                if self.stop_event.is_set(): break
                curr = Path(r)
                d[:] = [x for x in d if self.is_selected(curr/x, root)]
                if not self.is_selected(curr, root): continue
                for fname_item in f:
                    if self.should_exclude_file(fname_item): continue
                    fpath = curr / fname_item
                    tar.add(fpath, arcname=fpath.relative_to(root))
                    count += 1
                    if count % 10 == 0: self.schedule_log_message(f"Archiving: {fname_item}", "DEBUG")

        if self.stop_event.is_set():
            self.schedule_log_message("Backup Cancelled.", "WARNING")
        else:
            self.schedule_log_message(f"Backup saved: {fname}")

    def audit_system_impl(self):
        root = self._get_current_project_path() or DEFAULT_ROOT_DIR
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "system_audit", ".txt")
        out_file = out_dir / fname
        
        lines = [f"System Audit: {datetime.now()}", f"Platform: {platform.platform()}"]
        lines.append(f"Python: {sys.version}")
        lines.append("\nEnvironment Variables (Keys only):")
        for k in os.environ.keys(): lines.append(f"  {k}")
        
        with open(out_file, "w") as f: f.write("\n".join(lines))
        self.schedule_log_message(f"System audit saved: {fname}")

    def audit_conda_impl(self):
        env_name = self.widgets['conda_env_var'].get()
        if not env_name: return
        root = self._get_current_project_path() or DEFAULT_ROOT_DIR
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(f"conda_{env_name}", "audit", ".txt")
        out_file = out_dir / fname
        
        self.schedule_log_message(f"Auditing Conda Env: {env_name}...")
        try:
            res = subprocess.run(["conda", "list", "-n", env_name], capture_output=True, text=True, shell=True)
            with open(out_file, "w") as f: f.write(res.stdout)
            self.schedule_log_message(f"Conda audit saved: {fname}")
        except Exception as e:
            self.schedule_log_message(f"Conda audit failed: {e}", "ERROR")

    def _load_conda_info_impl(self):
        try:
            res = subprocess.run(["conda", "env", "list", "--json"], capture_output=True, text=True, shell=True)
            data = json.loads(res.stdout)
            envs = [Path(p).name for p in data.get('envs', [])]
            self.gui_queue.put(lambda: self.widgets['conda_env_combo'].config(values=envs))
            if envs: self.gui_queue.put(lambda: self.widgets['conda_env_combo'].current(0))
        except: pass

    # --- Persistence ---
    def save_project_config(self, root: Path):
        cfg = self.get_log_dir(root) / PROJECT_CONFIG_FILENAME
        rel_states = {}
        with self.state_lock:
            for k, v in self.folder_item_states.items():
                try: rel_states[str(Path(k).relative_to(root))] = v
                except: pass
            data = {
                "folder_states": rel_states,
                "dynamic_exclusions": list(self.dynamic_global_excluded_filenames)
            }
        with open(cfg, "w") as f: json.dump(data, f, indent=2)

    def load_project_config(self, root: Path):
        cfg = self.get_log_dir(root) / PROJECT_CONFIG_FILENAME
        if not cfg.exists(): return
        try:
            with open(cfg, "r") as f: data = json.load(f)
            for k, v in data.get("folder_states", {}).items():
                self.folder_item_states[str((root / k).resolve())] = v
            self.dynamic_global_excluded_filenames.update(data.get("dynamic_exclusions", []))
        except: pass

    # --- Dynamic Exclusions ---
    def add_excluded_filename(self, entry):
        val = entry.get().strip()
        if val:
            self.dynamic_global_excluded_filenames.add(val)
            entry.delete(0, tk.END)
            self.schedule_log_message(f"Added exclusion: {val}")

    def manage_dynamic_exclusions_popup(self):
        top = tk.Toplevel(self.root)
        top.title("Exclusions")
        lb = tk.Listbox(top)
        lb.pack(fill=tk.BOTH, expand=True)
        for x in self.dynamic_global_excluded_filenames: lb.insert(tk.END, x)
        def _rem():
            sel = lb.curselection()
            if not sel: return
            val = lb.get(sel[0])
            self.dynamic_global_excluded_filenames.remove(val)
            top.destroy()
            self.manage_dynamic_exclusions_popup()
        tk.Button(top, text="Remove Selected", command=_rem).pack()

    def open_main_log_directory(self):
        p = self._get_current_project_path()
        if not p: return
        d = self.get_log_dir(p)
        if platform.system() == "Windows": os.startfile(d)
        elif platform.system() == "Darwin": subprocess.run(["open", d])
        else: subprocess.run(["xdg-open", d])


# ==============================================================================
# 4. ENTRY POINTS
# ==============================================================================

def run_gui():
    root = tk.Tk()
    app = ProjectMapperApp(root)
    root.mainloop()

def run_cli():
    parser = argparse.ArgumentParser(description="ProjectMapper CLI")
    parser.add_argument("path", nargs="?", default=".", help="Root path to map")
    args = parser.parse_args()
    
    target = Path(args.path).resolve()
    print(f"--- Project Mapper CLI ---\nMapping: {target}\n")
    
    if not target.is_dir():
        print("Error: Invalid directory.")
        sys.exit(1)
        
    for item in target.rglob("*"):
        depth = len(item.relative_to(target).parts)
        indent = "  " * depth
        print(f"{indent}{item.name}")
        
    print("\nDone. For full features (backup, dumping, config), use GUI mode.")

def main():
    if len(sys.argv) > 1 and sys.argv[1] not in ["-m", "src.app"]:
        run_cli()
    else:
        run_gui()

if __name__ == "__main__":
    main()


--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\src\app.py
--------------------------------------------------------------------------------
import sys
import argparse
import tkinter as tk
from tkinter import filedialog, scrolledtext, ttk, messagebox
import tkinter.font as tkFont
from pathlib import Path
from datetime import datetime
import subprocess
import platform
import threading
import queue
import traceback
import fnmatch
import os
import json
import tarfile

# ==============================================================================
# 0. PYTHONW SAFETY CHECK
# ==============================================================================
# Fixes issues where pythonw crashes because it has no stdout/stderr attached
if sys.stdout is None:
    sys.stdout = open(os.devnull, "w")
if sys.stderr is None:
    sys.stderr = open(os.devnull, "w")

# ==============================================================================
# 1. CORE CONFIGURATION & CONSTANTS
# ==============================================================================

APP_DIR = Path(__file__).resolve().parent
DEFAULT_ROOT_DIR = APP_DIR

# --- Exclusions ---
EXCLUDED_FOLDERS = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "target", "out",
    "bin", "obj", "Debug", "Release", "logs", "venv"
}
PREDEFINED_EXCLUDED_FILENAMES = {
    "package-lock.json", "yarn.lock", ".DS_Store", "Thumbs.db",
    "*.pyc", "*.pyo", "*.swp", "*.swo"
}

# --- Binary Extensions (for skipping in dump) ---
FORCE_BINARY_EXTENSIONS_FOR_DUMP = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".aac", ".m4a",
    ".mp4", ".mkv", ".avi", ".mov", ".webm", ".flv", ".wmv",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".odt", ".ods",
    ".exe", ".dll", ".so", ".o", ".a", ".lib", ".app", ".dmg", ".deb", ".rpm",
    ".db", ".sqlite", ".mdb", ".accdb", ".dat", ".idx", ".pickle", ".joblib",
    ".pyc", ".pyo", ".class", ".jar", ".wasm",
    ".ttf", ".otf", ".woff", ".woff2",
    ".iso", ".img", ".bin", ".bak", ".data", ".asset", ".pak"
}

# --- Log Configuration ---
LOG_ROOT_NAME = "_logs"
PROJECT_CONFIG_FILENAME = "_project_mapper_config.json"

# --- State Constants ---
S_CHECKED = "checked"
S_UNCHECKED = "unchecked"

# ==============================================================================
# 2. HELPER FUNCTIONS (Pure Logic / Stateless)
# ==============================================================================

def is_binary(file_path: Path) -> bool:
    """Check if a file is binary by reading the first chunk."""
    try:
        with open(file_path, 'rb') as f:
            return b'\0' in f.read(1024)
    except (IOError, PermissionError):
        return True
    except Exception:
        return True

def get_folder_size_bytes(folder_path: Path) -> int:
    """Recursively calculate folder size."""
    total_size = 0
    try:
        for entry in os.scandir(folder_path):
            if entry.is_file(follow_symlinks=False):
                try: total_size += entry.stat(follow_symlinks=False).st_size
                except OSError: pass
            elif entry.is_dir(follow_symlinks=False):
                try: total_size += get_folder_size_bytes(Path(entry.path))
                except OSError: pass
    except OSError: pass
    return total_size

def format_display_size(size_bytes: int) -> str:
    """Format bytes into readable string."""
    if size_bytes < 1024: return f"{size_bytes} B"
    size_kb = size_bytes / 1024
    if size_kb < 1024: return f"{size_kb:.1f} KB"
    size_mb = size_kb / 1024
    if size_mb < 1024: return f"{size_mb:.1f} MB"
    size_gb = size_mb / 1024
    return f"{size_gb:.2f} GB"

# ==============================================================================
# 3. GUI COMPONENTS & PROGRESS POPUP
# ==============================================================================

class ProgressPopup:
    """A popup window that streams activity and allows cancellation."""
    def __init__(self, parent, title="Processing", on_cancel=None):
        self.top = tk.Toplevel(parent)
        self.top.title(title)
        self.top.geometry("500x300")
        self.top.configure(bg="#252526")
        self.top.transient(parent)
        self.top.grab_set()
        
        self.top.protocol("WM_DELETE_WINDOW", self._on_close_attempt)

        self.on_cancel = on_cancel
        self.is_cancelled = False

        # UI Elements
        lbl = tk.Label(self.top, text=f"{title}...", fg="white", bg="#252526", font=("Arial", 12, "bold"))
        lbl.pack(pady=10)

        self.log_display = scrolledtext.ScrolledText(self.top, height=10, bg="#1e1e1e", fg="#00ff00", font=("Consolas", 9))
        self.log_display.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        btn_frame = tk.Frame(self.top, bg="#252526")
        btn_frame.pack(fill=tk.X, pady=10)

        self.cancel_btn = tk.Button(btn_frame, text="CANCEL OPERATION", bg="#c23621", fg="white", 
                                    font=("Arial", 10, "bold"), command=self.trigger_cancel)
        self.cancel_btn.pack()

    def update_text(self, text):
        self.log_display.insert(tk.END, text + "\n")
        self.log_display.see(tk.END)

    def trigger_cancel(self):
        self.is_cancelled = True
        self.log_display.insert(tk.END, "\n!!! CANCELLATION REQUESTED - STOPPING !!!\n")
        self.log_display.see(tk.END)
        self.cancel_btn.config(state=tk.DISABLED, text="Stopping...")
        if self.on_cancel:
            self.on_cancel()

    def _on_close_attempt(self):
        if not self.is_cancelled:
            self.trigger_cancel()
        
    def close(self):
        self.top.destroy()


class ProjectMapperApp:
    def __init__(self, root: tk.Tk):
        self.root = root
        self.gui_queue = queue.Queue()

        # Application State
        self.folder_item_states = {}
        self.dynamic_global_excluded_filenames = set()

        # .gitignore support (best-effort, simple patterns)
        self.gitignore_dirnames = set()
        self.gitignore_file_patterns = set()
        self.gitignore_path_patterns = set()

        self.running_tasks = set()
        self._tree_is_ready = False
        
        # Threading Safety
        self.state_lock = threading.RLock()
        self.stop_event = threading.Event()

        # References
        self.widgets = {}
        self.current_progress_popup = None

        # --- GENERATE ICONS PROGRAMMATICALLY (Robust/No Base64) ---
        self.icon_imgs = {}

        # 1. Unchecked Icon (Gray Border, Transparent Center)
        img_u = tk.PhotoImage(width=14, height=14)
        img_u.put(("#808080",), to=(0, 0, 14, 1))    # Top border
        img_u.put(("#808080",), to=(0, 13, 14, 14))  # Bottom border
        img_u.put(("#808080",), to=(0, 0, 1, 14))    # Left border
        img_u.put(("#808080",), to=(13, 0, 14, 14))  # Right border
        self.icon_imgs[S_UNCHECKED] = img_u

        # 2. Checked Icon (Blue Fill, White Checkmarkish shape)
        img_c = tk.PhotoImage(width=14, height=14)
        img_c.put(("#007ACC",), to=(0, 0, 14, 14))   # Blue Background
        # Simple white "check" pixels
        img_c.put(("#FFFFFF",), to=(3, 7, 6, 10))    # Short leg
        img_c.put(("#FFFFFF",), to=(6, 5, 11, 8))    # Long leg
        self.icon_imgs[S_CHECKED] = img_c
        # ----------------------------------------------------------

        self._setup_styles()
        self._setup_ui()
        self.process_gui_queue()
        
        self._activity_blinker()

        # Initial Actions
        self.root.after(100, lambda: self.run_threaded_action(self._load_conda_info_impl, task_id='load_conda'))
        self.root.after(200, self._rescan_project_tree)

        # File Icon (Simple text document shape)
        img_f = tk.PhotoImage(width=14, height=14)
        # Outline
        img_f.put(("#FFFFFF",), to=(2, 1, 12, 2))   # Top
        img_f.put(("#FFFFFF",), to=(2, 1, 3, 13))   # Left
        img_f.put(("#FFFFFF",), to=(11, 1, 12, 13)) # Right
        img_f.put(("#FFFFFF",), to=(2, 12, 12, 13)) # Bottom
        # Lines representing text
        img_f.put(("#808080",), to=(4, 4, 10, 5))
        img_f.put(("#808080",), to=(4, 7, 10, 8))
        img_f.put(("#808080",), to=(4, 10, 8, 11))
        self.icon_imgs["file"] = img_f

    # --- UI Setup ---
    def _setup_styles(self):
        style = ttk.Style()
        if "clam" in style.theme_names(): style.theme_use("clam")
        
        self.default_ui_font = "Arial"
        if "DejaVu Sans" in tkFont.families(): self.default_ui_font = "DejaVu Sans"

        tree_font = tkFont.Font(family=self.default_ui_font, size=11)
        
        self.widgets['tree_bg_normal'] = "#252526"
        self.widgets['tree_bg_disabled'] = "#3a3a3a"
        
        style.configure("Treeview", background=self.widgets['tree_bg_normal'], 
                        foreground="lightgray", fieldbackground=self.widgets['tree_bg_normal'],
                        borderwidth=0, font=tree_font, rowheight=24)
        style.map("Treeview", background=[('selected', '#007ACC')], foreground=[('selected', 'white')])
        style.configure("Treeview.Heading", background="#333333", foreground="white", relief=tk.FLAT)
        
        style.configure('TCombobox', fieldbackground='#2a2a3f', background='#4a4a5a', foreground='white')

    def _setup_ui(self):
        self.root.title("Project Mapper - Systems Thinker Edition")
        self.root.configure(bg="#1e1e2f")
        self.root.geometry("1200x850")

        # 1. Top Bar
        top_frame = tk.Frame(self.root, bg="#1e1e2f")
        top_frame.pack(fill=tk.X, padx=10, pady=8)

        tk.Label(top_frame, text="Project Root:", bg="#1e1e2f", fg="white").pack(side=tk.LEFT)
        
        self.widgets['selected_root_var'] = tk.StringVar(value=str(DEFAULT_ROOT_DIR))
        self.widgets['project_path_entry'] = tk.Entry(top_frame, textvariable=self.widgets['selected_root_var'], 
                                                      bg="#2a2a3f", fg="lightblue", width=60)
        self.widgets['project_path_entry'].pack(side=tk.LEFT, fill=tk.X, expand=True, padx=5)
        self.widgets['project_path_entry'].bind("<Return>", self._on_project_root_commit)

        tk.Button(top_frame, text="Choose...", command=self._on_choose_project_directory, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT)
        tk.Button(top_frame, text="‚Üë", command=self._on_click_up_dir, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT, padx=5)

        # 2. Main Split (Changed to VERTICAL for pythonw layout stability)
        paned = ttk.PanedWindow(self.root, orient=tk.VERTICAL)
        paned.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)

        # Top Pane: Tree
        left_frame = tk.Frame(paned, bg="#1e1e2f")
        self.widgets['folder_tree'] = ttk.Treeview(left_frame, show="tree", columns=("size"), selectmode="none")
        self.widgets['folder_tree'].column("#0", width=800)
        self.widgets['folder_tree'].column("size", width=100, anchor="e")
        self.widgets['folder_tree'].heading("size", text="Size")
        
        vsb = ttk.Scrollbar(left_frame, orient="vertical", command=self.widgets['folder_tree'].yview)
        self.widgets['folder_tree'].configure(yscrollcommand=vsb.set)
        
        self.widgets['folder_tree'].pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
        vsb.pack(side=tk.RIGHT, fill=tk.Y)
        self.widgets['folder_tree'].bind("<ButtonRelease-1>", self.on_tree_item_click)
        
        paned.add(left_frame, weight=3) # Give tree more initial weight

        # Bottom Pane: Actions & Logs
        right_frame = tk.Frame(paned, bg="#1e1e2f")
        
        # Action Buttons Grid
        btn_grid = tk.Frame(right_frame, bg="#1e1e2f")
        btn_grid.pack(fill=tk.X, pady=5)
        
        self.widgets['buttons'] = {}
        actions = [
            ("Map Project Tree", self.build_folder_tree_impl, True),
            ("Dump Source Files", self.dump_files_impl, True),
            ("Backup Project (Zip)", self.backup_project_impl, True),
            ("Audit System Info", self.audit_system_impl, False)
        ]

        for idx, (lbl, func, save) in enumerate(actions):
            r, c = divmod(idx, 4) # Spread buttons horizontally
            b = tk.Button(btn_grid, text=lbl, bg="#007ACC", fg="white", font=("Arial", 11, "bold"), pady=8)
            task_id = lbl.split()[0].lower()
            b.config(command=lambda f=func, t=task_id, s=save: self.run_threaded_action(f, task_id=t, save_config_after=s, use_popup=True))
            b.grid(row=r, column=c, sticky="ew", padx=5, pady=5)
            btn_grid.columnconfigure(c, weight=1)
            self.widgets['buttons'][task_id] = b

        # Controls & Utility Section
        util_frame = tk.Frame(right_frame, bg="#1e1e2f")
        util_frame.pack(fill=tk.X, pady=5)

        # -- Timestamp Checkbox --
        self.widgets['use_timestamps'] = tk.BooleanVar(value=False)
        ts_chk = tk.Checkbutton(util_frame, text="Append Timestamps to Filenames", variable=self.widgets['use_timestamps'],
                                bg="#1e1e2f", fg="white", selectcolor="#252526", activebackground="#1e1e2f")
        ts_chk.pack(side=tk.LEFT, padx=10)

        # -- Exclusion / .gitignore Toggle (default ON) --
        self.widgets['respect_exclusions'] = tk.BooleanVar(value=True)
        excl_chk = tk.Checkbutton(
            util_frame,
            text="Respect .gitignore + exclusions",
            variable=self.widgets['respect_exclusions'],
            bg="#1e1e2f",
            fg="white",
            selectcolor="#252526",
            activebackground="#1e1e2f"
        )
        excl_chk.pack(side=tk.LEFT, padx=10)

        # -- Conda --
        tk.Label(util_frame, text="| Env:", bg="#1e1e2f", fg="gray").pack(side=tk.LEFT)
        self.widgets['conda_env_var'] = tk.StringVar()
        self.widgets['conda_env_combo'] = ttk.Combobox(util_frame, textvariable=self.widgets['conda_env_var'], state="readonly", width=15)
        self.widgets['conda_env_combo'].pack(side=tk.LEFT, padx=5)
        tk.Button(util_frame, text="Audit", bg="#4a4a5a", fg="white", font=("Arial", 8),
                  command=lambda: self.run_threaded_action(self.audit_conda_impl, task_id='audit_conda', use_popup=True)).pack(side=tk.LEFT)

        # -- Utility --
        tk.Button(util_frame, text="Open Logs", command=self.open_main_log_directory, bg="#4a4a5a", fg="white").pack(side=tk.RIGHT, padx=5)
        tk.Button(util_frame, text="Exclusions", command=self.manage_dynamic_exclusions_popup, bg="#007a7a", fg="white").pack(side=tk.RIGHT, padx=5)
        tk.Button(util_frame, text="All", command=lambda: self.set_global_selection(S_CHECKED), bg="#4a4a5a", fg="white", width=4).pack(side=tk.RIGHT, padx=2)
        tk.Button(util_frame, text="None", command=lambda: self.set_global_selection(S_UNCHECKED), bg="#4a4a5a", fg="white", width=4).pack(side=tk.RIGHT, padx=2)
        
        # -- Quick Add Exclusion --
        tk.Button(util_frame, text="Add", command=lambda: self.add_excluded_filename(self.exc_entry), bg="#007ACC", fg="white", font=("Arial", 8)).pack(side=tk.RIGHT, padx=5)
        self.exc_entry = tk.Entry(util_frame, bg="#3a3a4a", fg="white", width=15)
        self.exc_entry.pack(side=tk.RIGHT, padx=5)
        tk.Label(util_frame, text="Excl. Pattern:", bg="#1e1e2f", fg="gray").pack(side=tk.RIGHT)

        # Log Box
        self.widgets['log_box'] = scrolledtext.ScrolledText(right_frame, bg="#151521", fg="#E0E0E0", font=("Consolas", 9), state=tk.DISABLED, height=10)
        self.widgets['log_box'].pack(fill=tk.BOTH, expand=True, padx=5, pady=5)

        paned.add(right_frame, weight=1)

        # Status Bar
        self.widgets['status_var'] = tk.StringVar(value="Ready.")
        self.widgets['status_bar'] = tk.Label(self.root, textvariable=self.widgets['status_var'], bg="#111111", fg="#90EE90", anchor="w")
        self.widgets['status_bar'].pack(fill=tk.X, side=tk.BOTTOM)

    # --- Threading & Activity Logic ---
    def _activity_blinker(self):
        if self.running_tasks:
            current_color = self.widgets['status_bar'].cget("bg")
            next_color = "#333333" if current_color == "#111111" else "#111111"
            self.widgets['status_bar'].config(bg=next_color)
            task_names = ", ".join(self.running_tasks)
            self.widgets['status_var'].set(f"[ACTIVE] Processing: {task_names}")
        else:
            self.widgets['status_bar'].config(bg="#111111")
            
        self.root.after(500, self._activity_blinker)

    def cancel_current_operations(self):
        self.stop_event.set()
        self.log_message("Stop signal sent to background threads.", "WARNING")

    def run_threaded_action(self, target_function_impl, task_id: str, widgets_to_disable=None, save_config_after=False, use_popup=False):
        if task_id in self.running_tasks:
            self.log_message(f"Task '{task_id}' is already running.", "WARNING")
            return

        if use_popup:
            self.current_progress_popup = ProgressPopup(self.root, title=f"Working: {task_id}", on_cancel=self.cancel_current_operations)

        def thread_target_wrapper():
            self.running_tasks.add(task_id)
            self.stop_event.clear()
            
            try:
                target_function_impl()
                if save_config_after:
                    path = self._get_current_project_path()
                    if path: self.save_project_config(path)
            except Exception as e:
                err_msg = f"CRASH in {task_id}: {e}\n{traceback.format_exc()}"
                self.schedule_log_message(err_msg, "CRITICAL")
            finally:
                if task_id in self.running_tasks:
                    self.running_tasks.remove(task_id)
                if use_popup and self.current_progress_popup:
                    self.gui_queue.put(self.current_progress_popup.close)
                    self.current_progress_popup = None
                self.schedule_log_message(f"Task '{task_id}' finished.", "INFO")

        threading.Thread(target=thread_target_wrapper, daemon=True).start()

    def schedule_log_message(self, msg: str, level: str = "INFO"):
        self.gui_queue.put(lambda: self.log_message(msg, level))
        def _update_popup_safely():
            if self.current_progress_popup:
                self.current_progress_popup.update_text(f"[{level}] {msg}")
        self.gui_queue.put(_update_popup_safely)

    def log_message(self, msg: str, level: str = "INFO"):
        ts = datetime.now().strftime("[%H:%M:%S]")
        full_msg = f"{ts} [{level}] {msg}\n"
        lb = self.widgets.get('log_box')
        if lb:
            lb.config(state=tk.NORMAL)
            lb.insert(tk.END, full_msg)
            lb.config(state=tk.DISABLED)
            lb.see(tk.END)
        self.widgets['status_var'].set(f"{ts} {msg}")

    def process_gui_queue(self):
        while not self.gui_queue.empty():
            try:
                cb = self.gui_queue.get_nowait()
                try: cb()
                except Exception: pass
            except queue.Empty: pass
        self.root.after(100, self.process_gui_queue)

    # --- Project Management Logic ---
    def _on_choose_project_directory(self):
        d = filedialog.askdirectory()
        if d:
            self.widgets['selected_root_var'].set(d)
            self._rescan_project_tree()

    def _on_project_root_commit(self, event=None):
        self._rescan_project_tree()

    def _on_click_up_dir(self):
        p = self._get_current_project_path()
        if p:
            self.widgets['selected_root_var'].set(str(p.parent))
            self._rescan_project_tree()

    def _get_current_project_path(self) -> Path | None:
        p_str = self.widgets['selected_root_var'].get()
        if p_str:
            p = Path(p_str)
            if p.is_dir(): return p
        return None

    def _rescan_project_tree(self):
        path = self._get_current_project_path()
        tree = self.widgets['folder_tree']
        for i in tree.get_children(): tree.delete(i)
        
        if not path:
            tree.insert("", "end", text="Invalid Root Path")
            return
            
        tree.insert("", "end", text="Scanning...")
        self.run_threaded_action(lambda: self._initial_tree_load_impl(path), task_id='load_tree')

    def _initial_tree_load_impl(self, root_path: Path):
        with self.state_lock:
            self.folder_item_states.clear()
        
        self.load_project_config(root_path)
        tree_data = []

        def _recurse(current: Path, parent_iid: str):
            if self.stop_event.is_set(): return
            try:
                # LIST ALL ITEMS (Files + Folders)
                # Sort: Folders first, then files (case insensitive)
                items = sorted(list(current.iterdir()), key=lambda x: (not x.is_dir(), x.name.lower()))
                
                for p in items:
                    # 1. SAFETY: Skip Excluded Folders/Files immediately
                    if self.should_exclude_path(p, root_path):
                        continue

                    path_str = str(p.resolve())
                    
                    # 2. State Inheritance
                    # If we don't have a specific state saved, inherit from parent
                    if path_str not in self.folder_item_states:
                        parent_state = self.folder_item_states.get(parent_iid, S_CHECKED)
                        with self.state_lock:
                            self.folder_item_states[path_str] = parent_state
                    
                    # 3. Add to Tree Data
                    # We add a visual prefix for files since we are using the image slot for the checkbox
                    display_text = f" {p.name}"
                    
                    tree_data.append({
                        'parent': parent_iid, 
                        'iid': path_str, 
                        'text': display_text
                    })
                    
                    # 4. Recurse only if Directory
                    if p.is_dir():
                        _recurse(p, path_str)
                        
            except PermissionError: pass

        root_str = str(root_path.resolve())
        with self.state_lock: self.folder_item_states[root_str] = S_CHECKED
        tree_data.append({'parent': '', 'iid': root_str, 'text': f" {root_path.name}", 'open': True})
        
        _recurse(root_path, root_str)
        self.gui_queue.put(lambda: self._populate_tree(tree_data))
    def _populate_tree(self, data):
        tree = self.widgets['folder_tree']
        for i in tree.get_children(): tree.delete(i)
        for d in data:
            tree.insert(d['parent'], "end", iid=d['iid'], text=d['text'], open=d.get('open', False))
            tree.set(d['iid'], "size", "...")
        
        self.refresh_tree_visuals()
        root_path = self._get_current_project_path()
        if root_path:
             threading.Thread(target=self._calc_sizes_async, args=(str(root_path),), daemon=True).start()

    def _calc_sizes_async(self, root_iid):
        tree = self.widgets['folder_tree']
        q = [root_iid]
        while q:
            if self.stop_event.is_set(): break
            iid = q.pop(0)
            try:
                if not tree.exists(iid): continue
                sz = get_folder_size_bytes(Path(iid))
                fmt = format_display_size(sz)
                self.gui_queue.put(lambda i=iid, s=fmt: (tree.set(i, "size", s), self.refresh_tree_visuals(i)))
                q.extend(tree.get_children(iid))
            except: pass

    def refresh_tree_visuals(self, start_node=None):
        tree = self.widgets['folder_tree']
        def _refresh(iid):
            if not tree.exists(iid): return
            with self.state_lock:
                st = self.folder_item_states.get(iid, S_UNCHECKED)
            
            # Use Checkbox Icon
            icon = self.icon_imgs.get(st, self.icon_imgs[S_UNCHECKED])
            
            # Add File/Folder distinction to text
            p = Path(iid)
            prefix = "üìÑ " if p.is_file() else "" 
            
            tree.item(iid, text=f" {prefix}{p.name}", image=icon)
            
            # Recursion only needed for folders (files have no children)
            if tree.get_children(iid):
                for child in tree.get_children(iid): _refresh(child)
        
        if start_node: _refresh(start_node)
        else:
            root = self._get_current_project_path()
            if root: _refresh(str(root.resolve()))

    def on_tree_item_click(self, event):
        tree = event.widget
        
        # Identify specific element. 
        # Note: element name varies by theme (e.g., "image", "Treeitem.image", etc.)
        element = tree.identify("element", event.x, event.y)
        iid = tree.identify_row(event.y)
        
        if not iid: return

        # ROBUST FIX: Check if "image" is part of the element name
        if "image" in element:
            with self.state_lock:
                curr = self.folder_item_states.get(iid, S_UNCHECKED)
                new = S_CHECKED if curr != S_CHECKED else S_UNCHECKED
                self.folder_item_states[iid] = new
            self.refresh_tree_visuals(iid)

    def set_global_selection(self, state):
        with self.state_lock:
            for k in self.folder_item_states:
                self.folder_item_states[k] = state
        self.refresh_tree_visuals()

    def is_selected(self, path: Path, project_root: Path) -> bool:
        try: p = path.resolve()
        except: return False
        root = project_root.resolve()
        if p != root and not str(p).startswith(str(root)): return False
        curr = p
        while True:
            st = self.folder_item_states.get(str(curr))
            if st == S_UNCHECKED: return False
            if curr == root: return st != S_UNCHECKED
            if curr.parent == curr: break
            curr = curr.parent
        return True

    def _load_gitignore_patterns(self, root: Path):
        """Best-effort .gitignore parsing.

        Supported (simple):
          - dir ignores via trailing '/'
          - bare patterns like '*.log'
          - path-ish patterns containing '/'

        Not supported (yet):
          - negation rules starting with '!'
          - advanced gitignore semantics (root anchoring, '**' edge cases, etc.)
        """
        gi = root / ".gitignore"
        with self.state_lock:
            self.gitignore_dirnames = set()
            self.gitignore_file_patterns = set()
            self.gitignore_path_patterns = set()

        if not gi.exists():
            return

        try:
            lines = gi.read_text(encoding="utf-8", errors="ignore").splitlines()
        except Exception:
            return

        dirnames = set()
        file_pats = set()
        path_pats = set()

        for raw in lines:
            s = raw.strip()
            if not s or s.startswith("#"):
                continue
            # Skip negation rules for now
            if s.startswith("!"):
                continue

            # Normalize Windows separators to POSIX-ish for matching
            s = s.replace("\\", "/")

            if s.endswith("/"):
                # directory name or dir-pattern
                d = s[:-1].strip("/")
                if d:
                    dirnames.add(d)
                continue

            if "/" in s:
                path_pats.add(s.strip("/"))
            else:
                file_pats.add(s)

        with self.state_lock:
            self.gitignore_dirnames = dirnames
            self.gitignore_file_patterns = file_pats
            self.gitignore_path_patterns = path_pats

    def _rel_posix(self, p: Path, root: Path) -> str:
        """Return a posix-style relative path; fall back to name if relative fails."""
        try:
            return p.relative_to(root).as_posix()
        except Exception:
            return p.name

    def _respect_exclusions_enabled(self) -> bool:
        """UI/engine toggle: when False, include everything (verbose mapping)."""
        try:
            var = self.widgets.get('respect_exclusions')
            if var is None:
                return True
            return bool(var.get())
        except Exception:
            return True

    def should_exclude_dir(self, dir_path: Path, project_root: Path) -> bool:
        """Directory exclusion check: hard-coded exclusions + .gitignore (best-effort)."""
        if not self._respect_exclusions_enabled():
            return False

        name = dir_path.name
        if name in EXCLUDED_FOLDERS:
            return True

        rel = self._rel_posix(dir_path, project_root)

        with self.state_lock:
            if name in self.gitignore_dirnames:
                return True
            # Match path patterns against directory relpath
            for pat in self.gitignore_path_patterns:
                if fnmatch.fnmatch(rel, pat) or fnmatch.fnmatch(rel + "/", pat) or fnmatch.fnmatch(rel + "/", pat + "/"):
                    return True

        return False

    def should_exclude_file(self, filename: str, rel_posix: str | None = None) -> bool:
        """File exclusion check: predefined + dynamic + .gitignore (best-effort)."""
        if not self._respect_exclusions_enabled():
            return False

        with self.state_lock:
            pats = PREDEFINED_EXCLUDED_FILENAMES.union(self.dynamic_global_excluded_filenames)
            gi_files = set(self.gitignore_file_patterns)
            gi_paths = set(self.gitignore_path_patterns)

        # filename-based patterns
        if any(fnmatch.fnmatch(filename, p) for p in pats):
            return True
        if any(fnmatch.fnmatch(filename, p) for p in gi_files):
            return True

        # relative-path patterns (if available)
        if rel_posix:
            rel_posix = rel_posix.replace("\\", "/")
            for pat in gi_paths:
                if fnmatch.fnmatch(rel_posix, pat):
                    return True

        return False

    def should_exclude_path(self, path: Path, project_root: Path) -> bool:
        """Unified exclusion gate for BOTH files and directories.

        This centralizes the behavior so initial scan, map, dump, and backup stay consistent.
        Uses:
          - Hard excluded folders
          - Predefined + dynamic filename patterns
          - .gitignore patterns (best-effort)
          - UI toggle to disable all exclusions
        """
        if not self._respect_exclusions_enabled():
            return False

        try:
            root = project_root.resolve()
            p = path.resolve()
        except Exception:
            return False

        # Only apply exclusions inside the active project root
        if p != root and not str(p).startswith(str(root)):
            return False

        if p.is_dir():
            return self.should_exclude_dir(p, root)

        relp = self._rel_posix(p, root)
        return self.should_exclude_file(p.name, rel_posix=relp)

    # --- Core Actions ---
    def get_log_dir(self, root: Path) -> Path | None:
        if not root: return None
        # CHANGED: All logs go directly to _logs, no subdirectories
        d = root / LOG_ROOT_NAME
        try: d.mkdir(parents=True, exist_ok=True)
        except: return None
        return d

    def _generate_filename(self, root_name: str, base_suffix: str, extension: str) -> str:
        # CHANGED: Naming convention logic
        # Default: FolderName_suffix.ext
        # If timestamp enabled: FolderName_suffix_timestamp.ext
        name = f"{root_name}_{base_suffix}"
        if self.widgets['use_timestamps'].get():
            ts = datetime.now().strftime('%Y%m%d_%H%M%S')
            name += f"_{ts}"
        name += extension
        return name

    def build_folder_tree_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "project_folder_tree", ".txt")
        out_file = out_dir / fname
        
        lines = [f"Project Tree: {root}\nGenerated: {datetime.now()}\n"]
        
        def _write_recurse(curr, prefix):
            if self.stop_event.is_set(): 
                lines.append(f"{prefix}!!! CANCELLED !!!")
                return

            try: items = sorted(list(curr.iterdir()), key=lambda x: (x.is_file(), x.name.lower()))
            except: return
            
            for i, item in enumerate(items):
                is_last = (i == len(items) - 1)
                conn = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
                
                if item.is_dir():
                    # Respect exclusions/.gitignore (unless toggled off)
                    if self.should_exclude_path(item, root):
                        continue

                    if self.is_selected(item, root):
                        lines.append(f"{prefix}{conn}üìÅ {item.name}/")
                        _write_recurse(item, prefix + ("    " if is_last else "‚îÇ   "))
                else:
                    if (not self.should_exclude_path(item, root)) and self.is_selected(item.parent, root):
                         lines.append(f"{prefix}{conn}üìÑ {item.name}")
        
        _write_recurse(root, "")
        
        with open(out_file, "w", encoding="utf-8") as f: f.write("\n".join(lines))
        self.schedule_log_message(f"Tree saved: {fname}")

    def dump_files_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "filedump", ".txt")
        out_file = out_dir / fname
        
        count = 0
        with open(out_file, "w", encoding="utf-8") as f_out:
            f_out.write(f"Dump: {root}\n\n")
            
            for r, d, f in os.walk(root):
                if self.stop_event.is_set(): 
                    f_out.write("\n\n!!! DUMP CANCELLED BY USER !!!")
                    break
    
                curr = Path(r)
                # First remove excluded folders (hard + .gitignore), then apply selection logic
                kept_dirs = []
                for x in d:
                    dp = curr / x
                    if self.should_exclude_path(dp, root):
                        continue
                    if not self.is_selected(dp, root):
                        continue
                    kept_dirs.append(x)
                d[:] = kept_dirs
                if not self.is_selected(curr, root): continue
                
                for fname_item in f:
                    if self.stop_event.is_set(): break

                    fpath = curr / fname_item
                    if self.should_exclude_path(fpath, root):
                        continue
                    if fpath.stat().st_size > 1_000_000: continue
                    if is_binary(fpath) or "".join(fpath.suffixes).lower() in FORCE_BINARY_EXTENSIONS_FOR_DUMP: continue
                    
                    rel = fpath.relative_to(root)
                    if count % 5 == 0: self.schedule_log_message(f"Dumping: {rel}", "DEBUG")
                    
                    try:
                        f_out.write(f"\n{'-'*80}\nFILE: {rel}\n{'-'*80}\n")
                        with open(fpath, "r", encoding="utf-8", errors="ignore") as f_in:
                            f_out.write(f_in.read())
                        count += 1
                    except Exception as e:
                        f_out.write(f"\n[ERROR READING FILE: {e}]\n")
        
        self.schedule_log_message(f"Dump saved: {fname} ({count} files)")

    def backup_project_impl(self):
        root = self._get_current_project_path()
        if not root: return
        
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "backup", ".tar.gz")
        out_file = out_dir / fname
        
        count = 0
        with tarfile.open(out_file, "w:gz") as tar:
            for r, d, f in os.walk(root):
                if self.stop_event.is_set(): break
                curr = Path(r)
                kept_dirs = []
                for x in d:
                    dp = curr / x
                    if self.should_exclude_path(dp, root):
                        continue
                    if not self.is_selected(dp, root):
                        continue
                    kept_dirs.append(x)
                d[:] = kept_dirs
                if not self.is_selected(curr, root): continue
                for fname_item in f:
                    fpath = curr / fname_item
                    if self.should_exclude_path(fpath, root):
                        continue
                    tar.add(fpath, arcname=fpath.relative_to(root))
                    count += 1
                    if count % 10 == 0: self.schedule_log_message(f"Archiving: {fname_item}", "DEBUG")

        if self.stop_event.is_set():
            self.schedule_log_message("Backup Cancelled.", "WARNING")
        else:
            self.schedule_log_message(f"Backup saved: {fname}")

    def audit_system_impl(self):
        root = self._get_current_project_path() or DEFAULT_ROOT_DIR
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(root.name, "system_audit", ".txt")
        out_file = out_dir / fname
        
        lines = [f"System Audit: {datetime.now()}", f"Platform: {platform.platform()}"]
        lines.append(f"Python: {sys.version}")
        lines.append("\nEnvironment Variables (Keys only):")
        for k in os.environ.keys(): lines.append(f"  {k}")
        
        with open(out_file, "w") as f: f.write("\n".join(lines))
        self.schedule_log_message(f"System audit saved: {fname}")

    def audit_conda_impl(self):
        env_name = self.widgets['conda_env_var'].get()
        if not env_name: return
        root = self._get_current_project_path() or DEFAULT_ROOT_DIR
        out_dir = self.get_log_dir(root)
        fname = self._generate_filename(f"conda_{env_name}", "audit", ".txt")
        out_file = out_dir / fname
        
        self.schedule_log_message(f"Auditing Conda Env: {env_name}...")
        try:
            res = subprocess.run(["conda", "list", "-n", env_name], capture_output=True, text=True, shell=True)
            with open(out_file, "w") as f: f.write(res.stdout)
            self.schedule_log_message(f"Conda audit saved: {fname}")
        except Exception as e:
            self.schedule_log_message(f"Conda audit failed: {e}", "ERROR")

    def _load_conda_info_impl(self):
        try:
            res = subprocess.run(["conda", "env", "list", "--json"], capture_output=True, text=True, shell=True)
            data = json.loads(res.stdout)
            envs = [Path(p).name for p in data.get('envs', [])]
            self.gui_queue.put(lambda: self.widgets['conda_env_combo'].config(values=envs))
            if envs: self.gui_queue.put(lambda: self.widgets['conda_env_combo'].current(0))
        except: pass

    # --- Persistence ---
    def save_project_config(self, root: Path):
        cfg = self.get_log_dir(root) / PROJECT_CONFIG_FILENAME
        rel_states = {}
        with self.state_lock:
            for k, v in self.folder_item_states.items():
                try: rel_states[str(Path(k).relative_to(root))] = v
                except: pass
            data = {
                "folder_states": rel_states,
                "dynamic_exclusions": list(self.dynamic_global_excluded_filenames)
            }
        with open(cfg, "w") as f: json.dump(data, f, indent=2)

    def load_project_config(self, root: Path):
        # Always (re)load .gitignore for the active root (best-effort)
        self._load_gitignore_patterns(root)

        cfg = self.get_log_dir(root) / PROJECT_CONFIG_FILENAME
        if not cfg.exists(): return
        try:
            with open(cfg, "r") as f: data = json.load(f)
            for k, v in data.get("folder_states", {}).items():
                self.folder_item_states[str((root / k).resolve())] = v
            self.dynamic_global_excluded_filenames.update(data.get("dynamic_exclusions", []))
        except: pass

    # --- Dynamic Exclusions ---
    def add_excluded_filename(self, entry):
        val = entry.get().strip()
        if val:
            self.dynamic_global_excluded_filenames.add(val)
            entry.delete(0, tk.END)
            self.schedule_log_message(f"Added exclusion: {val}")

    def manage_dynamic_exclusions_popup(self):
        top = tk.Toplevel(self.root)
        top.title("Exclusions")
        lb = tk.Listbox(top)
        lb.pack(fill=tk.BOTH, expand=True)
        for x in self.dynamic_global_excluded_filenames: lb.insert(tk.END, x)
        def _rem():
            sel = lb.curselection()
            if not sel: return
            val = lb.get(sel[0])
            self.dynamic_global_excluded_filenames.remove(val)
            top.destroy()
            self.manage_dynamic_exclusions_popup()
        tk.Button(top, text="Remove Selected", command=_rem).pack()

    def open_main_log_directory(self):
        p = self._get_current_project_path()
        if not p: return
        d = self.get_log_dir(p)
        if platform.system() == "Windows": os.startfile(d)
        elif platform.system() == "Darwin": subprocess.run(["open", d])
        else: subprocess.run(["xdg-open", d])


# ==============================================================================
# 4. ENTRY POINTS
# ==============================================================================

def run_gui():
    root = tk.Tk()
    app = ProjectMapperApp(root)
    root.mainloop()

def run_cli():
    parser = argparse.ArgumentParser(description="ProjectMapper CLI")
    parser.add_argument("path", nargs="?", default=".", help="Root path to map")
    args = parser.parse_args()
    
    target = Path(args.path).resolve()
    print(f"--- Project Mapper CLI ---\nMapping: {target}\n")
    
    if not target.is_dir():
        print("Error: Invalid directory.")
        sys.exit(1)
        
    for item in target.rglob("*"):
        depth = len(item.relative_to(target).parts)
        indent = "  " * depth
        print(f"{indent}{item.name}")
        
    print("\nDone. For full features (backup, dumping, config), use GUI mode.")

def main():
    if len(sys.argv) > 1 and sys.argv[1] not in ["-m", "src.app"]:
        run_cli()
    else:
        run_gui()

if __name__ == "__main__":
    main()




--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\src\the-patch.txt
--------------------------------------------------------------------------------
{
  "hunks": [
    {
      "description": "Add .gitignore parsing + matching helpers (robust, supports !negation and dir patterns)",
      "search_block": "def is_binary(file_path: Path) -> bool:\n    \"\"\"Check if a file is binary by reading the first chunk.\"\"\"",
      "replace_block": "def _parse_gitignore_lines(lines: List[str]) -> List[Tuple[bool, str, bool]]:\n    \"\"\"Parse .gitignore lines into ordered rules.\n\n    Returns: list of (negate, pattern, dir_only)\n    - negate: True if rule starts with '!'\n    - pattern: normalized POSIX-ish pattern (no trailing spaces)\n    - dir_only: True if pattern ends with '/'\n\n    Notes:\n    - This is intentionally a pragmatic implementation (fnmatch-based), not a full git spec engine,\n      but it correctly handles the common cases (including .venv/, *.pyc, /dist, !keep.txt).\n    \"\"\"\n    rules: List[Tuple[bool, str, bool]] = []\n    for raw in lines:\n        s = raw.strip(\"\\n\")\n        if not s or s.lstrip().startswith(\"#\"):\n            continue\n        s = s.strip()\n        negate = s.startswith(\"!\")\n        if negate:\n            s = s[1:].strip()\n            if not s:\n                continue\n        dir_only = s.endswith(\"/\")\n        if dir_only:\n            s = s[:-1]\n        # normalize separators\n        s = s.replace(\"\\\\\", \"/\")\n        rules.append((negate, s, dir_only))\n    return rules\n\n\ndef _load_gitignore_rules(root: Path) -> List[Tuple[bool, str, bool]]:\n    \"\"\"Load .gitignore rules from the project root (if present).\"\"\"\n    gi = root / \".gitignore\"\n    if not gi.exists() or not gi.is_file():\n        return []\n    try:\n        txt = gi.read_text(encoding=\"utf-8\", errors=\"ignore\")\n    except Exception:\n        return []\n    return _parse_gitignore_lines(txt.splitlines())\n\n\ndef _gitignore_match(rel_posix: str, name: str, rule_pat: str) -> bool:\n    \"\"\"Match a gitignore-style pattern against a rel path.\n\n    Matching strategy:\n    - If pattern contains '/', match against rel_posix\n    - Else match against basename (name)\n    - If pattern starts with '/', anchor to root (strip leading '/').\n    \"\"\"\n    pat = rule_pat\n    if pat.startswith(\"/\"):\n        pat = pat[1:]\n        return fnmatch.fnmatch(rel_posix, pat)\n\n    if \"/\" in pat:\n        return fnmatch.fnmatch(rel_posix, pat)\n\n    return fnmatch.fnmatch(name, pat)\n\n\ndef _gitignore_is_ignored(rel_posix: str, name: str, is_dir: bool, rules: List[Tuple[bool, str, bool]]) -> bool:\n    \"\"\"Apply ordered gitignore rules. Later matches override earlier ones.\"\"\"\n    ignored = False\n    for negate, pat, dir_only in rules:\n        if dir_only and not is_dir:\n            continue\n        if _gitignore_match(rel_posix, name, pat):\n            ignored = (not negate)\n    return ignored\n\n\ndef is_binary(file_path: Path) -> bool:\n    \"\"\"Check if a file is binary by reading the first chunk.\"\"\"",
      "use_patch_indent": false
    },
    {
      "description": "Unify exclusions: add UI toggle + path-level exclusion function (default exclusions + dynamic patterns + .gitignore)",
      "search_block": "    def should_exclude_file(self, filename: str) -> bool:\n        with self.state_lock:\n            pats = PREDEFINED_EXCLUDED_FILENAMES.union(self.dynamic_global_excluded_filenames)\n        return any(fnmatch.fnmatch(filename, p) for p in pats)\n",
      "replace_block": "    def _respect_exclusions_enabled(self) -> bool:\n        \"\"\"UI/engine toggle: when False, we include everything (verbose mapping).\"\"\"\n        try:\n            var = self.widgets.get('respect_exclusions')\n            if var is None:\n                return True\n            return bool(var.get())\n        except Exception:\n            return True\n\n    def should_exclude_file(self, filename: str) -> bool:\n        \"\"\"Filename/pattern exclusions (predefined + dynamic).\"\"\"\n        if not self._respect_exclusions_enabled():\n            return False\n        with self.state_lock:\n            pats = PREDEFINED_EXCLUDED_FILENAMES.union(self.dynamic_global_excluded_filenames)\n        return any(fnmatch.fnmatch(filename, p) for p in pats)\n\n    def should_exclude_path(self, path: Path, project_root: Path) -> bool:\n        \"\"\"Unified exclusion check.\n\n        Applies:\n        - Default excluded folders (.venv, .git, node_modules, etc.)\n        - Pattern exclusions (predefined + dynamic)\n        - .gitignore (from project root)\n\n        When the UI toggle is OFF, returns False (include everything).\n        \"\"\"\n        if not self._respect_exclusions_enabled():\n            return False\n\n        try:\n            root = project_root.resolve()\n            p = path.resolve()\n        except Exception:\n            return False\n\n        # Ensure within project root\n        if p != root and not str(p).startswith(str(root)):\n            return False\n\n        name = p.name\n        is_dir = p.is_dir()\n\n        # 1) Hard folder exclusions\n        if name in EXCLUDED_FOLDERS:\n            return True\n\n        # 2) Pattern exclusions (apply to both dirs and files)\n        if self.should_exclude_file(name):\n            return True\n\n        # 3) .gitignore rules\n        try:\n            rel_posix = p.relative_to(root).as_posix()\n        except Exception:\n            rel_posix = name\n\n        rules = _load_gitignore_rules(root)\n        if rules and _gitignore_is_ignored(rel_posix, name, is_dir, rules):\n            return True\n\n        return False\n",
      "use_patch_indent": false
    },
    {
      "description": "Add UI checkbox to toggle respecting exclusions/.gitignore (default ON)",
      "search_block": "        # -- Timestamp Checkbox --\n        self.widgets['use_timestamps'] = tk.BooleanVar(value=False)\n        ts_chk = tk.Checkbutton(util_frame, text=\"Append Timestamps to Filenames\", variable=self.widgets['use_timestamps'],\n                                bg=\"#1e1e2f\", fg=\"white\", selectcolor=\"#252526\", activebackground=\"#1e1e2f\")\n        ts_chk.pack(side=tk.LEFT, padx=10)\n",
      "replace_block": "        # -- Timestamp Checkbox --\n        self.widgets['use_timestamps'] = tk.BooleanVar(value=False)\n        ts_chk = tk.Checkbutton(util_frame, text=\"Append Timestamps to Filenames\", variable=self.widgets['use_timestamps'],\n                                bg=\"#1e1e2f\", fg=\"white\", selectcolor=\"#252526\", activebackground=\"#1e1e2f\")\n        ts_chk.pack(side=tk.LEFT, padx=10)\n\n        # -- Exclusion / .gitignore Toggle (default ON) --\n        self.widgets['respect_exclusions'] = tk.BooleanVar(value=True)\n        excl_chk = tk.Checkbutton(\n            util_frame,\n            text=\"Respect .gitignore + exclusions\",\n            variable=self.widgets['respect_exclusions'],\n            bg=\"#1e1e2f\",\n            fg=\"white\",\n            selectcolor=\"#252526\",\n            activebackground=\"#1e1e2f\"\n        )\n        excl_chk.pack(side=tk.LEFT, padx=10)\n",
      "use_patch_indent": false
    },
    {
      "description": "Fix initial tree scan to use unified exclusion logic (ensures UI matches map/dump/backup behavior)",
      "search_block": "                    if p.name in EXCLUDED_FOLDERS: continue\n",
      "replace_block": "                    if self.should_exclude_path(p, root_path):\n                        continue\n",
      "use_patch_indent": false
    },
    {
      "description": "Fix Map Project Tree to exclude folders like .venv and respect .gitignore (and match UI toggle behavior)",
      "search_block": "        def _recurse(curr: Path, depth: int = 0):\n            if self.stop_event.is_set(): return\n\n            try: items = sorted(curr.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))\n            except: return\n\n            for item in items:\n                # Skip if not selected\n                if item.is_dir() and not self.is_selected(item, root):\n                    continue\n\n                indent = \"  \" * depth\n\n                if item.is_dir():\n                    lines.append(f\"{indent}üìÅ {item.name}/\")\n                    _recurse(item, depth + 1)\n                else:\n                    # Only include files inside selected folders\n                    if self.is_selected(item.parent, root):\n                        lines.append(f\"{indent}üìÑ {item.name}\")\n",
      "replace_block": "        def _recurse(curr: Path, depth: int = 0):\n            if self.stop_event.is_set():\n                return\n\n            try:\n                items = sorted(curr.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))\n            except Exception:\n                return\n\n            for item in items:\n                if self.stop_event.is_set():\n                    return\n\n                # 1) Unified exclusions (.gitignore + default + dynamic), unless user toggled it off\n                if self.should_exclude_path(item, root):\n                    continue\n\n                # 2) Selection gate\n                if item.is_dir() and not self.is_selected(item, root):\n                    continue\n\n                indent = \"  \" * depth\n\n                if item.is_dir():\n                    lines.append(f\"{indent}üìÅ {item.name}/\")\n                    _recurse(item, depth + 1)\n                else:\n                    # Only include files inside selected folders\n                    if self.is_selected(item.parent, root):\n                        lines.append(f\"{indent}üìÑ {item.name}\")\n",
      "use_patch_indent": false
    },
    {
      "description": "Make Dump Source Files respect .gitignore + exclusions (and match the same toggle as UI/map)",
      "search_block": "        for curr_dir, d, f in os.walk(root):\n            if self.stop_event.is_set(): break\n            d[:] = [x for x in d if x not in EXCLUDED_FOLDERS and self.is_selected(Path(curr_dir)/x, root)]\n\n            for fname in f:\n                if self.stop_event.is_set(): break\n                if self.should_exclude_file(fname): continue\n",
      "replace_block": "        for curr_dir, d, f in os.walk(root):\n            if self.stop_event.is_set():\n                break\n\n            curr_path = Path(curr_dir)\n\n            # Filter directories in-place so os.walk doesn't descend into excluded/ignored dirs\n            kept_dirs = []\n            for x in d:\n                p = curr_path / x\n                if not self.is_selected(p, root):\n                    continue\n                if self.should_exclude_path(p, root):\n                    continue\n                kept_dirs.append(x)\n            d[:] = kept_dirs\n\n            for fname in f:\n                if self.stop_event.is_set():\n                    break\n                p = curr_path / fname\n                if self.should_exclude_path(p, root):\n                    continue\n",
      "use_patch_indent": false
    },
    {
      "description": "Make Backup Project (Zip) respect .gitignore + exclusions (same toggle, same behavior)",
      "search_block": "        for curr_dir, d, f in os.walk(root):\n            if self.stop_event.is_set(): break\n            curr_path = Path(curr_dir)\n\n            # Prune traversal\n            d[:] = [x for x in d if self.is_selected(curr_path/x, root)]\n",
      "replace_block": "        for curr_dir, d, f in os.walk(root):\n            if self.stop_event.is_set():\n                break\n            curr_path = Path(curr_dir)\n\n            # Prune traversal (selection + unified exclusions)\n            kept_dirs = []\n            for x in d:\n                p = curr_path / x\n                if not self.is_selected(p, root):\n                    continue\n                if self.should_exclude_path(p, root):\n                    continue\n                kept_dirs.append(x)\n            d[:] = kept_dirs\n",
      "use_patch_indent": false
    },
    {
      "description": "Ensure Backup Project file loop also respects unified exclusions (.gitignore + default + dynamic)",
      "search_block": "            for fname in f:\n                if self.stop_event.is_set(): break\n                src = curr_path / fname\n\n                # Skip unselected\n                if not self.is_selected(curr_path, root):\n                    continue\n",
      "replace_block": "            for fname in f:\n                if self.stop_event.is_set():\n                    break\n                src = curr_path / fname\n\n                # Skip unselected\n                if not self.is_selected(curr_path, root):\n                    continue\n\n                # Skip excluded/ignored\n                if self.should_exclude_path(src, root):\n                    continue\n",
      "use_patch_indent": false
    }
  ]
}
--------------------------------------------------------------------------------
FILE: _ProjectMAPPER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _RagFORGE\.ragforge.json
--------------------------------------------------------------------------------
{
  ".": true,
  "_logs": false,
  "_logs/_project_mapper_config.json": false,
  "_logs/_RagFORGE_backup.tar.gz": false,
  "_logs/_RagFORGE_filedump.txt": false,
  "_logs/_RagFORGE_project_folder_tree.txt": false,
  "cartridges": false,
  "cartridges/_RagFORGE_CARTRIDGE.db": false,
  "src": true,
  "src/microservices": true,
  "src/microservices/__init__.py": true,
  "src/microservices/base_service.py": true,
  "src/microservices/cartridge_service.py": true,
  "src/microservices/document_utils.py": true,
  "src/microservices/graph_engine.py": true,
  "src/microservices/graph_view.py": true,
  "src/microservices/intake_service.py": true,
  "src/microservices/neural_service.py": true,
  "src/microservices/panels.py": true,
  "src/microservices/refinery_service.py": true,
  "src/microservices/scanner.py": true,
  "src/microservices/semantic_chunker.py": true,
  "src/microservices/telemetry_service.py": true,
  "src/microservices/thought_stream.py": true,
  "src/__init__.py": false,
  "src/app.py": true,
  "tests": false,
  "tests/dummy_source": false,
  "tests/dummy_source/concept.txt": false,
  "tests/test_cartridge.db": false,
  "tests/verify_forge.py": false,
  "LICENSE.md": true,
  "README.md": true,
  "requirements.txt": true,
  "setup_env.bat": true
}
--------------------------------------------------------------------------------
FILE: _RagFORGE\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _RagFORGE\README.md
--------------------------------------------------------------------------------
# **\_RagFORGE: The Neural Cartridge Factory**

**"Manufacture portable, self-contained RAG databases for your AI agents."**

\_RagFORGE is a "Knowledge Foundry." It ingests raw data (Source Code, PDFs, Websites) and refines it into a **Unified Neural Cartridge (.db)**. These cartridges are portable SQLite databases that contain the raw archive, the semantic vector index, and the relational knowledge graph‚Äîeverything an AI needs to understand a topic, in a single file.

## **üöÄ Features**

* **Universal Ingestion:**  
  * **Filesystem:** Recursively scans folders, respecting .gitignore.  
  * **Documents:** Automatically extracts text from **PDFs** and **HTML**.  
  * **Web Crawler:** Spiders websites to a specified depth and converts them into a navigable Virtual File System (VFS).  
* **The Refinery (Background Daemon):**  
  * **Smart Chunking:** Uses AST parsing for Python (splitting by Class/Function) and semantic windows for prose/text.  
  * **Parallel Embedding:** High-speed vector generation using local LLMs (via Ollama).  
  * **Graph Weaving:** Automatically links files based on imports and definitions.  
* **The Cartridge (UNCF v1.0):**  
  * Portable .db file (SQLite).  
  * Contains **Source** (Text/Blob), **Vectors** (sqlite-vec), and **Graph** (Nodes/Edges).  
  * Self-describing Manifest.  
* **Visual Verification:**  
  * Force-Directed Graph Visualization.  
  * Real-time **Neural Test** to verify vector search relevance immediately.

## **üõ†Ô∏è Installation**

1. **Prerequisites:**  
   * Python 3.10+  
   * [Ollama](https://ollama.ai/) running locally (ollama serve).  
   * Models pulled: ollama pull mxbai-embed-large (or your preferred embedder).  
2. **Setup:**  
   git clone \[https://github.com/yourusername/\_RagFORGE.git\](https://github.com/yourusername/\_RagFORGE.git)  
   cd \_RagFORGE  
   setup\_env.bat

3. **Run:**  
   \# Launch GUI  
   python \-m src.app

   \# Headless Mode (CLI)  
   python \-m src.app \--input "./my\_project" \--output "project\_brain.db"

## **üíæ The Cartridge Contract (UNCF v1.0)**

Every cartridge produced by \_RagFORGE adheres to the **Unified Neural Cartridge Format**. This ensures any consuming agent (like \_LocalMIND) can instantly mount and query the brain.

\[ YOUR CARTRIDGE (.db) \]  
‚îÇ  
‚îú‚îÄ‚îÄ 1\. The Archive (Verbatim Storage)   
‚îÇ   ‚îî‚îÄ‚îÄ Table: 'files'  
‚îÇ       ‚îú‚îÄ‚îÄ vfs\_path: "src/main.py"      (Hierarchy)  
‚îÇ       ‚îú‚îÄ‚îÄ content:  "import os..."     (Raw Text for LLM reading)  
‚îÇ       ‚îî‚îÄ‚îÄ blob:     \[Binary Data\]      (Original PDF/Image backup)  
‚îÇ  
‚îú‚îÄ‚îÄ 2\. The Index (Semantic Search)  
‚îÇ   ‚îú‚îÄ‚îÄ Table: 'chunks'                  (Text Segments)  
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ content: "def scan\_path..."  
‚îÇ   ‚îî‚îÄ‚îÄ Table: 'vec\_items'               (Mathematical Index)  
‚îÇ       ‚îî‚îÄ‚îÄ embedding: \[0.12, \-0.98...\]  (Fast Nearest-Neighbor Search)  
‚îÇ  
‚îî‚îÄ‚îÄ 3\. The Map (Knowledge Graph)  
    ‚îú‚îÄ‚îÄ Table: 'graph\_nodes'             (File & Function Nodes)  
    ‚îî‚îÄ‚îÄ Table: 'graph\_edges'             (Imports & Definitions)

### **1\. The Manifest (Boot Sector)**

Table: manifest  
Key-value store describing the cartridge's provenance and configuration.

* cartridge\_id: UUID4 unique identifier.  
* schema\_version: uncf\_v1.0.  
* created\_at\_utc: Timestamp of manufacture.  
* source\_root: Original path or URL of the source material.  
* ingest\_config: JSON record of which files were explicitly selected by the user.

### **2\. The Archive (Verbatim Storage)**

Table: files  
The "Physical" layer. Contains the raw data.

* vfs\_path: Portable, relative path (e.g., src/main.py or web/example.com/docs/intro.html).  
* content: UTF-8 Extracted Text (Input for the LLM).  
* blob\_data: Binary backup (Original PDF bytes, Images, etc.).  
* status: RAW (Needs processing), REFINED (Ready), or SKIPPED.

### **3\. The Index (Semantic Search)**

Tables: chunks, vec\_items  
The "Mathematical" layer. Enables similarity search.

* **chunks:** Text segments derived from the files.  
  * *Python:* Split by Class (class X) and Function (def y).  
  * *Docs:* Split by semantic window (e.g., 800 chars).  
* **vec\_items:** Virtual table (via sqlite-vec) storing the 1024-dimension embeddings.  
  * Queryable via KNN: WHERE embedding MATCH ? ORDER BY distance.

### **4\. The Map (Knowledge Graph)**

Tables: graph\_nodes, graph\_edges  
The "Relational" layer. Describes structure.

* **Nodes:** Represents Files (file), Web Pages (web), and Code Symbols (chunk).  
* **Edges:** Represents relationships like imports, defined\_in, or links\_to.

## **üñ•Ô∏è Usage Guide**

### **The Workflow**

1. **Select Source:** Choose a local folder or paste a URL.  
2. **Scan:** \_RagFORGE builds a file tree. Adjust "Web Depth" for crawlers.  
3. **Ingest:** Select the files you want. Click **INGEST**.  
4. **Refine:** The background daemon will wake up, chunk the data, and embed it. Watch the "System Log."  
5. **Verify:** Go to **Neural Topology**, type a query (e.g., "Authentication System"), and click **Neural Test**. Relevant nodes will light up.

### **Keyboard Controls (Graph View)**

* **Scroll:** Zoom In/Out.  
* **Left Click:** Pan Camera.  
* **Left Click \+ Drag Node:** Move Node (Physics active).  
* **Double Click Node:** Focus & Zoom.

## **üß© Architecture**

\[ \_RagFORGE App \]                 \[ External Services \]  
       ‚îÇ                                   ‚îÇ  
       ‚îú‚îÄ‚îÄ Intake Service  \<‚îÄ‚îÄ(Scan)‚îÄ‚îÄ‚îÄ\> Filesystem / Web  
       ‚îÇ        ‚îÇ                          ‚îÇ  
       ‚îú‚îÄ‚îÄ Refinery Service \<‚îÄ‚îÄ(Embed)‚îÄ‚îÄ\> Ollama (Localhost)  
       ‚îÇ        ‚îÇ  
       ‚îî‚îÄ‚îÄ Cartridge Service ‚îÄ‚îÄ\> \[ .db File (UNCF v1.0) \]

## **üìÑ License**

MIT License. Copyright (c) 2025 Jacob Lambert.
--------------------------------------------------------------------------------
FILE: _RagFORGE\requirements.txt
--------------------------------------------------------------------------------
# --- Critical UI/Graphing ---
pygame-ce>=2.3.0
Pillow>=10.0.0

# --- Networking/AI ---
requests>=2.30.0

# --- Database Extensions ---
# (Ensure your Python environment supports installing this, 
# otherwise you may need to manually place the DLL/SO)
sqlite-vec>=0.1.0

# --- Document Processing ---
pypdf>=3.0.0
beautifulsoup4>=4.12.0
--------------------------------------------------------------------------------
FILE: _RagFORGE\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _RagFORGE\src\.ragforge.json
--------------------------------------------------------------------------------
{
  ".": true,
  "microservices": true,
  "microservices/__init__.py": false,
  "microservices/base_service.py": true,
  "microservices/cartridge_service.py": true,
  "microservices/document_utils.py": true,
  "microservices/graph_engine.py": true,
  "microservices/graph_view.py": true,
  "microservices/intake_service.py": true,
  "microservices/neural_service.py": true,
  "microservices/panels.py": true,
  "microservices/refinery_service.py": true,
  "microservices/scanner.py": true,
  "microservices/semantic_chunker.py": true,
  "microservices/telemetry_service.py": true,
  "microservices/thought_stream.py": true,
  "__init__.py": false,
  "app.py": true
}
--------------------------------------------------------------------------------
FILE: _RagFORGE\src\app.py
--------------------------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
== _RagFORGE: Neural Cartridge Factory ==
"""

# 1. IMPORTS
import sys
import os

# --- PATH PATCH START ---
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)
# --- PATH PATCH END ---

import argparse
import threading
import tkinter as tk
from tkinter import ttk, messagebox

# Microservices
from microservices.base_service import BaseService
from microservices.cartridge_service import CartridgeService
from microservices.neural_service import NeuralService
from microservices.intake_service import IntakeService
from microservices.refinery_service import RefineryService
from microservices.telemetry_service import TelemetryService

# UI Components
from microservices.panels import (
    Sidebar,
    IngestToolbar,
    FileTreePanel,
    EditorPanel,
    SystemLog,
)
from microservices.graph_view import GraphView
from microservices.thought_stream import ThoughtStream

# 2. CONSTANTS
APP_TITLE = "_RagFORGE v1.0"
STORAGE_DIR = "./cartridges"
BG_COLOR = "#1e1e2f"


# 3. CORE FUNCTIONALITY (Headless Logic)

def headless_forge(source_path: str, db_name: str, verbose: bool = False):
    """CLI Entry point for automated cartridge creation."""
    if not db_name.endswith(".db"):
        db_name += ".db"

    db_path = os.path.join(STORAGE_DIR, db_name)

    print(f"[FORGE] Target Cartridge: {db_path}")
    print(f"[FORGE] Source Material: {source_path}")

    cartridge = CartridgeService(db_path)
    neural = NeuralService()
    intake = IntakeService(cartridge)
    refinery = RefineryService(cartridge, neural)

    print(">>> Phase 1: Intake (Vacuuming files...)")
    stats = intake.ingest_source(source_path)
    print(f"    Intake Result: {stats}")
    
    # Verify Manifest
    cid = cartridge.get_manifest("cartridge_id")
    print(f"    [Manifest] Cartridge ID: {cid}")

    print(">>> Phase 2: Refinery (Chunking & Weaving...)")
    while True:
        processed = refinery.process_pending(batch_size=10)
        if processed == 0:
            break
        if verbose:
            print(f"    Refined batch of {processed}...")

    print(f"[SUCCESS] Cartridge forged at {db_path}")
    return db_path


# 4. GUI LOGIC (The Workstation)

class RagForgeApp(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title(APP_TITLE)
        self.geometry("1200x800")
        self.configure(bg=BG_COLOR)

        self.neural = NeuralService()
        self.active_cartridge = None
        self.active_db_path = None
        self.refining = False

        self._apply_cyberpunk_theme()
        self._setup_ui()

    def _apply_cyberpunk_theme(self):
        """Injects the Dark/Cyberpunk visual style globally."""
        style = ttk.Style(self)

        try:
            style.theme_use("clam")
        except Exception:
            pass

        dark_bg = "#1e1e2f"
        darker_bg = "#151515"
        text_fg = "#e0e0e0"
        accent = "#007ACC"
        border = "#333344"

        # Treeviews
        style.configure(
            "Treeview",
            background=darker_bg,
            fieldbackground=darker_bg,
            foreground=text_fg,
            borderwidth=0,
            rowheight=26,
            font=("Segoe UI", 10),
        )
        style.map(
            "Treeview",
            background=[("selected", accent)],
            foreground=[("selected", "white")],
        )

        style.configure(
            "Treeview.Heading",
            background=dark_bg,
            foreground="#888",
            relief="flat",
            font=("Segoe UI", 9, "bold"),
        )
        style.map(
            "Treeview.Heading",
            background=[("active", "#2d2d44")],
        )

        # Scrollbars
        style.configure(
            "Vertical.TScrollbar",
            gripcount=0,
            background="#2d2d44",
            darkcolor=dark_bg,
            lightcolor=dark_bg,
            troughcolor=dark_bg,
            bordercolor=dark_bg,
            arrowcolor="#888",
        )
        style.map(
            "Vertical.TScrollbar",
            background=[("active", "#444"), ("disabled", dark_bg)],
        )

        # Tabs
        style.configure(
            "TNotebook",
            background=dark_bg,
            borderwidth=0,
        )
        style.configure(
            "TNotebook.Tab",
            background="#252526",
            foreground="#888",
            padding=[15, 8],
            font=("Segoe UI", 10),
        )
        style.map(
            "TNotebook.Tab",
            background=[("selected", accent)],
            foreground=[("selected", "white")],
        )

        # Panes
        style.configure("TPanedwindow", background=dark_bg)
        style.configure("Sash", background=border, handlecv_bg=border)

    def _setup_ui(self):
        self.sidebar = Sidebar(self, STORAGE_DIR, self.load_cartridge)
        self.sidebar.pack(side="left", fill="y")

        self.main_area = tk.Frame(self, bg=BG_COLOR)
        self.main_area.pack(side="right", fill="both", expand=True)

        self.notebook = ttk.Notebook(self.main_area)
        self.notebook.pack(fill="both", expand=True)

        self.tab_forge = tk.Frame(self.notebook, bg=BG_COLOR)
        self.notebook.add(self.tab_forge, text="  DATA INGESTION  ")

        self.ingest_toolbar = IngestToolbar(
            self.tab_forge,
            on_scan=self.run_scan_request,
            on_ingest=self.run_ingest_request,
        )
        self.ingest_toolbar.pack(fill="x", side="top")

        forge_panes = ttk.PanedWindow(self.tab_forge, orient="horizontal")
        forge_panes.pack(fill="both", expand=True, padx=5, pady=5)

        self.file_tree = FileTreePanel(forge_panes, None)
        forge_panes.add(self.file_tree, weight=1)

        right_col = ttk.PanedWindow(forge_panes, orient="vertical")
        forge_panes.add(right_col, weight=3)

        self.stream = ThoughtStream(right_col)
        right_col.add(self.stream, weight=3)

        self.sys_log = SystemLog(right_col)
        right_col.add(self.sys_log, weight=1)

        # Initialize the Nervous System (Telemetry)
        self.telemetry = TelemetryService(self, self.sys_log)
        self.telemetry.start()

        self.editor_panel = EditorPanel(self.notebook)
        self.notebook.add(self.editor_panel, text="  KNOWLEDGE INSPECTOR  ")

        self.graph_view = GraphView(self.notebook)
        self.notebook.add(self.graph_view, text="  NEURAL TOPOLOGY  ")

        status_frame = tk.Frame(
            self.main_area,
            bg="#101018",
            height=25,
            highlightbackground="#333",
            highlightthickness=1,
        )
        status_frame.pack(fill="x", side="bottom")
        status_frame.pack_propagate(False)

        self.status_var = tk.StringVar(value="Ready.")
        tk.Label(
            status_frame,
            textvariable=self.status_var,
            bg="#101018",
            fg="#888",
            font=("Arial", 9),
        ).pack(side="left", padx=10)

    # Remaining methods unchanged ‚Ä¶


    def load_cartridge(self, path):
        """Called when user clicks a DB in sidebar."""
        self.active_db_path = path
        self.status_var.set(f"Loaded: {os.path.basename(path)}")
        self.title(f"{APP_TITLE} - [{os.path.basename(path)}]")
        
        # 1. Init Backend for this specific cartridge
        self.active_cartridge = CartridgeService(path)
        
        # 2. Wire up panels
        new_intake = IntakeService(self.active_cartridge)
        self.file_tree.intake = new_intake
        self.editor_panel.load_db(path)
        self.sys_log.log(f"Cartridge loaded: {os.path.basename(path)}")
        
        # 3. Load Graph (Non-blocking)
        self.graph_view.bind_services(self.active_cartridge, self.neural)
        self.graph_view.load_from_db(path)
        
        # 4. Start Background Poller (The Refinery Daemon)
        if not self.refining:
            self.refining = True
            self.after(1000, self._refinery_loop)

    def run_scan_request(self, path, web_depth=0, binary_policy="Extract Text"):
        if not self.active_cartridge:
            messagebox.showwarning("No Cartridge", "Select a cartridge first.")
            return
        
        # Update Manifest with policy preference
        self.active_cartridge.set_manifest("binary_policy", binary_policy)
        self.active_cartridge.set_manifest("web_depth", web_depth)

        self.status_var.set(f"Scanning {path} (Depth: {web_depth})...")
        self.file_tree.load_tree(path, web_depth=web_depth)
        self.sys_log.log(f"Scanned source: {path}")
        self.status_var.set("Scan complete. Select files to ingest.")

    def run_ingest_request(self):
        if not self.active_cartridge:
            return
        
        files = self.file_tree.get_selected_files()
        root = self.file_tree.root_path
        
        if not files:
            messagebox.showwarning("No Files", "No files selected for ingestion.")
            return

        def worker():
            count = len(files)
            msg = f"Ingesting {count} items..."
            self.status_var.set(msg)
            self.sys_log.log(msg)
            try:
                # Access intake via file_tree which now holds the reference
                stats = self.file_tree.intake.ingest_selected(files, root)
                done_msg = f"Ingest Result: {stats}"
                self.status_var.set(done_msg)
                self.sys_log.log(done_msg)
            except Exception as e:
                err = f"Ingest Failed: {e}"
                self.sys_log.log(err)
            
            # Refresh UI elements on main thread
            def _refresh():
                self.editor_panel.refresh_list()
                self.graph_view.load_from_db(self.active_db_path)
                
            self.after(0, _refresh)

        threading.Thread(target=worker, daemon=True).start()

    def _refinery_loop(self):
        """
        The Heartbeat. Checks for RAW files and processes them in small batches.
        Keeps the UI responsive while chewing through data.
        """
        if self.active_cartridge:
            # We create a transient refinery instance to process the batch
            # Ideally, this should be persistent, but for now this works.
            refinery = RefineryService(self.active_cartridge, self.neural)
            
            try:
                # Process a small batch
                processed = refinery.process_pending(batch_size=1)
                
                if processed > 0:
                    self.status_var.set("Refining Knowledge... (Embedding & Weaving)")
                    # Update visuals occasionally
                    if processed % 5 == 0:
                        self.graph_view.load_from_db(self.active_db_path)
                        self.editor_panel.refresh_list()
                else:
                    current_status = self.status_var.get()
                    if "Refining" in current_status:
                        self.status_var.set("Refinery Idle. Cartridge up to date.")
                        self.graph_view.load_from_db(self.active_db_path)
                        self.editor_panel.refresh_list()
            except Exception as e:
                print(f"Refinery Loop Error: {e}")

        # Loop
        self.after(2000, self._refinery_loop)


# 5. CLI ENTRY POINT

def main():
    parser = argparse.ArgumentParser(description="_RagFORGE: Neural Cartridge Factory")
    
    # CLI Args for Headless Mode
    parser.add_argument("--input", "-i", type=str, help="Input source path (folder or URL)")
    parser.add_argument("--output", "-o", type=str, help="Output .db filename (e.g. 'my_brain.db')")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logs")
    
    args = parser.parse_args()

    if args.input and args.output:
        # HEADLESS MODE
        try:
            headless_forge(args.input, args.output, args.verbose)
        except Exception as e:
            print(f"[FATAL] {e}")
            sys.exit(1)
    else:
        # GUI MODE
        app = RagForgeApp()
        app.mainloop()

if __name__ == "__main__":
    main()







--------------------------------------------------------------------------------
FILE: _RagFORGE\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\.ragforge.json
--------------------------------------------------------------------------------
{
  ".": false,
  "__init__.py": false,
  "base_service.py": false,
  "cartridge_service.py": false,
  "document_utils.py": false,
  "graph_engine.py": false,
  "graph_view.py": false,
  "intake_service.py": false,
  "neural_service.py": false,
  "panels.py": false,
  "refinery_service.py": false,
  "scanner.py": false,
  "semantic_chunker.py": false,
  "telemetry_service.py": false,
  "thought_stream.py": false
}
--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\base_service.py
--------------------------------------------------------------------------------
import logging
import sys

class BaseService:
    """
    Standard base class for all _NeoCORTEX microservices.
    Provides unified logging and error handling.
    """
    def __init__(self, service_name: str):
        self.service_name = service_name
        self.log = logging.getLogger(service_name)
        
        # Configure logging if not already set up
        if not self.log.handlers:
            handler = logging.StreamHandler(sys.stdout)
            formatter = logging.Formatter('%(asctime)s [%(name)s] %(levelname)s: %(message)s', datefmt='%H:%M:%S')
            handler.setFormatter(formatter)
            self.log.addHandler(handler)
            self.log.setLevel(logging.INFO)

    def log_info(self, msg: str):
        self.log.info(msg)

    def log_error(self, msg: str):
        self.log.error(msg)

--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\cartridge_service.py
--------------------------------------------------------------------------------
import sqlite3
import json
import time
import os
import uuid
import datetime
import struct
from pathlib import Path

# Try to import sqlite-vec (pip install sqlite-vec)
try:
    import sqlite_vec
except ImportError:
    sqlite_vec = None
from typing import Dict, Any, Optional, List
from .base_service import BaseService

class CartridgeService(BaseService):
    """
    The Source of Truth.
    Manages the Unified Neural Cartridge Format (UNCF v1.0).
    """
    
    SCHEMA_VERSION = "uncf_v1.0"

    def __init__(self, db_path: str):
        super().__init__("CartridgeService")
        self.db_path = Path(db_path)
        self._init_db()

    def _get_conn(self):
        # Set generous timeout (60s) for multi-threaded Ingest/Refinery contention
        conn = sqlite3.connect(self.db_path, timeout=60.0)
        if sqlite_vec:
            try:
                conn.enable_load_extension(True)
                sqlite_vec.load(conn)
                conn.enable_load_extension(False)
            except Exception as e:
                self.log_error(f"Failed to load sqlite-vec: {e}")
        return conn

    def _init_db(self):
        """Initializes the standard Schema."""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = self._get_conn()
        cursor = conn.cursor()
        
        # Enable WAL Mode: Allows concurrent Readers (Refinery) & Writers (Ingest)
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL")
        
        # 1. Manifest (The Boot Sector)
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")
        
        # 1.5 Directories (The VFS Index)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS directories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT UNIQUE NOT NULL,
                parent_path TEXT,
                metadata TEXT DEFAULT '{}'
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_dir_parent ON directories(parent_path)")

        # 2. Files (The Content Store)
        # Supports Text AND Binary (blob_data)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT NOT NULL,       -- Portable path (e.g. "src/main.py")
                origin_path TEXT,             -- Provenance (e.g. "C:/Users/...")
                origin_type TEXT,             -- 'filesystem', 'web', 'github'
                content TEXT,                 -- Text content (UTF-8)
                blob_data BLOB,               -- Binary content (Images, PDFs)
                mime_type TEXT,
                status TEXT DEFAULT 'RAW',    -- RAW, REFINED, ERROR, SKIPPED
                metadata TEXT DEFAULT '{}',   -- JSON tags, summaries
                last_updated TIMESTAMP
            )
        """)
        # Index for fast lookups by VFS path
        cursor.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_vfs ON files(vfs_path)")

        # 3. Chunks (The Vector Store)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_id INTEGER,
                chunk_index INTEGER,
                content TEXT,
                embedding BLOB,
                name TEXT,
                type TEXT,
                start_line INTEGER,
                end_line INTEGER,
                FOREIGN KEY(file_id) REFERENCES files(id)
            )
        """)

        # 3.5 Vector Index (sqlite-vec)
        # Defaulting to 1024 dimensions (mxbai-embed-large). 
        # If you use a different model, this needs to match.
        if sqlite_vec:
            try:
                cursor.execute("CREATE VIRTUAL TABLE IF NOT EXISTS vec_items USING vec0(embedding float[1024])")
            except Exception as e:
                self.log_error(f"Vector Table Init Error: {e}")

        # 4. Graph Topology (The Neural Wiring)
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, relation TEXT, weight REAL)")

        # 5. Validation Logs
        cursor.execute("CREATE TABLE IF NOT EXISTS logs (timestamp REAL, level TEXT, message TEXT, context TEXT)")
        
        conn.commit()
        conn.close()
        
        # Initialize standard keys if new
        self.initialize_manifest()

    def initialize_manifest(self):
        """Populates the boot sector with strict RagFORGE Cartridge Schema (UNCF) v1.1."""
        if not self.get_manifest("cartridge_id"):
            now = datetime.datetime.utcnow().isoformat()

            # 1. Identity & Versioning
            self.set_manifest("schema_name", "ragforge_cartridge")
            self.set_manifest("schema_version", "1.1.0")
            self.set_manifest("cartridge_id", str(uuid.uuid4()))
            self.set_manifest("created_at_utc", now)
            self.set_manifest("created_by_app", "RagFORGE")

            # 2. Provenance / Sources
            # Agents can read this to understand where the content came from and what policies were used.
            self.set_manifest("sources", [])
            self.set_manifest("source_policies", {
                "binary_policy": "Extract Text",
                "web_depth": 0
            })

            # 3. Specs (Defaults - updated by RefineryService._stamp_specs)
            self.set_manifest("embedding_spec", {
                "provider": "unknown",
                "model": "pending_init",
                "dim": 0,
                "dtype": "unknown",
                "distance": "unknown"
            })
            self.set_manifest("chunking_spec", {
                "strategy": "semantic_hybrid",
                "python_ast": True,
                "generic_window": 1500
            })

            # 4. VFS + Content Stats (populated/updated over time)
            self.set_manifest("vfs", {
                "root_label": "",
                "directories": {"count": 0},
                "files": {
                    "count": 0,
                    "by_origin_type": {},
                    "by_mime": {}
                },
                "index_built": False
            })
            self.set_manifest("content_stats", {
                "chunks": {"count": 0},
                "vector_index": {
                    # Don't assume sqlite-vec is available until proven.
                    "enabled": False,
                    "table": "vec_items",
                    "backend": "sqlite-vec",
                    "dims": 0,
                    "status": "unknown"
                },
                "graph": {
                    "nodes": 0,
                    "edges": 0
                }
            })

            # 5. Capabilities Contract (what an agent can assume exists / how to navigate)
            self.set_manifest("capabilities", {
                "tables": {
                    "manifest": True,
                    "directories": True,
                    "files": True,
                    "chunks": True,
                    "vec_items": True,
                    "graph_nodes": True,
                    "graph_edges": True,
                    "logs": True
                },
                "navigation": {
                    "vfs_path": "files.vfs_path",
                    "directory_index": "directories.vfs_path",
                    "list_files_query": "SELECT vfs_path, mime_type, origin_type, status FROM files ORDER BY vfs_path",
                    "list_directories_query": "SELECT vfs_path, parent_path FROM directories ORDER BY vfs_path"
                },
                "retrieval": {
                    "raw_file_content_query": "SELECT content, blob_data, mime_type FROM files WHERE vfs_path=?",
                    "chunks_by_file_query": "SELECT chunk_index, name, type, start_line, end_line, content FROM chunks WHERE file_id=? ORDER BY chunk_index",
                    "vector_search": "sqlite-vec on vec_items if available"
                },
                "python_helper_api": {
                    "note": "Optional convenience layer for agents running inside Python. For non-Python consumers, use the SQL queries above.",
                    "methods": [
                        "CartridgeService.get_status_flags",
                        "CartridgeService.list_files",
                        "CartridgeService.list_directories",
                        "CartridgeService.get_file_record",
                        "CartridgeService.get_directory_tree",
                        "CartridgeService.get_status_summary",
                        "CartridgeService.add_node",
                        "CartridgeService.add_edge",
                        "CartridgeService.search_embeddings"
                    ]
                }
            })

            # 6. Status & Health
            self.set_manifest("cartridge_health", "FRESH")
            self.set_manifest("ingest_complete", False)
            self.set_manifest("refine_complete", False)
            self.set_manifest("last_ingest_at_utc", "")
            self.set_manifest("last_refine_at_utc", "")
            self.set_manifest("last_error", "")
            self.set_manifest("locks", {
                "write_lock_expected": False,
                "notes": "If DB locks occur, consider batching writes and shorter-lived connections."
            })

    def set_manifest(self, key: str, value: Any):
        """Upsert metadata key."""
        conn = self._get_conn()
        val_str = json.dumps(value) if isinstance(value, (dict, list)) else str(value)
        conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", (key, val_str))
        conn.commit()
        conn.close()

    def get_manifest(self, key: str) -> Optional[str]:
        """Retrieve metadata key."""
        conn = self._get_conn()
        row = conn.execute("SELECT value FROM manifest WHERE key=?", (key,)).fetchone()
        conn.close()
        return row[0] if row else None

    def validate_cartridge(self) -> Dict[str, Any]:
        """Quality Control: Checks if the cartridge is Agent-Safe."""
        report = {"valid": True, "health": "OK", "errors": []}
        
        # 1. Check Required Keys
        # These are the minimum contract keys an agent needs to understand what it loaded.
        required = [
            "schema_name",
            "schema_version",
            "cartridge_id",
            "created_at_utc",
            "created_by_app",
            "embedding_spec",
            "chunking_spec",
            "capabilities"
        ]
        for key in required:
            if not self.get_manifest(key):
                report["valid"] = False
                report["errors"].append(f"Missing Manifest Key: {key}")
        
        # 2. Check Vector Index Presence (and stamp reality into the manifest)
        conn = self._get_conn()
        vec_enabled = False
        vec_status = "unknown"
        try:
            # If this succeeds, the vec_items table exists and is queryable.
            conn.execute("SELECT count(*) FROM vec_items").fetchone()
            vec_enabled = True
            vec_status = "available"
        except Exception:
            vec_enabled = False
            vec_status = "unavailable"
            report["errors"].append("Vector Index (vec_items) missing or not loaded.")
            # Not fatal for 'valid' but impacts capability
            report["health"] = "WARN_NO_VECTORS"
        finally:
            conn.close()

        # Update manifest content_stats.vector_index to reflect truth
        try:
            content_stats = self.get_manifest("content_stats") or {}
            vec = content_stats.get("vector_index", {}) if isinstance(content_stats, dict) else {}

            # Use embedding_spec dim if present; otherwise keep existing dims value
            embed_spec = self.get_manifest("embedding_spec") or {}
            spec_dim = 0
            if isinstance(embed_spec, dict):
                spec_dim = int(embed_spec.get("dim", 0) or 0)

            vec["enabled"] = bool(vec_enabled)
            vec["status"] = vec_status
            if spec_dim > 0:
                vec["dims"] = spec_dim

            # Preserve backend/table fields if present
            if "table" not in vec:
                vec["table"] = "vec_items"
            if "backend" not in vec:
                vec["backend"] = "sqlite-vec"

            content_stats["vector_index"] = vec
            self.set_manifest("content_stats", content_stats)
        except Exception as e:
            # Non-fatal; validation should still return a report
            report["errors"].append(f"Failed to stamp vector_index status into manifest: {e}")
            report["health"] = "WARN_MANIFEST_STAMP_FAIL"
            
        return report

    def store_file(self, vfs_path: str, origin_path: str, content: str = None, blob: bytes = None, mime_type: str = "text/plain", origin_type: str = "filesystem"):
        """
        The Universal Input Method. 
        Stores raw data. If file exists, updates it and resets status to 'RAW' for re-refining.
        """
        conn = self._get_conn()
        try:
            conn.execute("""
                INSERT OR REPLACE INTO files 
                (vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, 'RAW', ?)
            """, (vfs_path, origin_path, origin_type, content, blob, mime_type, time.time()))
            conn.commit()
            return True
        except Exception as e:
            self.log_error(f"DB Store Error ({vfs_path}): {e}")
            return False
        finally:
            conn.close()

    def get_pending_files(self, limit: int = 10) -> List[Dict]:
        """Fetches files waiting for the Refinery."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        rows = conn.execute("SELECT * FROM files WHERE status = 'RAW' LIMIT ?", (limit,)).fetchall()
        conn.close()
        return [dict(row) for row in rows]

    def update_status(self, file_id: int, status: str, metadata: dict = None):
        conn = self._get_conn()
        if metadata:
            conn.execute("UPDATE files SET status = ?, metadata = ? WHERE id = ?", 
                         (status, json.dumps(metadata), file_id))
        else:
            conn.execute("UPDATE files SET status = ? WHERE id = ?", (status, file_id))
        conn.commit()
        conn.close()

    def ensure_directory(self, vfs_path: str):
        """Idempotent insert for VFS directories."""
        if not vfs_path: return
        parent = os.path.dirname(vfs_path).replace("\\", "/")
        if parent == vfs_path: parent = "" # Root case
        
        conn = self._get_conn()
        try:
            conn.execute("INSERT OR IGNORE INTO directories (vfs_path, parent_path) VALUES (?, ?)", (vfs_path, parent))
            conn.commit()
        except: pass
        finally:
            conn.close()

    # --- Agent-Friendly Helpers (No raw SQL required) ---
    def _coerce_bool(self, v: Any) -> bool:
        """Best-effort conversion for manifest values stored as strings."""
        if v is None:
            return False
        if isinstance(v, bool):
            return v
        s = str(v).strip().lower()
        return s in ("1", "true", "yes", "y", "on")

    def get_status_flags(self) -> Dict[str, Any]:
        """Returns key manifest status flags in a single call."""
        ingest_complete = self._coerce_bool(self.get_manifest("ingest_complete"))
        refine_complete = self._coerce_bool(self.get_manifest("refine_complete"))
        health = self.get_manifest("cartridge_health") or "UNKNOWN"
        return {
            "ingest_complete": ingest_complete,
            "refine_complete": refine_complete,
            "cartridge_health": health,
            "schema_name": self.get_manifest("schema_name") or "",
            "schema_version": self.get_manifest("schema_version") or "",
            "cartridge_id": self.get_manifest("cartridge_id") or ""
        }

    def list_files(self, prefix: str = "", status: Optional[str] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Enumerate files in the cartridge (optionally filtered by VFS prefix and/or status)."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            sql = "SELECT id, vfs_path, origin_path, origin_type, mime_type, status, last_updated, metadata FROM files"
            clauses = []
            params = []

            if prefix:
                # Prefix match on portable path
                clauses.append("vfs_path LIKE ?")
                params.append(prefix.rstrip("/") + "/%")

            if status:
                clauses.append("status = ?")
                params.append(status)

            if clauses:
                sql += " WHERE " + " AND ".join(clauses)

            sql += " ORDER BY vfs_path"

            if limit is not None:
                sql += " LIMIT ?"
                params.append(int(limit))

            rows = conn.execute(sql, tuple(params)).fetchall()
            out = []
            for r in rows:
                d = dict(r)
                # metadata is stored as JSON string
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    def get_file_record(self, vfs_path: str) -> Optional[Dict[str, Any]]:
        """Fetch a single file record by VFS path."""
        if not vfs_path:
            return None
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            row = conn.execute(
                "SELECT id, vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, metadata, last_updated FROM files WHERE vfs_path = ?",
                (vfs_path,)
            ).fetchone()
            if not row:
                return None
            d = dict(row)
            try:
                d["metadata"] = json.loads(d.get("metadata") or "{}")
            except Exception:
                d["metadata"] = {}
            return d
        finally:
            conn.close()

    def list_directories(self, prefix: str = "") -> List[Dict[str, Any]]:
        """Enumerate directories in the cartridge VFS."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            if prefix:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories WHERE vfs_path LIKE ? ORDER BY vfs_path",
                    (prefix.rstrip("/") + "/%",)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories ORDER BY vfs_path"
                ).fetchall()

            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    def get_directory_tree(self, root: str = "") -> Dict[str, Any]:
        """Builds a nested directory tree starting at `root` ("" for full tree)."""
        dirs = self.list_directories(prefix=root) if root else self.list_directories()
        files = self.list_files(prefix=root) if root else self.list_files()

        # Tree nodes are dicts: {"_dirs": {name: node}, "_files": [file_records...]}
        def new_node():
            return {"_dirs": {}, "_files": []}

        tree = new_node()

        # Insert directories
        for d in dirs:
            path = (d.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            cur = tree
            for p in parts:
                cur = cur["_dirs"].setdefault(p, new_node())

        # Insert files
        for f in files:
            path = (f.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            fname = parts[-1]
            cur = tree
            for p in parts[:-1]:
                cur = cur["_dirs"].setdefault(p, new_node())
            # Store a light file record for tree browsing
            cur["_files"].append({
                "name": fname,
                "vfs_path": f.get("vfs_path"),
                "mime_type": f.get("mime_type"),
                "origin_type": f.get("origin_type"),
                "status": f.get("status")
            })

        return tree

    def get_status_summary(self) -> Dict[str, Any]:
        """Counts files by status and provides a quick cartridge overview."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            rows = conn.execute("SELECT status, COUNT(*) as n FROM files GROUP BY status").fetchall()
            by_status = {r["status"]: r["n"] for r in rows}

            dcnt = conn.execute("SELECT COUNT(*) FROM directories").fetchone()[0]
            fcnt = conn.execute("SELECT COUNT(*) FROM files").fetchone()[0]
            ccnt = conn.execute("SELECT COUNT(*) FROM chunks").fetchone()[0]
            ncnt = conn.execute("SELECT COUNT(*) FROM graph_nodes").fetchone()[0]
            ecnt = conn.execute("SELECT COUNT(*) FROM graph_edges").fetchone()[0]

            return {
                "directories": int(dcnt),
                "files": int(fcnt),
                "chunks": int(ccnt),
                "graph_nodes": int(ncnt),
                "graph_edges": int(ecnt),
                "files_by_status": by_status,
                "flags": self.get_status_flags()
            }
        finally:
            conn.close()

    # --- Graph Helpers ---
    def add_node(self, node_id: str, node_type: str, label: str, data: dict = None):
        conn = self._get_conn()
        conn.execute("INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json) VALUES (?, ?, ?, ?)",
                     (node_id, node_type, label, json.dumps(data or {})))
        conn.commit()
        conn.close()

    def add_edge(self, source: str, target: str, relation: str = "related", weight: float = 1.0):
        conn = self._get_conn()
        conn.execute("INSERT OR IGNORE INTO graph_edges (source, target, relation, weight) VALUES (?, ?, ?, ?)",
                     (source, target, relation, weight))
        conn.commit()
        conn.close()

    # --- Vector Search ---
    def search_embeddings(self, query_vector: List[float], limit: int = 5) -> List[Dict]:
        """Performs semantic search using sqlite-vec."""
        if not sqlite_vec or not query_vector:
            return []

        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        results = []
        
        try:
            # Pack vector to binary if needed, but sqlite-vec usually handles raw lists in parameterized queries
            # dependent on the binding. We'll pass binary for safety if using standard bindings,
            # but typically raw list works with the extension's adapters. 
            # For now, we assume the extension handles the list->vector conversion.
            
            rows = conn.execute("""
                SELECT
                    rowid,
                    distance
                FROM vec_items
                WHERE embedding MATCH ?
                ORDER BY distance
                LIMIT ?
            """, (json.dumps(query_vector), limit)).fetchall()
            
            # Resolve back to chunks with VFS context
            for r in rows:
                chunk_id = r['rowid']
                # Join with files to get vfs_path
                query = """
                    SELECT c.*, f.vfs_path 
                    FROM chunks c 
                    JOIN files f ON c.file_id = f.id 
                    WHERE c.id=?
                """
                chunk = conn.execute(query, (chunk_id,)).fetchone()
                
                if chunk:
                    res = dict(chunk)
                    res['score'] = r['distance']
                    results.append(res)
                    
        except Exception as e:
            self.log_error(f"Vector Search Error: {e}")
        finally:
            conn.close()
            
        return results







--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\document_utils.py
--------------------------------------------------------------------------------
import io
import re
from typing import Tuple, Optional

# Third-party imports (from requirements.txt)
try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None

try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

def extract_text_from_pdf(file_bytes: bytes) -> str:
    """Extracts text from a PDF blob using pypdf."""
    if not PdfReader:
        return ""
    
    text_content = []
    try:
        # Wrap bytes in a stream for PdfReader
        stream = io.BytesIO(file_bytes)
        reader = PdfReader(stream)
        
        for page in reader.pages:
            extracted = page.extract_text()
            if extracted:
                text_content.append(extracted)
        
        return "\n".join(text_content)
    except Exception as e:
        print(f"[DocumentUtils] PDF Extraction Error: {e}")
        return ""

def extract_text_from_html(html_content: str) -> str:
    """Cleans HTML to raw text using BeautifulSoup."""
    if not BeautifulSoup:
        return strip_tags_regex(html_content)
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Kill all script and style elements
        for script in soup(["script", "style", "meta", "noscript"]):
            script.decompose()
            
        text = soup.get_text()
        
        # Collapse whitespace
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return text
    except Exception as e:
        print(f"[DocumentUtils] HTML Parsing Error: {e}")
        return strip_tags_regex(html_content)

def strip_tags_regex(html: str) -> str:
    """Fallback if BS4 is missing."""
    clean = re.compile('<.*?>')
    return re.sub(clean, '', html)

--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\graph_engine.py
--------------------------------------------------------------------------------
import pygame
import math
import random

# Initialize font module globally once
pygame.font.init()

class GraphRenderer:
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        self.width = width
        self.height = height
        self.bg_color = bg_color
        
        self.surface = pygame.Surface((width, height))
        
        # Camera
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction
        self.dragged_node_idx = None
        self.hovered_node_idx = None
        
        # Physics State
        self.settled = False

    def resize(self, width, height):
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    def set_data(self, nodes, links):
        self.nodes = nodes
        self.links = links
        self.settled = False # Wake up physics on new data
        
        # 1. Build an ID map so we can find parents
        node_map = {node['id']: node for node in self.nodes}

        for n in self.nodes:
            # GNN Injection: Use pre-calculated layout if available
            if 'gnn_x' in n and 'gnn_y' in n:
                n['x'] = n['gnn_x'] * self.width
                n['y'] = n['gnn_y'] * self.height

            elif 'x' not in n:
                # SMART SPAWN: If I am a satellite, spawn near my planet
                parent_id = n.get('meta', {}).get('parent')
                if parent_id and parent_id in node_map and 'x' in node_map[parent_id]:
                    p = node_map[parent_id]
                    angle = random.random() * 6.28
                    dist = 30
                    n['x'] = p['x'] + math.cos(angle) * dist
                    n['y'] = p['y'] + math.sin(angle) * dist
                else:
                    # Random spawn for Files
                    n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                    n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Semantic Coloring
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # Blue
                n['_radius'] = 6
            elif n.get('type') == 'web':
                n['_color'] = (204, 0, 122) # Purple/Pink
                n['_radius'] = 7
            elif n.get('type') == 'chunk':
                n['_color'] = (100, 200, 100) # Satellite Green
                n['_radius'] = 3
            else:
                n['_color'] = (160, 32, 240) # Default
                n['_radius'] = 6

    # --- INPUT HANDLING ---
    
    def screen_to_world(self, sx, sy):
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def get_node_at(self, sx, sy):
        wx, wy = self.screen_to_world(sx, sy)
        for n in self.nodes:
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                return n
        return None

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                self.dragged_node_idx = i
                self.settled = False # Wake up physics
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
            self.settled = False
        else:
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            return prev_hover != self.hovered_node_idx

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    def highlight_nodes(self, node_ids):
        """Highlights specific nodes by ID."""
        for n in self.nodes:
            # 1. Reset to defaults
            if n.get('type') == 'file': 
                n['_color'] = (0, 122, 204)
                n['_radius'] = 6
            elif n.get('type') == 'web': 
                n['_color'] = (204, 0, 122)
                n['_radius'] = 7
            elif n.get('type') == 'chunk': 
                n['_color'] = (100, 200, 100)
                n['_radius'] = 3
            else: 
                n['_color'] = (160, 32, 240)
                n['_radius'] = 6
                
            # 2. Apply Highlight
            if n['id'] in node_ids:
                n['_color'] = (255, 255, 0) # Bright Yellow
                n['_radius'] = 12
                
        self.settled = False # Wake up physics

    # --- PHYSICS (Damped) ---

    def step_physics(self):
        if not self.nodes or self.settled: return

        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.85 # Increased damping to settle faster
        
        cx, cy = self.width / 2, self.height / 2
        total_kinetic_energy = 0

        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue

            # LOD: Freeze satellites if zoomed out
            if self.zoom < 1.2 and a.get('type') == 'chunk':
                a['vx'] = 0
                a['vy'] = 0
                continue
            
            fx, fy = 0, 0
            
            # 1. Gravity (Center pull)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # 2. Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Performance opt: Ignore far away nodes
                if dist_sq > 25000: continue 

                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 3. Attraction (Links)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 4. Apply & Measure Energy
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']
            total_kinetic_energy += (abs(n['vx']) + abs(n['vy']))

        # 5. Sleep Threshold
        if total_kinetic_energy < 0.5:
            self.settled = True

    # --- RENDERING ---

    def get_image_bytes(self):
        self.surface.fill(self.bg_color)
        
        cx, cy = self.width / 2, self.height / 2
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # Links
        for u, v in self.links:
            if self.zoom < 1.2:
                if self.nodes[u].get('type') == 'chunk' or self.nodes[v].get('type') == 'chunk':
                    continue

            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # Nodes
        for i, n in enumerate(self.nodes):
            # LOD: Hide chunks if zoomed out
            if self.zoom < 1.2 and n.get('type') == 'chunk':
                continue

            sx, sy = to_screen(n['x'], n['y'])
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20: continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')



--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\graph_view.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import json
import os
from .graph_engine import GraphRenderer

class GraphView(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.pack(fill="both", expand=True)
        
        # Search Overlay
        self.controls = tk.Frame(self, bg="#101018")
        self.controls.pack(fill="x", side="top", padx=5, pady=5)
        
        self.entry_search = tk.Entry(self.controls, bg="#252526", fg="white", insertbackground="white", font=("Consolas", 10))
        self.entry_search.pack(side="left", fill="x", expand=True, padx=(0, 5))
        self.entry_search.bind("<Return>", self.run_search)
        
        btn = tk.Button(self.controls, text="NEURAL TEST", command=self.run_search, bg="#007ACC", fg="white", relief="flat")
        btn.pack(side="right")

        # UI Container
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Services
        self.cartridge = None
        self.neural = None
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None 
        
        # Input State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        self.is_panning = False

        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<Double-Button-1>', self.on_double_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Start the Heartbeat
        self.animate()

    def bind_services(self, cartridge, neural):
        self.cartridge = cartridge
        self.neural = neural

    def run_search(self, event=None):
        if not self.cartridge or not self.neural:
            return
            
        query = self.entry_search.get().strip()
        if not query: return
        
        # 1. Embed
        vec = self.neural.get_embedding(query)
        if not vec: return
        
        # 2. Search
        results = self.cartridge.search_embeddings(vec, limit=5)
        
        # 3. Resolve IDs for Graph
        # Graph Node ID format: "{vfs_path}::{chunk_name}"
        ids = set()
        for r in results:
            if 'vfs_path' in r and 'name' in r:
                ids.add(f"{r['vfs_path']}::{r['name']}")
                
        # 4. Highlight
        self.engine.highlight_nodes(ids)

    def load_from_db(self, db_path):
        """
        Loads graph data from SQLite.
        Does NOT block the UI. The physics engine will settle the nodes frame-by-frame.
        """
        if not os.path.exists(db_path): return
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # [cite_start]Fetch Nodes [cite: 198]
            db_nodes = cursor.execute("SELECT id, type, label, data_json FROM graph_nodes").fetchall()
            
            # [cite_start]Fetch Edges [cite: 198]
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
            
            conn.close()
        except Exception as e:
            print(f"Graph Load Error: {e}")
            return

        # Format for Engine
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label, raw_json = row
            meta = {}
            try:
                if raw_json: meta = json.loads(raw_json)
            except: pass
            
            id_to_index[node_id] = idx
            formatted_nodes.append({'id': node_id, 'type': n_type, 'label': label, 'meta': meta})

        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                formatted_links.append((id_to_index[src], id_to_index[tgt]))

        # Inject Data - The Physics Engine handles the "Explosion" logic internally
        self.engine.set_data(formatted_nodes, formatted_links)

    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_double_click(self, event):
        # Zoom in on the node we clicked
        hit_node = self.engine.get_node_at(event.x, event.y)
        if hit_node:
            # Center camera on node and zoom in
            self.engine.cam_x = hit_node['x']
            self.engine.cam_y = hit_node['y']
            self.engine.zoom = 2.0
            self.engine.settled = False

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        if hit:
            self.is_dragging_node = True
        else:
            self.is_panning = True

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False
        self.is_panning = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        elif self.is_panning:
            # Camera Pan
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)
        self.engine.settled = False # Wake up physics on zoom

    def on_windows_scroll(self, event):
        if event.delta > 0: self.on_zoom(1.1)
        else: self.on_zoom(0.9)

    def animate(self):
        """
        The Heartbeat Loop. 
        Runs at ~30 FPS. Handles Physics + Rendering.
        """
        # 1. Step Physics (Micro-calculations)
        self.engine.step_physics()
        
        # 2. Render to Buffer
        raw_data = self.engine.get_image_bytes()
        
        # 3. Blit to Screen
        if raw_data:
            img = Image.frombytes('RGB', (self.engine.width, self.engine.height), raw_data)
            self.photo = ImageTk.PhotoImage(img)
            self.canvas_lbl.configure(image=self.photo)
        
        # 4. Loop
        self.after(30, self.animate)

--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\intake_service.py
--------------------------------------------------------------------------------
import os
import mimetypes
import requests
import fnmatch
import json
from pathlib import Path
from typing import Dict, Set, List, Any
from .base_service import BaseService
from .cartridge_service import CartridgeService
from .scanner import ScannerMS
from . import document_utils

# Optional import for Web
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

class IntakeService(BaseService):
    """
    The Vacuum. 
    Now supports two-phase ingestion:
    1. Scan -> Build Tree (with .gitignore respect)
    2. Ingest -> Process selected paths
    """

    DEFAULT_IGNORE_DIRS = {
        '.git', '__pycache__', 'node_modules', 'venv', '.venv', 'env', '.env', 
        '.idea', '.vscode', 'dist', 'build', 'target', 'bin', 'obj', 
        '__cartridge__'
    }
    
    DEFAULT_IGNORE_EXTS = {
        '.pyc', '.pyd', '.exe', '.dll', '.so', '.db', '.sqlite', '.sqlite3', 
        '.bin', '.iso', '.img', '.zip', '.tar', '.gz', '.7z', '.jpg', '.png'
    }

    def __init__(self, cartridge: CartridgeService):
        super().__init__("IntakeService")
        self.cartridge = cartridge
        self.ignore_patterns: Set[str] = set()

    def ingest_source(self, source_path: str) -> Dict[str, int]:
        """Headless/CLI Entry point: Scans and Ingests in one go."""
        self.cartridge.initialize_manifest()
        
        # Update Manifest source info
        self.cartridge.set_manifest("source_root", source_path)
        
        is_web = source_path.startswith("http")
        self.cartridge.set_manifest("source_type", "web_root" if is_web else "filesystem_dir")

        scanner = ScannerMS()
        tree_node = scanner.scan_directory(source_path, web_depth=1 if is_web else 0)
        
        if not tree_node:
             return {"error": "Source not found"}

        # Flatten tree to list of paths
        files_to_ingest = scanner.flatten_tree(tree_node)
        self.cartridge.set_manifest("ingest_config", {"auto_flattened": True, "count": len(files_to_ingest)})
        
        return self.ingest_selected(files_to_ingest, source_path)

    # --- PHASE 1: SCANNING ---

    def scan_path(self, root_path: str, web_depth: int = 0) -> Dict[str, Any]:
        """
        Unified Scanner Interface.
        Delegates to ScannerMS for both Web and Local FS to ensure consistent node structure.
        """
        scanner = ScannerMS()

        # 1. Delegate to Scanner
        tree_root = scanner.scan_directory(root_path, web_depth=web_depth)
        if not tree_root: return None

        # 2. Apply Persistence / Checked State
        # (We only do this for FS usually, but we can try for web if we had it)
        if not root_path.startswith("http"):
            saved_config = self._load_persistence(os.path.abspath(root_path))
            self._apply_persistence(tree_root, saved_config)
    
        return tree_root

    def _apply_persistence(self, node: Dict, saved_config: Dict):
        """Recursively applies checked state from saved config."""
        if 'rel_path' in node and node['rel_path'] in saved_config:
            node['checked'] = saved_config[node['rel_path']]
        elif 'children' in node:
            # Default check all if no config? Or check logic from before?
            pass
    
        if 'children' in node:
            for child in node['children']:
                self._apply_persistence(child, saved_config)

    def _scan_recursive(self, current_path: str, root_path: str, saved_config: Dict) -> Dict:
        name = os.path.basename(current_path)
        is_dir = os.path.isdir(current_path)
        rel_path = os.path.relpath(current_path, root_path).replace("\\", "/")
        
        node = {
            'name': name,
            'path': current_path,
            'rel_path': rel_path,
            'type': 'dir' if is_dir else 'file',
            'children': [],
            'checked': True
        }

        # Determine Check State
        if saved_config and rel_path in saved_config:
            # Respect user persistence
            node['checked'] = saved_config[rel_path]
        elif self._is_ignored(name) or (not is_dir and self._is_binary_ext(name)):
            # Default to unchecked if ignored
            node['checked'] = False

        if is_dir:
            try:
                with os.scandir(current_path) as it:
                    entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                    for entry in entries:
                        child = self._scan_recursive(entry.path, root_path, saved_config)
                        node['children'].append(child)
            except PermissionError:
                pass
        
        return node

    # --- PHASE 2: INGESTION ---

    def ingest_selected(self, file_list: List[str], root_path: str) -> Dict[str, int]:
        """Ingests only the specific files passed in the list."""
        stats = {"added": 0, "skipped": 0, "errors": 0}
        
        for file_path in file_list:
            try:
                # Calculate VFS Path
                try:
                    vfs_path = os.path.relpath(file_path, root_path).replace("\\", "/")
                except ValueError:
                    vfs_path = os.path.basename(file_path)

                self._read_and_store(Path(file_path), vfs_path, "filesystem", stats)
            except Exception as e:
                self.log_error(f"Error ingesting {file_path}: {e}")
                stats["errors"] += 1
        
        # --- POST-INGESTION: Update Manifest ---
        self._rebuild_directory_index()
        
        return stats

    def _rebuild_directory_index(self):
        """
        Scans 'files' table and populates 'directories' table.
        This creates the navigable VFS structure.
        """
        self.log_info("Rebuilding VFS Directory Index...")
        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            seen_dirs = set()
            
            for r in rows:
                path = r[0]
                # Walk up the path to register all parents
                current = os.path.dirname(path).replace("\\", "/")
                while current and current != "." and current not in seen_dirs:
                    self.cartridge.ensure_directory(current)
                    seen_dirs.add(current)
                    current = os.path.dirname(current).replace("\\", "/")
            
        except Exception as e:
            self.log_error(f"Directory Index Error: {e}")
        finally:
            conn.close()

    # --- HELPERS ---

    def _load_persistence(self, root_path: str) -> Dict[str, bool]:
        """Loads config from DB Manifest (Portable) or fallback to local."""
        # 1. Try DB Manifest
        try:
            conn = self.cartridge._get_conn()
            row = conn.execute("SELECT value FROM manifest WHERE key='ingest_config'").fetchone()
            conn.close()
            if row:
                return json.loads(row[0])
        except: pass
        
        # 2. Fallback to local (Legacy)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        if os.path.exists(cfg_path):
            try:
                with open(cfg_path, 'r') as f: return json.load(f)
            except: pass
        return {}

    def save_persistence(self, root_path: str, checked_map: Dict[str, bool]):
        """Saves user selections into the Cartridge Manifest (Portable)."""
        # 1. Save to DB
        try:
            conn = self.cartridge._get_conn()
            conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", 
                         ("ingest_config", json.dumps(checked_map)))
            conn.commit()
            conn.close()
        except Exception as e:
            self.log_error(f"Failed to save persistence to DB: {e}")

        # 2. Save local backup (Optional, keeps scan state if DB is deleted)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        try:
            with open(cfg_path, 'w') as f: json.dump(checked_map, f, indent=2)
        except: pass

    def _load_gitignore(self, root_path: str):
        gitignore_path = os.path.join(root_path, '.gitignore')
        self.ignore_patterns = self.DEFAULT_IGNORE_DIRS.copy()
        if os.path.exists(gitignore_path):
            try:
                with open(gitignore_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            if line.endswith('/'): line = line[:-1]
                            self.ignore_patterns.add(line)
            except: pass

    def _is_ignored(self, name: str) -> bool:
        if name in self.ignore_patterns: return True
        for pattern in self.ignore_patterns:
            if fnmatch.fnmatch(name, pattern): return True
        return False

    def _is_binary_ext(self, name: str) -> bool:
        _, ext = os.path.splitext(name)
        return ext.lower() in self.DEFAULT_IGNORE_EXTS

    def _read_and_store(self, real_path: Path, vfs_path: str, origin_type: str, stats: Dict):
        mime_type, _ = mimetypes.guess_type(real_path)
        if not mime_type: mime_type = "application/octet-stream"
        
        content = None
        blob = None
        
        # 1. Try Binary Read First (Covers PDF/Images/Safe Read)
        try:
            with open(real_path, 'rb') as f:
                blob = f.read()
        except Exception as e:
            self.log_error(f"Read error {real_path}: {e}")
            stats["errors"] += 1
            return

        # 2. Text Extraction / Decoding Strategy
        lower_path = str(real_path).lower()
        
        if lower_path.endswith(".pdf"):
            # PDF: Extract text, keep blob
            content = document_utils.extract_text_from_pdf(blob)
            if not content: mime_type = "application/pdf" # Fallback if extraction fails
            
        elif lower_path.endswith(".html") or lower_path.endswith(".htm"):
            # HTML: Decode and Clean
            try:
                raw_text = blob.decode('utf-8', errors='ignore')
                content = document_utils.extract_text_from_html(raw_text)
            except: pass
            
        else:
            # Default: Try UTF-8 Decode
            try:
                content = blob.decode('utf-8')
            except UnicodeDecodeError:
                content = None # Leave as binary blob

        # 3. Store in Cartridge
        # If content is set, it will be chunked/indexed. If only blob, it's stored but skipped by refinery.
        success = self.cartridge.store_file(
            vfs_path, 
            str(real_path), 
            content=content, 
            blob=blob, 
            mime_type=mime_type, 
            origin_type=origin_type
        )
        
        if success: stats["added"] += 1
        else: stats["errors"] += 1
--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\neural_service.py
--------------------------------------------------------------------------------
import requests
import json
import concurrent.futures
from typing import Optional, Dict, Any, List
from .base_service import BaseService

# Configuration constants
OLLAMA_API_URL = "http://localhost:11434/api"

class NeuralService(BaseService):
    def __init__(self, max_workers: int = 4):
        super().__init__("NeuralService")
        self.max_workers = max_workers
        # Default configs
        self.config = {
            "fast": "qwen2.5-coder:1.5b-cpu",
            "smart": "qwen2.5:3b-cpu",
            "embed": "mxbai-embed-large:latest-cpu"
        }

    def update_models(self, fast_model: str, smart_model: str, embed_model: str):
        """Called by the UI Settings Modal to change models on the fly."""
        self.config["fast"] = fast_model
        self.config["smart"] = smart_model
        self.config["embed"] = embed_model
        self.log_info(f"Models Updated: Fast={fast_model}, Smart={smart_model}")

    def get_available_models(self) -> List[str]:
        """Fetches list from Ollama for the UI dropdown."""
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            if res.status_code == 200:
                return [m['name'] for m in res.json().get('models', [])]
        except:
            return []
        return []

    def check_connection(self) -> bool:
        """Pings Ollama to see if it's alive."""
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except requests.RequestException:
            self.log_error("Ollama connection failed. Is 'ollama serve' running?")
            return False

    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Generates a vector using the CPU embedder."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.config["embed"], "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            self.log_error(f"Embedding failed: {e}")
        return None

    def request_inference(self, prompt: str, tier: str = "fast", format_json: bool = False) -> str:
        """
        Synchronous inference request.
        tier: 'fast' (1.5b-cpu), 'smart' (3b-cpu), or 'architect' (7b-gpu)
        """
        model = self.config.get(tier, self.config["fast"])
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        if format_json:
            payload["format"] = "json"

        try:
            res = requests.post(f"{OLLAMA_API_URL}/generate", json=payload, timeout=60)
            if res.status_code == 200:
                return res.json().get("response", "").strip()
        except Exception as e:
            self.log_error(f"Inference ({tier}) failed: {e}")
        return ""

    def process_parallel(self, items: List[Any], worker_func) -> List[Any]:
        """
        Helper to run a function across many items using the ThreadPool.
        Useful for batch ingestion.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # worker_func should take a single item and return a result
            futures = {executor.submit(worker_func, item): item for item in items}
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    self.log_error(f"Worker task failed: {e}")
        return results


--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\panels.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext, simpledialog
import os
import time
import sqlite3
import threading
import time

# --- UI CONSTANTS ---
BG_COLOR = "#1e1e2f"
SIDEBAR_COLOR = "#171725"
ACCENT_COLOR = "#007ACC"
TEXT_COLOR = "#e0e0e0"
SUCCESS_COLOR = "#388E3C"

class SystemLog(tk.Frame):
    """A read-only scrolling log for system events."""
    def __init__(self, parent):
        super().__init__(parent, bg=BG_COLOR)
        tk.Label(self, text="SYSTEM LOG", bg=BG_COLOR, fg="#666", font=("Consolas", 9, "bold")).pack(anchor="w", padx=5, pady=(5,0))
        self.text = scrolledtext.ScrolledText(self, bg="#151515", fg="#00FF00", font=("Consolas", 9), height=8, bd=0)
        self.text.pack(fill="both", expand=True, padx=5, pady=5)
        self.text.config(state="disabled")

    def log(self, msg):
        self.text.config(state="normal")
        ts = time.strftime("%H:%M:%S")
        self.text.insert("end", f"[{ts}] {msg}\n")
        self.text.see("end")
        self.text.config(state="disabled")

class IngestToolbar(tk.Frame):
    """Top bar for selecting source and triggering actions."""
    def __init__(self, parent, on_scan, on_ingest):
        super().__init__(parent, bg=BG_COLOR, pady=5)
        self.on_scan = on_scan
        self.on_ingest = on_ingest
        self.path_var = tk.StringVar()

        # Label
        tk.Label(self, text="SOURCE:", bg=BG_COLOR, fg="#888", font=("Arial", 9, "bold")).pack(side="left", padx=(10, 5))
        
        # Entry
        self.entry = tk.Entry(self, textvariable=self.path_var, bg="#252526", fg="white", 
                              insertbackground="white", relief="flat", font=("Consolas", 10))
        self.entry.pack(side="left", fill="x", expand=True, padx=5, ipady=4)

        # Options
        tk.Label(self, text="Depth:", bg=BG_COLOR, fg="#666", font=("Arial", 8)).pack(side="left")
        self.spin_depth = tk.Spinbox(self, from_=0, to=5, width=3, bg="#333", fg="white", relief="flat")
        self.spin_depth.pack(side="left", padx=2)
        
        self.combo_policy = ttk.Combobox(self, values=["Extract Text", "Store Blob", "Skip Binary"], width=12, state="readonly")
        self.combo_policy.set("Extract Text")
        self.combo_policy.pack(side="left", padx=5)

        # Buttons
        btn_cfg = {"bg": "#444", "fg": "white", "relief": "flat", "padx": 10, "font": ("Arial", 9)}
        
        tk.Button(self, text="üìÑ File", command=self._browse_file, **btn_cfg).pack(side="left", padx=2)
        tk.Button(self, text="üìÇ Folder", command=self._browse_folder, **btn_cfg).pack(side="left", padx=2)
        
        # Separator
        tk.Frame(self, width=1, bg="#555").pack(side="left", fill="y", padx=10, pady=5)

        tk.Button(self, text="üîç SCAN", command=self._trigger_scan, **btn_cfg).pack(side="left", padx=2)
        
        # Ingest (Green)
        ing_cfg = btn_cfg.copy()
        ing_cfg["bg"] = SUCCESS_COLOR
        ing_cfg["font"] = ("Arial", 9, "bold")
        tk.Button(self, text="‚ñ∂ INGEST", command=self._trigger_ingest, **ing_cfg).pack(side="left", padx=(10, 10))

    def _browse_folder(self):
        d = filedialog.askdirectory()
        if d:
            self.path_var.set(d)
            self._trigger_scan()

    def _browse_file(self):
        f = filedialog.askopenfilename()
        if f:
            self.path_var.set(f)
            self._trigger_scan()

    def _trigger_scan(self):
        path = self.path_var.get().strip()
        try:
            depth = int(self.spin_depth.get())
        except: depth = 0
        
        policy = self.combo_policy.get()
        if path: self.on_scan(path, web_depth=depth, binary_policy=policy)

    def _trigger_ingest(self):
        self.on_ingest()


class FileTreePanel(tk.Frame):
    """Hierarchy Explorer with Checkboxes."""
    def __init__(self, parent, intake_service):
        super().__init__(parent, bg=BG_COLOR)
        self.intake = intake_service
        self.root_path = None
        self.node_map = {}
        
        # --- UI ASSETS: Programmatic Icons ---
        # Create 16x16 checkboxes using empty PhotoImages and coloring pixels (or simple rectangles)
        # 1. Unchecked (Gray outline)
        self.img_off = tk.PhotoImage(width=16, height=16)
        self.img_off.put(("#666",), to=(2, 2, 14, 14))   # Border
        self.img_off.put(("#1e1e2f",), to=(4, 4, 12, 12)) # Center (BG Color)
        
        # 2. Checked (Green Fill)
        self.img_on = tk.PhotoImage(width=16, height=16)
        self.img_on.put(("#388E3C",), to=(2, 2, 14, 14)) # Green Box
        self.img_on.put(("#ffffff",), to=(5, 7, 7, 12))  # Checkmark (simple L shape)
        self.img_on.put(("#ffffff",), to=(7, 10, 11, 7)) 

        # Header
        tk.Label(self, text="HIERARCHY EXPLORER", bg=BG_COLOR, fg="#666", 
                 font=("Consolas", 9, "bold")).pack(anchor="w", padx=5, pady=(5,0))

        # Tree (Inherits 'Treeview' style from app.py)
        self.tree = ttk.Treeview(self, show="tree", selectmode="none")
        self.tree.pack(fill="both", expand=True, padx=5, pady=5)
        self.tree.bind("<Button-1>", self._on_tree_click)
        
        sb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        sb.place(relx=1, rely=0, relheight=1, anchor="ne")
        self.tree.configure(yscrollcommand=sb.set)

    def load_tree(self, path, web_depth=0):
        self.root_path = path
        self.tree.delete(*self.tree.get_children())
        self.node_map = {}
        # Pass web_depth to scanner
        tree_data = self.intake.scan_path(path, web_depth=web_depth)
        if tree_data:
            self._insert_node("", tree_data)

    def _insert_node(self, parent_id, node):
        # Use the image property for the checkbox, keep text clean for the name
        img = self.img_on if node['checked'] else self.img_off
        
        item_id = self.tree.insert(parent_id, "end", text=f" {node['name']}", image=img, open=(parent_id==""))
        self.node_map[item_id] = node
        
        # Safely handle children
        children = node.get('children', [])
        for child in children:
            self._insert_node(item_id, child)

    def _on_tree_click(self, event):
        item_id = self.tree.identify_row(event.y)
        if not item_id: return
        
        # Identify what part of the item was clicked
        element = self.tree.identify_element(event.x, event.y)
        
        # Case 1: Clicked the Checkbox (Image) -> Toggle Selection
        if element == "image":
            self._toggle_check(item_id)
            return "break"
            
        # Case 2: Clicked the Row/Text -> Toggle Expand (if folder)
        else:
            # If it has children, toggle the open state
            if self.tree.get_children(item_id) or self.node_map[item_id]['type'] in ['dir', 'folder']:
                current_state = self.tree.item(item_id, "open")
                self.tree.item(item_id, open=not current_state)
                return "break"
            # If file, do nothing (or select row standard behavior)
            return

    def _toggle_check(self, item_id):
        node = self.node_map[item_id]
        new_state = not node['checked']
        self._set_node_state(item_id, new_state)
        self._save_config()

    def _set_node_state(self, item_id, state):
        node = self.node_map[item_id]
        node['checked'] = state
        
        # Update image only
        img = self.img_on if state else self.img_off
        self.tree.item(item_id, image=img)
        
        for child_id in self.tree.get_children(item_id):
            self._set_node_state(child_id, state)

    def _save_config(self):
        if not self.root_path or os.path.isfile(self.root_path): return
        flat_config = {n['rel_path']: n['checked'] for n in self.node_map.values()}
        self.intake.save_persistence(self.root_path, flat_config)

    def get_selected_files(self):
        selected = []
        for node in self.node_map.values():
            if node['checked'] and (node['type'] == 'file' or node['type'] == 'web'):
                selected.append(node['path'])
        return selected

class Sidebar(tk.Frame):
    """
    Manages the list of available Cartridges (.db files).
    """
    def __init__(self, parent, storage_dir, on_select_callback):
        super().__init__(parent, bg=SIDEBAR_COLOR, width=220)
        self.storage_dir = storage_dir
        self.on_select = on_select_callback
        self.pack_propagate(False)

        # Header
        tk.Label(self, text="_RagFORGE", bg=SIDEBAR_COLOR, fg=ACCENT_COLOR, 
                 font=("Consolas", 14, "bold"), pady=15).pack(fill="x")
        
        # List
        tk.Label(self, text="CARTRIDGES", bg=SIDEBAR_COLOR, fg="#666", 
                 font=("Arial", 8, "bold"), anchor="w", padx=10).pack(fill="x")
        
        self.listbox = tk.Listbox(self, bg=SIDEBAR_COLOR, fg=TEXT_COLOR, bd=0, 
                                  highlightthickness=0, selectbackground=ACCENT_COLOR)
        self.listbox.pack(fill="both", expand=True, padx=5, pady=5)
        self.listbox.bind("<<ListboxSelect>>", self._on_click)

        # Footer Actions
        btn_frame = tk.Frame(self, bg=SIDEBAR_COLOR, pady=10)
        btn_frame.pack(fill="x", side="bottom")
        
        row1 = tk.Frame(btn_frame, bg=SIDEBAR_COLOR)
        row1.pack(fill="x", pady=2)
        tk.Button(row1, text="üìÇ ROOT", bg="#2d2d44", fg="#aaa", relief="flat", font=("Arial", 8), 
                  command=self._change_root).pack(side="left", padx=10, fill="x", expand=True)
        
        row2 = tk.Frame(btn_frame, bg=SIDEBAR_COLOR)
        row2.pack(fill="x", pady=2)
        tk.Button(row2, text="REFRESH", bg="#2d2d44", fg="white", relief="flat", 
                  command=self.refresh).pack(side="left", padx=10, fill="x", expand=True)
        tk.Button(row2, text="+ NEW", bg="#2d2d44", fg="white", relief="flat", 
                  command=self._create_new).pack(side="right", padx=10, fill="x", expand=True)

        self.refresh()

    def _change_root(self):
        d = filedialog.askdirectory(title="Select Cartridge Library")
        if d:
            self.storage_dir = d
            self.refresh()

    def refresh(self):
        self.listbox.delete(0, tk.END)
        if not os.path.exists(self.storage_dir):
            os.makedirs(self.storage_dir)
        
        dbs = [f for f in os.listdir(self.storage_dir) if f.endswith(".db")]
        for db in dbs:
            self.listbox.insert(tk.END, db)

    def _on_click(self, event):
        sel = self.listbox.curselection()
        if sel:
            db_name = self.listbox.get(sel[0])
            self.on_select(os.path.join(self.storage_dir, db_name))

    def _create_new(self):
        name = simpledialog.askstring("New Cartridge", "Enter name (e.g. 'project_alpha'):")
        if name:
            if not name.endswith(".db"): name += ".db"
            path = os.path.join(self.storage_dir, name)
            # Just touching the file is enough, CartridgeService will init schema on load
            sqlite3.connect(path).close()
            self.refresh()

class EditorPanel(tk.Frame):
    """
    Views the 'files' table content.
    """
    def __init__(self, parent):
        super().__init__(parent, bg=BG_COLOR)
        self.active_db = None
        
        paned = ttk.PanedWindow(self, orient="horizontal")
        paned.pack(fill="both", expand=True)
        
        # Left: File Tree
        self.tree = ttk.Treeview(paned, show="tree", selectmode="browse")
        self.tree.heading("#0", text="VFS Path", anchor="w")
        self.tree.bind("<<TreeviewSelect>>", self._on_select)
        paned.add(self.tree, weight=1)
        
        # Right: Content
        self.editor = scrolledtext.ScrolledText(paned, bg="#252526", fg=TEXT_COLOR, 
                                                font=("Consolas", 10), insertbackground="white")
        paned.add(self.editor, weight=3)

    def load_db(self, db_path):
        self.active_db = db_path
        self.refresh_list()

    def refresh_list(self):
        self.tree.delete(*self.tree.get_children())
        self.editor.delete("1.0", tk.END)
        
        if not self.active_db or not os.path.exists(self.active_db): return
        
        try:
            conn = sqlite3.connect(self.active_db)
            rows = conn.execute("SELECT id, vfs_path, status FROM files ORDER BY vfs_path").fetchall()
            conn.close()
            
            for rid, path, status in rows:
                display = f"[{status}] {path}"
                self.tree.insert("", "end", iid=rid, text=display, values=(path,))
        except Exception as e:
            print(f"Tree load error: {e}")

    def _on_select(self, event):
        sel = self.tree.selection()
        if not sel: return
        file_id = sel[0]
        
        try:
            conn = sqlite3.connect(self.active_db)
            row = conn.execute("SELECT content FROM files WHERE id=?", (file_id,)).fetchone()
            conn.close()
            
            self.editor.delete("1.0", tk.END)
            if row and row[0]:
                self.editor.insert("1.0", row[0])
            else:
                self.editor.insert("1.0", "(Binary content or empty)")
        except: pass







--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\refinery_service.py
--------------------------------------------------------------------------------
import json
import re
import os
import ast
import concurrent.futures
from typing import Dict, List, Any, Optional, Tuple
from .base_service import BaseService
from .cartridge_service import CartridgeService
from .neural_service import NeuralService
from .semantic_chunker import SemanticChunker

class RefineryService(BaseService):
    """
    The Night Shift.
    Polls the DB for 'RAW' files and processes them into Chunks and Graph Nodes.

    Graph Enrichment:
    - Code: function/class nodes, resolved import edges when possible.
    - Docs: section/chapter nodes for long-form text (md/txt/rst).
    """

    def __init__(self, cartridge: CartridgeService, neural: NeuralService):
        super().__init__("RefineryService")
        self.cartridge = cartridge
        self.neural = neural
        self.chunker = SemanticChunker()

        # --- Spec Enforcement ---
        # Update the cartridge manifest to reflect the ACTUAL tools we are using.
        self._stamp_specs()

        # Import parsing / resolution
        # Regex remains as a fallback (JS + non-parseable cases)
        self.import_pattern = re.compile(r"""(?:from|import)\s+([\w\.]+)|require\(['"]([\w\.\-/]+)['"]\)""")

        # Lightweight module/path index cache for resolving imports to VFS files
        self._module_index: Dict[str, str] = {}
        self._path_index: Dict[str, str] = {}
        self._index_built: bool = False

        # Simple section/chapter detection
        self._md_heading = re.compile(r"^(#{1,6})\s+(.+?)\s*$")
        self._chapter_heading = re.compile(r"^\s*(chapter|CHAPTER)\s+([0-9]+|[IVXLC]+)\b\s*[:\-]?\s*(.*)$")

    def _stamp_specs(self):
        """Writes the active Neural/Chunker configuration to the Manifest."""
        try:
            # 1. Embedding Spec
            # We assume 1024 dim for mxbai-large, but ideally we'd probe it.
            embed_model = self.neural.config["embed"]
            spec = {
                "provider": "ollama",
                "model": embed_model,
                "dim": 1024,  # Hardcoded for now based on mxbai-embed-large
                "dtype": "float32",
                "distance": "cosine"
            }
            self.cartridge.set_manifest("embedding_spec", spec)

            # 2. Chunking Spec
            # (We could expose chunker config params here if they were dynamic)

        except Exception as e:
            self.log_error(f"Failed to stamp specs: {e}")

    def _build_import_index(self):
        """Builds caches for resolving imports to VFS targets.

        - _path_index: exact VFS path -> VFS path
        - _module_index: python module name (a.b.c) -> VFS path (a/b/c.py, a/b/c/__init__.py)
        """
        if self._index_built:
            return

        path_index: Dict[str, str] = {}
        module_index: Dict[str, str] = {}

        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            for (vp,) in rows:
                if not vp:
                    continue
                vfs_path = str(vp).replace("\\", "/")
                path_index[vfs_path] = vfs_path

                # Python module mapping
                if vfs_path.endswith(".py"):
                    mod = vfs_path[:-3].strip("/")
                    mod = mod.replace("/", ".")
                    if mod:
                        module_index[mod] = vfs_path

                    # If it's a package __init__.py, map the package name too
                    if vfs_path.endswith("/__init__.py"):
                        pkg = vfs_path[:-len("/__init__.py")].strip("/").replace("/", ".")
                        if pkg:
                            module_index[pkg] = vfs_path

        finally:
            conn.close()

        self._path_index = path_index
        self._module_index = module_index
        self._index_built = True

    def process_pending(self, batch_size: int = 5) -> int:
        """Main loop. Returns number of files processed."""
        pending = self.cartridge.get_pending_files(limit=batch_size)
        if not pending:
            return 0

        self.log_info(f"Refining batch of {len(pending)} files...")

        for file_row in pending:
            self._refine_file(file_row)

        return len(pending)

    def _refine_file(self, row: Dict):
        file_id = row['id']
        vfs_path = row['vfs_path']
        content = row['content']

        # Skip binary files for now (unless we add OCR later)
        if not content:
            self.cartridge.update_status(file_id, "SKIPPED_BINARY")
            return

        try:
            # 1. Semantic Chunking
            chunks = self.chunker.chunk_file(content, vfs_path)

            # 2. Vectorization & Storage
            chunk_texts = [c.content for c in chunks]

            # Buffer graph writes while DB transaction is open (prevents nested-writer locks)
            pending_nodes: List[Tuple[str, str, str, Dict[str, Any]]] = []
            pending_edges: List[Tuple[str, str, str, float]] = []

            # Use ThreadPool to embed in parallel (preserve order with map)
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.neural.max_workers) as executor:
                vectors = list(executor.map(self.neural.get_embedding, chunk_texts))

            conn = self.cartridge._get_conn()
            try:
                cursor = conn.cursor()

                for i, chunk in enumerate(chunks):
                    vector = vectors[i]
                    vec_blob = json.dumps(vector).encode('utf-8') if vector else None

                    # Store Chunk
                    cursor.execute(
                        """
                        INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (file_id, i, chunk.content, vec_blob, chunk.name, chunk.type, chunk.start_line, chunk.end_line)
                    )

                    chunk_row_id = cursor.lastrowid

                    # Insert into Vector Index (if vector exists)
                    if vector:
                        try:
                            cursor.execute(
                                "INSERT INTO vec_items(rowid, embedding) VALUES (?, ?)",
                                (chunk_row_id, json.dumps(vector))
                            )
                        except Exception as ve:
                            self.log_error(f"Vector Index Insert Failed: {ve}")

                    # Graph Node for Chunks (Functions/Classes)
                    # IMPORTANT: Do NOT call CartridgeService.add_node/add_edge while this conn is open.
                    if chunk.type in ['class', 'function']:
                        node_id = f"{vfs_path}::{chunk.name}"
                        pending_nodes.append(
                            (
                                node_id,
                                'chunk',
                                chunk.name,
                                {
                                    'parent': vfs_path,
                                    'file_id': file_id,
                                    'chunk_row_id': chunk_row_id,
                                    'chunk_type': chunk.type,
                                    'start_line': chunk.start_line,
                                    'end_line': chunk.end_line
                                }
                            )
                        )
                        pending_edges.append((node_id, vfs_path, "defined_in", 1.0))

                conn.commit()

            finally:
                conn.close()

            # 3. File Level Graph Node (after close)
            pending_nodes.append((vfs_path, 'file', vfs_path.split('/')[-1], {'path': vfs_path, 'file_id': file_id}))

            # 4. Section/Chapter Weaving (docs)
            self._weave_sections(vfs_path, content)

            # 5. Import Weaving (resolved when possible)
            self._weave_imports(vfs_path, content)

            # 6. Flush buffered graph writes
            for nid, ntype, label, data in pending_nodes:
                self.cartridge.add_node(nid, ntype, label, data)
            for src, tgt, rel, w in pending_edges:
                self.cartridge.add_edge(src, tgt, rel, w)

            # 7. Mark file refined
            self.cartridge.update_status(file_id, "REFINED")

        except Exception as e:
            self.log_error(f"Refining failed for {vfs_path}: {e}")
            self.cartridge.update_status(file_id, "ERROR", {"error": str(e)})

    def _extract_imports_python(self, source_path: str, content: str) -> List[Tuple[str, int, int]]:
        """Returns list of (module_or_path, level, lineno).

        - level: 0 for absolute imports, >=1 for relative import-from statements.
        """
        out: List[Tuple[str, int, int]] = []
        try:
            tree = ast.parse(content)
        except Exception:
            return out

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias and alias.name:
                        out.append((alias.name, 0, getattr(node, 'lineno', 0)))
            elif isinstance(node, ast.ImportFrom):
                level = int(getattr(node, 'level', 0) or 0)
                mod = getattr(node, 'module', None) or ""
                if mod:
                    out.append((mod, level, getattr(node, 'lineno', 0)))
                else:
                    # from . import x
                    for alias in node.names:
                        if alias and alias.name:
                            out.append((alias.name, level, getattr(node, 'lineno', 0)))

        return out

    def _resolve_python_import(self, source_path: str, module: str, level: int) -> List[str]:
        """Resolve a python import to possible VFS target paths."""
        self._build_import_index()

        # Absolute: try direct module mapping
        if level <= 0:
            if module in self._module_index:
                return [self._module_index[module]]
            return []

        # Relative: resolve from the source directory
        src_dir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        base_parts = src_dir.split("/") if src_dir else []

        # level=1 means "from .", so pop 0; level=2 means "from .." pop 1, etc.
        pops = max(level - 1, 0)
        if pops > 0 and pops <= len(base_parts):
            base_parts = base_parts[:-pops]

        rel_base = "/".join([p for p in base_parts if p])
        mod_path = module.replace(".", "/").strip("/")

        candidates: List[str] = []
        if rel_base:
            if mod_path:
                candidates.append(f"{rel_base}/{mod_path}.py")
                candidates.append(f"{rel_base}/{mod_path}/__init__.py")
            else:
                candidates.append(f"{rel_base}/__init__.py")
        else:
            if mod_path:
                candidates.append(f"{mod_path}.py")
                candidates.append(f"{mod_path}/__init__.py")

        return [c for c in candidates if c in self._path_index]

    def _resolve_js_like_import(self, source_path: str, imp: str) -> List[str]:
        """Resolve require('./x') / import ... from './x' to VFS candidates."""
        self._build_import_index()

        sdir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        raw = imp.strip().replace("\\", "/")

        # Only try to resolve relative-ish paths
        if not (raw.startswith(".") or raw.startswith("/")):
            return []

        # Normalize
        if raw.startswith("/"):
            rel = raw.lstrip("/")
        else:
            rel = os.path.normpath(os.path.join(sdir, raw)).replace("\\", "/").lstrip("./")

        ext_candidates = [rel]
        # Common extensions
        if not os.path.splitext(rel)[1]:
            ext_candidates.extend([rel + ".js", rel + ".ts", rel + ".json"]) 
            ext_candidates.extend([rel + "/index.js", rel + "/index.ts"]) 

        return [c for c in ext_candidates if c in self._path_index]

    def _weave_imports(self, source_path: str, content: str):
        """Scans content for imports and links them in the graph.

        Creates edges:
        - imports_file: source_path -> target_vfs_path (resolved)
        - imports_unresolved: source_path -> module_string (fallback)
        """
        targets_resolved: List[str] = []

        # Python: use AST when possible
        if source_path.endswith(".py"):
            for mod, level, lineno in self._extract_imports_python(source_path, content):
                resolved = self._resolve_python_import(source_path, mod, level)
                if resolved:
                    for tgt in resolved:
                        self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                        targets_resolved.append(tgt)
                else:
                    self.cartridge.add_edge(source_path, mod, "imports_unresolved", 0.25)

            return

        # JS / generic: regex fallback
        for line in content.splitlines():
            match = self.import_pattern.search(line)
            if not match:
                continue

            imp = match.group(1) or match.group(2)
            if not imp:
                continue

            resolved = self._resolve_js_like_import(source_path, imp)
            if resolved:
                for tgt in resolved:
                    self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                    targets_resolved.append(tgt)
            else:
                self.cartridge.add_edge(source_path, imp, "imports_unresolved", 0.25)

    def _weave_sections(self, vfs_path: str, content: str):
        """Creates section/chapter nodes for long-form text and links them to the file node."""
        ext = os.path.splitext(vfs_path)[1].lower()
        if ext not in (".md", ".markdown", ".txt", ".rst"):
            return

        lines = content.splitlines()
        for idx, line in enumerate(lines):
            lineno = idx + 1

            m = self._md_heading.match(line)
            if m:
                hashes = m.group(1)
                title = (m.group(2) or "").strip()
                level = len(hashes)
                if title:
                    node_id = f"{vfs_path}::section::{lineno}:{title}"
                    self.cartridge.add_node(node_id, "section", title, {
                        "parent": vfs_path,
                        "level": level,
                        "line": lineno
                    })
                    self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)
                continue

            c = self._chapter_heading.match(line)
            if c:
                chap_num = (c.group(2) or "").strip()
                chap_title = (c.group(3) or "").strip()
                title = f"Chapter {chap_num}" + (f": {chap_title}" if chap_title else "")
                node_id = f"{vfs_path}::chapter::{lineno}:{chap_num}"
                self.cartridge.add_node(node_id, "section", title, {
                    "parent": vfs_path,
                    "level": 1,
                    "line": lineno
                })
                self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)





--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\scanner.py
--------------------------------------------------------------------------------
import os
import time
import requests
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Any, Optional

# Try imports for Web/PDF support
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

class ScannerMS:
    """
    The Scanner: Walks file systems OR crawls websites (Depth-Aware).
    """
    
    def __init__(self):
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage', 'site-packages'
        }
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', 
            '.zip', '.tar', '.gz', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }
        self.visited_urls = set()

    def is_binary(self, file_path: str) -> bool:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS: return True
        return False

    def scan_directory(self, root_path: str, web_depth: int = 0) -> Optional[Dict[str, Any]]:
        """
        Main Entry Point.
        :param root_path: File path or URL.
        :param web_depth: How many links deep to crawl (0 = single page).
        """
        # 1. Web Crawl Mode
        if root_path.startswith("http://") or root_path.startswith("https://"):
            self.visited_urls.clear()
            return self._crawl_web_recursive(root_path, depth=web_depth, origin_domain=urlparse(root_path).netloc)

        # 2. Local File System Mode
        target = os.path.abspath(root_path)
        if not os.path.exists(target): return None
        
        if not os.path.isdir(target): 
            return self._create_node(target, is_dir=False)
            
        return self._scan_fs_recursive(target)

    # --- Web Logic ---
    def _crawl_web_recursive(self, url: str, depth: int, origin_domain: str) -> Dict[str, Any]:
        """
        Recursively fetches links.
        """
        # Generate a nice VFS path: web/domain/path
        parsed = urlparse(url)
        clean_path = parsed.path.strip("/")
        if not clean_path: clean_path = "index.html"
        rel_path = f"web/{parsed.netloc}/{clean_path}"

        node = {
            'name': url,
            'path': url,
            'rel_path': rel_path,
            'type': 'web',
            'children': [],
            'checked': True
        }
        
        if depth < 0 or url in self.visited_urls: return node
        self.visited_urls.add(url)

        if depth > 0 and BeautifulSoup:
            try:
                # Polite Delay
                time.sleep(0.1)
                resp = requests.get(url, timeout=5)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.content, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        parsed = urlparse(full_url)
                        
                        # Filter: Only same domain, valid schemes
                        if parsed.netloc == origin_domain and parsed.scheme in ['http', 'https']:
                            if full_url not in self.visited_urls:
                                child_node = self._crawl_web_recursive(full_url, depth - 1, origin_domain)
                                node['children'].append(child_node)
            except Exception as e:
                node['error'] = str(e)
                
        return node

    # --- File System Logic ---
    def _scan_fs_recursive(self, current_path: str, root_path: str = None) -> Dict[str, Any]:
        if root_path is None: root_path = current_path
        
        node = self._create_node(current_path, is_dir=True, root_path=root_path)
        node['children'] = []
        try:
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                for entry in entries:
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS: continue
                    if entry.name.startswith('.'): continue

                    if entry.is_dir():
                        child = self._scan_fs_recursive(entry.path, root_path=root_path)
                        if child: node['children'].append(child)
                    else:
                        node['children'].append(self._create_node(entry.path, is_dir=False, root_path=root_path))
        except PermissionError:
            node['error'] = "Access Denied"
        return node

    def _create_node(self, path: str, is_dir: bool, root_path: str = None) -> Dict[str, Any]:
        name = os.path.basename(path)
        # Calculate relative path for VFS
        rel_path = name
        if root_path:
            try:
                rel_path = os.path.relpath(path, root_path).replace("\\", "/")
            except ValueError:
                pass

        node = {
            'name': name, 
            'path': path, 
            'rel_path': rel_path,
            'type': 'folder' if is_dir else 'file', 
            'children': [],
            'checked': False
        }
        return node

    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        files = []
        if tree_node['type'] in ['file', 'web']:
            files.append(tree_node['path'])
        elif 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files


--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\semantic_chunker.py
--------------------------------------------------------------------------------
import ast
from dataclasses import dataclass
from typing import List

@dataclass
class CodeChunk:
    name: str          # e.g., "class AuthMS"
    type: str          # "class", "function", "text"
    content: str       # The raw source
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

class SemanticChunker:
    """
    Intelligent Code Splitter.
    Parses source code into logical units (Classes, Functions) 
    rather than arbitrary text windows.
    """
    
    def chunk_file(self, content: str, filename: str) -> List[CodeChunk]:
        # 1. Python Code
        if filename.endswith(".py"):
            return self._chunk_python(content)
            
        # 2. Text / Prose Documents (Smaller semantic windows)
        lower = filename.lower()
        if lower.endswith(('.md', '.txt', '.pdf', '.html', '.htm', '.rst')):
            return self._chunk_generic(content, window_size=800)
            
        # 3. Fallback (Generic Code/Binary)
        return self._chunk_generic(content, window_size=1500)

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                end = node.end_lineno if hasattr(node, 'end_lineno') else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", type="function", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"class {node.name}", type="class", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))

            # Fallback: If no classes/functions found (e.g., script file), treat as generic
            if not chunks:
                return self._chunk_generic(source)
                
        except SyntaxError:
            return self._chunk_generic(source)
            
        return chunks

    def _chunk_generic(self, text: str, window_size: int = 1500) -> List[CodeChunk]:
        """Sliding window for non-code files."""
        chunks = []
        # normalize newlines to avoid massive single-line blobs
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        lines = text.splitlines(keepends=True)
        
        current_chunk = []
        current_size = 0
        chunk_idx = 1
        start_line = 1
        
        for i, line in enumerate(lines):
            current_chunk.append(line)
            current_size += len(line)
            
            if current_size >= window_size:
                chunks.append(CodeChunk(
                    name=f"Chunk {chunk_idx}", type="text_block",
                    content="".join(current_chunk), start_line=start_line, end_line=i + 1
                ))
                current_chunk = []
                current_size = 0
                chunk_idx += 1
                start_line = i + 2
                
        if current_chunk:
            chunks.append(CodeChunk(
                name=f"Chunk {chunk_idx}", type="text_block",
                content="".join(current_chunk), start_line=start_line, end_line=len(lines)
            ))
            
        return chunks


--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\telemetry_service.py
--------------------------------------------------------------------------------
import logging
import queue
from .base_service import BaseService

class TelemetryService(BaseService):
    """
    The Nervous System.
    Watches the thread-safe LogQueue and updates the GUI Panels.
    """
    def __init__(self, root, panels):
        super().__init__("TelemetryService")
        self.root = root
        self.panels = panels
        self.log_queue = queue.Queue()
        
        # We set up the global logging hook HERE, inside the service
        self._setup_logging_hook()

    def _setup_logging_hook(self):
        """Redirects Python's standard logging to our Queue."""
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        
        # Create our custom handler that feeds the queue
        q_handler = QueueHandler(self.log_queue)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
        q_handler.setFormatter(formatter)
        logger.addHandler(q_handler)

    def start(self):
        """Begins the GUI update loop."""
        self.log_info("Telemetry Service starting...")
        self._poll_queue()

    def _poll_queue(self):
        """The heartbeat that drains the queue into the GUI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                msg = f"[{record.levelname}] {record.message}" # Removed \n because .log() adds it
                
                # Update the GUI
                self.panels.log(msg)
                
        except queue.Empty:
            pass
        finally:
            # Check again in 100ms
            self.root.after(100, self._poll_queue)

# Helper Class for the Queue
class QueueHandler(logging.Handler):
    def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.format(record)
        self.log_queue.put(record)


--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\thought_stream.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import datetime

class ThoughtStream(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340)
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", fg="#007ACC", bg="#1a1a25", font=("Consolas", 8)).pack(anchor="w", padx=5, pady=2)
        
        snippet = content[:400] + "..." if len(content) > 400 else content
        tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", font=("Consolas", 8), justify="left", wraplength=300).pack(fill="x", padx=5, pady=2)
        
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        if not vector: return
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        bar_w = w / len(vector)
        for i, val in enumerate(vector):
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")
--------------------------------------------------------------------------------
FILE: _RagFORGE\src\microservices\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _RagFORGE\tests\verify_forge.py
--------------------------------------------------------------------------------
import sys
import os
import time
import json

# Fix path so we can import src
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'src')))

from microservices.cartridge_service import CartridgeService
from microservices.neural_service import NeuralService
from microservices.intake_service import IntakeService
from microservices.refinery_service import RefineryService

TEST_DB = "tests/test_cartridge.db"
TEST_SOURCE_DIR = "tests/dummy_source"

def setup_dummy_data():
    if not os.path.exists(TEST_SOURCE_DIR):
        os.makedirs(TEST_SOURCE_DIR)
    
    # Create a text file with specific distinct concepts
    with open(f"{TEST_SOURCE_DIR}/concept.txt", "w") as f:
        f.write("""
        Project: Project Necromancy.
        Objective: To resurrect dead code using AI patches.
        Lead Engineer: Jacob Lambert.
        Tools: _RagFORGE, _TokenizingPATCHER.
        Status: ACTIVE.
        """)
    print("[TEST] Dummy data created.")

def run_test():
    # 1. Cleanup old test
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)

    setup_dummy_data()

    print(f"[TEST] Forging Cartridge: {TEST_DB}")
    
    # 2. Initialize Services
    cartridge = CartridgeService(TEST_DB)
    neural = NeuralService()
    intake = IntakeService(cartridge)
    refinery = RefineryService(cartridge, neural)

    # 3. Intake Phase
    print(">>> Phase 1: Intake")
    # Using the new ingest_source method we patched
    stats = intake.ingest_source(TEST_SOURCE_DIR)
    print(f"    Stats: {stats}")

    # 4. Refinery Phase
    print(">>> Phase 2: Refinery")
    processed = refinery.process_pending(batch_size=10)
    print(f"    Refined {processed} files.")
    
    # Allow DB to settle
    time.sleep(1)

    # 5. Verification: Manifest
    print("\n[VERIFY] Checking Manifest...")
    cid = cartridge.get_manifest("cartridge_id")
    schema = cartridge.get_manifest("schema_version")
    print(f"    Cartridge ID: {cid}")
    print(f"    Schema Ver:   {schema}")
    
    if not cid:
        print("    [FAIL] Manifest missing!")
        return

    # 6. Verification: Vector Search
    print("\n[VERIFY] Testing Vector Search...")
    # We search for a concept that shouldn't match by keyword alone but works semantically
    query = "Who is the lead engineer?" 
    print(f"    Query: '{query}'")
    
    q_vec = neural.get_embedding(query)
    results = cartridge.search_embeddings(q_vec, limit=1)
    
    if results:
        top = results[0]
        print(f"    [SUCCESS] Match found!")
        print(f"    Score: {top['score']}")
        print(f"    Content: {top['content'].strip()}")
    else:
        print("    [FAIL] No vector results found. Is sqlite-vec working?")

if __name__ == "__main__":
    try:
        run_test()
    except Exception as e:
        print(f"[FATAL] Test failed: {e}")

--------------------------------------------------------------------------------
FILE: _RagFORGE\tests\dummy_source\concept.txt
--------------------------------------------------------------------------------

        Project: Project Necromancy.
        Objective: To resurrect dead code using AI patches.
        Lead Engineer: Jacob Lambert.
        Tools: _RagFORGE, _TokenizingPATCHER.
        Status: ACTIVE.
        
--------------------------------------------------------------------------------
FILE: _TasklistWORKER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _TasklistWORKER\README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TasklistWORKER\requirements.txt
--------------------------------------------------------------------------------
tk
ollama
--------------------------------------------------------------------------------
FILE: _TasklistWORKER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\app - Copy.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
import json
import os
import datetime

# --- MICROSERVICES CHECK ---
try:
    from src._microservices.ollama_client import OllamaClient
    from src._microservices.template_engine import resolve_template
except ImportError:
    try:
        from _microservices.ollama_client import OllamaClient
        from _microservices.template_engine import resolve_template
    except ImportError:
        raise ImportError("Microservices missing. Ensure src/_microservices exists.")

# ==============================================================================
# CUSTOM WIDGET: Smart Task Step (Expander)
# ==============================================================================
class TaskStepWidget(tk.Frame):
    def __init__(self, parent, index, app_ref, initial_data=None):
        super().__init__(parent, bg="#334155", bd=1, relief="flat")
        self.app = app_ref
        self.index = index
        self.expanded = False
        
        # Data Model
        self.data = initial_data or {
            "system": "You are a helpful AI assistant.",
            "prompt": "",
            "use_context": True
        }

        self._build_ui()
        self._refresh_summary()

    def _build_ui(self):
        # --- HEADER (Always Visible) ---
        self.header = tk.Frame(self, bg="#334155", height=30)
        self.header.pack(fill="x", padx=2, pady=2)
        
        # Click header to toggle
        self.header.bind("<Button-1>", self.toggle)
        
        # Step Number / Status Indicator
        self.lbl_idx = tk.Label(self.header, text=f"#{self.index+1}", bg="#334155", fg="#94a3b8", font=("Segoe UI", 10, "bold"), width=4)
        self.lbl_idx.pack(side="left")
        self.lbl_idx.bind("<Button-1>", self.toggle)

        # Summary Text (Read-only representation)
        self.lbl_summary = tk.Label(self.header, text="(New Step)", bg="#334155", fg="white", anchor="w", font=("Segoe UI", 10))
        self.lbl_summary.pack(side="left", fill="x", expand=True, padx=5)
        self.lbl_summary.bind("<Button-1>", self.toggle)

        # Delete Button (Small 'x')
        lbl_del = tk.Label(self.header, text="√ó", bg="#334155", fg="#ef4444", font=("Arial", 12, "bold"), cursor="hand2")
        lbl_del.pack(side="right", padx=5)
        lbl_del.bind("<Button-1>", lambda e: self.app.delete_step(self.index))

        # --- BODY (Hidden by default) ---
        self.body = tk.Frame(self, bg="#1e293b", padx=5, pady=5)
        
        # 1. System Prompt
        tk.Label(self.body, text="System Prompt:", bg="#1e293b", fg="#64748b", font=("Segoe UI", 8)).pack(anchor="w")
        self.ent_sys = tk.Entry(self.body, bg="#0f172a", fg="#94a3b8", borderwidth=0, insertbackground="white")
        self.ent_sys.pack(fill="x", pady=(0, 5))
        self.ent_sys.insert(0, self.data["system"])

        # 2. User Prompt (Auto-growing Text)
        tk.Label(self.body, text="User Prompt:", bg="#1e293b", fg="#cbd5e1", font=("Segoe UI", 9, "bold")).pack(anchor="w")
        self.txt_prompt = tk.Text(self.body, height=4, bg="#0f172a", fg="white", borderwidth=0, insertbackground="white", font=("Segoe UI", 10), wrap="word")
        self.txt_prompt.pack(fill="x", pady=(0, 5))
        self.txt_prompt.insert("1.0", self.data["prompt"])
        
        # Bindings for "Save on Type" / "Auto-size"
        self.txt_prompt.bind("<KeyRelease>", self._on_text_change)
        self.txt_prompt.bind("<FocusOut>", self._sync_data)

        # 3. Context Toggle
        self.var_ctx = tk.BooleanVar(value=self.data["use_context"])
        chk = tk.Checkbutton(self.body, text="Include Previous Response", variable=self.var_ctx, bg="#1e293b", fg="#94a3b8", selectcolor="#0f172a", activebackground="#1e293b", activeforeground="white")
        chk.pack(anchor="w")

    def toggle(self, event=None):
        if self.expanded:
            self.collapse()
        else:
            self.expand()

    def expand(self):
        self.app.collapse_all_steps() # Singleton expansion (optional, feels cleaner)
        self.body.pack(fill="x", expand=True)
        self.header.config(bg="#475569")
        self.lbl_idx.config(bg="#475569", fg="white")
        self.lbl_summary.config(bg="#475569")
        self.expanded = True
        self.txt_prompt.focus_set()

    def collapse(self):
        self._sync_data()
        self.body.pack_forget()
        self.header.config(bg="#334155")
        self.lbl_idx.config(bg="#334155", fg="#94a3b8")
        self.lbl_summary.config(bg="#334155")
        self.expanded = False
        self._refresh_summary()

    def _sync_data(self, event=None):
        """Update internal data dict from widgets."""
        self.data["system"] = self.ent_sys.get()
        self.data["prompt"] = self.txt_prompt.get("1.0", "end-1c")
        self.data["use_context"] = self.var_ctx.get()

    def _on_text_change(self, event=None):
        # Auto-grow logic
        lines = int(self.txt_prompt.index('end-1c').split('.')[0])
        new_height = min(max(4, lines + 1), 15)
        if int(self.txt_prompt.cget("height")) != new_height:
            self.txt_prompt.config(height=new_height)

    def _refresh_summary(self):
        """Update the collapsed header text."""
        raw = self.data["prompt"].replace("\n", " ").strip()
        if not raw: raw = "(Empty Step)"
        if len(raw) > 50: raw = raw[:47] + "..."
        self.lbl_summary.config(text=raw)

    def set_active(self, active: bool):
        color = "#2563eb" if active else ("#475569" if self.expanded else "#334155")
        self.config(bg=color)
        if not self.expanded:
            self.header.config(bg=color)
            self.lbl_idx.config(bg=color)
            self.lbl_summary.config(bg=color)

# ==============================================================================
# MAIN APPLICATION
# ==============================================================================
class PromptChainerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("_PromptCHAINER v3.1 [IDE Mode]")
        self.root.geometry("1600x900")
        self.root.configure(bg="#0f172a")

        self.client = OllamaClient()
        
        # State
        self.state = {
            "chat": {"history": [], "last_response": ""},
            "working": {"step_outputs": {}, "thoughts": []},
            "outputs": {}
        }
        self.steps = [] # List of TaskStepWidget
        self.current_step_idx = 0
        self.is_running = False
        
        # UI Vars
        self.selected_model = tk.StringVar()
        self.helper_model = tk.StringVar()
        self.auto_run = tk.BooleanVar(value=False)

        self._build_layout()
        self._refresh_models()
        self.log_system("System Initialized.")

    def _build_layout(self):
        # Main container (3 Columns)
        self.paned = tk.PanedWindow(self.root, orient=tk.HORIZONTAL, bg="#0f172a", sashwidth=4, sashrelief="flat")
        self.paned.pack(fill="both", expand=True, pady=(0, 30)) # Leave room for bottom log

        # --- COL 1: HISTORY (Left) ---
        f_left = tk.Frame(self.paned, bg="#0f172a")
        self.paned.add(f_left, minsize=350, stretch="always")
        
        tk.Label(f_left, text="Conversation History", bg="#0f172a", fg="#64748b", font=("Segoe UI", 9, "bold")).pack(anchor="w", padx=5, pady=5)
        self.txt_chat = scrolledtext.ScrolledText(f_left, bg="#020617", fg="#94a3b8", borderwidth=0, font=("Consolas", 10))
        self.txt_chat.pack(fill="both", expand=True, padx=5)

        # --- COL 2: CHAIN EDITOR (Center) ---
        f_center = tk.Frame(self.paned, bg="#1e293b")
        self.paned.add(f_center, minsize=450, stretch="always")

        # 1. Header (Top)
        c_head = tk.Frame(f_center, bg="#1e293b", pady=10, padx=10)
        c_head.pack(side="top", fill="x")
        tk.Label(c_head, text="Task Chain", font=("Segoe UI", 14, "bold"), bg="#1e293b", fg="white").pack(side="left")
        
        tk.Label(c_head, text="Model:", bg="#1e293b", fg="#94a3b8").pack(side="left", padx=(20, 5))
        self.cb_model = ttk.Combobox(c_head, textvariable=self.selected_model, state="readonly", width=18)
        self.cb_model.pack(side="left")
        
        tk.Button(c_head, text="üìÇ", command=self.load_chain, bg="#334155", fg="white", width=3, relief="flat").pack(side="right")
        tk.Button(c_head, text="üíæ", command=self.save_chain, bg="#334155", fg="white", width=3, relief="flat").pack(side="right", padx=5)

        # 2. Footer Buttons (Bottom - PACK FIRST TO ENSURE VISIBILITY)
        f_ctrl = tk.Frame(f_center, bg="#1e293b", pady=10, padx=10)
        f_ctrl.pack(side="bottom", fill="x")
        
        # "+ Add Step"
        tk.Button(f_ctrl, text="+ Add Step", command=self.add_step, bg="#334155", fg="white", relief="flat", font=("Segoe UI", 10)).pack(fill="x")

        # 3. Scrollable Task Area (Fills remaining space)
        self.canvas = tk.Canvas(f_center, bg="#1e293b", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(f_center, orient="vertical", command=self.canvas.yview)
        self.frame_tasks = tk.Frame(self.canvas, bg="#1e293b")
        
        self.canvas.create_window((0, 0), window=self.frame_tasks, anchor="nw")
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="top", fill="both", expand=True, padx=5)
        self.scrollbar.pack(side="right", fill="y")
        self.frame_tasks.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))


        # --- COL 3: ANALYSIS & OUTPUT (Right) ---
        f_right = tk.Frame(self.paned, bg="#0f172a")
        self.paned.add(f_right, minsize=400, stretch="always")

        # Split Right Col into Thoughts (Top) and Staging (Bottom)
        paned_right = tk.PanedWindow(f_right, orient=tk.VERTICAL, bg="#0f172a", sashwidth=4)
        paned_right.pack(fill="both", expand=True)

        # Thoughts Pane
        f_thoughts = tk.Frame(paned_right, bg="#0f172a")
        h_thoughts = tk.Frame(f_thoughts, bg="#0f172a")
        h_thoughts.pack(fill="x", pady=5)
        tk.Label(h_thoughts, text="Thought Stream", bg="#0f172a", fg="#a78bfa", font=("Segoe UI", 10, "bold")).pack(side="left", padx=5)
        
        # Helper Model Select
        self.cb_helper = ttk.Combobox(h_thoughts, textvariable=self.helper_model, state="readonly", width=15)
        self.cb_helper.pack(side="right", padx=5)
        tk.Label(h_thoughts, text="Helper:", bg="#0f172a", fg="#64748b", font=("Segoe UI", 8)).pack(side="right")

        self.txt_thoughts = scrolledtext.ScrolledText(f_thoughts, bg="#1e1e1e", fg="#a78bfa", borderwidth=0, font=("Segoe UI", 9))
        self.txt_thoughts.pack(fill="both", expand=True, padx=5)
        paned_right.add(f_thoughts, minsize=200, stretch="always")

        # Staging Pane (Bottom Right)
        f_stage = tk.Frame(paned_right, bg="#0f172a")
        
        tk.Label(f_stage, text="FINAL OUTPUT / STAGING (Edit before continuing)", bg="#0f172a", fg="#facc15", font=("Segoe UI", 10, "bold")).pack(anchor="w", padx=5, pady=(10, 5))
        
        self.txt_staging = scrolledtext.ScrolledText(f_stage, bg="#1e293b", fg="#e2e8f0", borderwidth=0, font=("Consolas", 11), insertbackground="white")
        self.txt_staging.pack(fill="both", expand=True, padx=5)
        
        # ACTION BAR (The Green Arrow)
        f_action = tk.Frame(f_stage, bg="#0f172a", pady=10)
        f_action.pack(fill="x", padx=5)
        
        # This is the Master Action Button
        self.btn_action = tk.Button(f_action, text="START CHAIN ‚û°", command=self.on_action_click, bg="#2563eb", fg="white", font=("Segoe UI", 12, "bold"), relief="flat", height=2)
        self.btn_action.pack(fill="x")
        
        tk.Checkbutton(f_action, text="Auto-Commit (No Pause)", variable=self.auto_run, bg="#0f172a", fg="#94a3b8", selectcolor="#0f172a", activebackground="#0f172a").pack(anchor="e")

        paned_right.add(f_stage, minsize=300, stretch="always")

        # --- BOTTOM: SYSTEM LOG ---
        self.txt_log = tk.Text(self.root, height=8, bg="#020617", fg="#475569", font=("Consolas", 9), borderwidth=0, state="disabled")
        self.txt_log.place(relx=0, rely=1, anchor="sw", relwidth=1.0, height=150) # Use place to anchor firmly to bottom
        self.txt_log.pack(side="bottom", fill="x")

    # --- LOGIC ---

    def log_system(self, msg):
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        self.txt_log.config(state="normal")
        self.txt_log.insert("end", f"[{timestamp}] {msg}\n")
        self.txt_log.see("end")
        self.txt_log.config(state="disabled")

    def _refresh_models(self):
        def worker():
            try:
                models = self.client.list_models()
                self.root.after(0, lambda: self._update_combos(models))
            except Exception as e:
                self.log_system(f"Error listing models: {e}")
        threading.Thread(target=worker, daemon=True).start()

    def _update_combos(self, models):
        self.cb_model.config(values=models)
        self.cb_helper.config(values=models)
        if models:
            self.cb_model.set(models[0])
            helpers = [m for m in models if "mini" in m or "7b" in m]
            self.cb_helper.set(helpers[0] if helpers else models[0])
        self.log_system(f"Found {len(models)} models.")

    def collapse_all_steps(self):
        for s in self.steps:
            if s.expanded: s.collapse()

    def add_step(self, data=None):
        idx = len(self.steps)
        # Create widget
        w = TaskStepWidget(self.frame_tasks, idx, self, initial_data=data)
        w.pack(fill="x", pady=2)
        self.steps.append(w)
        
        # If adding manually (no data), expand it for editing
        if not data:
            w.expand()

    def delete_step(self, index):
        if not (0 <= index < len(self.steps)): return
        
        # Remove widget
        self.steps[index].destroy()
        self.steps.pop(index)
        
        # Re-index remaining
        for i, s in enumerate(self.steps):
            s.index = i
            s.lbl_idx.config(text=f"#{i+1}")
        
        self.log_system(f"Deleted step #{index+1}")

    def on_action_click(self):
        # Determine context: Are we starting? confirming? finishing?
        
        # Case 1: Not running -> Start
        if not self.is_running:
            self.start_chain()
            return

        # Case 2: Running -> Commit Staging & Move Next
        self.commit_staging()

    def start_chain(self):
        if not self.steps:
            messagebox.showwarning("Empty", "Add some steps first.")
            return

        self.is_running = True
        self.current_step_idx = 0
        
        # Clear logs
        self.txt_chat.delete("1.0", "end")
        self.txt_thoughts.delete("1.0", "end")
        self.state["chat"]["history"] = []
        self.state["chat"]["last_response"] = ""
        
        self.log_system("--- STARTED NEW CHAIN ---")
        self.run_step(0)

    def run_step(self, idx):
        if idx >= len(self.steps):
            self.finish_chain()
            return

        self.current_step_idx = idx
        step_widget = self.steps[idx]
        step_widget._sync_data() # Ensure we have latest text
        data = step_widget.data

        # UI Update
        for s in self.steps: s.set_active(False)
        step_widget.set_active(True)
        
        self.btn_action.config(text="Running...", bg="#475569", state="disabled")
        self.log_system(f"Executing Step #{idx+1}")

        # Construct Prompt
        # 1. Template Resolution
        raw_prompt = data["prompt"]
        final_prompt = resolve_template(raw_prompt, self.state)
        
        # 2. Context Injection
        if data["use_context"] and idx > 0:
            final_prompt += f"\n\n[CONTEXT FROM PREVIOUS STEP]:\n{self.state['chat']['last_response']}"

        self.txt_chat.insert("end", f"\n[USER - Step {idx+1}]\n{final_prompt}\n" + "-"*30 + "\n")
        self.txt_chat.see("end")

        # Threaded Inference
        threading.Thread(target=self._inference_worker, args=(data["system"], final_prompt), daemon=True).start()

    def _inference_worker(self, sys_prompt, user_prompt):
        try:
            main_model = self.selected_model.get()
            resp = self.client.generate(main_model, sys_prompt, user_prompt)

            helper_model = self.helper_model.get()
            thought = "..."
            if helper_model:
                thought = self.client.generate(helper_model, "Summarize this interaction and suggest the next logical thought.", f"Q: {user_prompt}\nA: {resp}")

            self.root.after(0, lambda: self._on_step_complete(resp, thought))
        except Exception as e:
            self.root.after(0, lambda: self.log_system(f"Inference Error: {e}"))
            self.root.after(0, lambda: self.btn_action.config(state="normal", text="Retry"))

    def _on_step_complete(self, response, thought):
        # 1. Populate Staging
        self.txt_staging.delete("1.0", "end")
        self.txt_staging.insert("1.0", response)
        
        # 2. Populate Thoughts
        self.txt_thoughts.insert("end", f"Step {self.current_step_idx+1}: {thought.strip()}\n\n")
        self.txt_thoughts.see("end")

        # 3. Enable 'Next' Button
        is_last = (self.current_step_idx == len(self.steps) - 1)
        btn_text = "Finish & Save üèÅ" if is_last else "Confirm & Next ‚û°"
        btn_color = "#16a34a" if is_last else "#2563eb"
        
        self.btn_action.config(state="normal", text=btn_text, bg=btn_color)
        self.log_system(f"Step #{self.current_step_idx+1} complete. Waiting for confirmation.")

        if self.auto_run.get():
            self.commit_staging()

    def commit_staging(self):
        # User may have edited text in staging
        final_text = self.txt_staging.get("1.0", "end-1c")
        
        self.state["chat"]["last_response"] = final_text
        self.txt_chat.insert("end", f"\n[ASSISTANT]\n{final_text}\n" + "="*40 + "\n")
        self.txt_chat.see("end")
        
        self.run_step(self.current_step_idx + 1)

    def finish_chain(self):
        self.is_running = False
        self.btn_action.config(text="START CHAIN ‚û°", bg="#2563eb")
        self.log_system("Chain Completed.")
        
        # Save Log
        if not os.path.exists("_logs"): os.makedirs("_logs")
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        fn = f"_logs/run_{ts}.json"
        
        log_data = {
            "chat": self.txt_chat.get("1.0", "end-1c"),
            "thoughts": self.txt_thoughts.get("1.0", "end-1c"),
            "final_output": self.state["chat"]["last_response"]
        }
        with open(fn, "w", encoding="utf-8") as f:
            json.dump(log_data, f, indent=2)
        
        messagebox.showinfo("Success", f"Run saved to {fn}")

    # --- SAVE / LOAD ---
    def save_chain(self):
        # Force sync of all widgets
        for s in self.steps: s._sync_data()
        
        data = [s.data for s in self.steps]
        f = filedialog.asksaveasfilename(defaultextension=".json", filetypes=[("JSON", "*.json")])
        if f:
            with open(f, 'w') as file:
                json.dump(data, file, indent=2)
            self.log_system(f"Saved chain to {os.path.basename(f)}")

    def load_chain(self):
        f = filedialog.askopenfilename(filetypes=[("JSON", "*.json")])
        if f:
            with open(f, 'r') as file:
                data = json.load(file)
            
            # Clear UI
            for s in self.steps: s.destroy()
            self.steps = []
            
            # Rebuild
            for step_data in data:
                self.add_step(step_data)
            self.log_system(f"Loaded chain from {os.path.basename(f)}")

if __name__ == "__main__":
    root = tk.Tk()
    app = PromptChainerApp(root)
    root.mainloop()
--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\app.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import messagebox, ttk, simpledialog, filedialog
import threading
import json
import os
import datetime

from src.gui_layout import WorkbenchUI

try:
    from src._microservices.ollama_client import OllamaClient
    from src._microservices.template_engine import resolve_template
except ImportError:
    from _microservices.ollama_client import OllamaClient
    from _microservices.template_engine import resolve_template

# ==============================================================================
# DATA MANAGER: Roles
# ==============================================================================
class RoleManager:
    def __init__(self):
        # Default Roles
        self.roles = {
            "Helpful Assistant": "You are a helpful AI assistant.",
            "Python Expert": "You are a senior Python developer. Be concise and precise.",
            "Creative Writer": "You are a creative writer. Use vivid imagery.",
            "Strict Analyst": "You are a logic-first analyst. No fluff."
        }
    
    def get_names(self):
        return list(self.roles.keys())
    
    def get_prompt(self, name):
        return self.roles.get(name, "")
    
    def add_role(self, name, prompt):
        self.roles[name] = prompt

# ==============================================================================
# DIALOGS
# ==============================================================================
class AddRoleDialog(simpledialog.Dialog):
    def body(self, master):
        tk.Label(master, text="Role Name:").grid(row=0)
        tk.Label(master, text="System Prompt:").grid(row=1)
        self.e1 = tk.Entry(master)
        self.e2 = tk.Entry(master)
        self.e1.grid(row=0, column=1)
        self.e2.grid(row=1, column=1)
        return self.e1
    def apply(self):
        self.result = (self.e1.get(), self.e2.get())

class LogConfigDialog(simpledialog.Dialog):
    def __init__(self, parent, current_config):
        self.config = current_config
        super().__init__(parent, title="Log Configuration")

    def body(self, master):
        tk.Label(master, text="Log Subdirectory:").grid(row=0, sticky="w")
        self.e_dir = tk.Entry(master)
        self.e_dir.insert(0, self.config["dir"])
        self.e_dir.grid(row=0, column=1)

        self.var_ts = tk.BooleanVar(value=self.config["timestamp"])
        tk.Checkbutton(master, text="Include Timestamp in Filename", variable=self.var_ts).grid(row=1, columnspan=2, sticky="w")
        
        return self.e_dir

    def apply(self):
        self.result = {
            "dir": self.e_dir.get(),
            "timestamp": self.var_ts.get()
        }

# ==============================================================================
# CONTROLLER: Task Step Row
# ==============================================================================
class TaskStepController(tk.Frame):
    def __init__(self, parent, index, app_ref, role_mgr, initial_data=None):
        super().__init__(parent, bg="#334155", bd=1, relief="flat")
        self.app = app_ref
        self.index = index
        self.role_mgr = role_mgr
        self.expanded = False
        
        self.data = initial_data or {
            "role": "Helpful Assistant",
            "prompt": "",
            "isolated": False,
            "skip": False
        }
        
        self._build_row_ui()
        self._refresh_summary()

    def _build_row_ui(self):
        # Header
        self.header = tk.Frame(self, bg="#334155", height=30)
        self.header.pack(fill="x", padx=2, pady=2)
        self.header.bind("<Button-1>", self.toggle)

        # Controls Left
        self.lbl_idx = tk.Label(self.header, text=f"#{self.index+1}", bg="#334155", fg="#94a3b8", font=("Segoe UI", 10, "bold"), width=3)
        self.lbl_idx.pack(side="left")

        # Run Single Step Button
        self.btn_run_single = tk.Label(self.header, text="‚ñ∂", bg="#334155", fg="#4ade80", cursor="hand2", font=("Arial", 10))
        self.btn_run_single.pack(side="left", padx=5)
        self.btn_run_single.bind("<Button-1>", self.run_this_step_only)
        
        # Summary
        self.lbl_summary = tk.Label(self.header, text="(New Step)", bg="#334155", fg="white", anchor="w")
        self.lbl_summary.pack(side="left", fill="x", expand=True, padx=5)
        self.lbl_summary.bind("<Button-1>", self.toggle)
        
        # Status Icons
        self.lbl_skip = tk.Label(self.header, text="‚õî", bg="#334155", fg="#ef4444", font=("Segoe UI", 8)) # Hidden default
        self.lbl_iso = tk.Label(self.header, text="üîí", bg="#334155", fg="#facc15", font=("Segoe UI", 8)) # Hidden default

        # Delete
        lbl_del = tk.Label(self.header, text="√ó", bg="#334155", fg="#ef4444", cursor="hand2", font=("Arial", 12, "bold"))
        lbl_del.pack(side="right", padx=5)
        lbl_del.bind("<Button-1>", lambda e: self.app.delete_step(self.index))

        # Body
        self.body = tk.Frame(self, bg="#1e293b", padx=5, pady=5)
        
        # Role Row
        f_role = tk.Frame(self.body, bg="#1e293b")
        f_role.pack(fill="x", pady=(0, 5))
        tk.Label(f_role, text="Role:", bg="#1e293b", fg="#64748b").pack(side="left")
        
        self.cb_role = ttk.Combobox(f_role, values=self.role_mgr.get_names(), state="readonly")
        self.cb_role.pack(side="left", fill="x", expand=True, padx=5)
        self.cb_role.set(self.data["role"])
        self.cb_role.bind("<<ComboboxSelected>>", self._sync_data)

        tk.Button(f_role, text="+", command=self._add_role_local, bg="#334155", fg="white", width=2).pack(side="right")

        # Prompt
        tk.Label(self.body, text="Prompt:", bg="#1e293b", fg="#cbd5e1", font=("Segoe UI", 8, "bold")).pack(anchor="w")
        self.txt_prompt = tk.Text(self.body, height=5, bg="#0f172a", fg="white", borderwidth=0, font=("Segoe UI", 9), wrap="word")
        self.txt_prompt.pack(fill="x", pady=(0, 5))
        self.txt_prompt.insert("1.0", self.data["prompt"])
        self.txt_prompt.bind("<FocusOut>", self._sync_data)

        # Settings Row
        f_set = tk.Frame(self.body, bg="#1e293b")
        f_set.pack(fill="x")
        
        self.var_iso = tk.BooleanVar(value=self.data["isolated"])
        tk.Checkbutton(f_set, text="Run in Isolation", variable=self.var_iso, bg="#1e293b", fg="#facc15", selectcolor="#0f172a", activebackground="#1e293b", command=self._sync_data).pack(side="left")
        
        self.var_skip = tk.BooleanVar(value=self.data["skip"])
        tk.Checkbutton(f_set, text="Skip Step", variable=self.var_skip, bg="#1e293b", fg="#ef4444", selectcolor="#0f172a", activebackground="#1e293b", command=self._sync_data).pack(side="right")

    def toggle(self, event=None):
        if self.expanded: self.collapse()
        else: self.expand()

    def expand(self):
        self.app.collapse_all_steps()
        self.body.pack(fill="x", expand=True)
        self.expanded = True
    
    def collapse(self):
        self._sync_data()
        self.body.pack_forget()
        self.expanded = False
        self._refresh_summary()

    def _sync_data(self, event=None):
        self.data["role"] = self.cb_role.get()
        self.data["prompt"] = self.txt_prompt.get("1.0", "end-1c")
        self.data["isolated"] = self.var_iso.get()
        self.data["skip"] = self.var_skip.get()
        self._refresh_summary()

    def _refresh_summary(self):
        raw = self.data["prompt"].replace("\n", " ").strip()
        if not raw: raw = "(Empty Step)"
        if len(raw) > 35: raw = raw[:32] + "..."
        
        summary = f"[{self.data['role']}] {raw}"
        self.lbl_summary.config(text=summary)
        
        # Toggle Icons
        if self.data["isolated"]: self.lbl_iso.pack(side="right", padx=2)
        else: self.lbl_iso.pack_forget()
        
        if self.data["skip"]: 
            self.lbl_skip.pack(side="right", padx=2)
            self.lbl_summary.config(fg="#64748b") # Grey out text
        else: 
            self.lbl_skip.pack_forget()
            self.lbl_summary.config(fg="white")

    def _add_role_local(self):
        self.app.add_new_role()
        # Refresh combo
        self.cb_role.config(values=self.app.roles.get_names())

    def run_this_step_only(self, event=None):
        # Stop propagation so we don't toggle expand
        # Run this step via app controller
        self.app.run_manual_step(self.index)
        return "break"

    def set_active(self, state):
        # state: "active", "pending", "none"
        if state == "active":
            bg = "#2563eb"
        elif state == "pending":
            bg = "#475569" # Grey to show it needs rerunning
        else:
            bg = "#334155"
        
        self.header.config(bg=bg)
        self.lbl_idx.config(bg=bg)
        self.btn_run_single.config(bg=bg)
        self.lbl_summary.config(bg=bg)

# ==============================================================================
# MAIN CONTROLLER
# ==============================================================================
class WorkbenchApp:
    def __init__(self, root):
        self.ui = WorkbenchUI(root)
        self.client = OllamaClient()
        self.roles = RoleManager()
        
        # App Config
        self.log_config = {"dir": "_logs", "timestamp": True}
        
        # State
        self.state = {
            "chat": {"last_response": ""},
            "history": [] 
        }
        self.steps = []
        self.current_step_idx = 0
        self.is_running = False
        
        # Bindings
        self.ui.widgets["btn_add_step"].config(command=self.add_step)
        self.ui.widgets["btn_run"].config(command=self.on_run_click)
        self.ui.widgets["btn_inject"].config(command=self.inject_to_chat)
        self.ui.widgets["btn_send_chat"].config(command=self.send_chat_message)
        
        self.ui.widgets["btn_add_role_chat"].config(command=self.add_new_role)
        self.ui.widgets["cb_chat_role"].bind("<<ComboboxSelected>>", lambda e: None) # Just holds value
        
        # Log Bindings
        self.ui.widgets["btn_log_save"].config(command=self.save_log_file)
        self.ui.widgets["btn_log_config"].config(command=self.open_log_config)

        # Init Data
        self._refresh_roles()
        self.add_step()
        threading.Thread(target=self._fetch_models, daemon=True).start()
        self.log("Workbench initialized.")

    def log(self, msg):
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        self.ui.widgets["log_console"].config(state="normal")
        self.ui.widgets["log_console"].insert("end", f"[{timestamp}] {msg}\n")
        self.ui.widgets["log_console"].see("end")
        self.ui.widgets["log_console"].config(state="disabled")

    def _fetch_models(self):
        try:
            models = self.client.list_models()
            self.ui.root.after(0, lambda: self._update_dropdowns(models))
        except Exception as e:
            self.ui.root.after(0, lambda: self.log(f"Model Error: {e}"))

    def _update_dropdowns(self, models):
        for w in ["cb_helper", "cb_chat_model", "cb_task_model"]:
            self.ui.widgets[w].config(values=models)
            if models: self.ui.widgets[w].set(models[0])

    def _refresh_roles(self):
        names = self.roles.get_names()
        self.ui.widgets["cb_chat_role"].config(values=names)
        self.ui.widgets["cb_chat_role"].set("Helpful Assistant")
        # Update steps if needed? No, they pull dynamically.

    def add_new_role(self):
        d = AddRoleDialog(self.ui.root)
        if d.result:
            name, prompt = d.result
            if name and prompt:
                self.roles.add_role(name, prompt)
                self._refresh_roles()
                self.log(f"Added role: {name}")

    # --- CHAT LOGIC ---
    def send_chat_message(self):
        # 1. Get Text
        txt = self.ui.widgets["chat_input"].get("1.0", "end-1c").strip()
        if not txt: return
        
        # 2. Clear Input
        self.ui.widgets["chat_input"].delete("1.0", "end")
        
        # 3. Log User
        self._append_session("USER", txt)
        
        # 4. Get Config
        model = self.ui.widgets["cb_chat_model"].get()
        role_name = self.ui.widgets["cb_chat_role"].get()
        sys_prompt = self.roles.get_prompt(role_name)
        
        self.log(f"Chatting with {model} as {role_name}...")
        
        # 5. Thread
        threading.Thread(target=self._chat_worker, args=(model, sys_prompt, txt), daemon=True).start()

    def _chat_worker(self, model, sys, prompt):
        try:
            # We don't maintain full history buffer in state yet for simplicity, 
            # we just send prompt. Ideally we'd send history.
            # V5 Update: Let's simple-chain.
            context = ""
            if self.state["chat"]["last_response"]:
                context = f"Previous Context: {self.state['chat']['last_response']}\n\n"
            
            resp = self.client.generate(model, sys, context + prompt)
            self.ui.root.after(0, lambda: self._on_chat_complete(resp))
        except Exception as e:
            self.ui.root.after(0, lambda: self.log(f"Chat Error: {e}"))

    def _on_chat_complete(self, resp):
        self._append_session("ASSISTANT", resp)
        self.state["chat"]["last_response"] = resp

    # --- TASK LOGIC ---
    def collapse_all_steps(self):
        for s in self.steps: 
            if s.expanded: s.collapse()

    def add_step(self, data=None):
        container = self.ui.widgets["task_container"]
        idx = len(self.steps)
        step = TaskStepController(container, idx, self, self.roles, initial_data=data)
        step.pack(fill="x", pady=2)
        self.steps.append(step)
        if not data: step.expand()

    def delete_step(self, idx):
        if 0 <= idx < len(self.steps):
            self.steps[idx].destroy()
            self.steps.pop(idx)
            for i, s in enumerate(self.steps):
                s.index = i
                s.lbl_idx.config(text=f"#{i+1}")

    def run_manual_step(self, idx):
        # Visual Reset logic
        # If we run step 2, steps 3, 4, 5 become "Pending" (dirty)
        for i in range(idx + 1, len(self.steps)):
            self.steps[i].set_active("pending")
        
        # Hijack the standard runner
        self.run_step(idx)

    def on_run_click(self):
        # Continue chain logic
        self.run_step(self.current_step_idx)

    def run_step(self, idx):
        if idx >= len(self.steps): return
        
        self.current_step_idx = idx
        step = self.steps[idx]
        step._sync_data()

        if step.data["skip"]:
            self.log(f"Skipping Step #{idx+1}...")
            # Auto-advance if running chain, but wait if manual?
            # For now, just stop and let user click next.
            self.current_step_idx += 1
            if self.current_step_idx < len(self.steps):
                self.run_step(self.current_step_idx) # Auto skip
            return

        # UI Update
        for s in self.steps: 
            if s != step and s.header.cget("bg") == "#2563eb": 
                s.set_active("none")
        step.set_active("active")

        self.ui.widgets["btn_run"].config(text="Running...", state="disabled", bg="#475569")
        self.ui.widgets["btn_inject"].config(state="disabled")

        # Build Prompt
        raw_p = step.data["prompt"]
        final_p = resolve_template(raw_p, self.state)
        
        if not step.data["isolated"] and idx > 0 and self.state["chat"]["last_response"]:
             final_p += f"\n\n[CONTEXT]:\n{self.state['chat']['last_response']}"
        
        model = self.ui.widgets["cb_task_model"].get() or "qwen2.5:7b-instruct"
        role_name = step.data["role"]
        sys_p = self.roles.get_prompt(role_name)

        self.log(f"Running Step #{idx+1} on {model}...")
        threading.Thread(target=self._task_worker, args=(model, sys_p, final_p), daemon=True).start()

    def _task_worker(self, model, sys, prompt):
        try:
            resp = self.client.generate(model, sys, prompt)
            
            helper = self.ui.widgets["cb_helper"].get()
            thought = self.client.generate(helper, "Analyze.", f"TASK: {prompt[:100]}\nRESULT: {resp[:100]}")
            
            self.ui.root.after(0, lambda: self._on_task_complete(resp, thought))
        except Exception as e:
            self.ui.root.after(0, lambda: self.log(f"Task Error: {e}"))

    def _on_task_complete(self, resp, thought):
        self.ui.widgets["staging"].delete("1.0", "end")
        self.ui.widgets["staging"].insert("1.0", resp)
        self.ui.widgets["thoughts"].insert("end", f"Step {self.current_step_idx+1}: {thought}\n\n")
        
        btn = self.ui.widgets["btn_run"]
        is_last = self.current_step_idx == len(self.steps) - 1
        btn.config(text="Finish üèÅ" if is_last else "Confirm & Next ‚û°", state="normal", bg="#16a34a" if is_last else "#2563eb")
        self.ui.widgets["btn_inject"].config(state="normal")

    def inject_to_chat(self):
        content = self.ui.widgets["staging"].get("1.0", "end-1c")
        self._append_session("INJECTED", content)
        self.state["chat"]["last_response"] = content
        
        if self.current_step_idx < len(self.steps) - 1:
            self.current_step_idx += 1
            self.steps[self.current_step_idx].set_active("active")

    def _append_session(self, role, text):
        w = self.ui.widgets["session_log"]
        w.insert("end", f"\n[{role}]\n{text}\n" + "-"*40 + "\n")
        w.see("end")

    # --- LOGGING UTILS ---
    def open_log_config(self):
        d = LogConfigDialog(self.ui.root, self.log_config)
        if d.result:
            self.log_config = d.result
            self.log(f"Log config updated: {self.log_config}")

    def save_log_file(self):
        d = self.log_config["dir"]
        if not os.path.exists(d): os.makedirs(d)
        
        name = "system_log"
        if self.log_config["timestamp"]:
            name += f"_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        name += ".txt"
        
        path = os.path.join(d, name)
        content = self.ui.widgets["log_console"].get("1.0", "end-1c")
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        
        messagebox.showinfo("Saved", f"Log saved to {path}")

if __name__ == "__main__":
    root = tk.Tk()
    app = WorkbenchApp(root)
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\gui_layout.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, scrolledtext

class WorkbenchUI:
    def __init__(self, root):
        self.root = root
        self.root.title("_CognitiveWORKBENCH v5.1 [Scaffolded]")
        self.root.geometry("1900x1000")
        self.root.configure(bg="#0f172a")
        
        self.widgets = {}
        self._setup_styles()
        self._build_layout()

    def _setup_styles(self):
        style = ttk.Style()
        style.theme_use('clam')
        style.configure("TFrame", background="#0f172a")
        style.configure("TLabel", background="#1e293b", foreground="#e2e8f0")
        style.configure("TButton", background="#334155", foreground="white", borderwidth=0)
        style.map("TButton", background=[('active', '#475569')])
        style.configure("Card.TFrame", background="#1e293b", relief="flat")
        
        # Log Panel
        style.configure("Log.TFrame", background="#020617", borderwidth=2, relief="sunken")
        
        # Notebook (Tabs)
        style.configure("TNotebook", background="#0f172a", borderwidth=0)
        style.configure("TNotebook.Tab", background="#334155", foreground="white", padding=[10, 5], font=("Segoe UI", 9))
        style.map("TNotebook.Tab", background=[("selected", "#1e293b")], foreground=[("selected", "#60a5fa")])

    def _build_layout(self):
        # Master Vertical Paned Window
        self.main_split = tk.PanedWindow(self.root, orient=tk.VERTICAL, bg="#0f172a", sashwidth=6)
        self.main_split.pack(fill="both", expand=True)

        # --- UPPER SECTION: 3-COLUMNS ---
        self.paned_cols = tk.PanedWindow(self.main_split, orient=tk.HORIZONTAL, bg="#0f172a", sashwidth=6)
        self.main_split.add(self.paned_cols, stretch="always", minsize=600)

        # === COL 1: SUBCONSCIOUS (Left) ===
        self.f_left = tk.Frame(self.paned_cols, bg="#020617")
        self.paned_cols.add(self.f_left, minsize=350, stretch="always")
        
        h_left = tk.Frame(self.f_left, bg="#020617", pady=5)
        h_left.pack(fill="x")
        tk.Label(h_left, text="‚ö° SUBCONSCIOUS", bg="#020617", fg="#64748b", font=("Segoe UI", 9, "bold")).pack(side="left", padx=5)
        
        self.cb_helper = ttk.Combobox(h_left, state="readonly", width=25)
        self.cb_helper.pack(side="right", padx=5)
        self.widgets["cb_helper"] = self.cb_helper
        
        self.txt_thoughts = scrolledtext.ScrolledText(self.f_left, bg="#020617", fg="#94a3b8", borderwidth=0, font=("Consolas", 9))
        self.txt_thoughts.pack(fill="both", expand=True, padx=5)
        self.widgets["thoughts"] = self.txt_thoughts

        f_left_foot = tk.Frame(self.f_left, bg="#1e293b", height=40)
        f_left_foot.pack(side="bottom", fill="x")
        self.btn_settings = tk.Button(f_left_foot, text="‚öô Settings / Embedder", bg="#1e293b", fg="#94a3b8", relief="flat")
        self.btn_settings.pack(side="left", padx=10, pady=5)
        self.widgets["btn_settings"] = self.btn_settings

        # === COL 2: SESSION LTM (Center) ===
        self.f_center = tk.Frame(self.paned_cols, bg="#0f172a")
        self.paned_cols.add(self.f_center, minsize=600, stretch="always")

        h_center = tk.Frame(self.f_center, bg="#1e293b", height=50)
        h_center.pack(fill="x")
        tk.Label(h_center, text="SESSION", bg="#1e293b", fg="white", font=("Segoe UI", 11, "bold")).pack(side="left", padx=10)
        
        tk.Label(h_center, text="Chat Model:", bg="#1e293b", fg="#94a3b8").pack(side="left", padx=(15, 5))
        self.cb_chat_model = ttk.Combobox(h_center, state="readonly", width=30)
        self.cb_chat_model.pack(side="left")
        self.widgets["cb_chat_model"] = self.cb_chat_model

        # Session Role Controls
        f_role_sess = tk.Frame(h_center, bg="#1e293b")
        f_role_sess.pack(side="left", padx=15)
        tk.Label(f_role_sess, text="Role:", bg="#1e293b", fg="#94a3b8").pack(side="left")
        self.cb_chat_role = ttk.Combobox(f_role_sess, state="readonly", width=20)
        self.cb_chat_role.pack(side="left", padx=2)
        self.widgets["cb_chat_role"] = self.cb_chat_role
        
        self.btn_add_role_chat = tk.Button(f_role_sess, text="+", bg="#334155", fg="white", width=3)
        self.btn_add_role_chat.pack(side="left")
        self.widgets["btn_add_role_chat"] = self.btn_add_role_chat

        self.btn_onboard = tk.Button(h_center, text="Catch Me Up", bg="#2563eb", fg="white", font=("Segoe UI", 9))
        self.btn_onboard.pack(side="right", padx=10)
        self.widgets["btn_onboard"] = self.btn_onboard

        self.txt_session = scrolledtext.ScrolledText(self.f_center, bg="#0f172a", fg="#e2e8f0", borderwidth=0, font=("Segoe UI", 10), insertbackground="white")
        self.txt_session.pack(fill="both", expand=True, padx=10, pady=(10, 0))
        self.widgets["session_log"] = self.txt_session

        f_chat_input = tk.Frame(self.f_center, bg="#1e293b", pady=5, padx=5)
        f_chat_input.pack(fill="x", padx=10, pady=10)
        
        self.txt_chat_input = tk.Text(f_chat_input, height=4, bg="#0f172a", fg="white", borderwidth=0, font=("Segoe UI", 10), insertbackground="white")
        self.txt_chat_input.pack(side="left", fill="x", expand=True, padx=(0, 5))
        self.widgets["chat_input"] = self.txt_chat_input
        
        self.btn_send_chat = tk.Button(f_chat_input, text="SEND ‚û§", bg="#16a34a", fg="white", font=("Segoe UI", 10, "bold"), width=10)
        self.btn_send_chat.pack(side="right", fill="y")
        self.widgets["btn_send_chat"] = self.btn_send_chat

        # === COL 3: WORKFLOW (Right) - NOW TABBED ===
        self.f_right_container = tk.Frame(self.paned_cols, bg="#1e293b")
        self.paned_cols.add(self.f_right_container, minsize=550, stretch="always")

        # The Notebook
        self.notebook = ttk.Notebook(self.f_right_container)
        self.notebook.pack(fill="both", expand=True, padx=5, pady=5)

        # -- TAB 1: EXECUTION (The Active Workbench) --
        self.tab_exec = tk.Frame(self.notebook, bg="#1e293b")
        self.notebook.add(self.tab_exec, text=" ‚ñ∂ Execution ")
        self._build_execution_tab(self.tab_exec)

        # -- TAB 2: TASKLISTS (Manager) --
        self.tab_tasks = tk.Frame(self.notebook, bg="#0f172a")
        self.notebook.add(self.tab_tasks, text=" Tasklists ")
        tk.Label(self.tab_tasks, text="Tasklist Manager / Engineering (Coming Soon)", bg="#0f172a", fg="#475569").pack(expand=True)

        # -- TAB 3: PROMPTS (Manager) --
        self.tab_prompts = tk.Frame(self.notebook, bg="#0f172a")
        self.notebook.add(self.tab_prompts, text=" Prompts ")
        tk.Label(self.tab_prompts, text="Prompt Engineering Library (Coming Soon)", bg="#0f172a", fg="#475569").pack(expand=True)

        # -- TAB 4: CARTRIDGES (Manager) --
        self.tab_carts = tk.Frame(self.notebook, bg="#0f172a")
        self.notebook.add(self.tab_carts, text=" Cartridges ")
        tk.Label(self.tab_carts, text="RAG Cartridge Manager (Coming Soon)", bg="#0f172a", fg="#475569").pack(expand=True)

        # --- LOWER SECTION: SYSTEM LOG ---
        self.f_log = ttk.Frame(self.main_split, style="Log.TFrame")
        self.main_split.add(self.f_log, minsize=150, stretch="never")
        
        # Log Content (Top)
        self.txt_log = scrolledtext.ScrolledText(self.f_log, height=8, bg="#020617", fg="#475569", font=("Consolas", 9), borderwidth=0)
        self.txt_log.pack(fill="both", expand=True, side="top")
        self.widgets["log_console"] = self.txt_log

        # Log Toolbar (Bottom)
        f_log_ctrl = tk.Frame(self.f_log, bg="#0f172a", height=25)
        f_log_ctrl.pack(side="bottom", fill="x")
        
        tk.Label(f_log_ctrl, text="SYSTEM LOG", bg="#0f172a", fg="#64748b", font=("Segoe UI", 8, "bold")).pack(side="left", padx=5)
        
        self.btn_log_save = tk.Button(f_log_ctrl, text="üíæ Save Log", bg="#334155", fg="white", font=("Segoe UI", 8))
        self.btn_log_save.pack(side="right", padx=2, pady=2)
        self.widgets["btn_log_save"] = self.btn_log_save
        
        self.btn_log_config = tk.Button(f_log_ctrl, text="‚öô Config", bg="#334155", fg="white", font=("Segoe UI", 8))
        self.btn_log_config.pack(side="right", padx=2, pady=2)
        self.widgets["btn_log_config"] = self.btn_log_config

    def _build_execution_tab(self, parent):
        # Header
        h_right = tk.Frame(parent, bg="#334155", height=45)
        h_right.pack(fill="x")
        tk.Label(h_right, text="WORKBENCH", bg="#334155", fg="white", font=("Segoe UI", 10, "bold")).pack(side="left", padx=10)
        
        tk.Label(h_right, text="Task Model:", bg="#334155", fg="#cbd5e1").pack(side="left", padx=(15, 5))
        self.cb_task_model = ttk.Combobox(h_right, state="readonly", width=30)
        self.cb_task_model.pack(side="left")
        self.widgets["cb_task_model"] = self.cb_task_model

        self.btn_cartridge = tk.Button(h_right, text="üì¶ Make Cartridge", bg="#475569", fg="white")
        self.btn_cartridge.pack(side="right", padx=5)
        self.widgets["btn_cartridge"] = self.btn_cartridge

        # Task Steps Editor
        self.canvas_tasks = tk.Canvas(parent, bg="#1e293b", highlightthickness=0)
        self.scroll_tasks = ttk.Scrollbar(parent, orient="vertical", command=self.canvas_tasks.yview)
        self.frame_tasks_inner = tk.Frame(self.canvas_tasks, bg="#1e293b")
        
        self.canvas_tasks.create_window((0, 0), window=self.frame_tasks_inner, anchor="nw")
        self.canvas_tasks.configure(yscrollcommand=self.scroll_tasks.set)
        
        self.canvas_tasks.pack(side="top", fill="both", expand=True, padx=5, pady=5)
        self.scroll_tasks.pack(side="right", fill="y", in_=parent)
        self.frame_tasks_inner.bind("<Configure>", lambda e: self.canvas_tasks.configure(scrollregion=self.canvas_tasks.bbox("all")))
        self.widgets["task_container"] = self.frame_tasks_inner

        # Action Area
        f_action = tk.Frame(parent, bg="#0f172a", pady=10, padx=10)
        f_action.pack(side="bottom", fill="x")

        tk.Label(f_action, text="STAGING (Output Buffer)", bg="#0f172a", fg="#facc15", font=("Segoe UI", 8, "bold")).pack(anchor="w")
        self.txt_staging = tk.Text(f_action, height=10, bg="#1e293b", fg="white", borderwidth=1, relief="solid", insertbackground="white")
        self.txt_staging.pack(fill="x", pady=(5, 5))
        self.widgets["staging"] = self.txt_staging

        f_btns = tk.Frame(f_action, bg="#0f172a")
        f_btns.pack(fill="x")
        
        self.btn_add_step = tk.Button(f_btns, text="+ Step", bg="#334155", fg="white")
        self.btn_add_step.pack(side="left", padx=(0, 5))
        self.widgets["btn_add_step"] = self.btn_add_step

        self.btn_inject = tk.Button(f_btns, text="‚Üô Inject to Chat", bg="#0f766e", fg="white", state="disabled")
        self.btn_inject.pack(side="left", padx=5)
        self.widgets["btn_inject"] = self.btn_inject
        
        tk.Frame(f_btns, bg="#0f172a").pack(side="left", expand=True)

        self.btn_run = tk.Button(f_btns, text="RUN STEP ‚û°", bg="#2563eb", fg="white", font=("Segoe UI", 10, "bold"), width=15)
        self.btn_run.pack(side="right")
        self.widgets["btn_run"] = self.btn_run

--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\_microservices\ollama_client.py
--------------------------------------------------------------------------------
import json
import urllib.request
import urllib.error


class OllamaClient:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url.rstrip("/")

    # -----------------------------
    # Low-level helpers
    # -----------------------------

    def _read_json(self, resp) -> dict:
        raw = resp.read().decode("utf-8", errors="replace")
        try:
            return json.loads(raw)
        except Exception as e:
            raise RuntimeError(f"Ollama returned non-JSON response: {e}\nRAW:\n{raw}")

    def _request_json(self, method: str, path: str, payload: dict | None = None, timeout: int = 60) -> dict:
        url = f"{self.base_url}{path}"

        data = None
        headers = {}
        if payload is not None:
            data = json.dumps(payload).encode("utf-8")
            headers["Content-Type"] = "application/json"

        req = urllib.request.Request(url, data=data, headers=headers, method=method)

        try:
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                return self._read_json(resp)
        except urllib.error.HTTPError as e:
            # HTTPError is also a file-like object; it may contain JSON
            try:
                body = e.read().decode("utf-8", errors="replace")
            except Exception:
                body = "<unable to read body>"
            raise RuntimeError(f"Ollama HTTPError {e.code} for {method} {path}: {e.reason}\nBODY:\n{body}")
        except urllib.error.URLError as e:
            raise RuntimeError(f"Ollama URLError for {method} {path}: {e}")

    # -----------------------------
    # Public API
    # -----------------------------

    def list_models(self) -> list[str]:
        """Return list of installed model names via Ollama /api/tags."""
        obj = self._request_json("GET", "/api/tags", payload=None, timeout=30)
        models = []
        for m in obj.get("models", []) or []:
            name = m.get("name")
            if name:
                models.append(name)
        return models

    def generate(self, model: str, system: str, prompt: str, options: dict | None = None) -> str:
        """Uses Ollama /api/generate with system+prompt. Returns response text."""
        payload = {
            "model": model,
            "prompt": prompt,
            "system": system,
            "stream": False
        }
        if options:
            payload["options"] = options

        obj = self._request_json("POST", "/api/generate", payload=payload, timeout=300)
        return obj.get("response", "")


--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\_microservices\runner.py
--------------------------------------------------------------------------------
import json
from typing import Any, Dict, List, Optional, Tuple

from src._microservices.ollama_client import OllamaClient
from src._microservices.template_engine import resolve_template

_client = OllamaClient()


def _set_by_path(state: dict, path: str, value):
    # path like "working.step_outputs.S1" or "chat.last_response"
    parts = path.split(".")
    cur = state
    for p in parts[:-1]:
        cur = cur.setdefault(p, {})
    cur[parts[-1]] = value


def _get_by_path(state: dict, path: str):
    parts = path.split(".")
    cur = state
    for p in parts:
        if not isinstance(cur, dict) or p not in cur:
            return None
        cur = cur[p]
    return cur


def _ensure_state_defaults(state: dict) -> dict:
    """
    Runner-friendly defaults so UI can pass a minimal state in.
    """
    state.setdefault("chat", {})
    state["chat"].setdefault("history", [])          # list[{role, content}]
    state["chat"].setdefault("last_user", "")
    state["chat"].setdefault("last_response", "")

    state.setdefault("working", {})
    state["working"].setdefault("step_outputs", {})  # dict step_id -> output
    state["working"].setdefault("thoughts", [])      # list[{step_id, name, summary, errors?}]
    state["working"].setdefault("notes", [])         # list[{step_id, name, errors}]

    state.setdefault("outputs", {})
    state["outputs"].setdefault("final", "")

    return state


def _chain_prompt(base_prompt: str, chain_mode: str, last: str) -> str:
    """
    chain_mode:
      - "none": base_prompt only
      - "last": base_prompt + "\n\n---\nLAST_OUTPUT:\n" + last
      - "replace": replace occurrences of "{{last}}" in base_prompt
    """
    last = last or ""
    chain_mode = (chain_mode or "none").strip().lower()

    if chain_mode == "none":
        return base_prompt
    if chain_mode == "last":
        if not last:
            return base_prompt
        return f"{base_prompt}\n\n---\nLAST_OUTPUT:\n{last}"
    if chain_mode == "replace":
        return base_prompt.replace("{{last}}", last)

    # unknown -> safe default
    return base_prompt


def _safe_json_loads(text: str) -> Tuple[Optional[Any], Optional[str]]:
    try:
        return json.loads(text), None
    except Exception as e:
        return None, str(e)


def _run_helper_summary(
    helper_model: str,
    helper_system: str,
    helper_template: str,
    step_name: str,
    step_output: str,
    state: dict,
    options: Optional[dict] = None
) -> str:
    """
    Summarize what happened in this step. The template can reference:
      - {{state....}} via template_engine
      - {{step_name}}
      - {{step_output}}
    """
    tmp_state = dict(state)
    tmp_state.setdefault("_step_ctx", {})
    tmp_state["_step_ctx"]["step_name"] = step_name
    tmp_state["_step_ctx"]["step_output"] = step_output

    # allow easy placeholders without complicating template_engine:
    prompt = helper_template.replace("{{step_name}}", step_name).replace("{{step_output}}", step_output)
    prompt = resolve_template(prompt, tmp_state)

    raw = _client.generate(
        model=helper_model,
        system=helper_system,
        prompt=prompt,
        options=options or {"temperature": 0.2}
    )
    return (raw or "").strip()


def run_tasklist(
    state: dict,
    tasklist: dict,
    chat_model_default: str,
    helper_model_default: str,
) -> dict:
    """
    Generic tasklist runner.

    Each step:
      - builds a prompt from user_prompt_template (+ optional chaining)
      - calls an Ollama model
      - stores output to output_key
      - optionally runs helper summary (thought bubble)

    Expected step fields (minimal):
      - id (str)
      - name (str)
      - enabled (bool)
      - model (optional; defaults to chat_model_default)
      - system_prompt (str)
      - user_prompt_template (str)
      - chain_mode ("none" | "last" | "replace") optional
      - output_key (defaults to "working.step_outputs.<id>")
      - expects ("text" | "json") optional
      - thought_enabled (bool) optional
      - thought_model (optional; defaults helper_model_default)
      - thought_system_prompt (optional)
      - thought_prompt_template (optional)
    """
    state = _ensure_state_defaults(state)

    steps = tasklist.get("steps", [])
    last = state["chat"].get("last_response", "")

    for step in steps:
        if not step.get("enabled", True):
            continue

        step_id = step.get("id", "STEP?")
        step_name = step.get("name", step_id)

        model = step.get("model") or chat_model_default
        system = step.get("system_prompt", "")
        template = step.get("user_prompt_template", "")
        options = step.get("ollama_options") or {}

        chain_mode = step.get("chain_mode", "none")
        expects = step.get("expects", "text").strip().lower()

        output_key = step.get("output_key") or f"working.step_outputs.{step_id}"

        # 1) build prompt
        base_prompt = resolve_template(template, state)
        prompt = _chain_prompt(base_prompt, chain_mode, last)

        # 2) call model
        raw = _client.generate(model=model, system=system, prompt=prompt, options=options)
        raw_stripped = (raw or "").strip()

        # 3) parse/store
        produced_obj = None
        if expects == "json":
            produced_obj, err = _safe_json_loads(raw_stripped)
            if err:
                state["working"]["notes"].append({
                    "step_id": step_id,
                    "name": step_name,
                    "errors": [f"json_parse failed: {err}"]
                })
                # fall back to raw text storage
                _set_by_path(state, output_key, raw_stripped)
                last = raw_stripped
            else:
                _set_by_path(state, output_key, produced_obj)
                last = json.dumps(produced_obj, ensure_ascii=False)
        else:
            _set_by_path(state, output_key, raw_stripped)
            last = raw_stripped

        # record runner-level last_response
        state["chat"]["last_response"] = last

        # 4) thought bubble (helper model)
        if step.get("thought_enabled", True):
            helper_model = step.get("thought_model") or helper_model_default
            helper_system = step.get("thought_system_prompt") or (
                "Summarize what happened in this step in 1-3 bullet points. "
                "Be concrete. No fluff. No preamble."
            )
            helper_template = step.get("thought_prompt_template") or (
                "STEP: {{step_name}}\n\nOUTPUT:\n{{step_output}}\n\n"
                "Return 1-3 bullets: what changed / what was decided / what to do next."
            )

            try:
                summary = _run_helper_summary(
                    helper_model=helper_model,
                    helper_system=helper_system,
                    helper_template=helper_template,
                    step_name=step_name,
                    step_output=last,
                    state=state,
                    options={"temperature": 0.2}
                )
            except Exception as e:
                summary = f"- Thought summary failed: {e}"

            state["working"]["thoughts"].append({
                "step_id": step_id,
                "name": step_name,
                "summary": summary
            })

    state["outputs"]["final"] = state["chat"].get("last_response", "")
    return state

--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\_microservices\state.py
--------------------------------------------------------------------------------
def new_state() -> dict:
    """Create a fresh state object for the Tasklist Chat Prototype.

    Structure notes:
      - chat: conversational inputs + last outputs
      - working: per-step artifacts (outputs, thought bubbles, errors)
      - outputs: final result produced by the tasklist runner
    """
    return {
        "chat": {
            "history": [],            # list of {"role": "user"|"assistant", "content": str}
            "last_user": "",          # last user message
            "last_response": ""       # last model output (string form)
        },
        "working": {
            "step_outputs": {},       # dict step_id -> output (text or parsed json)
            "thoughts": [],           # list of {step_id, name, summary}
            "notes": []               # list of {step_id, name, errors:[...]}
        },
        "outputs": {
            "final": ""              # final assistant message to display in chat
        }
    }


--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\_microservices\tasklists.py
--------------------------------------------------------------------------------
def get_tasklist_names(mode: str):
    tl = _TASKLISTS.get(mode, {})
    return list(tl.keys())

def load_tasklist_by_name(mode: str, name: str) -> dict:
    return _TASKLISTS[mode][name]

_TASKLISTS = {
    "validate_patch": {
        "validate_patch_v0": {
            "name": "validate_patch_v0",
            "mode": "validate_patch",
            "steps": [
                {
                    "id": "V1",
                    "name": "Normalize patch JSON",
                    "enabled": True,
                    "model": "qwen2.5:7b-coder",
                    "ollama_options": {"temperature": 0.0},
                    "system_prompt": "You are a strict JSON normalizer. Output ONLY valid JSON. No commentary.",
                    "user_prompt_template": "PATCH_JSON_INPUT:\n{{state.inputs.existing_patch_json}}\n\nReturn valid JSON only.",
                    "expects": "json",
                    "output_key": "working.candidate_patch",
                    "validators": ["json_parse"],
                    "on_fail": "retry"
                },
                {
                    "id": "V2",
                    "name": "Enforce schema and output final patch JSON",
                    "enabled": True,
                    "model": "qwen2.5:7b-coder",
                    "ollama_options": {"temperature": 0.0},
                    "system_prompt": "Output ONLY JSON that matches EXACT schema: {hunks:[{description,search_block,replace_block,use_patch_indent}]}. No extra keys. hunks non-empty. use_patch_indent boolean.",
                    "user_prompt_template": "CANDIDATE:\n{{state.working.candidate_patch}}\n\nReturn schema-valid patch JSON only.",
                    "expects": "patch_json",
                    "output_key": "outputs.final_patch",
                    "validators": ["schema_strict"],
                    "on_fail": "retry"
                }
            ]
        }
    },
    "repair_patch": {
        "repair_patch_v0": {
            "name": "repair_patch_v0",
            "mode": "repair_patch",
            "steps": [
                {
                    "id": "R1",
                    "name": "Rewrite hunks to match current file",
                    "enabled": True,
                    "model": "qwen2.5:7b-coder",
                    "ollama_options": {"temperature": 0.2},
                    "system_prompt": (
                        "You are a patch surgeon for TokenizingPATCHER.\n"
                        "MUST output ONLY schema-valid patch JSON.\n"
                        "Rules:\n"
                        "- Every search_block MUST appear verbatim in TARGET_FILE.\n"
                        "- replace_block must be concrete (no placeholders).\n"
                        "- No extra keys."
                    ),
                    "user_prompt_template": (
                        "TARGET_FILE:\n{{state.inputs.target_file_text}}\n\n"
                        "FAILING_PATCH_JSON:\n{{state.inputs.existing_patch_json}}\n\n"
                        "ERROR_LOG:\n{{state.inputs.error_log_text}}\n\n"
                        "Produce corrected patch JSON."
                    ),
                    "expects": "patch_json",
                    "output_key": "outputs.final_patch",
                    "validators": ["schema_strict", "match_search_blocks"],
                    "on_fail": "retry"
                }
            ]
        }
    },
    "create_patch": {
        "create_patch_v0": {
            "name": "create_patch_v0",
            "mode": "create_patch",
            "steps": [
                {
                    "id": "C1",
                    "name": "Generate patch from snippet + file",
                    "enabled": True,
                    "model": "qwen2.5:7b-coder",
                    "ollama_options": {"temperature": 0.2},
                    "system_prompt": (
                        "Generate TokenizingPATCHER patch JSON.\n"
                        "Hard rules:\n"
                        "- Output ONLY schema-valid patch JSON.\n"
                        "- Every search_block MUST be verbatim substring from TARGET_FILE.\n"
                        "- replace_block must be concrete and placeholder-free.\n"
                        "- No extra keys."
                    ),
                    "user_prompt_template": (
                        "TARGET_FILE:\n{{state.inputs.target_file_text}}\n\n"
                        "CHANGE_SNIPPET:\n{{state.inputs.snippet_text}}\n\n"
                        "Create patch JSON."
                    ),
                    "expects": "patch_json",
                    "output_key": "outputs.final_patch",
                    "validators": ["schema_strict", "match_search_blocks"],
                    "on_fail": "retry"
                }
            ]
        }
    }
}

--------------------------------------------------------------------------------
FILE: _TasklistWORKER\src\_microservices\template_engine.py
--------------------------------------------------------------------------------
def resolve_template(template: str, state: dict) -> str:
    """
    Minimal {{state.path.to.key}} templating.
    Only supports read access, no function calls.
    """
    out = template

    # very small & safe on purpose
    # pattern scanning without regex to keep it obvious
    while True:
        start = out.find("{{")
        if start == -1:
            break
        end = out.find("}}", start)
        if end == -1:
            break
        expr = out[start+2:end].strip()

        val = ""
        if expr.startswith("state."):
            path = expr[len("state."):].split(".")
            cur = state
            ok = True
            for p in path:
                if isinstance(cur, dict) and p in cur:
                    cur = cur[p]
                else:
                    ok = False
                    break
            if ok:
                val = cur if isinstance(cur, str) else str(cur)

        out = out[:start] + val + out[end+2:]
    return out

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\requirements.txt
--------------------------------------------------------------------------------
tk
ollama
requests
# Recommended additions for the new architecture:
typing-extensions  # Supports advanced type hinting used in MS files [cite: 70, 77]
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\setup_env.bat
--------------------------------------------------------------------------------
@echo off
setlocal
echo [SYSTEM] Performing Nuclear Reset and Environment Initialization...

:: 0. Clean up old environments and caches
echo [SYSTEM] Purging old .venv, venv, and __pycache__...
if exist .venv rd /s /q .venv 
if exist venv rd /s /q venv
for /d /r . %%d in (__pycache__) do @if exist "%%d" rd /s /q "%%d"
del /s /q *.pyc >nul 2>&1

:: 1. Create the venv
echo [SYSTEM] Creating fresh .venv...
py -m venv .venv 

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip 
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt 
) else (
    echo [WARNING] requirements.txt not found!
)

echo.
echo [SUCCESS] Environment is now clean and ready! 
echo To launch the Workbench, run: .venv\Scripts\python.exe -m src.app 
pause
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\app.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import threading
import json
import os
import sys
import datetime
import requests

# --- CRITICAL PATH RESOLUTION ---
# Since we run via 'python -m src.app', we must find the repo root for microservices/
_current_dir = os.path.dirname(os.path.abspath(__file__))
_root_dir = os.path.abspath(os.path.join(_current_dir, ".."))
if _root_dir not in sys.path:
    sys.path.insert(0, _root_dir)
# --------------------------------

# --- INTEGRATED MICROSERVICES ---
# Using internal package imports
from src._microservices._ToolsMS import MicroserviceTools

# =========================================================
# 1. CORE CLIENTS
# =========================================================

class OllamaClient:
    """Manages local inference via Ollama."""
    def list_models(self):
        try:
            res = requests.get("http://localhost:11434/api/tags", timeout=2)
            if res.status_code == 200:
                return [m["name"] for m in res.json().get("models", [])]
        except Exception:
            pass
        return ["qwen2.5:7b-instruct", "qwen2.5:7b-coder", "qwen2.5:1.5b"]

    def generate(self, model, system, prompt):
        url = "http://localhost:11434/api/chat"
        payload = {
            "model": model,
            "messages": [
                {"role": "system", "content": system},
                {"role": "user", "content": prompt}
            ],
            "stream": False,
            "options": {"temperature": 0.2}
        }
        res = requests.post(url, json=payload, timeout=60)
        res.raise_for_status()
        return res.json()['message']['content']

class RoleManager:
    """Defines personas for the AI or signals for mechanical tools."""
    def __init__(self, app_ref=None):
        self.app = app_ref
        # Resolve path relative to this file's directory
        _base = os.path.dirname(os.path.abspath(__file__))
        _roles_path = os.path.join(_base, "_roles", "default_roles.json")
        
        if os.path.exists(_roles_path):
            try:
                with open(_roles_path, "r", encoding="utf-8") as f:
                    self.roles = json.load(f)
            except Exception:
                self.roles = {"Helpful Assistant": "Fallback: Load Error"}
        else:
            self.roles = {"Helpful Assistant": "Fallback: roles.json not found"}
    def get_names(self): return list(self.roles.keys())

    def get_prompt(self, name):
        """Extract the base_prompt from the structured role object."""
        role_data = self.roles.get(name, "")
        if isinstance(role_data, dict):
            return role_data.get("base_prompt", "")
        return role_data

    def get_system_prompt_by_id(self, prompt_id):
        """Lookup a specific instruction set from the system_prompts library."""
        if not self.app or not hasattr(self.app, 'system_prompts'):
            return ""
        
        # Search through all loaded system prompt files
        for filename, library in self.app.system_prompts.items():
            if prompt_id in library:
                return library[prompt_id].get('content', "")
        return ""

# =========================================================
# 2. UI COMPONENTS
# =========================================================

class TaskStepController(tk.Frame):
    """A single row in the iterative task list."""
    def __init__(self, parent, index, app_ref, role_mgr):
        super().__init__(parent, bg="#1e293b", bd=1, relief="flat", pady=2)
        self.app = app_ref
        self.index = index
        self.role_mgr = role_mgr
        # model: "(default)" means use the global model dropdown
        self.data = {"role": "Helpful Assistant", "prompt": "", "model": "(default)"}
        self._build_ui()

    def _build_ui(self):
        self.lbl_idx = tk.Label(self, text=f"#{self.index+1}", bg="#1e293b", fg="#94a3b8", width=3)
        self.lbl_idx.pack(side="left")

        self.cb_role = ttk.Combobox(self, values=self.role_mgr.get_names(), state="readonly", width=18)
        self.cb_role.pack(side="left", padx=5)
        self.cb_role.set(self.data["role"])
        self.cb_role.bind("<<ComboboxSelected>>", self._sync)

        # Per-step model override
        model_values = ["(default)"] + list(self.app.client.list_models())
        self.cb_model = ttk.Combobox(self, values=model_values, state="readonly", width=18)
        self.cb_model.pack(side="left", padx=5)
        self.cb_model.set(self.data.get("model") or "(default)")
        self.cb_model.bind("<<ComboboxSelected>>", self._sync)

        self.txt_prompt = tk.Entry(self, bg="#0f172a", fg="white", insertbackground="white", borderwidth=0)
        self.txt_prompt.pack(side="left", fill="x", expand=True, padx=5)
        self.txt_prompt.bind("<FocusOut>", self._sync)
        self.txt_prompt.bind("<Return>", self._run_step_from_enter)
        self.txt_prompt.bind("<KP_Enter>", self._run_step_from_enter)

        tk.Button(self, text="‚ñ∂", command=lambda: self.app.run_step(self.index), 
                  bg="#3b82f6", fg="white", font=("Arial", 9, "bold")).pack(side="right", padx=5)

        # Click row to recall last output for this step
        self.bind("<Button-1>", self._on_click)
        self.lbl_idx.bind("<Button-1>", self._on_click)
        self.cb_role.bind("<Button-1>", self._on_click)
        self.cb_model.bind("<Button-1>", self._on_click)
        self.txt_prompt.bind("<Button-1>", self._on_click)

        # Initial enable/disable based on role
        self._update_model_state()

    def _sync(self, e=None):
        self.data["role"] = self.cb_role.get()
        self.data["prompt"] = self.txt_prompt.get()
        self.data["model"] = self.cb_model.get() if hasattr(self, "cb_model") else "(default)"
        self._update_model_state()

    def _update_model_state(self):
        try:
            if self.cb_role.get() == "Mechanical Tool":
                self.cb_model.configure(state="disabled")
            else:
                self.cb_model.configure(state="readonly")
        except Exception:
            pass

    def _on_click(self, e=None):
        # Recall output for this step if available
        try:
            self.app.show_step_output(self.index)
        except Exception:
            pass

    def _run_step_from_enter(self, e=None):
        """Enter key runs this step."""
        self._sync()
        self.app.run_step(self.index)
        return "break"

# =========================================================
# 3. MAIN WORKBENCH APPLICATION
# =========================================================

class WorkbenchApp(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Systems Thinker: Microservice Transformer")
        self.geometry("1200x800")
        self.configure(bg="#0f172a")

        self.client = OllamaClient()
        # Link roles to self so it can access self.system_prompts later
        self.roles = RoleManager(self)
        self.steps = []

        # Tool Palette: The 'Swiss Army' manual for the AI
        self.tool_palette = {
            "create_backup": "Creates a timestamped backup of the target directory.",
            "read_template": "Loads the standard microservice boilerplate text.",
            "scan_file_structure": "Extracts imports, classes, and functions from a file via AST.",
            "apply_safe_patch": "Applies a TokenizingPatcher JSON hunk to the target file.",
            "get_cleanup_patch": "Generates a patch to fix common whitespace/formatting issues."
        }
        self.state = {"last_response": "", "current_file": ""}

        # Per-step output history
        # step_outputs[idx] = {"content": str, "success": bool|None, "ts": str, "log_msg": str}
        self.step_outputs = {}
        self._selected_step_index = None

        # RUN-ALL state
        self._run_all_active = False
        self._run_all_index = 0
        self._run_all_last_stop_reason = ""

        # Initialize ToolsMS pointing to the repo root so it can find std_lib and base_service
        self.tools_engine = MicroserviceTools(_root_dir)
        self.recipe_path = tk.StringVar()

        # Build UI first so widgets like txt_log exist before any logging occurs
        self._build_ui()

        # Initialize storage for new domains
        self.system_prompts = self._load_json_dir("_system_prompts")
        self.workflows = self._load_json_dir("_workflows")
        self._workflow_active = False
        self.log("Workbench Ready. Tooling Cartridge initialized.")

    def _build_ui(self):
        # Top Config
        top = tk.Frame(self, bg="#1e293b", pady=10)
        top.pack(fill="x")
        
        tk.Label(top, text="TARGET DIR:", bg="#1e293b", fg="white").pack(side="left", padx=(10, 5))
        self.ent_dir = tk.Entry(top, width=50, bg="#0f172a", fg="white")
        self.ent_dir.insert(0, os.getcwd())
        self.ent_dir.pack(side="left", padx=5)
        tk.Button(top, text="Browse", command=self._browse).pack(side="left")

        tk.Label(top, text="FILE:", bg="#1e293b", fg="#fbbf24").pack(side="left", padx=(15, 5))
        self.ent_file = tk.Entry(top, width=20, bg="#0f172a", fg="white")
        self.ent_file.insert(0, "dirty_service.py")
        self.ent_file.pack(side="left", padx=5)

        self.var_test_mode = tk.BooleanVar(value=True)
        tk.Checkbutton(top, text="TEST MODE (SANDBOX)", variable=self.var_test_mode, 
                       bg="#1e293b", fg="#f87171", selectcolor="#0f172a").pack(side="left", padx=10)

        self.cb_model = ttk.Combobox(top, values=self.client.list_models(), width=25)
        self.cb_model.pack(side="right", padx=10)
        if self.cb_model['values']: self.cb_model.set(self.cb_model['values'][0])
        tk.Label(top, text="MODEL:", bg="#1e293b", fg="white").pack(side="right")

        # Layout
        main_panes = tk.PanedWindow(self, orient="horizontal", bg="#0f172a", sashwidth=4)
        main_panes.pack(fill="both", expand=True)

        # Left: Task Recipe
        left_frame = tk.Frame(main_panes, bg="#0f172a")
        main_panes.add(left_frame, width=500)
        
        tk.Label(left_frame, text="ITERATIVE TASK LIST", bg="#0f172a", fg="#3b82f6", font=("Arial", 10, "bold")).pack(pady=5)
        self.step_container = tk.Frame(left_frame, bg="#0f172a")
        self.step_container.pack(fill="both", expand=True, padx=5)
        
        f_recipe_actions = tk.Frame(left_frame, bg="#0f172a")
        f_recipe_actions.pack(fill="x", side="bottom", pady=5, padx=5)
        
        tk.Button(f_recipe_actions, text="LOAD RECIPE", command=self.load_tasklist, bg="#1e293b", fg="#3b82f6").pack(side="left", expand=True, fill="x", padx=2)
        tk.Button(f_recipe_actions, text="SAVE RECIPE", command=self.save_tasklist, bg="#1e293b", fg="#10b981").pack(side="left", expand=True, fill="x", padx=2)
        tk.Button(f_recipe_actions, text="RUN ALL", command=self.run_all_steps, bg="#1e293b", fg="#fbbf24").pack(side="left", expand=True, fill="x", padx=2)
        tk.Button(f_recipe_actions, text="üöÄ WORKFLOW", command=lambda: self.run_workflow("default_workflow"), bg="#4f46e5", fg="white").pack(side="left", expand=True, fill="x", padx=2)
        tk.Button(left_frame, text="+ ADD STEP", command=self.add_step, bg="#334155", fg="white").pack(fill="x", pady=(5, 0), padx=5)

        # Right: Response & Log
        right_panes = tk.PanedWindow(main_panes, orient="vertical", bg="#0f172a")
        main_panes.add(right_panes)

        self.txt_response = scrolledtext.ScrolledText(right_panes, bg="#1e1e1e", fg="white", font=("Consolas", 10))
        right_panes.add(self.txt_response, height=450)

        self.txt_summary = scrolledtext.ScrolledText(right_panes, bg="#0b1220", fg="#e2e8f0", font=("Consolas", 9))
        right_panes.add(self.txt_summary, height=140)

        self.txt_log = scrolledtext.ScrolledText(right_panes, bg="#000000", fg="#00ff00", font=("Consolas", 9))
        right_panes.add(self.txt_log)

        # Initial Default Steps
        self.add_step("Mechanical Tool", "scan_file_structure")
        self.add_step("Strict Analyst", "Analyze the AST and plan migration.")

    def _request_missing_template(self, name):
        """Opens a popup window to allow the user to paste a new boilerplate."""
        result = {"text": None}
        dialog = tk.Toplevel(self)
        dialog.title(f"Missing Boilerplate: {name}")
        dialog.geometry("600x500")
        
        tk.Label(dialog, text=f"Paste the boilerplate/template for '{name}' below:", pady=10).pack()
        txt = scrolledtext.ScrolledText(dialog, bg="#1e1e1e", fg="white", font=("Consolas", 10))
        txt.pack(fill="both", expand=True, padx=10, pady=10)

        def _save():
            result["text"] = txt.get("1.0", "end-1c")
            dialog.destroy()

        tk.Button(dialog, text="SAVE & RESUME", command=_save, bg="#10b981", fg="white", pady=5).pack(pady=10)
        self.wait_window(dialog)
        return result["text"]

    def _load_json_dir(self, folder_name):
        """Utility to scan src/_folder/ for all JSON configuration objects."""
        data = {}
        _base = os.path.dirname(os.path.abspath(__file__))
        target_path = os.path.join(_base, folder_name)
        if os.path.exists(target_path):
            for f_name in os.listdir(target_path):
                if f_name.endswith(".json"):
                    try:
                        with open(os.path.join(target_path, f_name), "r", encoding="utf-8") as f:
                            data[f_name] = json.load(f)
                    except Exception as e:
                        self.log(f"Error loading {f_name}: {e}")
        return data

    def log(self, msg):
        ts = datetime.datetime.now().strftime("%H:%M:%S")

        def _do_log():
            # Safety check: skip logging if UI isn't ready yet
            if not hasattr(self, 'txt_log'):
                print(f"[{ts}] {msg}")
                return
            self.txt_log.insert("end", f"[{ts}] {msg}\n")
            self.txt_log.see("end")

        # Tkinter widgets must only be touched from the main/UI thread
        if threading.current_thread() is threading.main_thread():
            _do_log()
        else:
            self.after(0, _do_log)

    def _browse(self):
        d = filedialog.askdirectory()
        if d:
            self.ent_dir.delete(0, "end")
            self.ent_dir.insert(0, d)
            # Keep ToolsMS anchored to repo root (not the selected working dir)
            self.tools_engine = MicroserviceTools(_root_dir)

    def add_step(self, role=None, prompt=None):
        s = TaskStepController(self.step_container, len(self.steps), self, self.roles)
        if role: s.cb_role.set(role)
        if prompt: s.txt_prompt.insert(0, prompt)
        s.pack(fill="x", pady=1)
        self.steps.append(s)

    def save_tasklist(self):
        """Export current steps to JSON recipe."""
        recipe = []
        for s in self.steps:
            s._sync()
            recipe.append(s.data)
        
        path = filedialog.asksaveasfilename(defaultextension=".json", filetypes=[("JSON Recipe", "*.json")])
        if path:
            with open(path, "w") as f:
                json.dump(recipe, f, indent=2)
            self.log(f"Recipe saved to {os.path.basename(path)}")

    def load_tasklist(self):
        """Import steps from JSON recipe."""
        path = filedialog.askopenfilename(filetypes=[("JSON Recipe", "*.json")])
        if path:
            with open(path, "r") as f:
                recipe = json.load(f)
            
            # Clear existing steps
            for s in self.steps: s.destroy()
            self.steps = []
            
            for item in recipe:
                self.add_step(item.get("role"), item.get("prompt"))
                # Apply optional per-step model override
                try:
                    if self.steps and item.get("model"):
                        self.steps[-1].data["model"] = item.get("model")
                        if hasattr(self.steps[-1], "cb_model"):
                            self.steps[-1].cb_model.set(item.get("model"))
                        self.steps[-1]._sync()
                except Exception:
                    pass
            self.log(f"Loaded {len(recipe)} steps from {os.path.basename(path)}")

    def show_step_output(self, idx: int):
        """Show the most recent output for a step (if it has run)."""
        self._selected_step_index = idx
        record = self.step_outputs.get(idx)
        if not record:
            return

        content = record.get("content", "")
        # Avoid triggering RUN ALL advance; this is a manual view action
        def _do_show():
            self.txt_response.delete("1.0", "end")
            self.txt_response.insert("1.0", content)
        if threading.current_thread() is threading.main_thread():
            _do_show()
        else:
            self.after(0, _do_show)

    def run_step(self, idx):
        step = self.steps[idx]
        step._sync()
        role = step.data["role"]
        prompt = step.data["prompt"]
        
        self.log(f"Initiating Step #{idx+1} ({role})")
        
        if role == "Mechanical Tool":
            threading.Thread(target=self._tool_worker, args=(prompt, idx), daemon=True).start()
        else:
            # Per-step model override ("(default)" falls back to global)
            step_model = step.data.get("model")
            model = self.cb_model.get() if not step_model or step_model == "(default)" else step_model
            sys_p = self.roles.get_prompt(role)
            # Inject history context
            full_p = f"{prompt}\n\n[LAST_OUTPUT]:\n{self.state['last_response']}"
            threading.Thread(target=self._ai_worker, args=(model, sys_p, full_p, idx), daemon=True).start()

    def run_workflow(self, workflow_name):
        """Executes a workflow by iterating a tasklist over files."""
        wf = self.workflows.get(workflow_name) if isinstance(self.workflows.get(workflow_name), dict) else {}
        if not wf and workflow_name.endswith(".json"):
             wf = self.workflows.get(workflow_name, {})
            
        if not wf:
            self.log(f"Workflow '{workflow_name}' not found.")
            return

        target_dir = wf.get("target_dir", self.ent_dir.get())
        extension = wf.get("extension", ".py")
        
        # Find files matching criteria
        files = [f for f in os.listdir(target_dir) if f.endswith(extension) and not f.startswith("TEST_")]

        def _workflow_loop():
            self._workflow_active = True
            for f_name in files:
                self.log(f"[WORKFLOW] Processing: {f_name}")
                self.after(0, lambda f=f_name: self.ent_file.delete(0, "end"))
                self.after(0, lambda f=f_name: self.ent_file.insert(0, f))
                
                # Start the tasklist
                self.after(0, self.run_all_steps)
                
                # Wait for the tasklist to complete before moving to next file
                import time
                while self._run_all_active:
                    time.sleep(1)
            
            self.log("[WORKFLOW] All files processed.")
            self._workflow_active = False

        threading.Thread(target=_workflow_loop, daemon=True).start()

    def run_all_steps(self):
        if self._run_all_active:
            self.log("RUN ALL already active.")
            return

        if not self.steps:
            self.log("No steps to run.")
            return

        self._run_all_active = True
        self._run_all_index = 0
        self._run_all_last_stop_reason = ""
        self.log("RUN ALL started.")
        self.run_step(self._run_all_index)

    def _is_failure_output(self, content: str) -> bool:
        """Best-effort failure detection across tool + AI outputs."""
        if not content:
            return False

        # Common explicit signals
        lowered = content.lower()
        if "tool error" in lowered or "ai error" in lowered:
            return True

        # JSON outputs from tools often include success/error
        try:
            obj = json.loads(content)
            if isinstance(obj, dict):
                if obj.get("success") is False:
                    return True
                if "error" in obj and obj.get("error"):
                    return True
                if obj.get("message") and isinstance(obj.get("message"), str) and "error" in obj.get("message").lower():
                    return True
        except Exception:
            pass

        # Plain-text errors
        if lowered.strip().startswith("error:"):
            return True

        return False

    def _run_all_advance(self):
        """Called after a step finishes (from _update_output)."""
        if not self._run_all_active:
            return

        # Stop if last output looks like a failure
        if self._is_failure_output(self.state.get("last_response", "")):
            self._run_all_last_stop_reason = f"Failure at step #{self._run_all_index + 1}"
            self.log(f"RUN ALL stopped on failure at step #{self._run_all_index + 1}.")
            self._run_all_active = False
            try:
                self._update_run_summary()
            except Exception:
                pass
            return

        # Next step
        self._run_all_index += 1
        if self._run_all_index >= len(self.steps):
            self._run_all_last_stop_reason = "Complete"
            self.log("RUN ALL complete.")
            self._run_all_active = False
            try:
                self._update_run_summary()
            except Exception:
                pass
            return

        self.run_step(self._run_all_index)

    def _tool_worker(self, cmd, step_idx=None):
        target_dir = self.ent_dir.get()
        original_file = self.ent_file.get()

        # SANDBOX LOGIC: If test mode is on, we work on a copy
        if self.var_test_mode.get():
            sandbox_file = f"TEST_{original_file}"
            source_path = os.path.join(target_dir, original_file)
            dest_path = os.path.join(target_dir, sandbox_file)

            # Create sandbox copy if it doesn't exist yet
            if not os.path.exists(dest_path) and os.path.exists(source_path):
                import shutil
                shutil.copy2(source_path, dest_path)
                self.log(f"[SANDBOX] Created test file: {sandbox_file}")
            target_file = sandbox_file
        else:
            target_file = original_file

        try:
            # 1. DIRECTORY BACKUP
            if cmd == "create_backup":
                import shutil
                ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = os.path.join(os.path.dirname(target_dir), f"BACKUP_{ts}")
                shutil.copytree(target_dir, backup_path)
                res = {"success": True, "message": f"Backup created: {backup_path}"}

            # 2. READ TEMPLATE (Boilerplate)
            elif cmd == "read_template":
                # Look for a template matching the current target file's extension or name
                ext = os.path.splitext(target_file)[1].replace('.', '') or "txt"
                template_name = f"{ext}_template.py" if ext == "py" else f"{ext}_template.txt"
                template_path = os.path.join(_root_dir, "src", "_microservices", template_name)
                
                if os.path.exists(template_path):
                    with open(template_path, "r", encoding="utf-8") as f:
                        res = f.read()
                else:
                    # GUARDRAIL: Prompt user to provide a boilerplate on the fly
                    self.log(f"[GUARDRAIL] Template missing: {template_name}. Requesting user input.")
                    user_input = self._request_missing_template(template_name)
                    if user_input:
                        with open(template_path, "w", encoding="utf-8") as f:
                            f.write(user_input)
                        res = user_input
                        self.log(f"[SUCCESS] Saved new template: {template_name}")
                    else:
                        res = {"success": False, "message": f"Aborted: No template provided for {template_name}"}

            # 3. MECHANICAL ANALYSIS (AST)
            elif cmd == "scan_file_structure":
                target_path = os.path.join(target_dir, target_file)
                res = self.tools_engine.scan_file_structure(target_path)

            # 4. PATCHING (Surgeon)
            elif cmd == "apply_safe_patch":
                target_path = os.path.join(target_dir, target_file)
                res = self.tools_engine.apply_patch(target_path, self.state["last_response"], dry_run=False)

            # 5. CLEANUP PATCH GENERATOR
            elif cmd == "get_cleanup_patch":
                res = self.tools_engine.generate_cleanup_patch()

            else:
                res = {"success": False, "message": f"Error: Tool '{cmd}' not recognized."}

        except Exception as e:
            self.log(f"TOOL ERROR: {e}")
            res = {"success": False, "message": f"TOOL ERROR: {e}"}

        output = json.dumps(res, indent=2) if isinstance(res, (dict, list)) else str(res)
        success = None
        if isinstance(res, dict):
            if res.get("success") is True:
                success = True
            elif res.get("success") is False or res.get("error"):
                success = False

        status = "succeeded" if success is True else ("failed" if success is False else "completed")
        self._update_output(output, f"Tool '{cmd}' {status}.", step_idx=step_idx, success=success)

    def _ai_worker(self, model, sys, prompt, step_idx=None):
        try:
            # Inject Tool Palette as a reference guide for the AI
            tools_ref = "\n### AVAILABLE MECHANICAL TOOLS ###\n" + json.dumps(self.tool_palette, indent=2)
            
            # Standardized context injection for the iteration swarm
            augmented_prompt = f"### PREVIOUS STEP RESULT ###\n{self.state['last_response']}{tools_ref}\n\n### CURRENT TASK ###\n{prompt}"
            res = self.client.generate(model, sys, augmented_prompt)
            self._update_output(res, "AI Generation complete.", step_idx=step_idx, success=True)
        except Exception as e:
            self.log(f"AI ERROR: {e}")
            self._update_output(f"AI ERROR: {e}", "AI Generation failed.", step_idx=step_idx, success=False)

    def _update_run_summary(self):
        """Render a compact wrap-up of the current run state."""
        lines = []
        lines.append("=== RUN SUMMARY ===")
        if self._run_all_active:
            lines.append(f"Status: RUNNING (step {self._run_all_index + 1} / {len(self.steps)})")
        else:
            if self._run_all_last_stop_reason:
                lines.append(f"Status: STOPPED - {self._run_all_last_stop_reason}")
            else:
                lines.append("Status: IDLE")

        lines.append("")
        for i, s in enumerate(self.steps):
            rec = self.step_outputs.get(i)
            if not rec:
                status = "(not run)"
            else:
                succ = rec.get("success")
                if succ is True:
                    status = "OK"
                elif succ is False:
                    status = "FAIL"
                else:
                    status = "DONE"
            role = s.data.get("role")
            model = s.data.get("model") if role != "Mechanical Tool" else "-"
            lines.append(f"#{i+1:02d} [{status}] {role} | model={model}")

        lines.append("")
        lines.append("Final output shown in main output pane.")

        text = "\n".join(lines)
        self.txt_summary.delete("1.0", "end")
        self.txt_summary.insert("1.0", text)

    def _update_output(self, content, log_msg, step_idx=None, success=None):
        def _do_update():
            self.state["last_response"] = content
            self.txt_response.delete("1.0", "end")
            self.txt_response.insert("1.0", content)
            self.log(log_msg)

            # Persist output per-step for later recall
            if step_idx is not None:
                ts = datetime.datetime.now().strftime("%H:%M:%S")
                self.step_outputs[step_idx] = {
                    "content": content,
                    "success": success,
                    "ts": ts,
                    "log_msg": log_msg,
                }

            # Update summary panel
            try:
                self._update_run_summary()
            except Exception:
                pass

            # If RUN ALL is active, advance after this step completes
            if getattr(self, "_run_all_active", False):
                # Always advance via after() to keep sequencing stable
                self.after(0, self._run_all_advance)

        # Tkinter widgets must only be touched from the main/UI thread
        if threading.current_thread() is threading.main_thread():
            _do_update()
        else:
            self.after(0, _do_update)

if __name__ == "__main__":
    import sys
    # Phase 2 CLI Hook: If arguments are passed, we could bypass the UI
    if len(sys.argv) > 1 and "--cli" in sys.argv:
        print("[SYSTEM] CLI Mode detected. Bulk iteration would start here in Phase 2.")
    else:
        app = WorkbenchApp()
        app.mainloop()

# --- FUNCTIONAL PATCH ENGINE ---
# These must be at the top-level (global scope) to be importable via 'from src.app import ...'

class PatchError(Exception): 
    """Exception raised for errors in the patching process."""
    pass

def apply_patch_text(original_text, patch_obj, global_force_indent=False):
    """
    Applies multiple JSON hunks to a target text. 
    Each hunk must match the search_block exactly.
    """
    new_text = original_text
    hunks = patch_obj.get("hunks", [])

    for i, hunk in enumerate(hunks):
        description = hunk.get("description", f"Hunk #{i}")
        search = hunk.get("search_block", "")
        replace = hunk.get("replace_block", "")

        if not search:
            continue

        if search in new_text:
            # Basic replacement
            new_text = new_text.replace(search, replace)
        else:
            raise PatchError(f"Failed to apply '{description}': Search block not found.")

    return new_text











--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\base_service.py
--------------------------------------------------------------------------------
import logging
from typing import Dict, Any

class BaseService:
    """
    Standard parent class for all microservices. 
    Provides consistent logging and identity management.
    """
    def __init__(self, name: str):
        self._service_info = {
            "name": name, 
            "id": name.lower().replace(" ", "_")
        }
        
        # Setup standard logging (only if app hasn't configured logging yet)
        root_logger = logging.getLogger()
        if not root_logger.handlers:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s [%(levelname)s] %(message)s',
                datefmt='%H:%M:%S'
            )
        self.logger = logging.getLogger(name)

    def log_info(self, message: str):
        self.logger.info(message)

    def log_error(self, message: str):
        self.logger.error(message)

    def log_warning(self, message: str):
        self.logger.warning(message)


--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\document_utils.py
--------------------------------------------------------------------------------
from typing import Optional

# ContentExtractorMS location can vary after refactors.
# Try a few likely import paths; if not found, raise a clear runtime error.
ContentExtractorMS: Optional[object] = None

try:
    # Try absolute package import first
    from src._microservices._ContentExtractorMS import ContentExtractorMS
except ImportError:
    try:
        # Fallback to relative import if running within the same directory
        from _ContentExtractorMS import ContentExtractorMS
    except ImportError as e:
        ContentExtractorMS = None
        _import_error = e

if ContentExtractorMS is None:
    raise ImportError(
        "ContentExtractorMS could not be imported. Expected one of: "
        "microservices._ContentExtractorMS, _ContentExtractorMS, __ContentExtractorMS. "
        f"Original error: {_import_error}"
    )

# Singleton instance to reuse the extractor logic
_extractor = ContentExtractorMS()  # type: ignore

def extract_text_from_pdf(blob: bytes) -> str:
    """Proxy to ContentExtractorMS PDF logic."""
    return _extractor._extract_pdf(blob)

def extract_text_from_html(html_text: str) -> str:
    """Proxy to ContentExtractorMS HTML logic."""
    return _extractor._extract_html(html_text)



--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\microservice_std_lib.py
--------------------------------------------------------------------------------
"""
LIBRARY: Microservice Standard Lib
VERSION: 2.0.0
ROLE: Provides decorators for tagging Python classes as AI-discoverable services.
"""

import functools
import inspect
from typing import Dict, List, Any, Optional, Type

# ==============================================================================
# DECORATORS (The "Writer" Tools)
# ==============================================================================

def service_metadata(name: str, version: str, description: str, tags: List[str], capabilities: List[str] = None, dependencies: List[str] = None, side_effects: List[str] = None):
    """
    Class Decorator.
    Labels a Microservice class with high-level metadata for the Catalog.
    """
    def decorator(cls):
        cls._is_microservice = True
        cls._service_info = {
            "name": name,
            "version": version,
            "description": description,
            "tags": tags,
            "capabilities": capabilities or [],
            "dependencies": dependencies or [],
            "side_effects": side_effects or []
        }
        return cls
    return decorator

def service_endpoint(inputs: Dict[str, str], outputs: Dict[str, str], description: str, tags: List[str] = None, side_effects: List[str] = None, mode: str = "sync"):
    """
    Method Decorator.
    Defines the 'Socket' that the AI Architect can plug into.
    
    :param inputs: Dict of {arg_name: type_string} (e.g. {"query": "str"})
    :param outputs: Dict of {return_name: type_string} (e.g. {"results": "List[Dict]"})
    :param description: What this specific function does.
    :param tags: Keywords for searching (e.g. ["search", "read-only"])
    :param side_effects: List of impact types (e.g. ["network:outbound", "disk:write"])
    :param mode: 'sync', 'async', or 'ui_event'
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        
        # Attach metadata to the function object itself
        wrapper._endpoint_info = {
            "name": func.__name__,
            "inputs": inputs,
            "outputs": outputs,
            "description": description,
            "tags": tags or [],
            "side_effects": side_effects or [],
            "mode": mode
        }
        return wrapper
    return decorator

# ==============================================================================
# INTROSPECTION (The "Reader" Tools)
# ==============================================================================

def extract_service_schema(service_cls: Type) -> Dict[str, Any]:
    """
    Scans a decorated Service Class and returns a JSON-serializable schema 
    of its metadata and all its exposed endpoints.
    
    This is what the AI Agent uses to 'read' the manual.
    """
    if not getattr(service_cls, "_is_microservice", False):
        raise ValueError(f"Class {service_cls.__name__} is not decorated with @service_metadata")

    schema = {
        "meta": getattr(service_cls, "_service_info", {}),
        "endpoints": []
    }

    # Inspect all methods of the class
    for name, method in inspect.getmembers(service_cls, predicate=inspect.isfunction):
        # Unwrap decorators if necessary to find our tags
        # (Though usually the wrapper has the tag attached)
        endpoint_info = getattr(method, "_endpoint_info", None)
        
        if endpoint_info:
            schema["endpoints"].append(endpoint_info)

    return schema

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\microservice_template.py
--------------------------------------------------------------------------------
"""
Standardized Microservice Template
"""
from src._microservices.microservice_std_lib import service_metadata, service_endpoint
from src._microservices.base_service import BaseService

@service_metadata(
    name="{{SERVICE_NAME}}",
    version="1.0.0",
    description="{{DESCRIPTION}}",
    tags=[],
    capabilities=[]
)
class {{CLASS_NAME}}(BaseService):
    def __init__(self):
        super().__init__("{{SERVICE_NAME}}")
        # {{INIT_LOGIC}}

    # {{ENDPOINTS}}


--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\py_template.py
--------------------------------------------------------------------------------
from microservice_std_lib import service_metadata, service_endpoint
from typing import Dict, Any, Optional

@service_metadata(
    name="YourServiceName",
    version="1.0.0",
    description="Briefly describe the 'Purpose' here.",
    tags=["category", "utility"],
    capabilities=["filesystem:read"] # Optional: what it actually touches
)
class YourServiceMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Initialize your core logic/engines here

    @service_endpoint(
        inputs={"param1": "str", "param2": "int"},
        outputs={"result": "str"},
        description="Detailed description of what this specific method does.",
        tags=["action"],
        side_effects=["filesystem:write"] # Be explicit for the AI safety
    )
    def perform_action(self, param1: str, param2: int = 10) -> Dict[str, Any]:
        # Your translated logic goes here
        return {"result": f"Processed {param1}"}

if __name__ == "__main__":
    # Standard independent test block for the catalogue
    svc = YourServiceMS()
    print("Service ready:", svc)
    # Add a print test of your logic here
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\_FileSaverMS.py
--------------------------------------------------------------------------------
from src._microservices.microservice_std_lib import service_metadata, service_endpoint
from typing import Dict, Any, Optional
from pathlib import Path


@service_metadata(
    name="FileSaverService",
    version="1.0.0",
    description="Safely writes text to a file inside a sandbox directory.",
    tags=["filesystem", "write", "utility"],
    capabilities=["filesystem:write"]
)
class FileSaverMS:
    """
    A minimal microservice that writes text to a file.
    It enforces strict sandboxing: the resolved path must remain inside base_dir.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

        base_dir = self.config.get("base_dir")
        if not base_dir:
            raise ValueError("FileSaverMS requires a 'base_dir' for safety.")

        self.base_dir = Path(base_dir).resolve()
        self.base_dir.mkdir(parents=True, exist_ok=True)

    @service_endpoint(
        inputs={
            "relative_path": "str  # Path relative to sandbox root.",
            "content": "str  # Text to write to the file."
        },
        outputs={
            "success": "bool",
            "message": "str",
            "written_path": "str"
        },
        description="Writes text to a file inside the sandbox. Overwrites existing files.",
        tags=["action", "filesystem"],
        side_effects=["filesystem:write"]
    )
    def save_file(self, relative_path: str, content: str) -> Dict[str, Any]:
        """
        Write text to a file inside the sandbox.
        The path must remain inside base_dir after resolution.
        """

        try:
            # Resolve path inside sandbox
            target = (self.base_dir / relative_path).resolve()

            # Enforce sandbox boundary
            try:
                target.relative_to(self.base_dir)
            except ValueError:
                return {
                    "success": False,
                    "message": "Refused: path escapes sandbox.",
                    "written_path": ""
                }

            # Ensure parent directories exist
            target.parent.mkdir(parents=True, exist_ok=True)

            # Write file
            with target.open("w", encoding="utf-8") as f:
                f.write(content)

            return {
                "success": True,
                "message": "File written successfully.",
                "written_path": str(target)
            }

        except Exception as e:
            return {
                "success": False,
                "message": f"Error writing file: {e}",
                "written_path": ""
            }


if __name__ == "__main__":
    svc = FileSaverMS(config={"base_dir": "./sandbox"})
    print("Service ready:", svc)


--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\_TokenizingPatcherMS.py
--------------------------------------------------------------------------------
from typing import Dict, Any, Optional
from pathlib import Path
import json

from src._microservices.microservice_std_lib import service_metadata, service_endpoint

from typing import Callable, Optional, Type

# NOTE:
# Do NOT import the patch engine from the UI module at import-time.
# That creates circular imports (UI -> ToolsMS -> TokenizingPatcherMS -> UI).
# Instead, we lazy-load on demand, or allow injection via config.


@service_metadata(
    name="TokenizingPatcherService",
    version="1.0.0",
    description=(
        "Applies structured JSON patch hunks to a target text file using the "
        "_TokenizingPATCHER engine (indentation-aware, non-overlapping, deterministic)."
    ),
    tags=["patching", "filesystem", "refactor", "automation"],
    capabilities=[
        "filesystem:read",
        "filesystem:write"
    ]
)
class TokenizingPatcherMS:
    """
    Microservice wrapper around the _TokenizingPATCHER core logic.

    This service is designed to:
    - Accept a target file path and a JSON patch schema.
    - Use the existing `apply_patch_text` engine for deterministic patching.
    - Optionally run as a dry run (no write).
    - Destructively overwrite the target file when requested (for sandbox use).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Optional config keys (all are optional):

        - base_dir: str
            If set, all target paths are resolved relative to this directory.
            Useful for sandboxing. Example: "/sandbox/workspace"

        - default_force_indent: bool
            Default for force_indent when the endpoint caller does not specify it.
            (Defaults to False if not provided.)

        - allow_absolute_paths: bool
            If False (default), absolute paths are rejected when base_dir is set,
            to enforce sandboxing. If True, caller can pass absolute paths.

        - patch_engine: callable
            Optional injection point for the patch engine function.
            Signature: (original_text: str, patch_obj: dict, global_force_indent: bool) -> str

        - patch_error_type: Exception type
            Optional injection point for the patch engine's expected exception class.
        """
        self.config = config or {}

        base_dir = self.config.get("base_dir")
        self.base_dir: Optional[Path] = Path(base_dir).resolve() if base_dir else None

        self.default_force_indent: bool = bool(
            self.config.get("default_force_indent", False)
        )
        self.allow_absolute_paths: bool = bool(
            self.config.get("allow_absolute_paths", False)
        )

        self._patch_engine: Optional[Callable[..., str]] = self.config.get("patch_engine")
        self._patch_error_type: Optional[Type[BaseException]] = self.config.get("patch_error_type")

    # -------------------------------------------------------------------------
    # Core endpoint: apply patch to a file
    # -------------------------------------------------------------------------

    @service_endpoint(
        inputs={
            "target_path": "str  # Relative or absolute path to the file to patch.",
            "patch_schema": (
                "str  # JSON string matching the _TokenizingPATCHER schema "
                "with a top-level 'hunks' list."
            ),
            "force_indent": (
                "bool (optional)  # If true, use patch indentation as-is; "
                "otherwise adapt indentation relative to target file."
            ),
            "dry_run": "bool (optional)  # If true, do not write back to disk.",
            "return_preview": (
                "bool (optional)  # If true, include patched text in response "
                "even for destructive runs."
            ),
        },
        outputs={
            "success": "bool",
            "message": "str",
            "target_path": "str",
            "dry_run": "bool",
            "force_indent_used": "bool",
            "written": "bool  # True if file was actually overwritten.",
            "patched_preview": "str (optional)  # May be omitted for large files.",
        },
        description=(
            "Apply a structured JSON patch to a target file using the "
            "_TokenizingPATCHER engine. Supports dry runs and destructive "
            "overwrites, with optional sandboxing via service config."
        ),
        tags=["action", "patch", "filesystem"],
        side_effects=[
            "filesystem:read",
            "filesystem:write"
        ]
    )
    def apply_patch_to_file(
        self,
        target_path: str,
        patch_schema: str,
        force_indent: Optional[bool] = None,
        dry_run: bool = False,
        return_preview: bool = True,
    ) -> Dict[str, Any]:
        """
        Apply patch hunks defined in `patch_schema` to the file at `target_path`.

        - Reads the target file from disk.
        - Parses the JSON schema into a patch object.
        - Uses `apply_patch_text(...)` to compute the new text.
        - Writes back to the SAME file when not in dry_run mode.
        - Returns a structured result describing what happened.
        """

        # -------------------------------------------------------------
        # 1. Resolve target path (respecting optional sandboxing)
        # -------------------------------------------------------------
        try:
            raw_path = Path(target_path)

            # Enforce sandboxing when base_dir is configured
            if self.base_dir:
                if raw_path.is_absolute():
                    if not self.allow_absolute_paths:
                        return {
                            "success": False,
                            "message": (
                                "Absolute paths are not allowed when 'base_dir' is configured. "
                                "Pass a path relative to the sandbox."
                            ),
                            "target_path": str(raw_path),
                            "dry_run": dry_run,
                            "force_indent_used": bool(
                                self.default_force_indent if force_indent is None else force_indent
                            ),
                            "written": False,
                        }
                    resolved_target = raw_path.resolve()
                else:
                    resolved_target = (self.base_dir / raw_path).resolve()

                # Optional: enforce that resolved_target stays inside base_dir
                try:
                    resolved_target.relative_to(self.base_dir)
                except ValueError:
                    return {
                        "success": False,
                        "message": (
                            "Resolved target path escapes the configured base_dir sandbox. "
                            "Refusing to patch."
                        ),
                        "target_path": str(resolved_target),
                        "dry_run": dry_run,
                        "force_indent_used": bool(
                            self.default_force_indent if force_indent is None else force_indent
                        ),
                        "written": False,
                    }
            else:
                # No base_dir configured; trust the caller
                resolved_target = raw_path.resolve()

        except Exception as e:
            return {
                "success": False,
                "message": f"Failed to resolve target path: {e}",
                "target_path": target_path,
                "dry_run": dry_run,
                "force_indent_used": bool(
                    self.default_force_indent if force_indent is None else force_indent
                ),
                "written": False,
            }

        # -------------------------------------------------------------
        # 2. Read target file
        # -------------------------------------------------------------
        try:
            with resolved_target.open("r", encoding="utf-8") as f:
                original_text = f.read()
        except FileNotFoundError:
            return {
                "success": False,
                "message": f"Target file not found: {resolved_target}",
                "target_path": str(resolved_target),
                "dry_run": dry_run,
                "force_indent_used": bool(
                    self.default_force_indent if force_indent is None else force_indent
                ),
                "written": False,
            }
        except Exception as e:
            return {
                "success": False,
                "message": f"Error reading target file: {e}",
                "target_path": str(resolved_target),
                "dry_run": dry_run,
                "force_indent_used": bool(
                    self.default_force_indent if force_indent is None else force_indent
                ),
                "written": False,
            }

        # -------------------------------------------------------------
        # 3. Parse patch schema JSON
        # -------------------------------------------------------------
        try:
            patch_obj = json.loads(patch_schema)
        except json.JSONDecodeError as e:
            return {
                "success": False,
                "message": f"Patch schema is not valid JSON: {e}",
                "target_path": str(resolved_target),
                "dry_run": dry_run,
                "force_indent_used": bool(
                    self.default_force_indent if force_indent is None else force_indent
                ),
                "written": False,
            }

        # -------------------------------------------------------------
        # 4. Apply patch using core engine
        # -------------------------------------------------------------
        force_indent_used = (
            self.default_force_indent if force_indent is None else bool(force_indent)
        )

        # Lazy-load engine if not injected
        patch_engine = self._patch_engine
        patch_error_type = self._patch_error_type

        if patch_engine is None:
            # Prefer the repo's UI module path (src.app) if available
            try:
                from src.app import apply_patch_text as _apply_patch_text
                from src.app import PatchError as _PatchError
                patch_engine = _apply_patch_text
                patch_error_type = _PatchError
            except Exception as e:
                return {
                    "success": False,
                    "message": (
                        "Patch engine not available. Provide config.patch_engine / config.patch_error_type "
                        "or ensure src.app exports apply_patch_text and PatchError. "
                        f"Import error: {e}"
                    ),
                    "target_path": str(resolved_target),
                    "dry_run": dry_run,
                    "force_indent_used": force_indent_used,
                    "written": False,
                }

        try:
            new_text = patch_engine(
                original_text,
                patch_obj,
                global_force_indent=force_indent_used,
            )
        except Exception as e:
            if patch_error_type and isinstance(e, patch_error_type):
                return {
                    "success": False,
                    "message": f"Patch engine failure: {e}",
                    "target_path": str(resolved_target),
                    "dry_run": dry_run,
                    "force_indent_used": force_indent_used,
                    "written": False,
                }
            return {
                "success": False,
                "message": f"Unexpected error during patching: {e}",
                "target_path": str(resolved_target),
                "dry_run": dry_run,
                "force_indent_used": force_indent_used,
                "written": False,
            }

        # -------------------------------------------------------------
        # 5. Optionally write back to disk (destructive overwrite)
        # -------------------------------------------------------------
        written = False
        if not dry_run:
            try:
                with resolved_target.open("w", encoding="utf-8") as f:
                    f.write(new_text)
                written = True
            except Exception as e:
                return {
                    "success": False,
                    "message": f"Failed to write patched file: {e}",
                    "target_path": str(resolved_target),
                    "dry_run": dry_run,
                    "force_indent_used": force_indent_used,
                    "written": False,
                }

        # -------------------------------------------------------------
        # 6. Build response payload
        # -------------------------------------------------------------
        result: Dict[str, Any] = {
            "success": True,
            "message": (
                "Dry run successful; patch applies cleanly."
                if dry_run
                else "Patch applied and file overwritten successfully."
            ),
            "target_path": str(resolved_target),
            "dry_run": dry_run,
            "force_indent_used": force_indent_used,
            "written": written,
        }

        if return_preview:
            # In dry_run mode, preview is the only observable side effect
            # In destructive mode, this is still useful for logging/inspection
            result["patched_preview"] = new_text

        return result


if __name__ == "__main__":
    # Standard independent test block for the catalogue
    svc = TokenizingPatcherMS(
        config={
            # Example: point this at a known sandbox root
            # "base_dir": "/path/to/sandbox",
            # "default_force_indent": False,
            # "allow_absolute_paths": False,
        }
    )
    print("Service ready:", svc)
    # Example manual smoke test (adjust paths and patch as needed):
    #
    # dummy_patch = json.dumps({
    #     "hunks": [
    #         {
    #             "description": "Example no-op hunk",
    #             "search_block": "original text",
    #             "replace_block": "modified text",
    #             "use_patch_indent": False
    #         }
    #     ]
    # })
    # print(
    #     svc.apply_patch_to_file(
    #         target_path="relative/or/absolute/path/to/file.py",
    #         patch_schema=dummy_patch,
    #         dry_run=True,
    #         return_preview=True,
    #     )
    # )





--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\_TokenizingPatcherMS_README.md
--------------------------------------------------------------------------------
# `_TokenizingPatcherMS_README.md`

## Overview
`TokenizingPatcherMS` is a deterministic, indentation‚Äëaware patch‚Äëapplication microservice built on the `_TokenizingPATCHER` engine. It applies structured JSON hunks to a target file, supports dry‚Äërun validation, and can destructively overwrite files when requested.

Agents use this service to perform safe, auditable, non‚Äëoverlapping code transformations.

---

## Capabilities
- Deterministic hunk‚Äëbased patching  
- Indentation‚Äëaware replacement  
- Collision detection  
- Dry‚Äërun + destructive modes  
- Optional sandboxing  
- Fully declarative JSON schema  

This is the **canonical** way agents modify files.

---

## When Agents Should Use This Service
Use it whenever you need to:
- Insert, replace, or remove code  
- Perform multi‚Äëfile refactors  
- Apply transformations generated by analysis  
- Maintain architectural consistency  

Do **not** rewrite files manually.  
Do **not** bypass the patcher.

---

## Endpoint: `apply_patch_to_file`

### Inputs
| Field | Type | Description |
|-------|------|-------------|
| `target_path` | str | File to patch (relative if sandboxed). |
| `patch_schema` | str | JSON string containing `"hunks"`. |
| `force_indent` | bool | Use patch indentation exactly. |
| `dry_run` | bool | Validate without writing. |
| `return_preview` | bool | Include patched text in response. |

### Outputs
| Field | Type | Description |
|-------|------|-------------|
| `success` | bool | Patch applied cleanly. |
| `message` | str | Status. |
| `target_path` | str | Resolved path. |
| `dry_run` | bool | Whether this was a dry run. |
| `force_indent_used` | bool | Final indentation mode. |
| `written` | bool | True if file overwritten. |
| `patched_preview` | str | Optional preview. |

---

## Patch Schema Contract

```json
{
  "hunks": [
    {
      "description": "Short description",
      "search_block": "text to find\n(multi-line allowed)",
      "replace_block": "replacement text",
      "use_patch_indent": false
    }
  ]
}
```

### Rules
1. Hunks must not overlap.  
2. `search_block` must match exactly (strict ‚Üí content‚Äëonly fallback).  
3. Indentation must be intentional.  
4. Hunks must be self‚Äëcontained.  
5. Validate with dry‚Äërun before destructive writes.

---

## How the Refactor Engine Calls This Service

### 1. Import and instantiate
```python
from microservice.tokenizing_patcher_ms import TokenizingPatcherMS

patcher = TokenizingPatcherMS(
    config={
        "base_dir": "/sandbox/workspace",
        "default_force_indent": False,
        "allow_absolute_paths": False
    }
)
```

### 2. Build patch schema
```python
patch_obj = {
    "hunks": [
        {
            "description": "Replace header",
            "search_block": "def old():",
            "replace_block": "def new():",
            "use_patch_indent": False
        }
    ]
}
```

### 3. Convert to JSON string
```python
import json
schema_str = json.dumps(patch_obj)
```

### 4. Dry‚Äërun
```python
result = patcher.apply_patch_to_file(
    target_path="src/module/example.py",
    patch_schema=schema_str,
    dry_run=True,
    return_preview=True
)
```

### 5. If correct ‚Üí destructive overwrite
```python
patcher.apply_patch_to_file(
    target_path="src/module/example.py",
    patch_schema=schema_str,
    dry_run=False
)
```

### 6. Inspect results
```python
if not result["success"]:
    raise RuntimeError(result["message"])
```

This is the **standard refactor‚Äëengine ritual**:
**analyze ‚Üí generate patch ‚Üí dry‚Äërun ‚Üí inspect ‚Üí apply ‚Üí recurse.**

---

## Sandbox Behavior
If configured with:

```python
{
  "base_dir": "/sandbox/workspace",
  "allow_absolute_paths": false
}
```

Then:
- All paths must be inside the sandbox  
- Escapes are rejected  
- Destructive writes are safe  

---

## Philosophy
This service enforces:
- determinism  
- clarity  
- explicit intent  
- safe automation  

Agents participate in the recursive ritual:  
**patch ‚Üí validate ‚Üí apply ‚Üí recurse.**

---


--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\_ToolsMS.py
--------------------------------------------------------------------------------
import ast
import os
import json
from typing import Dict, Any, List

# Import your existing patcher engine
from src._microservices._TokenizingPatcherMS import TokenizingPatcherMS

class MicroserviceTools:
    """
    The 'Cartridge' tools for the Microservice Refactor Domain.
    These are deterministic functions that the AI or Orchestrator can call.
    """

    def __init__(self, base_dir: str):
        self.base_dir = base_dir
        # Initialize the mechanical patcher once
        self.patcher = TokenizingPatcherMS(config={
            "base_dir": base_dir,
            "default_force_indent": False,
            "allow_absolute_paths": False
        })

    # --- THE SCOUT (Parser) ---
    def scan_file_structure(self, file_path: str) -> Dict[str, Any]:
        """
        Reads a Python file and extracts structured metadata via AST.
        Replaces the old 'ParserRole'.
        """
        # Resolve path inside the configured base_dir
        full_path = os.path.join(self.base_dir, file_path)
        if not os.path.exists(full_path):
            return {"error": f"File not found: {file_path} (resolved: {full_path})"}

        with open(full_path, "r", encoding="utf-8") as f:
            source = f.read()

        try:
            tree = ast.parse(source)
        except SyntaxError as e:
            return {"error": f"Syntax Error: {e}"}

        ir = {
            "file_path": file_path,
            "imports": [],
            "classes": [],
            "functions": []
        }

        for node in tree.body:
            # Extract Imports
            if isinstance(node, ast.Import):
                for alias in node.names:
                    ir["imports"].append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                for alias in node.names:
                    ir["imports"].append(f"{module}.{alias.name}")
            
            # Extract Classes (Potential Services)
            elif isinstance(node, ast.ClassDef):
                class_info = {"name": node.name, "decorators": []}
                for deco in node.decorator_list:
                    if isinstance(deco, ast.Call) and hasattr(deco.func, "id"):
                        class_info["decorators"].append(deco.func.id)
                ir["classes"].append(class_info)

            # Extract Functions (Potential Endpoints)
            elif isinstance(node, ast.FunctionDef):
                ir["functions"].append(node.name)

        return ir

    # --- THE SURGEON (Patcher) ---
    def apply_patch(self, file_path: str, patch_json_str: str, dry_run: bool = True) -> Dict[str, Any]:
        """
        Applies a JSON patch to a file.
        Replaces 'PatchRole._apply_patch'.

        This tool is intentionally defensive:
        - If the model returns markdown fences (```json ... ```), it will extract the JSON object.
        - If the model returns an empty object ({}), it will fail with a schema error.
        """

        def _extract_json_object(text: str) -> str:
            """Return the first {...} JSON object substring, or "" if not found."""
            if not text:
                return ""
            s = text.strip()

            # Remove common markdown code fences
            if s.startswith("```"):
                # Strip leading fence line
                first_nl = s.find("\n")
                if first_nl != -1:
                    s = s[first_nl + 1 :]
                # Strip trailing fence
                if s.rstrip().endswith("```"):
                    s = s.rstrip()
                    s = s[: -3]
                s = s.strip()

            # Extract JSON object boundaries
            start = s.find("{")
            end = s.rfind("}")
            if start == -1 or end == -1 or end < start:
                return ""
            return s[start : end + 1].strip()

        try:
            # Normalize incoming patch schema
            patch_obj = None

            if isinstance(patch_json_str, dict):
                patch_obj = patch_json_str
            elif isinstance(patch_json_str, str):
                extracted = _extract_json_object(patch_json_str)
                if not extracted:
                    return {
                        "success": False,
                        "message": "Patch schema did not contain a JSON object. Output must be a JSON object with top-level 'hunks'.",
                    }
                try:
                    patch_obj = json.loads(extracted)
                except json.JSONDecodeError as e:
                    return {
                        "success": False,
                        "message": f"Patch schema is not valid JSON after extraction: {e}",
                    }
            else:
                return {
                    "success": False,
                    "message": "Patch schema must be a JSON string or dict.",
                }

            # Validate required schema
            if not isinstance(patch_obj, dict) or "hunks" not in patch_obj or not isinstance(patch_obj.get("hunks"), list):
                return {
                    "success": False,
                    "message": "Patch schema must be a JSON object with a top-level 'hunks' list.",
                }

            # Re-serialize to a clean JSON string for the patcher
            clean_schema_str = json.dumps(patch_obj)

            result = self.patcher.apply_patch_to_file(
                target_path=file_path,
                patch_schema=clean_schema_str,
                dry_run=dry_run,
                return_preview=True,
            )
            return result

        except Exception as e:
            return {"success": False, "message": str(e)}

    # --- THE JANITOR (Helpers) ---
    def generate_cleanup_patch(self) -> str:
        """
        Returns the standard regex cleanup patch for this domain.
        Replaces 'PatchRole._generate_patch_hunk(final_cleanup)'.
        """
        cleanup_hunks = [
            {
                "description": "Collapse double blank lines",
                "search_block": "\n\n\n",
                "replace_block": "\n\n",
                "use_patch_indent": False
            }
        ]
        return json.dumps({"hunks": cleanup_hunks})



--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\_ZipCompressMS.py
--------------------------------------------------------------------------------
from src._microservices.microservice_std_lib import service_metadata, service_endpoint
from typing import Dict, Any, Optional, List
from pathlib import Path
import zipfile
import os


@service_metadata(
    name="ZipperService",
    version="1.0.0",
    description="Zips all files in a directory with optional exclusion filters.",
    tags=["filesystem", "utility", "backup"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ZipperMS:
    """
    A simple microservice that zips all files in a directory.
    Exclusions are substring-based: if any exclusion string appears
    in the file or folder name, it is skipped.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

        base_dir = self.config.get("base_dir")
        self.base_dir: Optional[Path] = Path(base_dir).resolve() if base_dir else None

        self.allow_absolute_paths: bool = bool(
            self.config.get("allow_absolute_paths", False)
        )

    # -------------------------------------------------------------------------
    # Endpoint: zip a directory
    # -------------------------------------------------------------------------

    @service_endpoint(
        inputs={
            "target_dir": "str  # Directory to zip.",
            "output_zip": "str (optional)  # Output zip filename.",
            "exclusions": "list[str] (optional)  # Skip files containing these substrings."
        },
        outputs={
            "success": "bool",
            "message": "str",
            "zip_path": "str",
            "skipped": "list[str]",
            "included": "list[str]"
        },
        description="Zips all files in a directory, skipping any whose names contain exclusion substrings.",
        tags=["action", "filesystem"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def zip_directory(
        self,
        target_dir: str,
        output_zip: Optional[str] = None,
        exclusions: Optional[List[str]] = None
    ) -> Dict[str, Any]:

        exclusions = exclusions or []

        # -------------------------------------------------------------
        # 1. Resolve directory path (sandbox-aware)
        # -------------------------------------------------------------
        try:
            raw_path = Path(target_dir)

            if self.base_dir:
                if raw_path.is_absolute() and not self.allow_absolute_paths:
                    return {
                        "success": False,
                        "message": "Absolute paths not allowed when sandboxing is enabled.",
                        "zip_path": "",
                        "skipped": [],
                        "included": []
                    }

                resolved_dir = (self.base_dir / raw_path).resolve() if not raw_path.is_absolute() else raw_path.resolve()

                try:
                    resolved_dir.relative_to(self.base_dir)
                except ValueError:
                    return {
                        "success": False,
                        "message": "Target directory escapes sandbox.",
                        "zip_path": "",
                        "skipped": [],
                        "included": []
                    }
            else:
                resolved_dir = raw_path.resolve()

        except Exception as e:
            return {
                "success": False,
                "message": f"Failed to resolve directory: {e}",
                "zip_path": "",
                "skipped": [],
                "included": []
            }

        if not resolved_dir.exists() or not resolved_dir.is_dir():
            return {
                "success": False,
                "message": f"Directory not found: {resolved_dir}",
                "zip_path": "",
                "skipped": [],
                "included": []
            }

        # -------------------------------------------------------------
        # 2. Determine output zip path
        # -------------------------------------------------------------
        if output_zip:
            zip_path = Path(output_zip)
            if not zip_path.is_absolute():
                zip_path = resolved_dir / zip_path
        else:
            zip_path = resolved_dir / "archive.zip"

        # If sandboxing is enabled, ensure zip output stays inside base_dir
        try:
            zip_path = zip_path.resolve()
        except Exception:
            zip_path = Path(str(zip_path)).resolve()

        if self.base_dir:
            try:
                zip_path.relative_to(self.base_dir)
            except ValueError:
                return {
                    "success": False,
                    "message": "Output zip path escapes sandbox.",
                    "zip_path": "",
                    "skipped": [],
                    "included": []
                }

        # Ensure output directory exists
        try:
            zip_path.parent.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            return {
                "success": False,
                "message": f"Failed to create output directory: {e}",
                "zip_path": "",
                "skipped": [],
                "included": []
            }

        # -------------------------------------------------------------
        # 3. Walk directory and collect files
        # -------------------------------------------------------------
        included = []
        skipped = []

        try:
            with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
                for root, dirs, files in os.walk(resolved_dir):
                    root_path = Path(root)

                    # Skip excluded directories
                    dirs[:] = [
                        d for d in dirs
                        if not any(ex in d for ex in exclusions)
                    ]

                    for file in files:
                        if any(ex in file for ex in exclusions):
                            skipped.append(str(root_path / file))
                            continue

                        full_path = root_path / file
                        arcname = full_path.relative_to(resolved_dir)

                        zf.write(full_path, arcname)
                        included.append(str(full_path))

        except Exception as e:
            return {
                "success": False,
                "message": f"Error creating zip: {e}",
                "zip_path": "",
                "skipped": skipped,
                "included": included
            }

        return {
            "success": True,
            "message": "Directory zipped successfully.",
            "zip_path": str(zip_path),
            "skipped": skipped,
            "included": included
        }


if __name__ == "__main__":
    svc = ZipperMS()
    print("Service ready:", svc)




--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_microservices\__AuthMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _AuthMS
ENTRY_POINT: __AuthMS.py
DEPENDENCIES: None
"""

import base64
import hashlib
import json
import logging
import time
from typing import Any, Dict, Optional

from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_SECRET_KEY = "super_secret_cortex_key"
DEFAULT_SALT = "cortex_salt"

logger = logging.getLogger("Auth")

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="Auth",
    version="1.0.0",
    description="Manages user authentication and signed session tokens.",
    tags=["auth", "security", "crypto"],
    capabilities=["crypto"],
    dependencies=["hashlib", "json", "time", "base64"],
    side_effects=[]
)
class AuthMS(BaseService):
    """
    ROLE: Simple authentication microservice providing username/password login
          and signed session tokens.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        super().__init__("Auth")
        self.config = config or {}
        self.secret_key: str = self.config.get("secret_key", DEFAULT_SECRET_KEY)

        # In a real scenario, this might load from a secure config file or DB.
        # For now, we keep a minimal in-memory user database[cite: 21].
        self.users_db: Dict[str, str] = {
            "admin": self._hash_password("admin123"),
        }

    # ==========================================================================
    # CORE ENDPOINTS
    # ==========================================================================

    @service_endpoint(
        inputs={"username": "str", "password": "str"},
        outputs={"token": "Optional[str]"},
        description="Attempts to log in and returns a signed session token.",
        tags=["auth", "security", "session"],
    )
    def login(self, username: str, password: str) -> Optional[str]:
        """
        Attempt to log in with the provided username and password.
        """
        if username not in self.users_db:
            return None

        stored_hash = self.users_db[username]
        if self._verify_password(password, stored_hash):
            return self._create_token(username)

        return None

    @service_endpoint(
        inputs={"token": "str"},
        outputs={"is_valid": "bool"},
        description="Checks whether a token is valid and not expired.",
        tags=["auth", "security"],
    )
    def validate_session(self, token: str) -> bool:
        """
        Check if a serialized token is valid and not expired[cite: 26].
        """
        payload = self._decode_token(token)
        return payload is not None

    # ==========================================================================
    # INTERNAL HELPERS
    # ==========================================================================

    def _hash_password(self, password: str) -> str:
        """
        Securely hashes a password using SHA-256 with a static salt[cite: 29].
        """
        return hashlib.sha256((password + DEFAULT_SALT).encode("utf-8")).hexdigest()

    def _verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verifies a provided password against the stored hash[cite: 30].
        """
        return self._hash_password(plain_password) == hashed_password

    def _create_token(self, user_id: str, expires_in: int = 3600) -> str:
        """
        Generates a signed session token containing sub, exp, iat, and scope[cite: 31].
        """
        now = int(time.time())
        payload = {
            "sub": user_id,
            "exp": now + expires_in,
            "iat": now,
            "scope": "admin",
        }

        json_payload = json.dumps(payload).encode("utf-8")
        token_part = base64.b64encode(json_payload).decode("utf-8")

        signature = hashlib.sha256((token_part + self.secret_key).encode("utf-8")).hexdigest()
        return f"{token_part}.{signature}"

    def _decode_token(self, token: str) -> Optional[Dict[str, Any]]:
        """
        Parses and validates the incoming token.
        Returns the payload if valid, None otherwise.
        """
        try:
            if not token or "." not in token:
                return None

            token_part, signature = token.split(".", 1)

            # Recalculate signature to verify integrity
            recalc_signature = hashlib.sha256(
                (token_part + self.secret_key).encode("utf-8")
            ).hexdigest()

            if signature != recalc_signature:
                return None  # Invalid signature

            # Decode payload
            payload_json = base64.b64decode(token_part).decode("utf-8")
            payload: Dict[str, Any] = json.loads(payload_json)

            # Check expiration
            if payload.get("exp", 0) < time.time():
                return None  # Expired

            return payload

        except Exception:
            # Intentionally swallow details here and just treat token as invalid[cite: 37].
            logger.exception("Failed to decode or validate auth token.")
            return None


# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    svc = AuthMS()
    print(f"Service Ready: {svc}")
    
    # Simple test
    token = svc.login("admin", "admin123")
    print(f"Token Generated: {token is not None}")
    if token:
        is_valid = svc.validate_session(token)
        print(f"Token Valid: {is_valid}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_roles\default_roles.json
--------------------------------------------------------------------------------
{
  "Helpful Assistant": {
    "description": "General purpose support and formatting.",
    "base_prompt": "You are a helpful AI assistant. Provide clear, concise answers.",
    "allowed_tools": [],
    "tags": ["utility"]
  },
  "Python Expert": {
    "description": "Senior developer focused on PEP 8 and AST efficiency.",
    "base_prompt": "You are a senior Python developer. Focus on PEP 8 compliance, efficiency, and clean Abstract Syntax Tree structures.",
    "allowed_tools": ["scan_file_structure"],
    "tags": ["coding", "analysis"]
  },
  "Strict Analyst": {
    "description": "Logic-first extractor that only speaks JSON.",
    "base_prompt": "You are a logic-first analyst. Your task is to extract facts from code. Output your findings in raw, valid JSON only. No markdown formatting.",
    "allowed_tools": ["scan_file_structure"],
    "tags": ["data", "json"]
  },
  "The Architect": {
    "description": "High-level structural planner for microservice migrations.",
    "base_prompt": "You are a software architect specializing in microservices. Your goal is to map legacy code logic into a modern, standardized boilerplate structure.",
    "allowed_tools": ["read_template"],
    "tags": ["architecture", "planning"]
  },
  "The Surgeon": {
    "description": "Precision code-patching specialist.",
    "base_prompt": "You are a code patching specialist. You write perfect TokenizingPatcher JSON hunks. Every search_block must match the original source EXACTLY.",
    "allowed_tools": ["apply_safe_patch", "get_cleanup_patch"],
    "tags": ["refactor", "patching"]
  },
  "Mechanical Tool": {
    "description": "Internal signal for orchestrator-led mechanical functions.",
    "base_prompt": "EXECUTE_TOOL",
    "allowed_tools": ["all"],
    "tags": ["system"]
  }
}
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_system_prompts\default_system_prompts.json
--------------------------------------------------------------------------------
{
  "standard_assistance": {
    "compatible_roles": ["Helpful Assistant"],
    "content": "You are a helpful AI assistant. Provide clear, concise, and technically accurate answers. If you don't know an answer, state that you don't know."
  },
  "pep8_strict_coder": {
    "compatible_roles": ["Python Expert"],
    "content": "You are a senior Python developer. Focus on PEP 8 compliance, efficiency, and clean Abstract Syntax Tree structures. Output code only unless asked for explanation."
  },
  "microservice_architect_v1": {
    "compatible_roles": ["The Architect"],
    "content": "You are a software architect specializing in microservices. Your goal is to map legacy code logic into a modern, standardized boilerplate structure using BaseService as a parent class."
  },
  "precision_patcher": {
    "compatible_roles": ["The Surgeon"],
    "content": "You are a code patching specialist. You write perfect TokenizingPatcher JSON hunks. Every search_block must match the original source EXACTLY. Do not truncate blocks."
  },
  "raw_json_extractor": {
    "compatible_roles": ["Strict Analyst"],
    "content": "You are a logic-first analyst. Your task is to extract facts from code. Output your findings in raw, valid JSON only. Do not include markdown code fences or conversational filler."
  },
  {
  "abstract_surgeon": 
    "compatible_roles": ["The Surgeon"],
    "content": "You are a master of source-to-source transformation. Your goal is to take 'Raw Logic' from a source file and inject it into a 'Boilerplate' provided in the context. 1. Identify placeholders in the boilerplate (e.g., {{LOGIC}}, {{IMPORTS}}). 2. Map the extracted logic to these placeholders. 3. Generate a TokenizingPatcher JSON where the search_block is the ENTIRE original file content and the replace_block is the completed boilerplate. 4. Ensure no partial hunks; perform a full-file replacement for structural transformations."
  }
}

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_tasklists\ast_patch_generator.json
--------------------------------------------------------------------------------
[
  { "role": "Mechanical Tool", "prompt": "create_backup" },
  { "role": "Mechanical Tool", "prompt": "scan_file_structure" },
  { 
    "role": "Strict Analyst", 
    "prompt": "Analyze the AST in [LAST_OUTPUT] and output a JSON migration plan for a new microservice." 
  },
  {
    "role": "The Surgeon",
    "prompt": "Using the analysis, generate the TokenizingPatcher JSON hunks to transform the target file."
  },
  { "role": "Mechanical Tool", "prompt": "apply_safe_patch" }
]
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_tasklists\basic_file_tranformation_tasklist.json
--------------------------------------------------------------------------------
[
  {
    "role": "Strict Analyst",
    "prompt": "Identify the file type and extract all unique logic blocks. Map the relationship between components."
  },
  {
    "role": "Mechanical Tool",
    "prompt": "read_template"
  },
  {
    "role": "The Architect",
    "prompt": "Using the template from Step 2, map the extracted logic into the required structure. Ensure all placeholders like {{NAME}} or {{LOGIC}} are defined."
  },
  {
    "role": "The Surgeon",
    "prompt": "Generate a full-file replacement patch that integrates the logic into the new boilerplate."
  },
  {
    "role": "Mechanical Tool",
    "prompt": "apply_safe_patch"
  }
]
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_tasklists\default_tasklist.json
--------------------------------------------------------------------------------
[
  {
    "role": "Strict Analyst",
    "prompt": "Analyze the user's request and the current environment. Output a JSON 'plan' consisting of the sequence of tools needed and the specific AI personas required."
  },
  {
    "role": "Mechanical Tool",
    "prompt": "scan_file_structure"
  },
  {
    "role": "Helpful Assistant",
    "prompt": "Based on the plan and the file structure, execute the first major phase of the user's request. Provide code or configuration as needed."
  },
  {
    "role": "The Surgeon",
    "prompt": "Generate the necessary TokenizingPatcher hunks to apply the changes discussed in the previous step."
  },
  {
    "role": "Mechanical Tool",
    "prompt": "apply_safe_patch"
  },
  {
    "role": "Strict Analyst",
    "prompt": "Review the final state. Output a JSON 'validation' report: {\"status\": \"success|fail\", \"remaining_steps\": []}."
  }
]
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_tasklists\tasklist_microservices_generator.json
--------------------------------------------------------------------------------
[
  { "role": "Mechanical Tool", "prompt": "create_backup" },

  { "role": "Mechanical Tool", "prompt": "read_template" },

  {
    "role": "Mechanical Tool",
    "prompt": "scan_file_structure"
  },

  {
    "role": "Strict Analyst",
    "prompt": "From [LAST_OUTPUT] (AST IR), output RAW JSON with: {\"file_path\":..., \"current_imports\":[], \"classes\":[...], \"functions\":[...], \"likely_service_class\":..., \"candidate_endpoints\":[], \"io_side_effects\":[], \"external_deps\":[], \"risks\":[]}. JSON only."
  },

  {
    "role": "Helpful Assistant",
    "prompt": "Using the microservice template (from earlier step) and the analysis JSON, write a migration plan as bullet points: target service name, module layout, @service_metadata fields, endpoint list, inputs/outputs contract, and any safe path/sandbox rules."
  },

  {
    "role": "Helpful Assistant",
    "prompt": "Now draft the final microservice code as a single Python file (full file contents). It must use @service_metadata and @service_endpoint, accept config in __init__, and avoid any hard-coded paths. Keep functions small and deterministic."
  },

  { "role": "Mechanical Tool", "prompt": "read_target_file" }

  {
    "role": "Helpful Assistant",
    "prompt": "Using the exact file text in [LAST_OUTPUT], generate a patch JSON that transforms it into the final version you drafted. Use verbatim search_block anchors copied from [LAST_OUTPUT]. Output ONLY JSON."
  },

  { "role": "Mechanical Tool", "prompt": "apply_safe_patch" },

  { "role": "Mechanical Tool", "prompt": "scan_file_structure" },

  {
    "role": "Strict Analyst",
    "prompt": "Validate the refactor using the new AST IR in [LAST_OUTPUT]. Output RAW JSON: {\"looks_like_microservice\":bool, \"has_service_metadata\":bool, \"has_endpoints\":bool, \"missing\":[], \"problems\":[], \"next_patch_needed\":bool, \"recommended_next_steps\":[]}."
  },

  {
    "role": "Helpful Assistant",
    "prompt": "If next_patch_needed is true, generate the next TokenizingPatcher JSON hunks to fix the problems list. Output ONLY raw JSON patch."
  },

  { "role": "Mechanical Tool", "prompt": "apply_safe_patch" },

  {
    "role": "Mechanical Tool",
    "prompt": "get_cleanup_patch"
  },

  { "role": "Mechanical Tool", "prompt": "apply_safe_patch" },

  {
    "role": "Helpful Assistant",
    "prompt": "Final review checklist: confirm metadata is complete, endpoints have explicit inputs/outputs, no circular imports, no hard-coded paths, and config supports sandboxing where needed. Output a short pass/fail list."
  }
]

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_tasklists\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_workflows\default_workflow.json
--------------------------------------------------------------------------------
{
  "name": "Default Batch Runner",
  "description": "Applies the currently loaded tasklist to all matching files in the target directory.",
  "target_dir": "./src",
  "extension": ".py",
  "stop_on_error": true,
  "sandbox_mode": true
}
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\src\_workflows\microservice_processing_workflow.json
--------------------------------------------------------------------------------
{
  "name": "Default Microservice Extraction",
  "description": "Iterates through Python files and runs the AST-based extraction tasklist.",
  "tasklist_to_use": "default_ast_patch_generator.json",
  "target_dir": "./src",
  "extension": ".py",
  "stop_on_error": true
}
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\dirty_service.py
--------------------------------------------------------------------------------
# Legacy Microservice - Hard to parse with regex
import sys, os
from datetime import datetime

def helper_tool(data):
    return f"PROCESSED: {data}"

class LegacyDataService:
    def __init__(self, config=None):
        self.cfg = config
        self.started = datetime.now()

    def get_user_data(self, user_id):
        # This logic needs to be moved to @service_endpoint
        print(f"Fetching {user_id}")
        return {"id": user_id, "status": "active"}

    def update_record(self, record):
        # Messy inline logic
        if not record: return False
        return helper_tool(record)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\TEST_dirty_service.py
--------------------------------------------------------------------------------
# Legacy Microservice - Hard to parse with regex
import sys, os
from datetime import datetime

def helper_tool(data):
    return f"PROCESSED: {data}"

class LegacyDataService:
    def __init__(self, config=None):
        self.cfg = config
        self.started = datetime.now()

    def get_user_data(self, user_id):
        # This logic needs to be moved to @service_endpoint
        print(f"Fetching {user_id}")
        return {"id": user_id, "status": "active"}

    def update_record(self, record):
        # Messy inline logic
        if not record: return False
        return helper_tool(record)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\TEST_TEST_dirty_service.py
--------------------------------------------------------------------------------
# Legacy Microservice - Hard to parse with regex
import sys, os
from datetime import datetime

def helper_tool(data):
    return f"PROCESSED: {data}"

class LegacyDataService:
    def __init__(self, config=None):
        self.cfg = config
        self.started = datetime.now()

    def get_user_data(self, user_id):
        # This logic needs to be moved to @service_endpoint
        print(f"Fetching {user_id}")
        return {"id": user_id, "status": "active"}

    def update_record(self, record):
        # Messy inline logic
        if not record: return False
        return helper_tool(record)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\base_service.py.txt
--------------------------------------------------------------------------------
import logging
from typing import Dict, Any

class BaseService:
    """
    Standard parent class for all microservices. 
    Provides consistent logging and identity management.
    """
    def __init__(self, name: str):
        self._service_info = {
            "name": name, 
            "id": name.lower().replace(" ", "_")
        }
        
        # Setup standard logging
        logging.basicConfig(
            level=logging.INFO, 
            format='%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(name)

    def log_info(self, message: str):
        self.logger.info(message)

    def log_error(self, message: str):
        self.logger.error(message)

    def log_warning(self, message: str):
        self.logger.warning(message)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\boiler_plate.py
--------------------------------------------------------------------------------
from microservice_std_lib import service_metadata, service_endpoint
from typing import Dict, Any, Optional

@service_metadata(
    name="YourServiceName",
    version="1.0.0",
    description="Briefly describe the 'Purpose' here.",
    tags=["category", "utility"],
    capabilities=["filesystem:read"] # Optional: what it actually touches
)
class YourServiceMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Initialize your core logic/engines here

    @service_endpoint(
        inputs={"param1": "str", "param2": "int"},
        outputs={"result": "str"},
        description="Detailed description of what this specific method does.",
        tags=["action"],
        side_effects=["filesystem:write"] # Be explicit for the AI safety
    )
    def perform_action(self, param1: str, param2: int = 10) -> Dict[str, Any]:
        # Your translated logic goes here
        return {"result": f"Processed {param1}"}

if __name__ == "__main__":
    # Standard independent test block for the catalogue
    svc = YourServiceMS()
    print("Service ready:", svc)
    # Add a print test of your logic here
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__ArchiveBotMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ArchiveBotMS
ENTRY_POINT: __ArchiveBotMS.py
DEPENDENCIES: None
"""

"""
SERVICE: ArchiveBot
VERSION: 1.1.0
ROLE: Create timestamped compressed .tar.gz backups of directory trees.
INPUTS:
- source_path: (str) The root folder to backup.
- output_dir: (str) Where to save the resulting file.
OUTPUTS:
- archive_path: (str) Absolute path to the created file.
- file_count: (int) Total files compressed.
DEPENDENCIES: None (Standard Library)
NOTES:
Includes default ignore lists for common dev artifacts (node_modules, venv, etc).
"""

import datetime
import fnmatch
import logging
import os
import tarfile
from pathlib import Path
from typing import Any, Dict, Optional, Set, Tuple

from microservice_std_lib import service_metadata, service_endpoint

# === [ CONFIGURATION ] ========================================================
SERVICE_TITLE = "ArchiveBot"
SERVICE_VERSION = "1.1.0"
LOG_LEVEL = logging.INFO

# Default exclusions (Dev artifacts, caches, system files)
DEFAULT_IGNORE_DIRS: Set[str] = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", "dist",
    "build", "coverage", "target", "out", "bin", "obj"
}

DEFAULT_IGNORE_FILES: Set[str] = {
    ".DS_Store", "Thumbs.db", "*.pyc", "*.pyo", "*.log", "*.tmp"
}

logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(SERVICE_TITLE)

# === [ SERVICE DEFINITION ] ===================================================
@service_metadata(
    name=SERVICE_TITLE,
    version=SERVICE_VERSION,
    description="Creates timestamped .tar.gz backups of directory trees.",
    tags=["utility", "backup", "filesystem"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ArchiveBotMS:
    
    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}

    # === [ CORE ENDPOINTS ] ===================================================
    @service_endpoint(
        inputs={
            "source_path": "str",
            "output_dir": "str",
            "extra_exclusions": "List[str] | None",
            "use_default_exclusions": "bool"
        },
        outputs={
            "archive_path": "str",
            "file_count": "int"
        },
        description="Compresses a directory into a .tar.gz archive.",
        tags=["action", "backup"],
        side_effects=["filesystem:write"]
    )
    def create_backup(
        self,
        source_path: str,
        output_dir: str,
        extra_exclusions: Optional[Set[str]] = None,
        use_default_exclusions: bool = True,
    ) -> Dict[str, Any]:
        
        src = Path(source_path).resolve()
        out = Path(output_dir).resolve()

        if not src.exists():
            logger.error(f"Source not found: {src}")
            raise FileNotFoundError(f"Source path does not exist: {src}")

        out.mkdir(parents=True, exist_ok=True)

        # Build exclusion set
        exclude_patterns: Set[str] = set()
        if use_default_exclusions:
            exclude_patterns.update(DEFAULT_IGNORE_DIRS)
            exclude_patterns.update(DEFAULT_IGNORE_FILES)
        if extra_exclusions:
            exclude_patterns.update(extra_exclusions)

        # Generate filename: backup_FOLDERNAME_YYYY-MM-DD_HH-MM-SS.tar.gz
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        archive_name = f"backup_{src.name}_{timestamp}.tar.gz"
        archive_path = out / archive_name

        file_count = 0

        try:
            with tarfile.open(archive_path, "w:gz") as tar:
                for root, dirs, files in os.walk(src):
                    # Filter directories in-place to prevent walking into them
                    dirs[:] = [d for d in dirs if not self._is_excluded(d, exclude_patterns)]

                    for file_name in files:
                        if self._is_excluded(file_name, exclude_patterns):
                            continue

                        full_path = Path(root) / file_name
                        # Don't zip the file if we are writing it inside the source folder
                        if full_path == archive_path: continue

                        rel_path = full_path.relative_to(src)
                        tar.add(full_path, arcname=rel_path)
                        file_count += 1

            logger.info(f"Archive created: {archive_path} ({file_count} files)")
            return {
                "archive_path": str(archive_path),
                "file_count": file_count
            }

        except Exception as exc:
            logger.exception(f"Backup failed: {exc}")
            if archive_path.exists():
                try:
                    archive_path.unlink()
                except Exception: pass
            raise exc

    # === [ HELPERS ] ==========================================================
    def _is_excluded(self, name: str, patterns: Set[str]) -> bool:
        for pattern in patterns:
            if name == pattern or fnmatch.fnmatch(name, pattern):
                return True
        return False

# === [ SELF-TEST / RUNNER ] ===================================================
if __name__ == "__main__":
    # Create a dummy file to backup for testing
    import tempfile
    
    bot = ArchiveBotMS()
    
    with tempfile.TemporaryDirectory() as tmp_source:
        with tempfile.TemporaryDirectory() as tmp_out:
            # Create a test file
            p = Path(tmp_source) / "test_file.txt"
            p.write_text("Hello Archive")
            
            print(f"Backing up {tmp_source} to {tmp_out}...")
            result = bot.create_backup(tmp_source, tmp_out)
            print(f"Result: {result}")
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__AuthMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _AuthMS
ENTRY_POINT: __AuthMS.py
DEPENDENCIES: None
"""

import base64
import hashlib
import json
import logging
import time
from typing import Any, Dict, Optional

from microservice_std_lib import service_metadata, service_endpoint

"""
SERVICE: Auth
ROLE: Manage user authentication and signed session tokens.
INPUTS:
  - username: Login identifier.
  - password: Secret credential.
  - token: Serialized session token string.
OUTPUTS:
  - token: Signed session token (str) or None on failure.
  - is_valid: Boolean indicating whether a token is valid and not expired.
NOTES:
  This is a simplified in-memory auth system intended for local tools and
  pipelines, not production-grade security.
"""

logger = logging.getLogger(__name__)

# ==============================================================================
# CONFIGURATION
# ==============================================================================

DEFAULT_SECRET_KEY = "super_secret_cortex_key"
DEFAULT_SALT = "cortex_salt"


# ==============================================================================


@service_metadata(
    name="Auth",
    version="1.0.0",
    description="Manages user authentication and session tokens.",
    tags=["auth", "security", "crypto"],
    capabilities=["crypto"],
)
class AuthMS:
    """
    ROLE: Simple authentication microservice providing username/password login
          and signed session tokens.

    INPUTS:
      - config: Optional configuration dict. Recognized keys:
          - 'secret_key': Secret used to sign tokens.

    OUTPUTS:
      - Exposes `login` and `validate_session` endpoints for use in pipelines.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}
        self.secret_key: str = self.config.get("secret_key", DEFAULT_SECRET_KEY)

        # In a real scenario, this might load from a secure config file or DB.
        # For now, we keep a minimal in-memory user database.
        self.users_db: Dict[str, str] = {
            "admin": self._hash_password("admin123"),
        }

    # --------------------------------------------------------------------- #
    # Public endpoints
    # --------------------------------------------------------------------- #

    @service_endpoint(
        inputs={"username": "str", "password": "str"},
        outputs={"token": "Optional[str]"},
        description="Attempts to log in and returns a signed session token.",
        tags=["auth", "security", "session"],
    )
    def login(self, username: str, password: str) -> Optional[str]:
        """
        Attempt to log in with the provided username and password.

        :param username: Login identifier.
        :param password: Plain-text password.
        :returns: Signed session token if successful, None otherwise.
        """
        if username not in self.users_db:
            return None

        stored_hash = self.users_db[username]
        if self._verify_password(password, stored_hash):
            return self._create_token(username)

        return None

    @service_endpoint(
        inputs={"token": "str"},
        outputs={"is_valid": "bool"},
        description="Checks whether a token is valid and not expired.",
        tags=["auth", "security"],
    )
    def validate_session(self, token: str) -> bool:
        """
        Check if a serialized token is valid and not expired.

        :param token: Session token string.
        :returns: True if token is valid and not expired, False otherwise.
        """
        payload = self._decode_token(token)
        return payload is not None

    # --------------------------------------------------------------------- #
    # Internal helpers
    # --------------------------------------------------------------------- #

    def _hash_password(self, password: str) -> str:
        """
        Securely hashes a password using SHA-256 with a static salt.
        """
        return hashlib.sha256((password + DEFAULT_SALT).encode("utf-8")).hexdigest()

    def _verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verifies a provided password against the stored hash.
        """
        return self._hash_password(plain_password) == hashed_password

    def _create_token(self, user_id: str, expires_in: int = 3600) -> str:
        """
        Generates a signed session token.

        Payload includes:
          - 'sub' (subject)
          - 'exp' (expiration time)
          - 'iat' (issued-at time)
          - 'scope' (authorization scope)
        """
        now = int(time.time())
        payload = {
            "sub": user_id,
            "exp": now + expires_in,
            "iat": now,
            "scope": "admin",
        }

        json_payload = json.dumps(payload).encode("utf-8")
        token_part = base64.b64encode(json_payload).decode("utf-8")

        signature = hashlib.sha256((token_part + self.secret_key).encode("utf-8")).hexdigest()
        return f"{token_part}.{signature}"

    def _decode_token(self, token: str) -> Optional[Dict[str, Any]]:
        """
        Parses and validates the incoming token.

        Returns the payload if valid, None otherwise.
        """
        try:
            if not token or "." not in token:
                return None

            token_part, signature = token.split(".", 1)

            # Recalculate signature to verify integrity
            recalc_signature = hashlib.sha256(
                (token_part + self.secret_key).encode("utf-8")
            ).hexdigest()

            if signature != recalc_signature:
                return None  # Invalid signature

            # Decode payload
            payload_json = base64.b64decode(token_part).decode("utf-8")
            payload: Dict[str, Any] = json.loads(payload_json)

            # Check expiration
            if payload.get("exp", 0) < time.time():
                return None  # Expired

            return payload

        except Exception:
            # Intentionally swallow details here and just treat token as invalid.
            logger.exception("Failed to decode or validate auth token.")
            return None


if __name__ == "__main__":
    svc = AuthMS()
    print("Service ready:", svc)
    # Example (manual) usage:
    # token = svc.login("admin", "admin123")
    # print("Token:", token)
    # print("Valid:", svc.validate_session(token) if token else False)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__CartridgeServiceMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CartridgeServiceMS
ENTRY_POINT: __CartridgeServiceMS.py
DEPENDENCIES: None
"""

import sqlite3
import json
import time
import os
import uuid
import datetime
import struct
from pathlib import Path

# Try to import sqlite-vec (pip install sqlite-vec)
try:
    import sqlite_vec
except ImportError:
    sqlite_vec = None
from typing import Dict, Any, Optional, List
from base_service import BaseService
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="CartridgeServiceMS",
    version="1.1.0",
    description="The Source of Truth. Manages the Unified Neural Cartridge Format (UNCF v1.0).",
    tags=["storage", "database", "RAG"],
    capabilities=["sqlite", "vector-search", "graph-storage"]
)
class CartridgeServiceMS(BaseService):
    """
    The Source of Truth.
    Manages the Unified Neural Cartridge Format (UNCF v1.0).
    """
    
    SCHEMA_VERSION = "uncf_v1.0"

    def __init__(self, db_path: str):
        super().__init__("CartridgeServiceMS")
        self.db_path = Path(db_path)
        self._init_db()

    def _get_conn(self):
        # Set generous timeout (60s) for multi-threaded Ingest/Refinery contention
        conn = sqlite3.connect(self.db_path, timeout=60.0)
        if sqlite_vec:
            try:
                conn.enable_load_extension(True)
                sqlite_vec.load(conn)
                conn.enable_load_extension(False)
            except Exception as e:
                self.log_error(f"Failed to load sqlite-vec: {e}")
        return conn

    def get_vector_dim(self) -> int:
        """Retrieves the expected vector dimension from the manifest spec."""
        spec = self.get_manifest("embedding_spec") or {}
        if isinstance(spec, str):
            try: spec = json.loads(spec)
            except: spec = {}
        return int(spec.get("dim", 0))

    def _init_db(self):
        """Initializes the standard Schema."""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = self._get_conn()
        cursor = conn.cursor()
        
        # Enable WAL Mode: Allows concurrent Readers (Refinery) & Writers (Ingest)
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL")
        
        # 1. Manifest (The Boot Sector)
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")
        
        # 1.5 Directories (The VFS Index)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS directories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT UNIQUE NOT NULL,
                parent_path TEXT,
                metadata TEXT DEFAULT '{}'
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_dir_parent ON directories(parent_path)")

        # 2. Files (The Content Store)
        # Supports Text AND Binary (blob_data)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT NOT NULL,       -- Portable path (e.g. "src/main.py")
                origin_path TEXT,             -- Provenance (e.g. "C:/Users/...")
                origin_type TEXT,             -- 'filesystem', 'web', 'github'
                content TEXT,                 -- Text content (UTF-8)
                blob_data BLOB,               -- Binary content (Images, PDFs)
                mime_type TEXT,
                status TEXT DEFAULT 'RAW',    -- RAW, REFINED, ERROR, SKIPPED
                metadata TEXT DEFAULT '{}',   -- JSON tags, summaries
                last_updated TIMESTAMP
            )
        """)
        # Index for fast lookups by VFS path
        cursor.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_vfs ON files(vfs_path)")

        # 3. Chunks (The Vector Store)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_id INTEGER,
                chunk_index INTEGER,
                content TEXT,
                embedding BLOB,
                name TEXT,
                type TEXT,
                start_line INTEGER,
                end_line INTEGER,
                FOREIGN KEY(file_id) REFERENCES files(id)
            )
        """)

        # 3.5 Vector Index (sqlite-vec)
        if sqlite_vec:
            dim = self.get_vector_dim()
            if dim > 0:
                try:
                    cursor.execute(f"CREATE VIRTUAL TABLE IF NOT EXISTS vec_items USING vec0(embedding float[{dim}])")
                except Exception as e:
                    self.log_error(f"Vector Table Init Error: {e}")
            else:
                self.log_info("Vector table creation deferred: No dimensions found in manifest yet.")

        # 4. Graph Topology (The Neural Wiring)
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, relation TEXT, weight REAL)")

        # 5. Validation Logs
        cursor.execute("CREATE TABLE IF NOT EXISTS logs (timestamp REAL, level TEXT, message TEXT, context TEXT)")
        
        conn.commit()
        conn.close()
        
        # Initialize standard keys if new
        self.initialize_manifest()

    def initialize_manifest(self):
        """Populates the boot sector with strict RagFORGE Cartridge Schema (UNCF) v1.1."""
        if not self.get_manifest("cartridge_id"):
            now = datetime.datetime.utcnow().isoformat()

            # 1. Identity & Versioning
            self.set_manifest("schema_name", "ragforge_cartridge")
            self.set_manifest("schema_version", "1.1.0")
            self.set_manifest("cartridge_id", str(uuid.uuid4()))
            self.set_manifest("created_at_utc", now)
            self.set_manifest("created_by_app", "RagFORGE")

            # 2. Provenance / Sources
            # Agents can read this to understand where the content came from and what policies were used.
            self.set_manifest("sources", [])
            self.set_manifest("source_policies", {
                "binary_policy": "Extract Text",
                "web_depth": 0
            })

            # 3. Specs (Defaults - updated by RefineryService._stamp_specs)
            self.set_manifest("embedding_spec", {
                "provider": "unknown",
                "model": "pending_init",
                "dim": 0,
                "dtype": "unknown",
                "distance": "unknown"
            })
            self.set_manifest("chunking_spec", {
                "strategy": "semantic_hybrid",
                "python_ast": True,
                "generic_window": 1500
            })

            # 4. VFS + Content Stats (populated/updated over time)
            self.set_manifest("vfs", {
                "root_label": "",
                "directories": {"count": 0},
                "files": {
                    "count": 0,
                    "by_origin_type": {},
                    "by_mime": {}
                },
                "index_built": False
            })
            self.set_manifest("content_stats", {
                "chunks": {"count": 0},
                "vector_index": {
                    # Don't assume sqlite-vec is available until proven.
                    "enabled": False,
                    "table": "vec_items",
                    "backend": "sqlite-vec",
                    "dims": 0,
                    "status": "unknown"
                },
                "graph": {
                    "nodes": 0,
                    "edges": 0
                }
            })

            # 5. Capabilities Contract (what an agent can assume exists / how to navigate)
            self.set_manifest("capabilities", {
                "tables": {
                    "manifest": True,
                    "directories": True,
                    "files": True,
                    "chunks": True,
                    "vec_items": True,
                    "graph_nodes": True,
                    "graph_edges": True,
                    "logs": True
                },
                "navigation": {
                    "vfs_path": "files.vfs_path",
                    "directory_index": "directories.vfs_path",
                    "list_files_query": "SELECT vfs_path, mime_type, origin_type, status FROM files ORDER BY vfs_path",
                    "list_directories_query": "SELECT vfs_path, parent_path FROM directories ORDER BY vfs_path"
                },
                "retrieval": {
                    "raw_file_content_query": "SELECT content, blob_data, mime_type FROM files WHERE vfs_path=?",
                    "chunks_by_file_query": "SELECT chunk_index, name, type, start_line, end_line, content FROM chunks WHERE file_id=? ORDER BY chunk_index",
                    "vector_search": "sqlite-vec on vec_items if available"
                },
                "python_helper_api": {
                    "note": "Optional convenience layer for agents running inside Python. For non-Python consumers, use the SQL queries above.",
                    "methods": [
                        "CartridgeServiceMS.get_status_flags",
                        "CartridgeServiceMS.list_files",
                        "CartridgeServiceMS.list_directories",
                        "CartridgeServiceMS.get_file_record",
                        "CartridgeServiceMS.get_directory_tree",
                        "CartridgeServiceMS.get_status_summary",
                        "CartridgeServiceMS.add_node",
                        "CartridgeServiceMS.add_edge",
                        "CartridgeServiceMS.search_embeddings"
                    ]
                }
            })

            # 6. Status & Health
            self.set_manifest("cartridge_health", "FRESH")
            self.set_manifest("ingest_complete", False)
            self.set_manifest("refine_complete", False)
            self.set_manifest("last_ingest_at_utc", "")
            self.set_manifest("last_refine_at_utc", "")
            self.set_manifest("last_error", "")
            self.set_manifest("locks", {
                "write_lock_expected": False,
                "notes": "If DB locks occur, consider batching writes and shorter-lived connections."
            })

    def set_manifest(self, key: str, value: Any):
        """Upsert metadata key."""
        conn = self._get_conn()
        val_str = json.dumps(value) if isinstance(value, (dict, list)) else str(value)
        conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", (key, val_str))
        conn.commit()
        conn.close()

    def get_manifest(self, key: str) -> Optional[str]:
        """Retrieve metadata key."""
        conn = self._get_conn()
        row = conn.execute("SELECT value FROM manifest WHERE key=?", (key,)).fetchone()
        conn.close()
        return row[0] if row else None

    def validate_cartridge(self) -> Dict[str, Any]:
        """Quality Control: Checks if the cartridge is Agent-Safe."""
        report = {"valid": True, "health": "OK", "errors": []}
        
        # 1. Check Required Keys
        # These are the minimum contract keys an agent needs to understand what it loaded.
        required = [
            "schema_name",
            "schema_version",
            "cartridge_id",
            "created_at_utc",
            "created_by_app",
            "embedding_spec",
            "chunking_spec",
            "capabilities"
        ]
        for key in required:
            if not self.get_manifest(key):
                report["valid"] = False
                report["errors"].append(f"Missing Manifest Key: {key}")
        
        # 2. Check Vector Index Presence (and stamp reality into the manifest)
        conn = self._get_conn()
        vec_enabled = False
        vec_status = "unknown"
        try:
            # If this succeeds, the vec_items table exists and is queryable.
            conn.execute("SELECT count(*) FROM vec_items").fetchone()
            vec_enabled = True
            vec_status = "available"
        except Exception:
            vec_enabled = False
            vec_status = "unavailable"
            report["errors"].append("Vector Index (vec_items) missing or not loaded.")
            # Not fatal for 'valid' but impacts capability
            report["health"] = "WARN_NO_VECTORS"
        finally:
            conn.close()

        # Update manifest content_stats.vector_index to reflect truth
        try:
            content_stats = self.get_manifest("content_stats") or {}
            vec = content_stats.get("vector_index", {}) if isinstance(content_stats, dict) else {}

            # Use embedding_spec dim if present; otherwise keep existing dims value
            embed_spec = self.get_manifest("embedding_spec") or {}
            spec_dim = 0
            if isinstance(embed_spec, dict):
                spec_dim = int(embed_spec.get("dim", 0) or 0)

            vec["enabled"] = bool(vec_enabled)
            vec["status"] = vec_status
            if spec_dim > 0:
                vec["dims"] = spec_dim

            # Preserve backend/table fields if present
            if "table" not in vec:
                vec["table"] = "vec_items"
            if "backend" not in vec:
                vec["backend"] = "sqlite-vec"

            content_stats["vector_index"] = vec
            self.set_manifest("content_stats", content_stats)
        except Exception as e:
            # Non-fatal; validation should still return a report
            report["errors"].append(f"Failed to stamp vector_index status into manifest: {e}")
            report["health"] = "WARN_MANIFEST_STAMP_FAIL"
            
        return report

    def store_file(self, vfs_path: str, origin_path: str, content: str = None, blob: bytes = None, mime_type: str = "text/plain", origin_type: str = "filesystem"):
        """
        The Universal Input Method. 
        Stores raw data. If file exists, updates it and resets status to 'RAW' for re-refining.
        """
        conn = self._get_conn()
        try:
            conn.execute("""
                INSERT OR REPLACE INTO files 
                (vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, 'RAW', ?)
            """, (vfs_path, origin_path, origin_type, content, blob, mime_type, time.time()))
            conn.commit()
            return True
        except Exception as e:
            self.log_error(f"DB Store Error ({vfs_path}): {e}")
            return False
        finally:
            conn.close()

    def get_pending_files(self, limit: int = 10) -> List[Dict]:
        """Fetches files waiting for the Refinery."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        rows = conn.execute("SELECT * FROM files WHERE status = 'RAW' LIMIT ?", (limit,)).fetchall()
        conn.close()
        return [dict(row) for row in rows]

    def update_status(self, file_id: int, status: str, metadata: dict = None):
        conn = self._get_conn()
        if metadata:
            conn.execute("UPDATE files SET status = ?, metadata = ? WHERE id = ?", 
                         (status, json.dumps(metadata), file_id))
        else:
            conn.execute("UPDATE files SET status = ? WHERE id = ?", (status, file_id))
        conn.commit()
        conn.close()

    def ensure_directory(self, vfs_path: str):
        """Idempotent insert for VFS directories."""
        if not vfs_path: return
        parent = os.path.dirname(vfs_path).replace("\\", "/")
        if parent == vfs_path: parent = "" # Root case
        
        conn = self._get_conn()
        try:
            conn.execute("INSERT OR IGNORE INTO directories (vfs_path, parent_path) VALUES (?, ?)", (vfs_path, parent))
            conn.commit()
        except: pass
        finally:
            conn.close()

    # --- Agent-Friendly Helpers (No raw SQL required) ---
    def _coerce_bool(self, v: Any) -> bool:
        """Best-effort conversion for manifest values stored as strings."""
        if v is None:
            return False
        if isinstance(v, bool):
            return v
        s = str(v).strip().lower()
        return s in ("1", "true", "yes", "y", "on")

    @service_endpoint(
        inputs={},
        outputs={"ingest_complete": "bool", "refine_complete": "bool", "cartridge_health": "str"},
        description="Returns key manifest status flags (ingest/refine status and health) in a single call.",
        tags=["status", "health"]
    )
    def get_status_flags(self) -> Dict[str, Any]:
        """Returns key manifest status flags in a single call."""
        ingest_complete = self._coerce_bool(self.get_manifest("ingest_complete"))
        refine_complete = self._coerce_bool(self.get_manifest("refine_complete"))
        health = self.get_manifest("cartridge_health") or "UNKNOWN"
        return {
            "ingest_complete": ingest_complete,
            "refine_complete": refine_complete,
            "cartridge_health": health,
            "schema_name": self.get_manifest("schema_name") or "",
            "schema_version": self.get_manifest("schema_version") or "",
            "cartridge_id": self.get_manifest("cartridge_id") or ""
        }

    def list_files(self, prefix: str = "", status: Optional[str] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Enumerate files in the cartridge (optionally filtered by VFS prefix and/or status)."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            sql = "SELECT id, vfs_path, origin_path, origin_type, mime_type, status, last_updated, metadata FROM files"
            clauses = []
            params = []

            if prefix:
                # Prefix match on portable path
                clauses.append("vfs_path LIKE ?")
                params.append(prefix.rstrip("/") + "/%")

            if status:
                clauses.append("status = ?")
                params.append(status)

            if clauses:
                sql += " WHERE " + " AND ".join(clauses)

            sql += " ORDER BY vfs_path"

            if limit is not None:
                sql += " LIMIT ?"
                params.append(int(limit))

            rows = conn.execute(sql, tuple(params)).fetchall()
            out = []
            for r in rows:
                d = dict(r)
                # metadata is stored as JSON string
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    def get_file_record(self, vfs_path: str) -> Optional[Dict[str, Any]]:
        """Fetch a single file record by VFS path."""
        if not vfs_path:
            return None
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            row = conn.execute(
                "SELECT id, vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, metadata, last_updated FROM files WHERE vfs_path = ?",
                (vfs_path,)
            ).fetchone()
            if not row:
                return None
            d = dict(row)
            try:
                d["metadata"] = json.loads(d.get("metadata") or "{}")
            except Exception:
                d["metadata"] = {}
            return d
        finally:
            conn.close()

    def list_directories(self, prefix: str = "") -> List[Dict[str, Any]]:
        """Enumerate directories in the cartridge VFS."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            if prefix:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories WHERE vfs_path LIKE ? ORDER BY vfs_path",
                    (prefix.rstrip("/") + "/%",)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories ORDER BY vfs_path"
                ).fetchall()

            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    @service_endpoint(
        inputs={"root": "str"},
        outputs={"tree": "dict"},
        description="Builds a nested directory tree structure for UI navigation or context mapping.",
        tags=["vfs", "navigation"]
    )
    def get_directory_tree(self, root: str = "") -> Dict[str, Any]:
        """Builds a nested directory tree starting at `root` ("" for full tree)."""
        dirs = self.list_directories(prefix=root) if root else self.list_directories()
        files = self.list_files(prefix=root) if root else self.list_files()

        # Tree nodes are dicts: {"_dirs": {name: node}, "_files": [file_records...]}
        def new_node():
            return {"_dirs": {}, "_files": []}

        tree = new_node()

        # Insert directories
        for d in dirs:
            path = (d.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            cur = tree
            for p in parts:
                cur = cur["_dirs"].setdefault(p, new_node())

        # Insert files
        for f in files:
            path = (f.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            fname = parts[-1]
            cur = tree
            for p in parts[:-1]:
                cur = cur["_dirs"].setdefault(p, new_node())
            # Store a light file record for tree browsing
            cur["_files"].append({
                "name": fname,
                "vfs_path": f.get("vfs_path"),
                "mime_type": f.get("mime_type"),
                "origin_type": f.get("origin_type"),
                "status": f.get("status")
            })

        return tree

    def get_status_summary(self) -> Dict[str, Any]:
        """Counts files by status and provides a quick cartridge overview."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            rows = conn.execute("SELECT status, COUNT(*) as n FROM files GROUP BY status").fetchall()
            by_status = {r["status"]: r["n"] for r in rows}

            dcnt = conn.execute("SELECT COUNT(*) FROM directories").fetchone()[0]
            fcnt = conn.execute("SELECT COUNT(*) FROM files").fetchone()[0]
            ccnt = conn.execute("SELECT COUNT(*) FROM chunks").fetchone()[0]
            ncnt = conn.execute("SELECT COUNT(*) FROM graph_nodes").fetchone()[0]
            ecnt = conn.execute("SELECT COUNT(*) FROM graph_edges").fetchone()[0]

            return {
                "directories": int(dcnt),
                "files": int(fcnt),
                "chunks": int(ccnt),
                "graph_nodes": int(ncnt),
                "graph_edges": int(ecnt),
                "files_by_status": by_status,
                "flags": self.get_status_flags()
            }
        finally:
            conn.close()

    # --- Graph Helpers ---
    def add_node(self, node_id: str, node_type: str, label: str, data: dict = None):
        conn = self._get_conn()
        conn.execute("INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json) VALUES (?, ?, ?, ?)",
                     (node_id, node_type, label, json.dumps(data or {})))
        conn.commit()
        conn.close()

    def add_edge(self, source: str, target: str, relation: str = "related", weight: float = 1.0):
        conn = self._get_conn()
        conn.execute("INSERT OR IGNORE INTO graph_edges (source, target, relation, weight) VALUES (?, ?, ?, ?)",
                     (source, target, relation, weight))
        conn.commit()
        conn.close()

    # --- Vector Search ---
    @service_endpoint(
        inputs={"query_vector": "list", "limit": "int"},
        outputs={"results": "list"},
        description="Performs semantic vector search using sqlite-vec against the cartridge chunks.",
        tags=["search", "vector"]
    )
    def search_embeddings(self, query_vector: List[float], limit: int = 5) -> List[Dict]:
        """Performs semantic search using sqlite-vec."""
        if not sqlite_vec or not query_vector:
            return []

        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        results = []
        
        try:
            # Pack vector to binary if needed, but sqlite-vec usually handles raw lists in parameterized queries
            # dependent on the binding. We'll pass binary for safety if using standard bindings,
            # but typically raw list works with the extension's adapters. 
            # For now, we assume the extension handles the list->vector conversion.
            
            rows = conn.execute("""
                SELECT
                    rowid,
                    distance
                FROM vec_items
                WHERE embedding MATCH ?
                ORDER BY distance
                LIMIT ?
            """, (json.dumps(query_vector), limit)).fetchall()
            
            # Resolve back to chunks with VFS context
            for r in rows:
                chunk_id = r['rowid']
                # Join with files to get vfs_path
                query = """
                    SELECT c.*, f.vfs_path 
                    FROM chunks c 
                    JOIN files f ON c.file_id = f.id 
                    WHERE c.id=?
                """
                chunk = conn.execute(query, (chunk_id,)).fetchone()
                
                if chunk:
                    res = dict(chunk)
                    res['score'] = r['distance']
                    results.append(res)
                    
        except Exception as e:
            self.log_error(f"Vector Search Error: {e}")
        finally:
            conn.close()
            
        return results










--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__ChalkBoardMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ChalkBoardMS
ENTRY_POINT: __ChalkBoardMS.py
DEPENDENCIES: None
"""

import webview 
import json
import os

# --- EMBEDDED HTML WITH NEW THEMES ---
HTML_CONTENT = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>OBS Signboard</title>
    <link href="https://fonts.googleapis.com/css2?family=Neonderthaw&family=Press+Start+2P&family=Fredericka+the+Great&family=Orbitron:wght@700&family=Special+Elite&display=swap" rel="stylesheet">
    <style>
        body, html { margin: 0; padding: 0; height: 100%; overflow: hidden; display: flex; justify-content: center; align-items: center; transition: all 0.5s ease; }
        #sign-container { width: 90%; text-align: center; outline: none; cursor: text; transition: transform 0.2s; }
        
        /* --- THEME 1: NEON NIGHTS --- */
        body.neon { background-color: #050505; font-family: 'Neonderthaw', cursive; }
        body.neon #sign-container { color: #fff; font-size: 8rem; text-shadow: 0 0 7px #fff, 0 0 42px #bc13fe, 0 0 102px #bc13fe; animation: flicker 1.5s infinite alternate; }

        /* --- THEME 2: 8-BIT HACKER --- */
        body.terminal { background-color: #000; font-family: 'Press Start 2P', cursive; }
        body.terminal #sign-container { color: #00ff41; font-size: 3.5rem; text-shadow: 0 0 10px #00ff41; text-transform: uppercase; }
        body.terminal #sign-container::after { content: '_'; animation: blink 1s step-end infinite; }

        /* --- THEME 3: CHALKBOARD --- */
        body.chalk { background-color: #2b3a28; font-family: 'Fredericka the Great', cursive; background-image: radial-gradient(circle, rgba(255,255,255,0.05) 1px, transparent 1px); background-size: 20px 20px; }
        body.chalk #sign-container { color: rgba(255,255,255,0.9); font-size: 6rem; transform: rotate(-1deg); }

        /* --- NEW THEME 4: BLUEPRINT (Technical) --- */
        body.blueprint { background-color: #003366; font-family: 'Orbitron', sans-serif; background-image: linear-gradient(#004080 1px, transparent 1px), linear-gradient(90deg, #004080 1px, transparent 1px); background-size: 50px 50px; }
        body.blueprint #sign-container { color: #00d9ff; font-size: 5rem; text-transform: uppercase; border: 2px solid #00d9ff; padding: 20px; box-shadow: 0 0 15px #00d9ff; }

        /* --- NEW THEME 5: RETRO WOOD --- */
        body.retro { background-color: #3d2b1f; font-family: 'Special Elite', serif; background-image: repeating-linear-gradient(90deg, transparent, transparent 40px, rgba(0,0,0,0.1) 41px); }
        body.retro #sign-container { color: #e6b450; font-size: 5.5rem; text-shadow: 2px 2px 0px #20150d; }

        /* --- NEW THEME 6: CYBERPUNK (Yellow/Black) --- */
        body.cyber { background-color: #fcee0a; font-family: 'Orbitron', sans-serif; }
        body.cyber #sign-container { color: #000; font-size: 5rem; font-weight: 900; text-transform: uppercase; font-style: italic; background: #000; color: #fcee0a; padding: 10px 40px; clip-path: polygon(0% 0%, 100% 0%, 95% 100%, 5% 100%); }

        /* ANIMATIONS & EFFECTS */
        @keyframes flicker { 0%, 19%, 21%, 100% { opacity: 1; } 20% { opacity: 0.5; } }
        @keyframes blink { 0%, 100% { opacity: 1; } 50% { opacity: 0; } }
        .shake { animation: shake 0.5s cubic-bezier(.36,.07,.19,.97) both; }
        @keyframes shake { 10%, 90% { transform: translate3d(-1px, 0, 0); } 20%, 80% { transform: translate3d(2px, 0, 0); } 30%, 50%, 70% { transform: translate3d(-4px, 0, 0); } 40%, 60% { transform: translate3d(4px, 0, 0); } }
    </style>
</head>
<body class="neon">
    <div id="sign-container" contenteditable="true" spellcheck="false">ON AIR</div>

    <script>
        const container = document.getElementById('sign-container');

        function updateDisplay(text, theme) {
            container.innerText = text;
            document.body.className = theme;
        }

        function triggerEffect(effect) {
            if (effect === 'shake') {
                container.classList.add('shake');
                setTimeout(() => container.classList.remove('shake'), 500);
            }
        }

        // Notify Python on load
        window.addEventListener('pywebviewready', () => {
            window.pywebview.api.loaded().then(state => {
                updateDisplay(state.text, state.theme);
            });
        });

        document.addEventListener('keydown', (e) => {
            const themes = { 'F1': 'neon', 'F2': 'terminal', 'F3': 'chalk', 'F4': 'blueprint', 'F5': 'retro', 'F6': 'cyber' };
            if (themes[e.key]) {
                document.body.className = themes[e.key];
                window.pywebview.api.log_action('switch_theme_' + themes[e.key]);
            }
        });
    </script>
</body>
</html>
"""

# --- PYTHON MICROSERVICE ---
try:
    from microservice_std_lib import service_metadata, service_endpoint
except ImportError:
    def service_metadata(**kwargs): return lambda c: c
    def service_endpoint(**kwargs): return lambda f: f

@service_metadata(
    name="ChalkboardWeb",
    version="2.0.1",
    description="Integrated HTML5/CSS3 Digital Signage Engine",
    tags=["ui", "webview", "obs"],
    capabilities=["ui:gui"]
)
class ChalkBoardMS:
    def __init__(self):
        self._window = None
        self.state = {"text": "ON AIR", "theme": "neon"}

    def loaded(self):
        """Called by JS when the page is ready."""
        print("Frontend handshake complete.")
        return self.state

    def log_action(self, action_name):
        """Called by JS when user interacts."""
        print(f"Webview Event: {action_name}")

    @service_endpoint(inputs={"text": "str", "theme": "str"}, outputs={})
    def update_sign(self, text: str, theme: str = "neon"):
        """Updates the embedded HTML via JS injection."""
        self.state["text"] = text
        self.state["theme"] = theme
        if self._window:
            sanitized_text = json.dumps(text)
            self._window.evaluate_js(f"updateDisplay({sanitized_text}, '{theme}')")

    @service_endpoint(inputs={"effect": "str"}, outputs={})
    def trigger_effect(self, effect: str):
        """Triggers CSS animations like 'shake'."""
        if self._window:
            self._window.evaluate_js(f"triggerEffect('{effect}')")

def start_app():
    api = ChalkBoardMS()
    window = webview.create_window(
        'OBS Signboard v2', 
        html=HTML_CONTENT, # No external file dependency now!
        js_api=api,
        width=1000, 
        height=700,
        background_color='#000000'
    )
    api._window = window
    webview.start(debug=True)

if __name__ == "__main__":
    start_app()
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__ChunkingRouterMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ChunkingRouterMS
ENTRY_POINT: __ChunkingRouterMS.py
DEPENDENCIES: None
"""

import re
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint
from __PythonChunkerMS import PythonChunkerMS, CodeChunk

@service_metadata(
    name="ChunkingRouterMS",
    version="1.1.0",
    description="The Dispatcher: Routes files to specialized chunkers based on extension (AST for Python, Recursive for Prose).",
    tags=["orchestration", "chunking", "nlp"],
    capabilities=["routing", "text-processing"]
)
class ChunkingRouterMS:
    """
The Editor: A 'Recursive' text splitter.
It respects the natural structure of text (Paragraphs -> Sentences -> Words)
rather than just hacking it apart by character count.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    self.python_specialist = PythonChunkerMS()
    # Separators for the Prose Specialist logic
    self.separators = ["\n\n", "\n", "(?<=[.?!])\s+", " ", ""]

    @service_endpoint(
        inputs={"text": "str", "filename": "str", "max_size": "int", "overlap": "int"},
        outputs={"chunks": "list"},
        description="Routes text to the appropriate specialist. Returns a list of CodeChunk objects or raw strings.",
        tags=["routing", "chunking"]
    )
    def chunk_file(self, text: str, filename: str, max_size: int = 1000, overlap: int = 100) -> List[Any]:
        """
        Extension-aware router.
        """
        if filename.endswith(".py"):
            return self.python_specialist.chunk(text)
        
        # Fallback to the internal Prose Specialist (Recursive Splitter)
        raw_chunks = self._recursive_split(text, self.separators, max_size, overlap)
        
        # Standardize output for the Refinery: Wrap prose in CodeChunk objects
        return [
            CodeChunk(
                name=f"prose_chunk_{i}", 
                type="text", 
                content=c, 
                start_line=0, 
                end_line=0
            ) for i, c in enumerate(raw_chunks)
        ]

    def _recursive_split(self, text: str, separators: List[str], max_size: int, overlap: int) -> List[str]:
        final_chunks = []
        
        # 1. Base Case: If the text fits, return it
        if len(text) <= max_size:
            return [text]
        
        # 2. Edge Case: No more separators, forced hard split
        if not separators:
            return self._hard_split(text, max_size, overlap)

        # 3. Recursive Step: Try to split by the current separator
        current_sep = separators[0]
        next_separators = separators[1:]
        
        # Regex split to keep delimiters if possible (logic varies by regex complexity)
        # For simple string splits like \n\n, we just split.
        if len(current_sep) > 1 and "(" in current_sep: 
            # It's a regex lookbehind (sentence splitter), use re.split
            splits = re.split(current_sep, text)
        else:
            splits = text.split(current_sep)

        # Now we have a list of smaller pieces. We need to merge them back together
        # until they fill the 'max_size' bucket, then start a new bucket.
        current_doc = []
        current_length = 0
        
        for split in splits:
            if not split: continue
            
            # If a single split is STILL too big, recurse deeper on it
            if len(split) > max_size:
                # If we have stuff in the buffer, flush it first
                if current_doc:
                    final_chunks.append(current_sep.join(current_doc))
                    current_doc = []
                    current_length = 0
                
                # Recurse on the big chunk using the NEXT separator
                sub_chunks = self._recursive_split(split, next_separators, max_size, overlap)
                final_chunks.extend(sub_chunks)
                continue

            # Check if adding this split would overflow
            if current_length + len(split) + len(current_sep) > max_size:
                # Flush the current buffer
                doc_text = current_sep.join(current_doc)
                final_chunks.append(doc_text)
# Start new buffer with overlap logic?
                # For simplicity in recursion, we often just start fresh or carry over 
                # a small tail if we implemented a rolling window here.
                # To keep this "Pure logic" simple, we start fresh with the current split.
                current_doc = [split]
                current_length = len(split)
            else:
                # Add to buffer
                current_doc.append(split)
                current_length += len(split) + len(current_sep)

        # Flush remaining
        if current_doc:
            final_chunks.append(current_sep.join(current_doc))

        return final_chunks

    def _hard_split(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Last resort: naive character sliding window."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    chunker = SmartChunkerMS()
    print("Service ready:", chunker)
    
    # Example: A technical document with structure
    doc = """
    # Intro to AI
    Artificial Intelligence is great. It helps us code.
    
    ## How it works
    1. Ingestion: Reading data.
    2. Processing: Thinking about data.
    
    This is a very long paragraph that effectively serves as a stress test for the sentence splitter. It should hopefully not break in the middle of a thought! We want to keep sentences whole.
    """
    
    print("--- Testing Smart Chunking (Max 60 chars) ---")
    # We set max_size very small to force it to use the sentence/word splitters
    chunks = chunker.chunk(doc, max_size=60, overlap=0)
    
    for i, c in enumerate(chunks):
        print(f"[{i}] {repr(c)}")


--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__CodeChunkerMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeChunkerMS
ENTRY_POINT: __CodeChunkerMS.py
DEPENDENCIES: None
"""

import re
from typing import List, Dict, Any, Optional
from pathlib import Path
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="CodeChunker",
version="1.0.0",
description="Splits code into semantic blocks (Classes, Functions) using indentation and regex heuristics.",
tags=["parsing", "chunking", "code"],
capabilities=["filesystem:read"]
)
class CodeChunkerMS:
    """
The Surgeon (Pure Python Edition): Splits code into semantic blocks
    (Classes, Functions) using indentation and regex heuristics.
    
    Advantages: Zero dependencies. Works on any machine.
    Disadvantages: Slightly less precise than Tree-Sitter for messy code.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    # Regex to find definitions. Capture group 1 is the indentation.
    # Supports Python, JS, TS, Go signatures loosely.
        self.def_pattern = re.compile(
            r'^(\s*)(?:async\s+)?(?:class|def|function|func|var|const)\s+([a-zA-Z0-9_]+)', 
            re.MULTILINE
        )

    @service_endpoint(
    inputs={"file_path": "str", "max_chars": "int"},
    outputs={"chunks": "List[Dict]"},
    description="Reads a file and breaks it into logical blocks based on indentation.",
    tags=["parsing", "chunking"],
    side_effects=["filesystem:read"]
    )
    def chunk_file(self, file_path: str, max_chars: int = 1500) -> List[Dict[str, Any]]:
    """
    Reads a file and breaks it into logical blocks based on indentation.
    """
        path = Path(file_path)
        try:
            code = path.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return []

        return self._chunk_by_indentation(code, max_chars)

    def _chunk_by_indentation(self, code: str, max_chars: int) -> List[Dict]:
lines = code.splitlines()
        chunks = []
        
        current_chunk_lines = []
        current_start_line = 0
        current_indent = 0
        in_block = False
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # 1. Skip empty lines if we aren't in a block
            if not stripped and not in_block:
                continue

            # 2. Calculate Indentation
            indent_match = re.match(r'^(\s*)', line)
            indent_level = len(indent_match.group(1)) if indent_match else 0

            # 3. Check for Block Start (def/class at root level or low indent)
            # We allow indent < 4 spaces to catch top-level stuff or slight nesting
            match = self.def_pattern.match(line)
            is_def = match is not None and indent_level <= 4
            
            # IF we hit a new definition AND we have a chunk pending:
            if is_def and current_chunk_lines:
                # Save previous chunk
                self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                # Reset
                current_chunk_lines = []
                current_start_line = i + 1
                in_block = True
                current_indent = indent_level

            # IF we hit a line with LESS indentation than the current block start,
            # the block has ended. (Python/Yaml logic, mostly holds for C-style too if formatted)
            if in_block and stripped and indent_level <= current_indent and not is_def:
                # Special case: Closing braces '}' often have same indent as start
                if not stripped.startswith('}'):
                    self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                    current_chunk_lines = []
                    current_start_line = i + 1
                    in_block = False

            current_chunk_lines.append(line)

        # Flush remaining
        if current_chunk_lines:
            self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)

        return chunks

    def _finalize_chunk(self, chunks, lines, start_line, max_chars):
        """Recursively splits huge chunks if they exceed max_chars."""
        full_text = "\n".join(lines)
        if not full_text.strip(): return

        # If chunk is too big, split it by lines (naive fallback for massive functions)
        if len(full_text) > max_chars:
            self._split_large_block(chunks, lines, start_line, max_chars)
        else:
            chunks.append({
                "type": "block", # Generic type since we aren't parsing AST
                "text": full_text,
                "start_line": start_line,
                "end_line": start_line + len(lines)
            })

    def _split_large_block(self, chunks, lines, start_line, max_chars):
        """Force split a large block while keeping line boundaries."""
        current_sub = []
        current_len = 0
        sub_start = start_line
        
        for i, line in enumerate(lines):
            if current_len + len(line) > max_chars:
                if current_sub:
                    chunks.append({
                        "type": "fragment",
                        "text": "\n".join(current_sub),
                        "start_line": sub_start,
                        "end_line": sub_start + len(current_sub)
                    })
                current_sub = []
                current_len = 0
                sub_start = start_line + i
            
            current_sub.append(line)
            current_len += len(line)
            
        if current_sub:
            chunks.append({
                "type": "fragment",
                "text": "\n".join(current_sub),
                "start_line": sub_start,
                "end_line": sub_start + len(current_sub)
            })

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = CodeChunkerMS()
print("Service ready:", chunker)
    
# Test Python Code
    py_code = """
import os

def small_helper():
    return True

class DataProcessor:
    def __init__(self):
        self.data = []

    def process(self, raw_input):
        # This is a comment inside the function
        if raw_input:
            self.data.append(raw_input)
        return True
    """
    
    # Write temp file
    import tempfile
    with tempfile.NamedTemporaryFile(suffix=".py", mode="w+", delete=False) as tmp:
        tmp.write(py_code)
        tmp_path = tmp.name
        
    print(f"--- Chunking {tmp_path} (Pure Python) ---")
    chunks = chunker.chunk_file(tmp_path)
    
    for i, c in enumerate(chunks):
        print(f"\n[Chunk {i}] Lines {c['start_line']}-{c['end_line']}")
        print(f"{'-'*20}\n{c['text'].strip()}\n{'-'*20}")
        
    os.remove(tmp_path)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__CodeGrapherMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeGrapherMS
ENTRY_POINT: __CodeGrapherMS.py
DEPENDENCIES: None
"""

import ast
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="CodeGrapher",
version="1.0.0",
description="Parses Python code to extract symbols (nodes) and call relationships (edges).",
tags=["parsing", "graph", "analysis"],
capabilities=["filesystem:read"]
)
class CodeGrapherMS:
    """
    The Cartographer of Logic: Parses Python code to extract high-level 
    symbols (classes, functions) and maps their 'Call' relationships.
    
    Output: A graph structure (Nodes + Edges) suitable for visualization 
    or dependency analysis.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
    self.nodes = [] # List of symbols (functions, classes)
    self.edges = [] # List of relationships (source -> target)

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"graph_data": "Dict[str, Any]"},
    description="Recursively scans a directory for .py files and builds the graph.",
    tags=["parsing", "graph"],
    side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Dict[str, Any]:
    """
    Recursively scans a directory for .py files and builds the graph.
    """
        root = Path(root_path).resolve()
        self.nodes = []
        self.edges = []
        
        if not root.exists():
            return {"error": f"Path {root} does not exist"}

        # 1. Parsing Pass (Create Nodes)
        for path in root.rglob("*.py"):
            try:
                # Skip hidden/venv folders
                if any(p.startswith('.') for p in path.parts) or "venv" in path.parts:
                    continue
                    
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    source = f.read()
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_symbols = self._parse_source(source, rel_path)
                self.nodes.extend(file_symbols)
                
            except Exception as e:
                print(f"Failed to parse {path.name}: {e}")

        # 2. Linking Pass (Create Edges)
        self._build_edges()

        return {
            "root": str(root),
            "node_count": len(self.nodes),
            "edge_count": len(self.edges),
            "nodes": self.nodes,
            "edges": self.edges
        }

    def _parse_source(self, source: str, file_path: str) -> List[Dict]:
        """
        Uses Python's AST to extract surgical symbol info.
        """
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return []

        visitor = SurgicalVisitor(file_path)
        visitor.visit(tree)
        return visitor.symbols

    def _build_edges(self):
        """
        Resolves 'calls' strings into explicit graph edges.
        """
        # Create a quick lookup map: "my_function" -> NodeID
        # Note: This is a naive lookup (name collision possible). 
        # A robust version would use "module.class.method" fully qualified names.
        name_map = {n['name']: n['id'] for n in self.nodes}

        for node in self.nodes:
            source_id = node['id']
            calls = node.get('calls', [])
            
            for target_name in calls:
                if target_name in name_map:
                    target_id = name_map[target_name]
                    
                    # Avoid self-loops for cleanliness
                    if source_id != target_id:
                        self.edges.append({
                            "source": source_id,
                            "target": target_id,
                            "type": "calls"
                        })

# --- Helper Class: The AST Walker ---

class SurgicalVisitor(ast.NodeVisitor):
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.symbols = []

    def visit_FunctionDef(self, node):
        self._handle_func(node, "function")

    def visit_AsyncFunctionDef(self, node):
        self._handle_func(node, "async_function")

    def visit_ClassDef(self, node):
        # Record the class
        class_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": class_id,
            "file": self.file_path,
            "name": node.name,
            "type": "class",
            "line": node.lineno,
            "calls": [] # Classes don't 'call' things directly usually, their methods do
        })
        # Visit children (methods)
        self.generic_visit(node)

    def _handle_func(self, node, type_name):
        # Extract outgoing calls from the function body
        calls = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.append(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.append(child.func.attr)
        
        unique_calls = list(set(calls))
        
        node_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": node_id,
            "file": self.file_path,
            "name": node.name,
            "type": type_name,
            "line": node.lineno,
            "calls": unique_calls
        })

# --- Independent Test Block ---
if __name__ == "__main__":
    import sys
    
    # Defaults to current directory
    target_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    
    print(f"Mapping Logic in: {target_dir}")
    grapher = CodeGrapherMS()
    print("Service ready:", grapher)
    graph_data = grapher.scan_directory(target_dir)
    
    print(f"\n--- Scan Complete ---")
    print(f"Nodes Found: {graph_data['node_count']}")
    print(f"Edges Built: {graph_data['edge_count']}")
    
    # Save to JSON for inspection
    out_file = "code_graph_dump.json"
    with open(out_file, "w") as f:
        json.dump(graph_data, f, indent=2)
    print(f"Graph saved to {out_file}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__CognitiveMemoryMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CognitiveMemoryMS
ENTRY_POINT: __CognitiveMemoryMS.py
DEPENDENCIES: pip install pydantic
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pip install pydantic"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _CognitiveMemoryMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import uuid
import json
import logging
import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_MEMORY_FILE = Path("working_memory.jsonl")
FLUSH_THRESHOLD = 5  # Number of turns before summarizing to Long Term Memory
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("CognitiveMem")
# ==============================================================================

class CognitiveMemoryMS(BaseModel):
    """Atomic unit of memory."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)
    role: str # 'user', 'assistant', 'system', 'tool'
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

@service_metadata(
name="CognitiveMemory",
version="1.0.0",
description="Manages Short-Term (Working) Memory and orchestrates flushing to Long-Term Memory.",
tags=["memory", "history", "context"],
capabilities=["filesystem:read", "filesystem:write"]
)
class CognitiveMemoryMS:
    """
The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
flushing to Long-Term Memory (Vector Store).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.file_path = Path(self.config.get("persistence_path", DEFAULT_MEMORY_FILE))
self.summarizer = self.config.get("summarizer_func")
self.ingestor = self.config.get("long_term_ingest_func")
        
self.working_memory: List[CognitiveMemoryMS] = []
self._load_working_memory()

    # --- Working Memory Operations ---

    @service_endpoint(
    inputs={"role": "str", "content": "str", "metadata": "Dict"},
    outputs={"entry": "CognitiveMemoryMS"},
    description="Adds an item to working memory and persists it.",
    tags=["memory", "write"],
    side_effects=["filesystem:write"]
    )
    def add_entry(self, role: str, content: str, metadata: Dict = None) -> CognitiveMemoryMS:
    """Adds an item to working memory and persists it."""
        entry = CognitiveMemoryMS(role=role, content=content, metadata=metadata or {})
        self.working_memory.append(entry)
        self._append_to_file(entry)
        log.info(f"Added memory: [{role}] {content[:30]}...")
        return entry

    @service_endpoint(
    inputs={"limit": "int"},
    outputs={"context": "str"},
    description="Returns the most recent conversation history formatted for an LLM.",
    tags=["memory", "read", "llm"],
    side_effects=["filesystem:read"]
    )
    def get_context(self, limit: int = 10) -> str:
    """
    Returns the most recent conversation history formatted for an LLM.
    """
        recent = self.working_memory[-limit:]
        return "\n".join([f"{e.role.upper()}: {e.content}" for e in recent])

    def get_full_history(self) -> List[Dict]:
        """Returns the raw list of memory objects."""
        return [e.dict() for e in self.working_memory]

    # --- Consolidation (The "Sleep" Cycle) ---

    @service_endpoint(
    inputs={},
    outputs={},
    description="Signals that a turn is complete; checks if memory flush is needed.",
    tags=["memory", "maintenance"],
    side_effects=["filesystem:write"]
def commit_turn(self):
        """
        Signal that a "Turn" (User + AI response) is complete.
        Checks if memory is full and triggers a flush if needed.
        """
        if len(self.working_memory) >= FLUSH_THRESHOLD:
            self._flush_to_long_term()

    def _flush_to_long_term(self):
        """
        Compresses working memory into a summary and moves it to Long-Term storage.
        """
        if not self.summarizer or not self.ingestor:
            log.warning("Flush triggered but Summarizer/Ingestor not configured. Skipping.")
            return

        log.info("üåÄ Flushing Working Memory to Long-Term Storage...")
        
        # 1. Combine Text
        full_text = "\n".join([f"{e.role}: {e.content}" for e in self.working_memory])
        
        # 2. Summarize
        try:
            summary = self.summarizer(full_text)
            log.info(f"Summary generated: {summary[:50]}...")
        except Exception as e:
            log.error(f"Summarization failed: {e}")
            return

        # 3. Ingest into Vector DB
        try:
            meta = {
                "source": "cognitive_memory_flush", 
                "date": datetime.datetime.utcnow().isoformat(),
                "original_entry_count": len(self.working_memory)
            }
            self.ingestor(summary, meta)
            log.info("‚úÖ Saved to Long-Term Memory.")
        except Exception as e:
            log.error(f"Ingestion failed: {e}")
            return

        # 4. Clear Working Memory (but keep file history or archive it?)
        # For this pattern, we clear the 'Active' RAM, and maybe rotate the log file.
        self.working_memory.clear()
        self._rotate_log_file()

    # --- Persistence Helpers ---

    def _load_working_memory(self):
        """Rehydrates memory from the JSONL file."""
        if not self.file_path.exists():
            return
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.working_memory.append(CognitiveMemoryMS.parse_raw(line))
            log.info(f"Loaded {len(self.working_memory)} items from {self.file_path}")
        except Exception as e:
            log.error(f"Corrupt memory file: {e}")

    def _append_to_file(self, entry: CognitiveMemoryMS):
        """Appends a single entry to the JSONL log."""
        with open(self.file_path, 'a', encoding='utf-8') as f:
            f.write(entry.json() + "\n")

    def _rotate_log_file(self):
        """Renames the current log to an archive timestamp."""
        if self.file_path.exists():
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = self.file_path.with_name(f"memory_archive_{timestamp}.jsonl")
            self.file_path.rename(archive_name)
            log.info(f"Rotated memory log to {archive_name}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
# 1. Setup Mock Dependencies
def mock_summarizer(text):
    return f"SUMMARY OF {len(text)} CHARS: The user and AI discussed AI architecture."

def mock_ingest(text, metadata):
    print(f"\n[VectorDB] Indexing: '{text}'\n[VectorDB] Meta: {metadata}")

# 2. Initialize
print("--- Initializing Cognitive Memory ---")
mem = CognitiveMemoryMS({
    "summarizer_func": mock_summarizer,
    "long_term_ingest_func": mock_ingest
})
print("Service ready:", mem)

# 3. Simulate Conversation
print("\n--- Simulating Conversation ---")
mem.add_entry("user", "Hello, who are you?")
mem.add_entry("assistant", "I am a Cognitive Agent.")
mem.add_entry("user", "What is your memory capacity?")
mem.add_entry("assistant", "I have a tiered memory system.")
mem.add_entry("user", "That sounds complex.")

print(f"\nCurrent Context:\n{mem.get_context()}")

# 4. Trigger Flush (Threshold is 5)
print("\n--- Triggering Memory Flush ---")
mem.commit_turn() # Should trigger flush because count is 5

print(f"\nWorking Memory after flush: {len(mem.working_memory)} items")

# Cleanup
if Path("working_memory.jsonl").exists():
    os.remove("working_memory.jsonl")
# Clean up archives if any were made
for p in Path(".").glob("memory_archive_*.jsonl"):
    os.remove(p)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__HeuristicSumMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _HeuristicSumMS
ENTRY_POINT: __HeuristicSumMS.py
DEPENDENCIES: None
"""

import re
import os
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: REGEX PATTERNS
# ==============================================================================
# Captures: def my_func, class MyClass, function myFunc, interface MyInterface
SIG_RE = re.compile(r'^\s*(def|class|function|interface|struct|impl|func)\s+([A-Za-z_][A-Za-z0-9_]*)')

# Captures: # Heading, ## Subheading
MD_HDR_RE = re.compile(r'^\s{0,3}(#{1,3})\s+(.+)')

# Captures: """ Docstring """ or ''' Docstring ''' (Start of block)
DOC_RE = re.compile(r'^\s*("{3}|\'{3})(.*)', re.DOTALL)
# ==============================================================================

@service_metadata(
name="HeuristicSum",
version="1.0.0",
description="Generates quick summaries of code/text files using regex heuristics (No AI).",
tags=["parsing", "summary", "heuristics"],
capabilities=["compute"]
)
class HeuristicSumMS:
    """
The Skimmer: Generates quick summaries of code/text files without AI.
Scans for high-value lines (headers, signatures, docstrings) and concatenates them.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"text": "str", "filename": "str", "max_chars": "int"},
outputs={"summary": "str"},
description="Generates a summary string from the provided text.",
tags=["summary", "parsing"]
)
def summarize(self, text: str, filename: str = "", max_chars: int = 480) -> str:
        """
        Generates a summary string from the provided text.
        """
        lines = text.splitlines()
        picks = []

        # 1. Scan top 20 lines for Markdown Headers
        for ln in lines[:20]:
            m = MD_HDR_RE.match(ln)
            if m:
                picks.append(f"Heading: {m.group(2).strip()}")

        # 2. Scan top 40 lines for Code Signatures (Functions/Classes)
        for ln in lines[:40]:
            m = SIG_RE.match(ln)
            if m:
                picks.append(f"{m.group(1)} {m.group(2)}")

        # 3. Check for Docstrings / Preamble
        if lines:
            # Join first 80 lines to check for multi-line docstrings
            joined = "\n".join(lines[:80])
            m = DOC_RE.match(joined)
            if m:
                # Grab the first few lines of the docstring content
                after = joined.splitlines()[1:3]
                if after:
                    clean_doc = " ".join(s.strip() for s in after).strip()
                    picks.append(f"Doc: {clean_doc}")

        # 4. Fallback: First non-empty line if nothing else found
        if not picks:
            head = " ".join(l.strip() for l in lines[:2] if l.strip())
            if head:
                picks.append(head)

        # 5. Add Filename Context
        if filename:
            picks.append(f"[{os.path.basename(filename)}]")

        # 6. Deduplicate and Format
        seen = set()
        uniq = []
        for p in picks:
            if p and p not in seen:
                uniq.append(p)
                seen.add(p)

        summary = " | ".join(uniq)
        
        # 7. Truncate
        if len(summary) > max_chars:
            summary = summary[:max_chars-3] + "..."
            
        return summary.strip() if summary else "[No summary available]"

# --- Independent Test Block ---
if __name__ == "__main__":
skimmer = HeuristicSumMS()
print("Service ready:", skimmer)
    
# Test 1: Python Code
    py_code = """
    class DataProcessor:
        '''
        Handles the transformation of raw input data into structured formats.
        '''
        def process(self, data):
            pass
    """
    print(f"Python Summary: {skimmer.summarize(py_code, 'processor.py')}")

    # Test 2: Markdown
    md_text = """
    # Project Roadmap
    ## Phase 1
    We begin with ingestion.
    """
    print(f"Markdown Summary: {skimmer.summarize(md_text, 'README.md')}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__IngestEngineMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IngestEngineMS
ENTRY_POINT: __IngestEngineMS.py
DEPENDENCIES: requests
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["requests"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _IngestEngineMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import os
import time
import re
import sqlite3
import requests
import json
from typing import List, Generator, Dict, Any, Optional, Set
from dataclasses import dataclass
from microservice_std_lib import service_metadata, service_endpoint

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass IngestEngineMS IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

IngestEngineMS SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    Used to generate the 'DEPENDS_ON' edges in the Knowledge Graph.
    """
    def __init__(self):
        # Python: "from x import y", "import x"
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        # JS/TS: "import ... from 'x'", "require('x')"
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        ext = os.path.splitext(file_path)[1].lower()
        
        lines = content.split('\n')
        for line in lines:
            match = None
            if ext == '.py':
                match = self.py_pattern.match(line)
            elif ext in ['.js', '.ts', '.tsx', '.jsx']:
                match = self.js_pattern.search(line)
            
            if match:
                # Clean up the module name (e.g., "backend.database" -> "database")
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        
        return dependencies

@service_metadata(
name="IngestEngine",
version="1.0.0",
description="Reads files, chunks text, fetches embeddings, and weaves graph edges.",
tags=["ingest", "rag", "parsing", "embedding"],
capabilities=["filesystem:read", "network:outbound", "db:sqlite"]
)
IngestEngineMS IngestEngineMS:
    """
The Heavy Lifter: Reads files, chunks text, fetches embeddings,
populates the Graph Nodes, and weaves Graph Edges.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", "knowledge.db")
self.stop_signal = False
self.weaver = SynapseWeaver()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    @service_endpoint(
    inputs={"file_paths": "List[str]", "model_name": "str"},
    outputs={"status": "IngestStatus"},
    description="Processes a list of files, ingesting them into the knowledge graph.",
    tags=["ingest", "processing"],
    mode="generator",
    side_effects=["db:write", "network:outbound"]
    )
    def process_files(self, file_paths: List[str], model_name: str = "none") -> Generator[IngestStatus, None, None]:
    total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Optimization settings
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # Memory for graph weaving (Node Name -> Node ID)
        node_registry = {}
        file_contents = {} # Cache content for the weaving pass

        # --- PHASE 1: INGESTION (Files, Chunks, Nodes) ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)

            # 1. Read
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                file_contents[filename] = content # Cache for Phase 2
            except Exception as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"Error: {e}")
                continue

            # 2. Track File
            try:
                cursor.execute("INSERT OR REPLACE INTO files (path, last_updated) VALUES (?, ?)", 
                              (file_path, time.time()))
                file_id = cursor.lastrowid
            except sqlite3.Error:
                continue

            # 3. Create Graph Node (for Visualization)
            # We use the filename as the unique ID for the graph to make linking easier
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (filename, 'file', filename, json.dumps({"path": file_path})))
            
            node_registry[filename] = filename

            # 4. Chunking & Embedding
            chunks = self._chunk_text(content)
            
            for i, chunk_text in enumerate(chunks):
                if self.stop_signal: break
                
                embedding = None
                if model_name != "none":
                    embedding = self._get_embedding(model_name, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                    INSERT INTO chunks (file_id, chunk_index, content, embedding)
                    VALUES (?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": filename,
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            # Checkpoint per file
            conn.commit()

        # --- PHASE 2: WEAVING (Edges) ---
        yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
        
        edge_count = 0
        for filename, content in file_contents.items():
            if self.stop_signal: break
            
            # Find imports
            deps = self.weaver.extract_dependencies(content, filename)
            
            for dep in deps:
                # Naive matching: if 'database' is imported, look for 'database.py' or 'database.ts'
                # in our registry.
                target_id = None
                for potential_match in node_registry.keys():
                    if potential_match.startswith(dep + '.') or potential_match == dep:
                        target_id = potential_match
                        break
                
                if target_id and target_id != filename:
                    try:
                        cursor.execute("""
                            INSERT OR IGNORE INTO graph_edges (source, target, weight)
VALUES (?, ?, 1.0)
                        """, (filename, target_id))
                        edge_count += 1
                    except:
                        pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. Created {edge_count} dependency edges."
        )

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        if len(text) < chunk_size: return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += (chunk_size - overlap)
        return chunks

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": model, "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
# --- Independent Test Block ---
if __name__ == "__main__":
    TEST_DB = "test_ingest_v2.db"
    
    # Init DB Schema manually for test
    conn = sqlite3.connect(TEST_DB)
    conn.execute("CREATE TABLE IF NOT EXISTS files (id INTEGER PRIMARY KEY, path TEXT, last_updated REAL)")
    conn.execute("CREATE TABLE IF NOT EXISTS chunks (id INTEGER PRIMARY KEY, file_id INT, chunk_index INT, content TEXT, embedding BLOB)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)")
    conn.close()

    engine = IngestEngineMS({"db_path": TEST_DB})
    # Self-ingest to test dependency parsing
    files = ["_IngestEngineMS.py"] 
    
    print("Running Ingest V2...")
    for status in engine.process_files(files, "none"):
        print(f"[{status.progress_percent:.0f}%] {status.log_message}")
    
    # Verify Edges
    conn = sqlite3.connect(TEST_DB)
    edges = conn.execute("SELECT * FROM graph_edges").fetchall()
    nodes = conn.execute("SELECT * FROM graph_nodes").fetchall()
    print(f"\nResult: {len(nodes)} Nodes, {len(edges)} Edges.")
    conn.close()
    
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__IsoProcessMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IsoProcessMS
ENTRY_POINT: __IsoProcessMS.py
DEPENDENCIES: None
"""

import multiprocessing as mp
import logging
import logging.handlers
import time
import queue
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# WORKER LOGIC (Runs in Child Process)
# ==============================================================================
def _isolated_worker(result_queue: mp.Queue, log_queue: mp.Queue, payload: Any, config: Dict[str, Any]):
    """
    Entry point for the child process.
    Configures a logging handler to send records back to the parent.
    """
    # 1. Setup Logging Bridge
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    # Clear default handlers to avoid duplicate prints in child
    for h in root.handlers[:]:
        root.removeHandler(h)
    
    # Send all logs to the parent via the queue
    qh = logging.handlers.QueueHandler(log_queue)
    root.addHandler(qh)
    
    log = logging.getLogger("IsoWorker")

    try:
        log.info(f"Worker PID {mp.current_process().pid} started.")
        
        # --- 2. Heavy Imports (Simulated) ---
        log.info("Loading heavy libraries (Torch/Transformers)...")
        # from transformers import pipeline
        time.sleep(0.2) # Simulate import time

        # --- 3. The Logic ---
        model_name = config.get("model_name", "default-model")
        log.info(f"Initializing model '{model_name}'...")
        
        # Simulate processing steps with progress reporting
        for i in range(1, 4):
            time.sleep(0.3)
            log.info(f"Processing chunk {i}/3...")
        
        processed_data = f"Processed({payload}) via {model_name}"
        
        # --- 4. Return Result ---
        log.info("Work complete. Returning result.")
        result_queue.put({"success": True, "data": processed_data})

    except Exception as e:
        log.exception("Critical failure in worker process.")
        result_queue.put({"success": False, "error": str(e)})

# ==============================================================================
# PARENT CONTROLLER (Runs in Main Process)
# ==============================================================================
@service_metadata(
name="IsoProcess",
version="1.0.0",
description="Spawns isolated processes with real-time logging feedback.",
tags=["process", "isolation", "safety"],
capabilities=["process:spawn"]
)
class IsoProcessMS:
    """
The Safety Valve: Spawns isolated processes with real-time logging feedback.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.timeout = self.config.get("timeout_seconds", 60)
        
# Setup main logger
        self.log = logging.getLogger("IsoParent")
        if not self.log.handlers:
            logging.basicConfig(
                level=logging.INFO, 
                format='%(asctime)s [%(name)s] %(message)s',
                datefmt='%H:%M:%S'
            )

    @service_endpoint(
    inputs={"payload": "Any", "config": "Dict"},
    outputs={"result": "Any"},
    description="Executes a payload in an isolated child process.",
    tags=["process", "execution"],
    side_effects=["process:spawn"]
    )
def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        config = config or {}
        
        # 1. Setup Queues
        ctx = mp.get_context("spawn")
        result_queue = ctx.Queue()
        log_queue = ctx.Queue()

        # 2. Setup Log Listener (The "Ear" of the parent)
        # This thread pulls logs from the queue and handles them in the main process
        listener = logging.handlers.QueueListener(log_queue, *logging.getLogger().handlers)
        listener.start()

        # 3. Launch Process
        process = ctx.Process(
            target=_isolated_worker,
            args=(result_queue, log_queue, payload, config)
        )
        
        self.log.info("üöÄ Spawning isolated process...")
        process.start()
        
        try:
            # 4. Wait for Result
            result_packet = result_queue.get(timeout=self.timeout)
            process.join()

            if result_packet["success"]:
                return result_packet["data"]
            else:
                raise RuntimeError(f"Worker Error: {result_packet['error']}")

        except queue.Empty:
            self.log.error("‚è≥ Worker timed out! Terminating...")
            process.terminate()
            process.join()
            raise TimeoutError(f"Task exceeded {self.timeout}s limit.")
            
        finally:
            # Clean up the log listener so it doesn't hang
listener.stop()

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing IsoProcessMS with Live Logging ---")
    iso = IsoProcessMS({"timeout_seconds": 5})
    print("Service ready:", iso)
    
try:
        result = iso.execute("Sensitive Data", {"model_name": "DeepSeek-V3"})
        print(f"\n[Parent] Final Result: {result}")
    except Exception as e:
        print(f"\n[Parent] Failed: {e}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__LexicalSearchMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _LexicalSearchMS
ENTRY_POINT: __LexicalSearchMS.py
DEPENDENCIES: None
"""

import sqlite3
import json
import os
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="LexicalSearch",
version="1.0.0",
description="Lightweight BM25 keyword search using SQLite FTS5 (No AI required).",
tags=["search", "index", "sqlite"],
capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class LexicalSearchMS:
    """
The Librarian's Index: A lightweight, AI-free search engine.
    
Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
Ideal for environments where installing PyTorch/Transformers is impossible
or overkill.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
default_db = str(Path(__file__).parent / "lexical_index.db")
self.db_path = self.config.get("db_path", default_db)
self._init_db()

    def _init_db(self):
        """
        Sets up the schema. 
        Uses Triggers to automatically keep the FTS index in sync with the main table.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1. Main Content Table (Stores the actual data)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                metadata TEXT  -- JSON blob for extra info (path, author, etc)
            );
        """)
        
        # 2. Virtual FTS Table (The Search Index)
        # content='documents' means it references the table above (saves space)
        cur.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                content,
                content='documents',
                content_rowid='rowid'  -- Internal SQLite mapping
            );
        """)

        # 3. Triggers (The "Magic" - Auto-sync index on Insert/Delete/Update)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_au AFTER UPDATE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        
        conn.commit()
        conn.close()

    @service_endpoint(
    inputs={"doc_id": "str", "text": "str", "metadata": "Dict"},
    outputs={},
    description="Adds or updates a document in the FTS index.",
    tags=["search", "write"],
    side_effects=["db:write"]
    )
    def add_document(self, doc_id: str, text: str, metadata: Dict[str, Any] = None):
    """
    Adds or updates a document in the index.
    """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        meta_json = json.dumps(metadata or {})
        
        # Upsert logic (Replace if ID exists)
        cur.execute("""
            INSERT OR REPLACE INTO documents (id, content, metadata)
            VALUES (?, ?, ?)
        """, (doc_id, text, meta_json))
        
        conn.commit()
        conn.close()

    @service_endpoint(
    inputs={"query": "str", "top_k": "int"},
    outputs={"results": "List[Dict]"},
    description="Performs a BM25 ranked keyword search.",
    tags=["search", "read"],
    side_effects=["db:read"]
    )
    def search(self, query: str, top_k: int = 20) -> List[Dict]:
    """
    Performs a BM25 Ranked Search.
    """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row # Allows dict-like access
        cur = conn.cursor()
        
        try:
            # The SQL Magic: 'bm25(documents_fts)' calculates relevance score
            sql = """
                SELECT 
                    d.id, 
                    d.content, 
                    d.metadata,
                    snippet(documents_fts, 0, '<b>', '</b>', '...', 15) as preview,
                    bm25(documents_fts) as score
                FROM documents_fts 
                JOIN documents d ON d.rowid = documents_fts.rowid
                WHERE documents_fts MATCH ? 
                ORDER BY score ASC
                LIMIT ?
            """
            # FTS5 query syntax: quotes typically help with special chars
            safe_query = f'"{query}"'
            rows = cur.execute(sql, (safe_query, top_k)).fetchall()
            
            results = []
            for r in rows:
                results.append({
                    "id": r['id'],
                    "score": round(r['score'], 4),
                    "preview": r['preview'], # FTS5 auto-generates snippets!
                    "metadata": json.loads(r['metadata']),
                    "full_content": r['content']
                })
            
            return results
            
        except sqlite3.OperationalError as e:
            # Usually happens if query syntax is bad (e.g. unmatched quotes)
            print(f"Search syntax error: {e}")
            return []
        finally:
            conn.close()

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_name = "test_lexical.db"
    
    # 1. Init
    engine = LexicalSearchMS({"db_path": db_name})
    print("Service ready:", engine)
    
    # 2. Ingest Data
    print("Ingesting test data...")
    engine.add_document("doc1", "Python is a great language for data science.", {"category": "coding"})
    engine.add_document("doc2", "The snake python is a reptile found in jungles.", {"category": "biology"})
    engine.add_document("doc3", "Data science involves python, pandas, and SQL.", {"category": "coding"})
    
# 3. Search
query = "python data"
print(f"\nSearching for: '{query}'")
hits = engine.search(query)
    
for hit in hits:
    print(f"[{hit['score']:.4f}] {hit['id']} ({hit['metadata']['category']})")
    print(f"   Preview: {hit['preview']}")
        
# Cleanup
if os.path.exists(db_name):
    os.remove(db_name)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__LibrarianServiceMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _LibrarianServiceMS
ENTRY_POINT: __LibrarianServiceMS.py
DEPENDENCIES: None
"""

import os
import shutil
import sqlite3
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="LibrarianService",
version="1.0.0",
description="Manages the lifecycle (create, list, delete) of Knowledge Base files.",
tags=["kb", "management", "filesystem"],
capabilities=["filesystem:read", "filesystem:write", "db:sqlite"]
)
class LibrarianServiceMS:
    """
The Librarian: Manages the physical creation, deletion, and listing
of Knowledge Base (KB) files.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
storage_dir = self.config.get("storage_dir", "./cortex_dbs")
self.storage_dir = Path(storage_dir)
self.storage_dir.mkdir(parents=True, exist_ok=True)

    @service_endpoint(
    inputs={},
    outputs={"kbs": "List[str]"},
    description="Lists available Knowledge Base files.",
    tags=["kb", "read"],
    side_effects=["filesystem:read"]
    )
    def list_kbs(self) -> List[str]:
    """
    Scans the storage directory for .db files.
    Equivalent to api.listKBs() in Sidebar.tsx.
    """
        if not self.storage_dir.exists():
            return []
        
        # Return simple filenames sorted by modification time (newest first)
        files = list(self.storage_dir.glob("*.db"))
        files.sort(key=os.path.getmtime, reverse=True)
        return [f.name for f in files]

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"status": "Dict"},
    description="Creates a new Knowledge Base with the standard schema.",
    tags=["kb", "create"],
    side_effects=["filesystem:write", "db:write"]
    )
    def create_kb(self, name: str) -> Dict[str, str]:
    """
    Creates a new SQLite database and initializes the Cortex Schema.
    """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            raise FileExistsError(f"Knowledge Base '{safe_name}' already exists.")

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # --- THE CORTEX SCHEMA ---
            # 1. System Config: Stores version and global metadata
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS config (
                    key TEXT PRIMARY KEY,
                    value TEXT
                )
            """)
            
            # 2. Files: Tracks scanned files to avoid re-ingesting unchanged ones
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    path TEXT UNIQUE NOT NULL,
                    checksum TEXT,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'indexed'
                )
            """)
            
            # 3. Chunks: The actual atomic units of knowledge
            # Note: 'embedding' is stored as a BLOB (bytes) for raw vector data
cursor.execute("""
                CREATE TABLE IF NOT EXISTS chunks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_id INTEGER,
                    chunk_index INTEGER,
                    content TEXT,
                    embedding BLOB, 
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)
            
            # 4. Graph Nodes: For the GraphView visualization
            # Distinguishes between 'file' nodes and 'concept' nodes
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_nodes (
                    id TEXT PRIMARY KEY,
                    type TEXT,  -- 'file' or 'concept'
                    label TEXT,
                    data_json TEXT -- Flexible JSON for positions/colors
                )
            """)
            
            # 5. Graph Edges: The connections
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_edges (
                    source TEXT,
                    target TEXT,
                    weight REAL DEFAULT 1.0,
                    FOREIGN KEY(source) REFERENCES graph_nodes(id),
                    FOREIGN KEY(target) REFERENCES graph_nodes(id)
                )
            """)
            
            # Timestamp creation
            cursor.execute("INSERT INTO config (key, value) VALUES (?, ?)", 
                           ("created_at", str(time.time())))
            
            conn.commit()
            conn.close()
            return {"status": "success", "path": str(db_path), "name": safe_name}
            
        except Exception as e:
            # Cleanup on failure
            if db_path.exists():
                os.remove(db_path)
            raise e

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"success": "bool"},
    description="Deletes a Knowledge Base file.",
    tags=["kb", "delete"],
    side_effects=["filesystem:write"]
    )
    def delete_kb(self, name: str) -> bool:
    """
    Physically removes the database file.
    """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            os.remove(db_path)
            return True
        return False

    @service_endpoint(
    inputs={"source_name": "str"},
    outputs={"status": "Dict"},
    description="Creates a copy of an existing KB.",
    tags=["kb", "copy"],
    side_effects=["filesystem:write"]
    )
    def duplicate_kb(self, source_name: str) -> Dict[str, str]:
    """
    Creates a copy of an existing KB.
    """
        safe_source = self._sanitize_name(source_name)
        source_path = self.storage_dir / safe_source
        
if not source_path.exists():
            raise FileNotFoundError(f"Source KB '{safe_source}' not found.")

# Generate new name
base = safe_source.replace('.db', '')
new_name = f"{base}_copy.db"
dest_path = self.storage_dir / new_name

# Handle collision if copy already exists
counter = 1
while dest_path.exists():
    new_name = f"{base}_copy_{counter}.db"
    dest_path = self.storage_dir / new_name
    counter += 1

shutil.copy2(source_path, dest_path)
return {"status": "success", "name": new_name}

def _sanitize_name(self, name: str) -> str:
    """Ensures the filename ends in .db and has no illegal chars."""
    clean = "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).strip()
    clean = clean.replace(' ', '_')
    if not clean.endswith('.db'):
        clean += '.db'
    return clean

# --- Independent Test Block ---
if __name__ == "__main__":
print("Initializing Librarian Service...")
lib = LibrarianServiceMS({"storage_dir": "./test_brains"})
print("Service ready:", lib)
    
    # 1. Create
    print("Creating 'Project_Alpha'...")
    try:
        lib.create_kb("Project Alpha")
    except FileExistsError:
        print("Project Alpha already exists.")
        
    # 2. List
kbs = lib.list_kbs()
print(f"Available Brains: {kbs}")

# 3. Duplicate
if "Project_Alpha.db" in kbs:
    print("Duplicating Alpha...")
    lib.duplicate_kb("Project_Alpha.db")

# 4. Final List
print(f"Final Brains: {lib.list_kbs()}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__LogViewMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _LogViewMS
ENTRY_POINT: __LogViewMS.py
DEPENDENCIES: None
"""

import tkinter as tk
from tkinter import scrolledtext, filedialog
import queue
import logging
import datetime
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

class LogViewMS(logging.Handler):
    """Sends log records to a Tkinter-safe queue."""
def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.log_queue.put(record)

@service_metadata(
name="LogView",
version="1.0.0",
description="A thread-safe log viewer widget for Tkinter.",
tags=["ui", "logs", "widget"],
capabilities=["ui:gui", "filesystem:write"]
)
class LogViewMS(tk.Frame):
    """
The Console: A professional log viewer widget.
Features:
- Thread-safe (consumes from a Queue).
- Message Consolidation ("Error occurred (x5)").
- Level Filtering (Toggle INFO/DEBUG/ERROR).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    parent = self.config.get("parent")
    super().__init__(parent)
    self.log_queue = self.config.get("log_queue")

    # State for consolidation
    self.last_msg = None
    self.last_count = 0
    self.last_line_index = None

    self._build_ui()
    self._poll_queue()

def _build_ui(self):
    # Toolbar
    toolbar = tk.Frame(self, bg="#2d2d2d", height=30)
    toolbar.pack(fill="x", side="top")
# Filters
        self.filters = {
            "INFO": tk.BooleanVar(value=True),
            "DEBUG": tk.BooleanVar(value=True),
            "WARNING": tk.BooleanVar(value=True),
            "ERROR": tk.BooleanVar(value=True)
        }
        
        for level, var in self.filters.items():
            cb = tk.Checkbutton(
                toolbar, text=level, variable=var, 
                bg="#2d2d2d", fg="white", selectcolor="#444",
                activebackground="#2d2d2d", activeforeground="white"
            )
            cb.pack(side="left", padx=5)

        tk.Button(toolbar, text="Clear", command=self.clear, bg="#444", fg="white", relief="flat").pack(side="right", padx=5)
        tk.Button(toolbar, text="Save", command=self.save, bg="#444", fg="white", relief="flat").pack(side="right")

        # Text Area
        self.text = scrolledtext.ScrolledText(
            self, state="disabled", bg="#1e1e1e", fg="#d4d4d4", 
            font=("Consolas", 10), insertbackground="white"
        )
        self.text.pack(fill="both", expand=True)
        
        # Color Tags
        self.text.tag_config("INFO", foreground="#d4d4d4")
        self.text.tag_config("DEBUG", foreground="#569cd6")
        self.text.tag_config("WARNING", foreground="#ce9178")
        self.text.tag_config("ERROR", foreground="#f44747")
        self.text.tag_config("timestamp", foreground="#608b4e")

    def _poll_queue(self):
        """Pulls logs from the queue and updates UI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                self._display(record)
except queue.Empty:
            pass
        finally:
            self.after(100, self._poll_queue)

    def _display(self, record):
        level = record.levelname
        if not self.filters.get(level, tk.BooleanVar(value=True)).get():
            return

        msg = record.getMessage()
        ts = datetime.datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        
        self.text.config(state="normal")
        
        # Consolidation Logic
        if msg == self.last_msg:
            self.last_count += 1
            # Delete previous line content (keep timestamp)
            # This is complex in Tk text, simplified approach:
            # We just append (xN) if we can, otherwise standard print
            # For simplicity in this microservice, we will just append standard lines 
            # to ensure stability, or implement simple dedup:
            pass 
        else:
            self.last_msg = msg
            self.last_count = 1
        
        self.text.insert("end", f"[{ts}] ", "timestamp")
        self.text.insert("end", f"{msg}\n", level)
        self.text.see("end")
        self.text.config(state="disabled")

    @service_endpoint(
    inputs={},
    outputs={},
    description="Clears the log console.",
    tags=["ui", "logs"],
    side_effects=["ui:update"]
    )
    def clear(self):
    self.text.config(state="normal")
    self.text.delete("1.0", "end")
    self.text.config(state="disabled")

    @service_endpoint(
    inputs={},
    outputs={},
    description="Opens a dialog to save logs to a file.",
    tags=["ui", "filesystem"],
    side_effects=["filesystem:write", "ui:dialog"]
    )
    def save(self):
    path = filedialog.asksaveasfilename(defaultextension=".log", filetypes=[("Log Files", "*.log")])
        if path:
            try:
                with open(path, "w", encoding="utf-8") as f:
                    f.write(self.text.get("1.0", "end"))
            except Exception as e:
                print(f"Save failed: {e}")

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Log View Test")
    root.geometry("600x400")
    
    # 1. Setup Queue
    q = queue.Queue()
    
    # 2. Setup Logger
    logger = logging.getLogger("TestApp")
    logger.setLevel(logging.DEBUG)
    logger.addHandler(LogViewMS(q))
    
    # 3. Mount View
    log_view = LogViewMS({"parent": root, "log_queue": q})
    print("Service ready:", log_view)
    log_view.pack(fill="both", expand=True)
    
    # 4. Generate Logs
    def generate_noise():
        logger.info("System initializing...")
        logger.debug("Checking sensors...")
        logger.warning("Sensor 4 response slow.")
        logger.error("Connection failed!")
        root.after(2000, generate_noise)
        
    generate_noise()
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__MonacoHostMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _MonacoHostMS
ENTRY_POINT: __MonacoHostMS.py
DEPENDENCIES: pywebview
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pywebview"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _MonacoHostMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import webview
import threading
import json
from pathlib import Path
from typing import Any, Dict, Optional, Callable
from microservice_std_lib import service_metadata, service_endpoint

# --- EMBEDDED MONACO HTML ---
MONACO_HTML = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Monaco Host</title>
    <style>
        html, body { margin: 0; padding: 0; width: 100%; height: 100%; overflow: hidden; background-color: #1e1e1e; font-family: sans-serif; }
        #container { display: flex; flex-direction: column; height: 100%; }
        #tabs { background: #252526; display: flex; overflow-x: auto; height: 35px; border-bottom: 1px solid #3e3e3e; }
        .tab { 
            padding: 8px 15px; color: #969696; background: #2d2d2d; cursor: pointer; border-right: 1px solid #1e1e1e; font-size: 12px;
            display: flex; align-items: center; white-space: nowrap;
        }
        .tab.active { background: #1e1e1e; color: #fff; border-top: 1px solid #007acc; }
        .tab:hover { background: #323233; color: #fff; }
        #editor { flex-grow: 1; }
    </style>
</head>
<body>
    <div id="container">
        <div id="tabs"></div>
        <div id="editor"></div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs/loader.js"></script>
    <script>
        require.config({ paths: { 'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs' }});
        let editor;
        let models = {}; 
        let currentPath = null;

        require(['vs/editor/editor.main'], function() {
            editor = monaco.editor.create(document.getElementById('editor'), {
                value: "# Monaco Editor Ready\\n",
                language: 'python',
                theme: 'vs-dark',
                automaticLayout: true,
                fontSize: 14
            });

            if (window.pywebview) window.pywebview.api.signal_editor_ready();

            editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.KeyS, function() {
                if (currentPath) {
                    window.pywebview.api.save_file(currentPath, editor.getValue());
                }
            });
        });

        window.pywebview = window.pywebview || {};
        window.pywebview.api = window.pywebview.api || {};

        window.pywebview.api.open_in_tab = function(filepath, content) {
            let ext = filepath.split('.').pop();
            let langMap = { 'py': 'python', 'js': 'javascript', 'html': 'html', 'json': 'json', 'css': 'css' };
            let lang = langMap[ext] || 'plaintext';

            if (!models[filepath]) {
                models[filepath] = monaco.editor.createModel(content, lang, monaco.Uri.file(filepath));
                const tab = document.createElement('div');
                tab.className = 'tab';
                tab.innerText = filepath.split(/[\\\\/]/).pop();
                tab.title = filepath;
                tab.onclick = () => switchTo(filepath);
                tab.dataset.path = filepath;
                document.getElementById('tabs').appendChild(tab);
            }
            switchTo(filepath);
        };

        window.pywebview.api.reveal_range = function(filepath, startLine, endLine) {
            if (filepath !== currentPath) switchTo(filepath);
            editor.revealLineInCenter(startLine);
            editor.setSelection({ startLineNumber: startLine, startColumn: 1, endLineNumber: endLine, endColumn: 1000 });
        };

        function switchTo(filepath) {
            if (!models[filepath]) return;
            editor.setModel(models[filepath]);
            currentPath = filepath;
            document.querySelectorAll('.tab').forEach(t => t.classList.toggle('active', t.dataset.path === filepath));
        }
    </script>
</body>
</html>
"""

class MonacoHostMS:
    def __init__(self):
        self._window = None
        self._ready_event = threading.Event()
        self.on_save_callback: Optional[Callable[[str, str], None]] = None

    def set_window(self, window):
        self._window = window

    def signal_editor_ready(self):
        self._ready_event.set()
        print("Monaco Editor is ready.")

    def save_file(self, filepath: str, content: str):
        if self.on_save_callback:
            self.on_save_callback(filepath, content)
        else:
            print(f"Saved {filepath} (No callback registered)")

    def open_file(self, filepath: str, content: str):
        self._ready_event.wait(timeout=10)
        if not self._window: return
        # Using json.dumps for both handles escaping perfectly
        js = f"window.pywebview.api.open_in_tab({json.dumps(filepath)}, {json.dumps(content)})"
        self._window.evaluate_js(js)

@service_metadata(
    name="MonacoHost",
    version="1.1.0",
    description="Hosts an embedded Monaco Editor instance.",
    tags=["ui", "editor", "webview"],
    capabilities=["ui:gui"]
)
class MonacoHostMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.api = MonacoHostMS()
        self.window = None

    @service_endpoint(mode="sync")
    def launch(self, title="Monaco Editor", width=1000, height=700, func=None):
        self.window = webview.create_window(
            title, 
            html=MONACO_HTML, # Pass the string directly here
            js_api=self.api,
            width=width, 
            height=height
        )
        self.api.set_window(self.window)
        webview.start(func, debug=True) if func else webview.start(debug=True)

if __name__ == "__main__":
    host = MonacoHostMS()
    
    def background_actions():
        host.api._ready_event.wait()
        host.api.open_file("demo.py", "print('Hello World')\\n# Try Ctrl+S")
        host.api.on_save_callback = lambda p, c: print(f"File: {p} was saved with {len(c)} chars.")

    host.launch(func=background_actions)
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__NetworkLayoutMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NetworkLayoutMS
ENTRY_POINT: __NetworkLayoutMS.py
DEPENDENCIES: pip install networkx
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pip install networkx"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _NetworkLayoutMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import networkx as nx
import logging
from typing import List, Dict, Any, Tuple, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("NetLayout")
# ==============================================================================

@service_metadata(
name="NetworkLayout",
version="1.0.0",
description="Calculates visual (x,y) coordinates for graph nodes using NetworkX.",
tags=["graph", "layout", "visualization"],
capabilities=["compute"]
)
class NetworkLayoutMS:
    """
The Topologist: Calculates visual coordinates for graph nodes using
server-side algorithms (NetworkX). 
Useful for generating static map snapshots or pre-calculating positions 
to offload client-side rendering.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"nodes": "List[str]", "edges": "List[Tuple]", "algorithm": "str"},
    outputs={"positions": "Dict[str, Tuple]"},
    description="Computes (x, y) coordinates for the given graph nodes and edges.",
    tags=["graph", "compute"],
    side_effects=[]
    )
    def calculate_layout(self, nodes: List[str], edges: List[Tuple[str, str]], 
    algorithm: str = "spring", **kwargs) -> Dict[str, Tuple[float, float]]:
    """
    Computes (x, y) coordinates for the given graph.
        
    :param nodes: List of node IDs.
        :param edges: List of (source, target) tuples.
        :param algorithm: 'spring' (Force-directed) or 'circular'.
        :return: Dictionary {node_id: (x, y)}
        """
        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        G.add_edges_from(edges)
        
        log.info(f"Computing layout for {len(nodes)} nodes, {len(edges)} edges...")
        
        try:
            if algorithm == "circular":
                pos = nx.circular_layout(G)
            else:
                # Spring layout (Fruchterman-Reingold) is standard for knowledge graphs
                k_val = kwargs.get('k', 0.15) # Optimal distance between nodes
                iter_val = kwargs.get('iterations', 50)
                pos = nx.spring_layout(G, k=k_val, iterations=iter_val, seed=42)
                
            # Convert numpy arrays to simple lists/tuples for JSON serialization
            return {n: (float(p[0]), float(p[1])) for n, p in pos.items()}
            
        except Exception as e:
            log.error(f"Layout calculation failed: {e}")
            return {}

# --- Independent Test Block ---
if __name__ == "__main__":
layout = NetworkLayoutMS()
print("Service ready:", layout)
    
# 1. Define a simple graph
    test_nodes = ["Main", "Utils", "Config", "DB", "Auth"]
    test_edges = [
        ("Main", "Utils"),
        ("Main", "Config"),
        ("Main", "DB"),
        ("Main", "Auth"),
        ("DB", "Config"),
        ("Auth", "DB")
    ]
    
    # 2. Compute Layout
    positions = layout.calculate_layout(test_nodes, test_edges, k=0.5)
    
    print("--- Calculated Positions ---")
    for node, (x, y) in positions.items():
        print(f"{node:<10}: ({x: .4f}, {y: .4f})")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__NeuralServiceMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NeuralServiceMS
ENTRY_POINT: __NeuralServiceMS.py
DEPENDENCIES: None
"""

import requests
import json
import concurrent.futures
from typing import Optional, Dict, Any, List
from base_service import BaseService
from microservice_std_lib import service_metadata, service_endpoint

# Configuration constants
OLLAMA_API_URL = "http://localhost:11434/api"

@service_metadata(
    name="NeuralServiceMS",
    version="1.0.0",
    description="The Brain Interface: Orchestrates local AI operations via Ollama for inference and embeddings.",
    tags=["ai", "neural", "inference", "ollama"],
    capabilities=["text-generation", "embeddings", "parallel-processing"]
)
class NeuralServiceMS(BaseService):
    def __init__(self, max_workers: int = 4):
        super().__init__("NeuralServiceMS")
        self.max_workers = max_workers
        # Default configs
        self.config = {
            "fast": "qwen2.5-coder:1.5b-cpu",
            "smart": "qwen2.5:3b-cpu",
            "embed": "mxbai-embed-large:latest-cpu"
        }

    def update_models(self, fast_model: str, smart_model: str, embed_model: str):
        """Called by the UI Settings Modal to change models on the fly."""
        self.config["fast"] = fast_model
        self.config["smart"] = smart_model
        self.config["embed"] = embed_model
        self.log_info(f"Models Updated: Fast={fast_model}, Smart={smart_model}")

    def get_available_models(self) -> List[str]:
        """Fetches list from Ollama for the UI dropdown."""
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            if res.status_code == 200:
                return [m['name'] for m in res.json().get('models', [])]
        except:
            return []
        return []

    def check_connection(self) -> bool:
        """Pings Ollama to see if it's alive."""
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except requests.RequestException:
            self.log_error("Ollama connection failed. Is 'ollama serve' running?")
            return False

    @service_endpoint(
        inputs={"text": "str"},
        outputs={"embedding": "list"},
        description="Generates a high-dimensional vector embedding for the provided text using the configured model.",
        tags=["nlp", "vector"]
    )
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Generates a vector using the CPU embedder."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.config["embed"], "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            self.log_error(f"Embedding failed: {e}")
        return None

    @service_endpoint(
        inputs={"prompt": "str", "tier": "str", "format_json": "bool"},
        outputs={"response": "str"},
        description="Requests a synchronous text generation/inference from a local LLM tier.",
        tags=["llm", "inference"]
    )
    def request_inference(self, prompt: str, tier: str = "fast", format_json: bool = False) -> str:
        """
        Synchronous inference request.
        tier: 'fast' (1.5b-cpu), 'smart' (3b-cpu), or 'architect' (7b-gpu)
        """
        model = self.config.get(tier, self.config["fast"])
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        if format_json:
            payload["format"] = "json"

        try:
            res = requests.post(f"{OLLAMA_API_URL}/generate", json=payload, timeout=60)
            if res.status_code == 200:
                return res.json().get("response", "").strip()
        except Exception as e:
            self.log_error(f"Inference ({tier}) failed: {e}")
        return ""

    def process_parallel(self, items: List[Any], worker_func) -> List[Any]:
        """
        Helper to run a function across many items using the ThreadPool.
        Useful for batch ingestion.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # worker_func should take a single item and return a result
            futures = {executor.submit(worker_func, item): item for item in items}
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    self.log_error(f"Worker task failed: {e}")
                            return results

                    if __name__ == "__main__":
                        svc = NeuralServiceMS()
                        print("Service ready:", svc._service_info["name"])
                        if svc.check_connection():
                            print("Ollama Connection: OK")
                        else:
                            print("Ollama Connection: FAILED")



--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__PromptOptimizerMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _PromptOptimizerMS
ENTRY_POINT: __PromptOptimizerMS.py
DEPENDENCIES: None
"""

import json
import logging
from typing import List, Dict, Any, Callable, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: META-PROMPTS
# ==============================================================================
# The system prompt used to turn the LLM into a Prompt Engineer
REFINE_SYSTEM_PROMPT = (
    "You are a world-class prompt engineer. "
    "Given an original prompt and specific feedback, "
    "provide an improved, refined version of the prompt that incorporates the feedback. "
    "Return ONLY the refined prompt text, no preamble."
)

# The system prompt used to generate A/B test variations
VARIATION_SYSTEM_PROMPT = (
    "You are a creative AI assistant. "
    "Generate {num} innovative and diverse variations of the following prompt. "
    "Return the result as a valid JSON array of strings. "
    "Example: [\"variation 1\", \"variation 2\"]"
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptOpt")
# ==============================================================================

@service_metadata(
name="PromptOptimizer",
version="1.0.0",
description="Uses an LLM to refine prompts or generate variations.",
tags=["llm", "prompt-engineering", "optimization"],
capabilities=["network:outbound"]
)
class PromptOptimizerMS:
    """
The Tuner: Uses an LLM to refine prompts or generate variations.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.infer = self.config.get("inference_func")

    @service_endpoint(
    inputs={"draft_prompt": "str", "feedback": "str"},
    outputs={"refined_prompt": "str"},
    description="Rewrites a prompt based on feedback.",
    tags=["llm", "refine"],
    side_effects=["network:outbound"]
    )
def refine_prompt(self, draft_prompt: str, feedback: str) -> str:
    """
    Rewrites a prompt based on feedback.
    """
        full_prompt = (
            f"{REFINE_SYSTEM_PROMPT}\n\n"
            f"[Original Prompt]:\n{draft_prompt}\n\n"
            f"[Feedback]:\n{feedback}\n\n"
            f"[Refined Prompt]:"
        )
        
        log.info("Refining prompt...")
        try:
            result = self.infer(full_prompt)
            return result.strip()
        except Exception as e:
            log.error(f"Refinement failed: {e}")
            return draft_prompt # Fallback to original

@service_endpoint(
    inputs={"draft_prompt": "str", "num_variations": "int", "context_data": "Dict"},
    outputs={"variations": "List[str]"},
    description="Generates multiple versions of a prompt for testing.",
    tags=["llm", "variations"],
    side_effects=["network:outbound"]
)
def generate_variations(self, draft_prompt: str, num_variations: int = 3, context_data: Optional[Dict] = None) -> List[str]:
    """
    Generates multiple versions of a prompt for testing.
    """
        meta_prompt = VARIATION_SYSTEM_PROMPT.format(num=num_variations)
        
        prompt_content = draft_prompt
        if context_data:
            prompt_content += f"\n\n--- Context ---\n{json.dumps(context_data, indent=2)}"

        full_prompt = (
            f"{meta_prompt}\n\n"
            f"[Original Prompt]:\n{prompt_content}\n\n"
            f"[JSON Array of Variations]:"
        )

        log.info(f"Generating {num_variations} variations...")
        try:
            # We explicitly ask for JSON, but LLMs are chatty, so we might need cleaning logic here
            raw_response = self.infer(full_prompt)
            
            # Simple cleanup to find the JSON array if the LLM added text around it
            start = raw_response.find('[')
            end = raw_response.rfind(']') + 1
            if start == -1 or end == 0:
                raise ValueError("No JSON array found in response")
                
            clean_json = raw_response[start:end]
            variations = json.loads(clean_json)
            
            if isinstance(variations, list):
                return [str(v) for v in variations]
            return []
            
        except Exception as e:
            log.error(f"Variation generation failed: {e}")
            return []

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Mock Inference Engine (Simulating an LLM)
    def mock_llm(prompt: str) -> str:
        if "[Refined Prompt]" in prompt:
            return "You are a helpful assistant who speaks like a pirate. How may I help ye?"
        if "[JSON Array]" in prompt:
            return '["Variation A: Pirate Mode", "Variation B: Formal Mode", "Variation C: Concise Mode"]'
        return "Error"

    optimizer = PromptOptimizerMS({"inference_func": mock_llm})
    print("Service ready:", optimizer)

    # 2. Test Refine
    print("--- Test: Refine ---")
    draft = "Help me."
feedback = "Make it sound like a pirate."
    refined = optimizer.refine_prompt(draft, feedback)
    print(f"Original: {draft}")
    print(f"Refined:  {refined}")

    # 3. Test Variations
    print("\n--- Test: Variations ---")
    vars = optimizer.generate_variations(draft, num_variations=3)
    for i, v in enumerate(vars):
        print(f" {i+1}. {v}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__PromptVaultMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _PromptVaultMS
ENTRY_POINT: __PromptVaultMS.py
DEPENDENCIES: pydantic, jinja2
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pydantic", "jinja2"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _PromptVaultMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Callable
from pydantic import BaseModel, Field, ValidationError
from jinja2 import Environment, BaseLoader

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "prompt_vault.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptVault")
# ==============================================================================

# --- Data Models ---

class PromptVaultMS(BaseModel):
    """A specific historical version of a prompt."""
    version_num: int
    content: str
    author: str
    timestamp: datetime.datetime
    embedding: Optional[List[float]] = None

class PromptTemplate(BaseModel):
    """The master record for a prompt."""
    id: str
    slug: str
    title: str
    description: Optional[str] = ""
    tags: List[str] = []
    latest_version_num: int
    versions: List[PromptVaultMS] = []
    
    @property
    def latest(self) -> PromptVaultMS:
        """Helper to get the most recent content."""
        if not self.versions:
            raise ValueError("No versions found.")
        # versions are stored sorted by DB insertion usually, but let's be safe
        return sorted(self.versions, key=lambda v: v.version_num)[-1]

# --- Database Management ---

class PromptVaultMS:
    """
    The Vault: A persistent SQLite store for managing, versioning, 
    and rendering AI prompts.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()
        self.jinja_env = Environment(loader=BaseLoader())

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        """Bootstraps the schema."""
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    slug TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    tags_json TEXT,
                    latest_version INTEGER DEFAULT 1,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS versions (
                    id TEXT PRIMARY KEY,
                    template_id TEXT,
                    version_num INTEGER,
                    content TEXT,
                    author TEXT,
                    timestamp TIMESTAMP,
                    embedding_json TEXT,
                    FOREIGN KEY(template_id) REFERENCES templates(id)
                )
            """)
# --- CRUD Operations ---

    def create_template(self, slug: str, title: str, content: str, author: str = "system", tags: List[str] = None) -> PromptTemplate:
        """Creates a new prompt template with an initial version 1."""
        tags = tags or []
        now = datetime.datetime.utcnow()
        t_id = str(uuid.uuid4())
        v_id = str(uuid.uuid4())

        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO templates (id, slug, title, description, tags_json, latest_version, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                    (t_id, slug, title, "", json.dumps(tags), 1, now, now)
                )
                conn.execute(
                    "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                    (v_id, t_id, 1, content, author, now)
                )
            log.info(f"Created template: {slug}")
            return self.get_template(slug)
        except sqlite3.IntegrityError:
            raise ValueError(f"Template '{slug}' already exists.")

    def add_version(self, slug: str, content: str, author: str = "user") -> PromptTemplate:
        """Adds a new version to an existing template."""
        current = self.get_template(slug)
        if not current:
            raise ValueError(f"Template '{slug}' not found.")

        new_ver = current.latest_version_num + 1
        now = datetime.datetime.utcnow()
        v_id = str(uuid.uuid4())

        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                (v_id, current.id, new_ver, content, author, now)
            )
conn.execute(
                "UPDATE templates SET latest_version = ?, updated_at = ? WHERE id = ?",
                (new_ver, now, current.id)
            )
        log.info(f"Updated {slug} to v{new_ver}")
        return self.get_template(slug)

    def get_template(self, slug: str) -> Optional[PromptTemplate]:
        """Retrieves a full template with all history."""
        with self._get_conn() as conn:
            # 1. Fetch Template
            row = conn.execute("SELECT * FROM templates WHERE slug = ?", (slug,)).fetchone()
            if not row: return None

            # 2. Fetch Versions
            v_rows = conn.execute("SELECT * FROM versions WHERE template_id = ? ORDER BY version_num ASC", (row['id'],)).fetchall()

            versions = []
            for v in v_rows:
                versions.append(PromptVaultMS(
                    version_num=v['version_num'],
                    content=v['content'],
                    author=v['author'],
                    timestamp=v['timestamp']
                    # embedding logic skipped for brevity
                ))

            return PromptTemplate(
                id=row['id'],
                slug=row['slug'],
                title=row['title'],
                description=row['description'],
                tags=json.loads(row['tags_json']),
                latest_version_num=row['latest_version'],
                versions=versions
            )

    def render(self, slug: str, context: Dict[str, Any] = None) -> str:
        """Fetches the latest version and renders it with Jinja2."""
        template = self.get_template(slug)
if not template:
            raise ValueError(f"Template '{slug}' not found.")
        
        raw_text = template.latest.content
        jinja_template = self.jinja_env.from_string(raw_text)
        return jinja_template.render(**(context or {}))

    def list_slugs(self) -> List[str]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT slug FROM templates").fetchall()
            return [r[0] for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup
    if DB_PATH.exists(): os.remove(DB_PATH)
    vault = PromptVaultMS()
    
    # 2. Create
    print("--- Creating Prompt ---")
    vault.create_template(
        slug="greet_user",
        title="Greeting Protocol",
        content="Hello {{ name }}, welcome to the {{ system_name }}!",
        tags=["ui", "onboarding"]
    )
    
    # 3. Versioning
    print("--- Updating Prompt ---")
    vault.add_version("greet_user", "Greetings, {{ name }}. System {{ system_name }} is online.")
    
    # 4. Retrieval & Rendering
    print("--- Rendering ---")
    final_text = vault.render("greet_user", {"name": "Alice", "system_name": "Nexus"})
    print(f"Rendered Output: {final_text}")
    
    # 5. Inspection
    tpl = vault.get_template("greet_user")
print(f"Current Version: v{tpl.latest_version_num}")
print(f"History: {[v.content for v in tpl.versions]}")
# Cleanup
if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__PythonChunkerMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _PythonChunkerMS
ENTRY_POINT: __PythonChunkerMS.py
DEPENDENCIES: None
"""

import ast
import time
from dataclasses import dataclass
from typing import List, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

@dataclass PythonChunkerMS CodeChunk:
    name: str          # e.g., "PythonChunkerMS AuthMS"
    type: str          # "PythonChunkerMS", "function", "text"
    content: str       # The raw source
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

@service_metadata(
    name="PythonChunkerMS",
    version="1.2.0",
    description="The Python Surgeon: Specialist in Abstract Syntax Tree (AST) parsing for Python source code.",
    tags=["chunking", "python", "ast"],
    capabilities=["python-ast"]
)
PythonChunkerMS PythonChunkerMS:
    """
    Specialized Python AST Chunker.
    Focuses exclusively on identifying classes and functions to preserve code logic.
    """
    
    def __init__(self):
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "specialty": "str"},
        description="Standardized health check for the Python specialist service.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the PythonChunkerMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "specialty": "python_ast"
        }

    @service_endpoint(
        inputs={"content": "str"},
        outputs={"chunks": "list"},
        description="Primary entry point for high-fidelity Python-specific AST chunking.",
        tags=["processing", "python"]
    )
    def chunk(self, content: str) -> List[CodeChunk]:
        """Parses Python source into semantic CodeChunks."""
        return self._chunk_python(content)

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                end = node.end_lineno if hasattr(node, "end_lineno") else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", type="function", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"PythonChunkerMS {node.name}", type="PythonChunkerMS", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))

            # Fallback: Return as single chunk if no structures found (e.g., flat script)
            if not chunks:
                return [CodeChunk(
                    name="module_level", 
                    type="text", 
                    content=source, 
                    start_line=1, 
                    end_line=len(lines)
                )]
                
        except SyntaxError:
            # If the code is unparseable, return the whole block as a raw chunk
            return [CodeChunk(
                name="syntax_error_fallback", 
                type="text", 
                content=source, 
                start_line=1, 
                end_line=source.count('\n') + 1
            )]
            
        return chunks

if __name__ == "__main__":
    svc = PythonChunkerMS()
    print("Service ready:", svc._service_info["name"])
    # Test on a Python snippet
    test_code = "PythonChunkerMS Test:\n    def run(self):\n        pass"
    results = svc.chunk(test_code)
    for c in results:
        print(f"[{c.type}] {c.name} (Lines {c.start_line}-{c.end_line})")
--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__RefineryServiceMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _RefineryServiceMS
ENTRY_POINT: __RefineryServiceMS.py
DEPENDENCIES: None
"""

import json
import re
import os
import time
import ast
import concurrent.futures
from typing import Dict, List, Any, Optional, Tuple
from base_service import BaseService
from __CartridgeServiceMS import CartridgeService
from __NeuralServiceMS import NeuralService
from __ChunkingRouterMS import ChunkingRouterMS
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="RefineryServiceMS",
    version="1.1.0",
    description="The Night Shift: Processes 'RAW' files into semantic chunks and weaves them into a knowledge graph.",
    tags=["processing", "refinery", "graph", "RAG"],
    capabilities=["smart-chunking", "graph-weaving", "parallel-embedding"]
)
class RefineryServiceMS(BaseService):
    """
    The Night Shift.
    Polls the DB for 'RAW' files and processes them into Chunks and Graph Nodes.

    Graph Enrichment:
    - Code: function/class nodes, resolved import edges when possible.
    - Docs: section/chapter nodes for long-form text (md/txt/rst).
    """

    def __init__(self, cartridge: CartridgeService, neural: NeuralService):
        super().__init__("RefineryServiceMS")
        self.cartridge = cartridge
        self.neural = neural
        self.chunker = ChunkingRouterMS()
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "cartridge_health": "str"},
        description="Standardized health check to verify the operational state of the Refinery service.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the RefineryServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "cartridge_health": self.cartridge.get_status_flags().get("cartridge_health", "UNKNOWN")
        }

        # --- Spec Enforcement ---
        # Update the cartridge manifest to reflect the ACTUAL tools we are using.
        self._stamp_specs()

        # Import parsing / resolution
        # Regex remains as a fallback (JS + non-parseable cases)
        self.import_pattern = re.compile(r"""(?:from|import)\s+([\w\.]+)|require\(['"]([\w\.\-/]+)['"]\)""")

        # Lightweight module/path index cache for resolving imports to VFS files
        self._module_index: Dict[str, str] = {}
        self._path_index: Dict[str, str] = {}
        self._index_built: bool = False

        # Simple section/chapter detection
        self._md_heading = re.compile(r"^(#{1,6})\s+(.+?)\s*$")
        self._chapter_heading = re.compile(r"^\s*(chapter|CHAPTER)\s+([0-9]+|[IVXLC]+)\b\s*[:\-]?\s*(.*)$")

    def _stamp_specs(self):
        """Writes the active Neural/Chunker configuration to the Manifest."""
        try:
            # 1. Embedding Spec
            # We assume 1024 dim for mxbai-large, but ideally we'd probe it.
            embed_model = self.neural.config["embed"]
            spec = {
                "provider": "ollama",
                "model": embed_model,
                "dim": 1024,  # Hardcoded for now based on mxbai-embed-large
                "dtype": "float32",
                "distance": "cosine"
            }
            self.cartridge.set_manifest("embedding_spec", spec)

            # 2. Chunking Spec
            # (We could expose chunker config params here if they were dynamic)

        except Exception as e:
            self.log_error(f"Failed to stamp specs: {e}")

    def _build_import_index(self):
        """Builds caches for resolving imports to VFS targets.

        - _path_index: exact VFS path -> VFS path
        - _module_index: python module name (a.b.c) -> VFS path (a/b/c.py, a/b/c/__init__.py)
        """
        if self._index_built:
            return

        path_index: Dict[str, str] = {}
        module_index: Dict[str, str] = {}

        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            for (vp,) in rows:
                if not vp:
                    continue
                vfs_path = str(vp).replace("\\", "/")
                path_index[vfs_path] = vfs_path

                # Python module mapping
                if vfs_path.endswith(".py"):
                    mod = vfs_path[:-3].strip("/")
                    mod = mod.replace("/", ".")
                    if mod:
                        module_index[mod] = vfs_path

                    # If it's a package __init__.py, map the package name too
                    if vfs_path.endswith("/__init__.py"):
                        pkg = vfs_path[:-len("/__init__.py")].strip("/").replace("/", ".")
                        if pkg:
                            module_index[pkg] = vfs_path

        finally:
            conn.close()

        self._path_index = path_index
        self._module_index = module_index
        self._index_built = True

    @service_endpoint(
        inputs={"batch_size": "int"},
        outputs={"processed_count": "int"},
        description="Polls the database for files with 'RAW' status and processes them into chunks and graph nodes.",
        tags=["pipeline", "batch"],
        side_effects=["cartridge:write", "neural:inference"]
    )
    def process_pending(self, batch_size: int = 5) -> int:
        """Main loop. Returns number of files processed."""
        pending = self.cartridge.get_pending_files(limit=batch_size)
        if not pending:
            return 0

        self.log_info(f"Refining batch of {len(pending)} files...")

        for file_row in pending:
            self._refine_file(file_row)

        return len(pending)

    def _refine_file(self, row: Dict):
        file_id = row['id']
        vfs_path = row['vfs_path']
        content = row['content']

        # Skip binary files for now (unless we add OCR later)
        if not content:
            self.cartridge.update_status(file_id, "SKIPPED_BINARY")
            return

        try:
            # 1. Specialized Chunking via Router
            chunks = self.chunker.chunk_file(content, vfs_path)

            # 2. Vectorization & Storage
            chunk_texts = [c.content for c in chunks]

            # Buffer graph writes while DB transaction is open (prevents nested-writer locks)
            pending_nodes: List[Tuple[str, str, str, Dict[str, Any]]] = []
            pending_edges: List[Tuple[str, str, str, float]] = []

            # Use ThreadPool to embed in parallel (preserve order with map)
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.neural.max_workers) as executor:
                vectors = list(executor.map(self.neural.get_embedding, chunk_texts))

            conn = self.cartridge._get_conn()
            try:
                cursor = conn.cursor()

                for i, chunk in enumerate(chunks):
                    vector = vectors[i]
                    vec_blob = json.dumps(vector).encode('utf-8') if vector else None

                    # Store Chunk
                    cursor.execute(
                        """
                        INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (file_id, i, chunk.content, vec_blob, chunk.name, chunk.type, chunk.start_line, chunk.end_line)
                    )

                    chunk_row_id = cursor.lastrowid

                    # Insert into Vector Index (if vector exists)
                    if vector:
                        try:
                            cursor.execute(
                                "INSERT INTO vec_items(rowid, embedding) VALUES (?, ?)",
                                (chunk_row_id, json.dumps(vector))
                            )
                        except Exception as ve:
                            self.log_error(f"Vector Index Insert Failed: {ve}")

                    # Graph Node for Chunks (Functions/Classes)
                    # IMPORTANT: Do NOT call CartridgeService.add_node/add_edge while this conn is open.
                    if chunk.type in ['class', 'function']:
                        node_id = f"{vfs_path}::{chunk.name}"
                        pending_nodes.append(
                            (
                                node_id,
                                'chunk',
                                chunk.name,
                                {
                                    'parent': vfs_path,
                                    'file_id': file_id,
                                    'chunk_row_id': chunk_row_id,
                                    'chunk_type': chunk.type,
                                    'start_line': chunk.start_line,
                                    'end_line': chunk.end_line
                                }
                            )
                        )
                        pending_edges.append((node_id, vfs_path, "defined_in", 1.0))

                conn.commit()

            finally:
                conn.close()

            # 3. File Level Graph Node (after close)
            pending_nodes.append((vfs_path, 'file', vfs_path.split('/')[-1], {'path': vfs_path, 'file_id': file_id}))

            # 4. Section/Chapter Weaving (docs)
            self._weave_sections(vfs_path, content)

            # 5. Import Weaving (resolved when possible)
            self._weave_imports(vfs_path, content)

            # 6. Flush buffered graph writes
            for nid, ntype, label, data in pending_nodes:
                self.cartridge.add_node(nid, ntype, label, data)
            for src, tgt, rel, w in pending_edges:
                self.cartridge.add_edge(src, tgt, rel, w)

            # 7. Mark file refined
            self.cartridge.update_status(file_id, "REFINED")

        except Exception as e:
            self.log_error(f"Refining failed for {vfs_path}: {e}")
            self.cartridge.update_status(file_id, "ERROR", {"error": str(e)})

    def _extract_imports_python(self, source_path: str, content: str) -> List[Tuple[str, int, int]]:
        """Returns list of (module_or_path, level, lineno).

        - level: 0 for absolute imports, >=1 for relative import-from statements.
        """
        out: List[Tuple[str, int, int]] = []
        try:
            tree = ast.parse(content)
        except Exception:
            return out

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias and alias.name:
                        out.append((alias.name, 0, getattr(node, 'lineno', 0)))
            elif isinstance(node, ast.ImportFrom):
                level = int(getattr(node, 'level', 0) or 0)
                mod = getattr(node, 'module', None) or ""
                if mod:
                    out.append((mod, level, getattr(node, 'lineno', 0)))
                else:
                    # from . import x
                    for alias in node.names:
                        if alias and alias.name:
                            out.append((alias.name, level, getattr(node, 'lineno', 0)))

        return out

    def _resolve_python_import(self, source_path: str, module: str, level: int) -> List[str]:
        """Resolve a python import to possible VFS target paths."""
        self._build_import_index()

        # Absolute: try direct module mapping
        if level <= 0:
            if module in self._module_index:
                return [self._module_index[module]]
            return []

        # Relative: resolve from the source directory
        src_dir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        base_parts = src_dir.split("/") if src_dir else []

        # level=1 means "from .", so pop 0; level=2 means "from .." pop 1, etc.
        pops = max(level - 1, 0)
        if pops > 0 and pops <= len(base_parts):
            base_parts = base_parts[:-pops]

        rel_base = "/".join([p for p in base_parts if p])
        mod_path = module.replace(".", "/").strip("/")

        candidates: List[str] = []
        if rel_base:
            if mod_path:
                candidates.append(f"{rel_base}/{mod_path}.py")
                candidates.append(f"{rel_base}/{mod_path}/__init__.py")
            else:
                candidates.append(f"{rel_base}/__init__.py")
        else:
            if mod_path:
                candidates.append(f"{mod_path}.py")
                candidates.append(f"{mod_path}/__init__.py")

        return [c for c in candidates if c in self._path_index]

    def _resolve_js_like_import(self, source_path: str, imp: str) -> List[str]:
        """Resolve require('./x') / import ... from './x' to VFS candidates."""
        self._build_import_index()

        sdir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        raw = imp.strip().replace("\\", "/")

        # Only try to resolve relative-ish paths
        if not (raw.startswith(".") or raw.startswith("/")):
            return []

        # Normalize
        if raw.startswith("/"):
            rel = raw.lstrip("/")
        else:
            rel = os.path.normpath(os.path.join(sdir, raw)).replace("\\", "/").lstrip("./")

        ext_candidates = [rel]
        # Common extensions
        if not os.path.splitext(rel)[1]:
            ext_candidates.extend([rel + ".js", rel + ".ts", rel + ".json"]) 
            ext_candidates.extend([rel + "/index.js", rel + "/index.ts"]) 

        return [c for c in ext_candidates if c in self._path_index]

    def _weave_imports(self, source_path: str, content: str):
        """Scans content for imports and links them in the graph.

        Creates edges:
        - imports_file: source_path -> target_vfs_path (resolved)
        - imports_unresolved: source_path -> module_string (fallback)
        """
        targets_resolved: List[str] = []

        # Python: use AST when possible
        if source_path.endswith(".py"):
            for mod, level, lineno in self._extract_imports_python(source_path, content):
                resolved = self._resolve_python_import(source_path, mod, level)
                if resolved:
                    for tgt in resolved:
                        self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                        targets_resolved.append(tgt)
                else:
                    self.cartridge.add_edge(source_path, mod, "imports_unresolved", 0.25)

            return

        # JS / generic: regex fallback
        for line in content.splitlines():
            match = self.import_pattern.search(line)
            if not match:
                continue

            imp = match.group(1) or match.group(2)
            if not imp:
                continue

            resolved = self._resolve_js_like_import(source_path, imp)
            if resolved:
                for tgt in resolved:
                    self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                    targets_resolved.append(tgt)
            else:
                self.cartridge.add_edge(source_path, imp, "imports_unresolved", 0.25)

    def _weave_sections(self, vfs_path: str, content: str):
        """Creates section/chapter nodes for long-form text and links them to the file node."""
        ext = os.path.splitext(vfs_path)[1].lower()
        if ext not in (".md", ".markdown", ".txt", ".rst"):
            return

        lines = content.splitlines()
        for idx, line in enumerate(lines):
            lineno = idx + 1

            m = self._md_heading.match(line)
            if m:
                hashes = m.group(1)
                title = (m.group(2) or "").strip()
                level = len(hashes)
                if title:
                    node_id = f"{vfs_path}::section::{lineno}:{title}"
                    self.cartridge.add_node(node_id, "section", title, {
                        "parent": vfs_path,
                        "level": level,
                        "line": lineno
                    })
                    self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)
                                    continue

                    if __name__ == "__main__":
                        # Requires Cartridge and Neural services for testing
                        from __CartridgeServiceMS import CartridgeService
                        from __NeuralServiceMS import NeuralService
                        c = CartridgeService(":memory:")
                        n = NeuralService()
                        svc = RefineryServiceMS(c, n)
                        print("Service ready:", svc._service_info["name"])

            c = self._chapter_heading.match(line)
            if c:
                chap_num = (c.group(2) or "").strip()
                chap_title = (c.group(3) or "").strip()
                title = f"Chapter {chap_num}" + (f": {chap_title}" if chap_title else "")
                node_id = f"{vfs_path}::chapter::{lineno}:{chap_num}"
                self.cartridge.add_node(node_id, "section", title, {
                    "parent": vfs_path,
                    "level": 1,
                    "line": lineno
                })
                self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)








--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__RegexWeaverMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _RegexWeaverMS
ENTRY_POINT: __RegexWeaverMS.py
DEPENDENCIES: None
"""

import re
import logging
from typing import Any, Dict, List, Optional, Set
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: PATTERNS
# ==============================================================================
# Python: "import x", "from x import y"
PY_IMPORT = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')

# JS/TS: "import ... from 'x'", "require('x')"
JS_IMPORT = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RegexWeaver")
# ==============================================================================

@service_metadata(
name="RegexWeaver",
version="1.0.0",
description="Fault-tolerant dependency extractor using Regex.",
tags=["parsing", "dependencies", "regex"],
capabilities=["compute"]
)
class RegexWeaverMS:
    """
The Weaver: A fault-tolerant dependency extractor.
Uses Regex to find imports, making it faster and more permissive
than AST parsers (works on broken code).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"content": "str", "language": "str"},
    outputs={"dependencies": "List[str]"},
    description="Scans code content for import statements.",
    tags=["parsing", "dependencies"],
    side_effects=[]
    )
    def extract_dependencies(self, content: str, language: str) -> List[str]:
    """
    Scans code content for import statements.
    :param language: 'python' or 'javascript' (includes ts/jsx).
    """
        dependencies: Set[str] = set()
        lines = content.splitlines()
        
        pattern = PY_IMPORT if language == 'python' else JS_IMPORT
        
        for line in lines:
            # Skip comments roughly
            if line.strip().startswith(('#', '//')):
                continue
                
            if language == 'python':
                match = pattern.match(line)
            else:
                match = pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                # Clean up: "backend.database" -> "database"
                # We usually want the leaf name for simple linking
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                dependencies.add(clean_dep)
                
        return sorted(list(dependencies))

# --- Independent Test Block ---
if __name__ == "__main__":
weaver = RegexWeaverMS()
print("Service ready:", weaver)
    
    # 1. Python Test
    py_code = """
    import os
    from backend.utils import helper
    # from commented.out import ignore_me
    import pandas as pd
    """
    print(f"Python Deps: {weaver.extract_dependencies(py_code, 'python')}")
    
    # 2. JS Test
    js_code = """
    import React from 'react';
    const utils = require('./lib/utils');
    // import hidden from 'hidden';
    """

print(f"JS Deps:     {weaver.extract_dependencies(js_code, 'javascript')}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__RoleManagerMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _RoleManagerMS
ENTRY_POINT: __RoleManagerMS.py
DEPENDENCIES: pydantic
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pydantic"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _RoleManagerMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("roles.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RoleManager")
# ==============================================================================

class RoleManagerMS(BaseModel):
    id: str
    name: str
    description: Optional[str] = ""
    system_prompt: str
    knowledge_bases: List[str] = []
    memory_policy: str = "scratchpad" # or 'auto_commit'
    created_at: datetime.datetime

@service_metadata(
name="RoleManager",
version="1.0.0",
description="Manages Agent Personas (Roles), including System Prompts and Memory Settings.",
tags=["roles", "personas", "db"],
capabilities=["db:sqlite"]
)
class RoleManagerMS:
    """
The Casting Director: Manages Agent Personas (Roles).
Persists configuration for System Prompts, Attached KBs, and Memory Settings.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", DB_PATH)
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS roles (
                    id TEXT PRIMARY KEY,
                    name TEXT UNIQUE NOT NULL,
                    description TEXT,
                    system_prompt TEXT NOT NULL,
                    knowledge_bases_json TEXT,
                    memory_policy TEXT,
                    created_at TIMESTAMP
                )
            """)

    @service_endpoint(
    inputs={"name": "str", "system_prompt": "str", "description": "str", "kbs": "List[str]"},
    outputs={"role": "RoleManagerMS"},
    description="Creates a new Agent Persona.",
    tags=["roles", "create"],
    side_effects=["db:write"]
    )
    def create_role(self, name: str, system_prompt: str, description: str = "", kbs: List[str] = None) -> RoleManagerMS:
    """Creates a new Agent Persona."""
        role_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        kbs_json = json.dumps(kbs or [])
        
        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO roles (id, name, description, system_prompt, knowledge_bases_json, memory_policy, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    (role_id, name, description, system_prompt, kbs_json, "scratchpad", now)
                )
            log.info(f"Created RoleManagerMS: {name}")
            return self.get_role(name)
        except sqlite3.IntegrityError:
            raise ValueError(f"RoleManagerMS '{name}' already exists.")

    @service_endpoint(
    inputs={"name_or_id": "str"},
    outputs={"role": "Optional[RoleManagerMS]"},
    description="Retrieves a role by Name or ID.",
    tags=["roles", "read"]
    )
    def get_role(self, name_or_id: str) -> Optional[RoleManagerMS]:
    """Retrieves a role by Name or ID."""
        with self._get_conn() as conn:
            # Try ID first
            row = conn.execute("SELECT * FROM roles WHERE id = ?", (name_or_id,)).fetchone()
            if not row:
                # Try Name
                row = conn.execute("SELECT * FROM roles WHERE name = ?", (name_or_id,)).fetchone()
            
            if not row: return None

            return RoleManagerMS(
                id=row['id'],
                name=row['name'],
                description=row['description'],
                system_prompt=row['system_prompt'],
                knowledge_bases=json.loads(row['knowledge_bases_json']),
                memory_policy=row['memory_policy'],
                created_at=row['created_at'] # Adapter might need datetime.fromisoformat if stored as str
            )

    @service_endpoint(
    inputs={},
    outputs={"roles": "List[Dict]"},
    description="Lists all available roles.",
    tags=["roles", "read"]
    )
    def list_roles(self) -> List[Dict]:
    with self._get_conn() as conn:
            rows = conn.execute("SELECT id, name, description FROM roles").fetchall()
            return [dict(r) for r in rows]

    @service_endpoint(
    inputs={"name": "str"},
    outputs={},
    description="Deletes a role by name.",
    tags=["roles", "delete"],
    side_effects=["db:write"]
    )
    def delete_role(self, name: str):
    with self._get_conn() as conn:
            conn.execute("DELETE FROM roles WHERE name = ?", (name,))
        log.info(f"Deleted RoleManagerMS: {name}")

# --- Independent Test Block ---
if __name__ == "__main__":
import os
if DB_PATH.exists(): os.remove(DB_PATH)
    
mgr = RoleManagerMS()
print("Service ready:", mgr)
    
    # 1. Create
    mgr.create_role(
        name="SeniorDev", 
        system_prompt="You are a senior Python developer. Prefer Clean Code principles.",
        description="Expert coding assistant",
        kbs=["python_docs", "project_repo"]
    )
    
    # 2. Retrieve
    role = mgr.get_role("SeniorDev")
    print(f"RoleManagerMS: {role.name}")
    print(f"Prompt: {role.system_prompt}")
    print(f"KBs: {role.knowledge_bases}")
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__SandboxManagerMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SandboxManagerMS
ENTRY_POINT: __SandboxManagerMS.py
DEPENDENCIES: None
"""

import shutil
import hashlib
import os
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Default folders to ignore when syncing or diffing
DEFAULT_EXCLUDES = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "_sandbox", "_project_library"
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("SandboxMgr")
# ==============================================================================

class SandboxManagerMS:
    """
    The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
    Allows for safe experimentation, diffing, and atomic promotion of changes.
    """
    def __init__(self, live_path: str, sandbox_path: str):
        self.live_root = Path(live_path).resolve()
        self.sandbox_root = Path(sandbox_path).resolve()

    def init_sandbox(self, force: bool = False):
        """
        Creates or resets the sandbox by mirroring the live project.
        """
        if self.sandbox_root.exists():
            if not force:
                raise FileExistsError(f"Sandbox already exists at {self.sandbox_root}")
            log.info("Wiping existing sandbox...")
            shutil.rmtree(self.sandbox_root)
        
        log.info(f"Cloning {self.live_root} -> {self.sandbox_root}...")
        self._mirror_tree(self.live_root, self.sandbox_root)
        log.info("Sandbox initialized.")

    def reset_sandbox(self):
        """
        Discards all sandbox changes and re-syncs from live.
        """
        self.init_sandbox(force=True)

    def get_diff(self) -> Dict[str, List[str]]:
        Compares Sandbox vs Live. Returns added, modified, and deleted files.
        """
        sandbox_files = self._scan_files(self.sandbox_root)
        live_files = self._scan_files(self.live_root)
        
        sandbox_paths = set(sandbox_files.keys())
        live_paths = set(live_files.keys())

        # 1. Added: In sandbox but not in live
        added = sorted(list(sandbox_paths - live_paths))
        
        # 2. Deleted: In live but not in sandbox
        deleted = sorted(list(live_paths - sandbox_paths))
        
        # 3. Modified: In both, but hashes differ
        common = sandbox_paths.intersection(live_paths)
        modified = []
        for rel_path in common:
            if sandbox_files[rel_path] != live_files[rel_path]:
                modified.append(rel_path)
        modified.sort()

        return {
            "added": added,
            "modified": modified,
            "deleted": deleted
        }

    def promote_changes(self) -> Tuple[int, int, int]:
        """
        Applies changes from Sandbox to Live.
        Returns (added_count, modified_count, deleted_count).
        """
        diff = self.get_diff()
        
        # 1. Additions & Modifications (Copy file -> file)
        for rel_path in diff['added'] + diff['modified']:
            src = self.sandbox_root / rel_path
            dst = self.live_root / rel_path
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
            
        # 2. Deletions (Remove file)
        for rel_path in diff['deleted']:
            target = self.live_root / rel_path
            if target.exists():
                os.remove(target)
                
        log.info(f"Promoted: {len(diff['added'])} added, {len(diff['modified'])} modified, {len(diff['deleted'])} deleted.")
        return len(diff['added']), len(diff['modified']), len(diff['deleted'])

    # --- Internal Helpers ---

    def _mirror_tree(self, src_root: Path, dst_root: Path):
        """Recursive copy that respects the exclusion list."""
        if not dst_root.exists():
            dst_root.mkdir(parents=True, exist_ok=True)

        for item in src_root.iterdir():
            if item.name in DEFAULT_EXCLUDES:
                continue
                
            dst_path = dst_root / item.name
            
            if item.is_dir():
                self._mirror_tree(item, dst_path)
            else:
                shutil.copy2(item, dst_path)

    def _scan_files(self, root: Path) -> Dict[str, str]:
        """
        Scans directory and returns {relative_path: sha256_hash}.
        """
        file_map = {}
        if not root.exists():
            return {}
            
        for path in root.rglob("*"):
            if path.is_file() and not self._is_excluded(path, root):
                rel = str(path.relative_to(root)).replace("\\", "/")
                file_map[rel] = self._get_hash(path)
        return file_map

    def _is_excluded(self, path: Path, root: Path) -> bool:
        """Checks if any part of the path is in the exclusion list."""
        try:
            rel_parts = path.relative_to(root).parts
            return any(p in DEFAULT_EXCLUDES for p in rel_parts)
        except ValueError:
            return False

    def _get_hash(self, path: Path) -> str:
        """Fast SHA-256 for file content."""
        try:
            # Skip binary files if needed, or hash them too (hashing is safe)
            return hashlib.sha256(path.read_bytes()).hexdigest()
        except Exception:
            return "read_error"

# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup test environment
    base = Path("test_env")
    live = base / "live_project"
    box = base / "sandbox"
    
    if base.exists(): shutil.rmtree(base)
    live.mkdir(parents=True)
    
    # 1. Create Mock Live Project
    (live / "main.py").write_text("print('v1')")
    (live / "utils.py").write_text("def help(): pass")
    (live / "node_modules").mkdir() # Should be ignored
    (live / "node_modules" / "junk.js").write_text("junk")
    
    print("--- Initializing Sandbox ---")
    mgr = SandboxManagerMS(str(live), str(box))
    mgr.init_sandbox()
    
    # 2. Make Changes in Sandbox
print("\n--- Modifying Sandbox ---")
(box / "main.py").write_text("print('v2')")  # Modify
(box / "new_feature.py").write_text("print('new')")  # Add
os.remove(box / "utils.py")  # Delete

# 3. Check Diff
diff = mgr.get_diff()
print(f"Diff Analysis:\n Added: {diff['added']}\n Modified: {diff['modified']}\n Deleted: {diff['deleted']}")

# 4. Promote
print("\n--- Promoting Changes ---")
mgr.promote_changes()

# Verify Live
print(f"Live 'main.py' content: {(live / 'main.py').read_text()}")
print(f"Live 'utils.py' exists? {(live / 'utils.py').exists()})

# Cleanup
if base.exists(): shutil.rmtree(base)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__ScoutMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ScoutMS
ENTRY_POINT: __ScoutMS.py
DEPENDENCIES: None
"""

import os
import time
import requests
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Any, Optional

# Try imports for Web/PDF support
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

@service_metadata(
    name="ScoutMS",
    version="1.0.0",
    description="The Scout: A depth-aware utility for recursively walking local file systems or crawling websites.",
    tags=["utility", "scanner", "crawler"],
    capabilities=["filesystem:read", "web:crawl"]
)
class ScoutMS:
    """
    The Scanner: Walks file systems OR crawls websites (Depth-Aware).
    """
    
    def __init__(self):
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage', 'site-packages'
        }
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', 
            '.zip', '.tar', '.gz', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }
        self.visited_urls = set()

    def is_binary(self, file_path: str) -> bool:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS: return True
        return False

    @service_endpoint(
        inputs={"root_path": "str", "web_depth": "int"},
        outputs={"tree": "dict"},
        description="Main entry point to perform a recursive scan of a directory or a web crawl.",
        tags=["discovery", "recursive"]
    )
    def scan_directory(self, root_path: str, web_depth: int = 0) -> Optional[Dict[str, Any]]:
        """
        Main Entry Point.
        :param root_path: File path or URL.
        :param web_depth: How many links deep to crawl (0 = single page).
        """
        # 1. Web Crawl Mode
        if root_path.startswith("http://") or root_path.startswith("https://"):
            self.visited_urls.clear()
            return self._crawl_web_recursive(root_path, depth=web_depth, origin_domain=urlparse(root_path).netloc)

        # 2. Local File System Mode
        target = os.path.abspath(root_path)
        if not os.path.exists(target): return None
        
        if not os.path.isdir(target): 
            return self._create_node(target, is_dir=False)
            
        return self._scan_fs_recursive(target)

    # --- Web Logic ---
    def _crawl_web_recursive(self, url: str, depth: int, origin_domain: str) -> Dict[str, Any]:
        """
        Recursively fetches links.
        """
        # Generate a nice VFS path: web/domain/path
        parsed = urlparse(url)
        clean_path = parsed.path.strip("/")
        if not clean_path: clean_path = "index.html"
        rel_path = f"web/{parsed.netloc}/{clean_path}"

        node = {
            'name': url,
            'path': url,
            'rel_path': rel_path,
            'type': 'web',
            'children': [],
            'checked': True
        }
        
        if depth < 0 or url in self.visited_urls: return node
        self.visited_urls.add(url)

        if depth > 0 and BeautifulSoup:
            try:
                # Polite Delay
                time.sleep(0.1)
                resp = requests.get(url, timeout=5)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.content, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        parsed = urlparse(full_url)
                        
                        # Filter: Only same domain, valid schemes
                        if parsed.netloc == origin_domain and parsed.scheme in ['http', 'https']:
                            if full_url not in self.visited_urls:
                                child_node = self._crawl_web_recursive(full_url, depth - 1, origin_domain)
                                node['children'].append(child_node)
            except Exception as e:
                node['error'] = str(e)
                
        return node

    # --- File System Logic ---
    def _scan_fs_recursive(self, current_path: str, root_path: str = None) -> Dict[str, Any]:
        if root_path is None: root_path = current_path
        
        node = self._create_node(current_path, is_dir=True, root_path=root_path)
        node['children'] = []
        try:
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                for entry in entries:
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS: continue
                    if entry.name.startswith('.'): continue

                    if entry.is_dir():
                        child = self._scan_fs_recursive(entry.path, root_path=root_path)
                        if child: node['children'].append(child)
                    else:
                        node['children'].append(self._create_node(entry.path, is_dir=False, root_path=root_path))
        except PermissionError:
            node['error'] = "Access Denied"
        return node

    def _create_node(self, path: str, is_dir: bool, root_path: str = None) -> Dict[str, Any]:
        name = os.path.basename(path)
        # Calculate relative path for VFS
        rel_path = name
        if root_path:
            try:
                rel_path = os.path.relpath(path, root_path).replace("\\", "/")
            except ValueError:
                pass

        node = {
            'name': name, 
            'path': path, 
            'rel_path': rel_path,
            'type': 'folder' if is_dir else 'file', 
            'children': [],
            'checked': False
        }
        return node

    @service_endpoint(
        inputs={"tree_node": "dict"},
        outputs={"file_list": "list"},
        description="Flattens a hierarchical tree node structure into a simple list of paths.",
        tags=["utility", "processing"]
    )
    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        files = []
        if tree_node['type'] in ['file', 'web']:
            files.append(tree_node['path'])
        elif 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files

    if __name__ == "__main__":
        svc = ScoutMS()
        print("Service ready:", svc._service_info["name"])
    # Basic local test
    current_dir = os.path.dirname(os.path.abspath(__file__))
    tree = svc.scan_directory(current_dir)
    if tree:
        print(f"Scanned {len(svc.flatten_tree(tree))} files in current directory.")




--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__SearchEngineMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SearchEngineMS
ENTRY_POINT: __SearchEngineMS.py
DEPENDENCIES: requests, sqlite-vec
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["requests", "sqlite-vec"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _SearchEngineMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import sqlite3
import json
import struct
import requests
import os
from typing import List, Dict, Any, Optional

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    
    Architecture:
    1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
    2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
    3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).
    """

    def __init__(self, model_name: str = "phi3:mini-128k"):
        self.model = model_name

    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict]:
        """
        Main entry point. Returns a list of results sorted by relevance.
        """
        if not os.path.exists(db_path):
            return []

        conn = sqlite3.connect(db_path)
        # Enable sqlite-vec extension if needed, though standard connect might miss it 
        # depending on system install. For now, we assume the DB is pre-populated 
        # and standard SQL queries work if the extension is loaded globally or unnecessary 
        # for simple selects (standard SQLite can read vec0 tables usually, just not query them efficiently without ext).
        # Note: If sqlite-vec is not loaded, the vec0 MATCH queries below will fail.
        # We try to load it here just in case.
        conn.enable_load_extension(True)
        try:
            import sqlite_vec
            sqlite_vec.load(conn)
        except:
            print("Warning: sqlite_vec not loaded in Search Engine. Vector search may fail.")

        cursor = conn.cursor()

        # 1. Vectorize the User Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails
            return self._keyword_search_only(cursor, query, limit)

        # Pack vector for sqlite-vec (Float32 Little Endian)
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)

        # 2. HYBRID QUERY (The "Magic" SQL)
        # We use CTEs to get top 50 from Vector and top 50 from Keyword, then merge.
        sql = """
        WITH 
        vec_matches AS (
            SELECT rowid, distance,
            row_number() OVER (ORDER BY distance) as rank
            FROM knowledge_vectors
            WHERE embedding MATCH ? 
            AND k = 50
        ),
        fts_matches AS (
            SELECT rowid, rank as fts_score,
            row_number() OVER (ORDER BY rank) as rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT 50
        )
        SELECT 
            kc.file_path,
            kc.content,
            (
                -- RRF Formula: 1 / (k + rank)
                COALESCE(1.0 / (60 + v.rank), 0.0) +
                COALESCE(1.0 / (60 + f.rank), 0.0)
            ) as rrf_score
        FROM knowledge_chunks kc
        LEFT JOIN vec_matches v ON kc.id = v.rowid
        LEFT JOIN fts_matches f ON kc.id = f.rowid
        WHERE v.rowid IS NOT NULL OR f.rowid IS NOT NULL
        ORDER BY rrf_score DESC
        LIMIT ?;
        """

        try:
            # Escape quotes for FTS
            fts_query = f'"{query}"' 
            rows = cursor.execute(sql, (vec_bytes, fts_query, limit)).fetchall()
        except sqlite3.OperationalError as e:
            print(f"Search Error (likely missing sqlite-vec): {e}")
            return []

        results = []
        for r in rows:
            path, content, score = r
            snippet = self._extract_snippet(content, query)
            results.append({
                "path": path,
                "score": round(score, 4),
                "snippet": snippet,
                "full_content": content # Keeping this for "Reconstruct" later
            })

        conn.close()
        return results

    def _keyword_search_only(self, cursor, query: str, limit: int) -> List[Dict]:
        """Fallback if embeddings are offline."""
        sql = """
            SELECT file_path, content
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        rows = cursor.execute(sql, (f'"{query}"', limit)).fetchall()
        return [{
            "path": r[0], 
            "score": 0.0, 
            "snippet": self._extract_snippet(r[1], query),
            "full_content": r[1]
        } for r in rows]

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        """Call Ollama to get the vector for the search query."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

    def _extract_snippet(self, content: str, query: str) -> str:
        """Finds the best window of text around the keyword."""
        lower_content = content.lower()
        lower_query = query.lower().split()[0] # Take first word for simple centering
        
        idx = lower_content.find(lower_query)
        if idx == -1:
            return content[:200].replace('\n', ' ') + "..."
            
        start = max(0, idx - 60)
        end = min(len(content), idx + 140)
        snippet = content[start:end].replace('\n', ' ')
        return f"...{snippet}..."

# --- Independent Test Block ---
if __name__ == "__main__":
    # Note: Requires a real DB path to work
    print("Initializing Search Engine...")
    engine = SearchEngineMS()
    # Test would go here

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__SemanticChunkerMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SemanticChunkerMS
ENTRY_POINT: __SemanticChunkerMS.py
DEPENDENCIES: None
"""

import ast
from dataclasses import dataclass
from typing import List
from microservice_std_lib import service_metadata, service_endpoint

@dataclass SemanticChunkerMS CodeChunk:
    name: str          # e.g., "SemanticChunkerMS AuthMS"
    type: str          # "SemanticChunkerMS", "function", "text"
    content: str       # The raw source
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

@service_metadata(
    name="SmartChunkerMS",
    version="1.0.0",
    description="The Surgeon: Intelligent Code Splitter that parses source code into logical semantic units (Classes, Functions) using AST.",
    tags=["utility", "nlp", "parser"],
    capabilities=["python-ast", "semantic-chunking"]
)
SemanticChunkerMS SemanticChunker:
    """
    Intelligent Code Splitter.
    Parses source code into logical units (Classes, Functions) 
    rather than arbitrary text windows.
    """
    
    @service_endpoint(
        inputs={"content": "str", "filename": "str"},
        outputs={"chunks": "list"},
        description="Main entry point to split a file into semantic chunks based on its extension and content.",
        tags=["processing", "chunking"]
    )
    def chunk_file(self, content: str, filename: str) -> List[CodeChunk]:
        # 1. Python Code
        if filename.endswith(".py"):
            return self._chunk_python(content)
            
        # 2. Text / Prose Documents (Smaller semantic windows)
        lower = filename.lower()
        if lower.endswith(('.md', '.txt', '.pdf', '.html', '.htm', '.rst')):
            return self._chunk_generic(content, window_size=800)
            
        # 3. Fallback (Generic Code/Binary)
        return self._chunk_generic(content, window_size=1500)

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                end = node.end_lineno if hasattr(node, 'end_lineno') else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", type="function", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"SemanticChunkerMS {node.name}", type="SemanticChunkerMS", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))

            # Fallback: If no classes/functions found (e.g., script file), treat as generic
            if not chunks:
                return self._chunk_generic(source)
                
        except SyntaxError:
            return self._chunk_generic(source)
            
        return chunks

    def _chunk_generic(self, text: str, window_size: int = 1500) -> List[CodeChunk]:
        """Sliding window for non-code files."""
        chunks = []
        # normalize newlines to avoid massive single-line blobs
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        lines = text.splitlines(keepends=True)
        
        current_chunk = []
        current_size = 0
        chunk_idx = 1
        start_line = 1
        
        for i, line in enumerate(lines):
            current_chunk.append(line)
            current_size += len(line)
            
            if current_size >= window_size:
                chunks.append(CodeChunk(
                    name=f"Chunk {chunk_idx}", type="text_block",
                    content="".join(current_chunk), start_line=start_line, end_line=i + 1
                ))
                current_chunk = []
                current_size = 0
                chunk_idx += 1
                start_line = i + 2
                
        if current_chunk:
            chunks.append(CodeChunk(
                name=f"Chunk {chunk_idx}", type="text_block",
                content="".join(current_chunk), start_line=start_line, end_line=len(lines)
            ))
            
                    return chunks

            if __name__ == "__main__":
                svc = SemanticChunker()
                print("Service ready:", svc._service_info["name"])
                # Basic test on a snippet of Python code
                test_code = "def hello():\n    print('world')\n\nclass Test:\n    pass"
                chunks = svc.chunk_file(test_code, "test.py")
                print(f"Extracted {len(chunks)} semantic chunks.")
                for c in chunks:
                    print(f" - [{c.type}] {c.name} ({c.start_line}-{c.end_line})")



--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__ServiceRegistryMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ServiceRegistryMS
ENTRY_POINT: __ServiceRegistryMS.py
DEPENDENCIES: None
"""

import ast
import json
import uuid
import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("ServiceRegistry")
# ==============================================================================

@service_metadata(
name="ServiceRegistry",
version="1.0.0",
description="Scans a library of Python microservices and generates standardized JSON Service Tokens.",
tags=["introspection", "registry", "parsing"],
capabilities=["filesystem:read", "filesystem:write"]
)
class ServiceRegistryMS:
    """
The Tokenizer (v2): Scans a library of Python microservices and generates
standardized JSON 'Service Tokens'.
Feature: Hybrid AST/Regex parsing for maximum robustness.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    self.root = Path(self.config.get("root_path", ".")).resolve()
    self.registry = []

    @service_endpoint(
    inputs={"save_to": "str"},
    outputs={"registry": "List[Dict]"},
    description="Scans the file system for microservices and builds a registry.",
    tags=["introspection", "scan"],
    side_effects=["filesystem:read", "filesystem:write"]
    )
    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict]:
    log.info(f"Scanning for microservices in: {self.root}")
        
        # 1. Walk directories
        for item in self.root.iterdir():
            if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                self._process_folder(item)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            log.info(f"‚úÖ Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            log.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_folder(self, folder: Path):
        # Find the main .py file (usually matches folder name, or is the only .py file)
        candidates = list(folder.glob("*.py"))
        for file in candidates:
            if file.name.startswith("__"): continue
            
            token = self._tokenize_file(file)
            if token:
                self.registry.append(token)
                log.info(f"  + Tokenized: {token['name']}")
                break 

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source = f.read()
            
            # Attempt 1: Strict AST Parsing (The "Right" Way)
            try:
                return self._ast_parse(source, file_path)
            except Exception:
                # Attempt 2: Regex Fallback (The "Survival" Way)
                return self._regex_parse(source, file_path)
                
        except Exception as e:
            log.warning(f"  - Failed to read {file_path.name}: {e}")
            return None

    def _ast_parse(self, source: str, file_path: Path):
tree = ast.parse(source)
        target_class = None
        
        # Find class ending in 'MS'
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name.endswith("MS"):
                target_class = node
                break
        
        if not target_class: return None

        # Extract Metadata
        return self._build_token(
            name=target_class.name,
            doc=ast.get_docstring(target_class) or "",
            methods=[
                (n.name, [a.arg for a in n.args.args if a.arg != 'self'], ast.get_docstring(n) or "")
                for n in target_class.body if isinstance(n, ast.FunctionDef) and not n.name.startswith("_")
            ],
            deps=self._extract_ast_imports(tree),
            file_path=file_path
        )

    def _regex_parse(self, source: str, file_path: Path):
        # Find class definition
        class_match = re.search(r'class\s+(\w+MS)', source)
        if not class_match: return None
        name = class_match.group(1)
        
        # Find methods (def name(args):)
        methods = []
        for match in re.finditer(r'def\s+(\w+)\s*\((.*?)\):', source):
            m_name = match.group(1)
            if not m_name.startswith("_"):
                args = [a.strip().split(':')[0] for a in match.group(2).split(',') if a.strip() != 'self']
                methods.append((m_name, args, "Regex extracted"))
                
        return self._build_token(name, "Parsed via Regex", methods, [], file_path)

    def _build_token(self, name, doc, methods, deps, file_path):
        # Generate deterministic ID
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, name).hex[:8].upper()}"
        
        method_dict = {
            m_name: {"args": m_args, "doc": m_doc.strip()} 
            for m_name, m_args, m_doc in methods
        }

        return {
            "token_id": token_id,
            "name": name,
            "path": str(file_path.relative_to(self.root)).replace('\\', '/'),
            "description": doc.strip(),
            "methods": method_dict,
            "dependencies": sorted(deps)
        }

    def _extract_ast_imports(self, tree):
        deps = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: deps.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: deps.add(node.module.split('.')[0])
        return list(deps)

if __name__ == "__main__":
svc = ServiceRegistryMS()
print("Service ready:", svc)
svc.scan()

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__SpinnerThingyMaBobberMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SpinnerThingyMaBobberMS
ENTRY_POINT: __SpinnerThingyMaBobberMS.py
DEPENDENCIES: None
"""

import tkinter as tk
import math
import colorsys
import time
from typing import Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SpinnerTHINGYMABOBBER",
version="1.0.0",
description="Interactive visual spinner widget for OBS/UI overlays.",
tags=["ui", "widget", "visuals"],
capabilities=["ui:gui"]
)
class SpinnerThingyMaBobberMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.root = tk.Tk()
self.root.title("OBS Interactive Spinner")
        self.root.configure(bg="black")
        
        # Default size
        self.root.geometry("600x600")
        
        # Canvas for drawing
        self.canvas = tk.Canvas(
            self.root, 
            bg="black", 
            highlightthickness=0
        )
        self.canvas.pack(fill="both", expand=True)

        # --- STATE VARIABLES ---
        self.angle_1 = 0
        self.angle_2 = 0
        self.angle_3 = 0
        self.hue = 0
        
        # Text Input State
        self.user_text = "PROCESSING"
        self.cursor_visible = True
        self.last_cursor_toggle = time.time()
        
        # Bind keyboard events to the window
        self.root.bind("<Key>", self.handle_keypress)
        
        # Start animation
        self.animate()

        @service_endpoint(
@service_endpoint(
    inputs={},
    outputs={},
    description="Launches the GUI main loop.",
    tags=["ui", "execution"],
    mode="sync",
    side_effects=["ui:block"]
)
def launch(self):
    self.root.mainloop()

def handle_keypress(self, event):
    # Handle Backspace
    if event.keysym == "BackSpace":
        self.user_text = self.user_text[:-1]
    # Handle Escape (Reset to default)
    elif event.keysym == "Escape":
        self.user_text = "PROCESSING"
    # Ignore special keys (Shift, Ctrl, Alt, F-keys, etc.)
    elif len(event.char) == 1 and ord(event.char) >= 32:
        # Limit length to prevent chaos (optional, but 20 is a safe max)
        if len(self.user_text) < 25: 
            self.user_text += event.char.upper()

def get_neon_color(self, offset=0):
    h = (self.hue + offset) % 1.0
    r, g, b = colorsys.hsv_to_rgb(h, 1.0, 1.0)
    return f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}'

def draw_arc(self, cx, cy, radius, width, start, extent, color):
    x0 = cx - radius
    y0 = cy - radius
    x1 = cx + radius
    y1 = cy + radius
    
    self.canvas.create_arc(
        x0, y0, x1, y1,
        start=start, extent=extent,
        outline=color, width=width, style="arc"
    )
def animate(self):
    self.canvas.delete("all")
    
    # Window Dimensions
    w = self.canvas.winfo_width()
    h = self.canvas.winfo_height()
    
    if w < 10 or h < 10:
        self.root.after(50, self.animate)
        return

    cx, cy = w / 2, h / 2
    base_size = min(w, h) / 2
    
    # Update Hue
    self.hue += 0.005
    if self.hue > 1: self.hue = 0
    c1 = self.get_neon_color(0.0)
    c2 = self.get_neon_color(0.3)
    c3 = self.get_neon_color(0.6)

    # --- RINGS ---
        
    # Ring 1
    r1 = base_size * 0.85
    self.angle_1 -= 3
    for i in range(3):
        self.draw_arc(cx, cy, r1, base_size*0.08, self.angle_1 + (i*120), 80, c1)

    # Ring 2
    r2 = base_size * 0.65
    self.angle_2 += 5
    self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2, 160, c2)
    self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2 + 180, 160, c2)

    # Ring 3
    r3 = base_size * 0.45
    self.angle_3 -= 8
    self.draw_arc(cx, cy, r3, base_size*0.04, self.angle_3, 300, c3)
        # --- TEXT LOGIC ---
        
        # Toggle cursor every 0.5 seconds
        if time.time() - self.last_cursor_toggle > 0.5:
            self.cursor_visible = not self.cursor_visible
            self.last_cursor_toggle = time.time()
            
        display_text = self.user_text + ("_" if self.cursor_visible else " ")

        # Dynamic Font Scaling
        # We start with a base size (0.15 of window).
        # If text is long (> 8 chars), we shrink it proportionally so it fits.
        text_len = max(len(self.user_text), 1)
        scaling_factor = 1.0
        if text_len > 8:
            scaling_factor = 8 / text_len
            
        font_size = int(base_size * 0.15 * scaling_factor)
        # Ensure font doesn't vanish
        font_size = max(font_size, 10) 

        self.canvas.create_text(
            cx, cy, 
            text=display_text, 
            fill="white", 
            font=("Courier", font_size, "bold")
        )

        self.root.after(30, self.animate)

if __name__ == "__main__":
svc = SpinnerThingyMaBobberMS()
svc.launch()

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__SysInspectorMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SysInspectorMS
ENTRY_POINT: __SysInspectorMS.py
DEPENDENCIES: None
"""

import platform
import subprocess
import sys
import datetime
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SysInspector",
version="1.0.0",
description="Gathers hardware and environment statistics via shell commands.",
tags=["system", "audit", "hardware"],
capabilities=["os:shell", "compute"]
)
class SysInspectorMS:
    """
The Auditor: Gathers hardware and environment statistics.
Supports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={},
outputs={"report": "str"},
description="Runs the full audit and returns a formatted string report.",
tags=["system", "report"],
side_effects=["os:read"]
)
def generate_report(self) -> str:
        """
        Runs the full audit and returns a formatted string report.
        """
        system_os = platform.system()
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = [
            f"System Audit Report",
            f"Generated: {timestamp}",
            f"OS: {system_os} {platform.release()} ({platform.machine()})",
            "-" * 40,
            ""
        ]

        # 1. Hardware Section
        report.append("--- Hardware Information ---")
        if system_os == "Windows":
            report.extend(self._audit_windows())
        elif system_os == "Linux":
            report.extend(self._audit_linux())
        elif system_os == "Darwin":
            report.extend(self._audit_mac())
        else:
            report.append("Unsupported Operating System for detailed hardware audit.")

        # 2. Software Section
        report.append("\n--- Software Environment ---")
        report.append(f"Python Version: {platform.python_version()}")
        report.append(f"Python Executable: {sys.executable}")
        
        return "\n".join(report)

    def _run_cmd(self, cmd: str) -> str:
        """Helper to run shell commands safely."""
        try:
            # shell=True is often required for piped commands, specifically on Windows/Linux
            result = subprocess.run(
                cmd, 
                text=True, 
                capture_output=True, 
                check=False, 
                shell=True, 
                timeout=5
            )
            if result.returncode == 0 and result.stdout:
                return result.stdout.strip()
            elif result.stderr:
                return f"[Cmd Error]: {result.stderr.strip()}"
            return "[No Output]"
        except Exception as e:
            return f"[Execution Error]: {e}"

    # --- OS Specific Implementations ---

    def _audit_windows(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("wmic cpu get name"))
        # GPU
        data.append("GPU: " + self._run_cmd("wmic path win32_videocontroller get name"))
        # RAM
        try:
            mem_str = self._run_cmd("wmic computersystem get totalphysicalmemory").splitlines()[-1]
            mem_bytes = int(mem_str)
            data.append(f"Memory: {mem_bytes / (1024**3):.2f} GB")
        except:
            data.append("Memory: Could not retrieve total physical memory.")
        # Disk
        data.append("\nDisks:")
        data.append(self._run_cmd("wmic diskdrive get model,size"))
        return data

    def _audit_linux(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("lscpu | grep 'Model name'"))
        # GPU (Requires lspci, usually in pciutils)
        data.append("GPU: " + self._run_cmd("lspci | grep -i vga"))
        # RAM
        data.append("Memory:\n" + self._run_cmd("free -h"))
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("lsblk -o NAME,SIZE,MODEL"))
        return data

    def _audit_mac(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("sysctl -n machdep.cpu.brand_string"))
        # GPU
        data.append("GPU:\n" + self._run_cmd("system_profiler SPDisplaysDataType | grep -E 'Chipset Model|VRAM'"))
        # RAM
        data.append("Memory Details:\n" + self._run_cmd("system_profiler SPMemoryDataType | grep -E 'Size|Type|Speed'"))
        # RAM Total
        try:
            mem_bytes = int(self._run_cmd('sysctl -n hw.memsize'))
            data.append(f"Total Memory: {mem_bytes / (1024**3):.2f} GB")
        except: 
            pass
        # Disk
data.append("\nDisks:\n" + self._run_cmd("diskutil list physical"))
        return data

# --- Independent Test Block ---
if __name__ == "__main__":
    inspector = SysInspectorMS()
    print("Service ready:", inspector)
    print("Running System Inspector...")
    print("\n" + inspector.generate_report())

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__TasklistVaultMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TasklistVaultMS
ENTRY_POINT: __TasklistVaultMS.py
DEPENDENCIES: None
"""

import sqlite3
import uuid
import logging
import datetime
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "task_vault.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("TaskVault")

TaskStatus = Literal["Pending", "Running", "Complete", "Error", "Awaiting-Approval"]
# ==============================================================================

@service_metadata(
name="TaskVault",
version="1.0.0",
description="Persistent SQLite engine for hierarchical task management.",
tags=["tasks", "db", "project-management"],
capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class TasklistVaultMS:
    """
The Taskmaster: A persistent SQLite engine for hierarchical task management.
Supports infinite nesting of sub-tasks and status tracking.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", DB_PATH)
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. Task Lists (The containers)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_lists (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP
                )
            """)
            # 2. Tasks (The items, supporting hierarchy via parent_id)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id TEXT PRIMARY KEY,
                    list_id TEXT NOT NULL,
                    parent_id TEXT,
                    content TEXT NOT NULL,
                    status TEXT DEFAULT 'Pending',
                    result TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    FOREIGN KEY(list_id) REFERENCES task_lists(id) ON DELETE CASCADE,
                    FOREIGN KEY(parent_id) REFERENCES tasks(id) ON DELETE CASCADE
                )
            """)

    # --- List Management ---

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"list_id": "str"},
    description="Creates a new task list and returns its ID.",
    tags=["tasks", "create"],
    side_effects=["db:write"]
    )
    def create_list(self, name: str) -> str:
    """Creates a new task list and returns its ID."""
        list_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO task_lists (id, name, created_at) VALUES (?, ?, ?)",
                (list_id, name, now)
            )
        log.info(f"Created Task List: '{name}' ({list_id})")
        return list_id

    @service_endpoint(
    inputs={},
    outputs={"lists": "List[Dict]"},
    description="Returns metadata for all task lists.",
    tags=["tasks", "read"],
    side_effects=["db:read"]
    )
    def get_lists(self) -> List[Dict]:
    """Returns metadata for all task lists."""
        with self._get_conn() as conn:
            rows = conn.execute("SELECT * FROM task_lists ORDER BY created_at DESC").fetchall()
            return [dict(r) for r in rows]

    # --- Task Management ---

    @service_endpoint(
    inputs={"list_id": "str", "content": "str", "parent_id": "Optional[str]"},
    outputs={"task_id": "str"},
    description="Adds a task (or sub-task) to a list.",
    tags=["tasks", "write"],
    side_effects=["db:write"]
    )
    def add_task(self, list_id: str, content: str, parent_id: Optional[str] = None) -> str:
    """Adds a task (or sub-task) to a list."""
        task_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                """INSERT INTO tasks (id, list_id, parent_id, content, status, created_at, updated_at) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (task_id, list_id, parent_id, content, "Pending", now, now)
            )
        return task_id

    @service_endpoint(
    inputs={"task_id": "str", "content": "str", "status": "str", "result": "str"},
    outputs={},
    description="Updates a task's details.",
    tags=["tasks", "update"],
    side_effects=["db:write"]
    )
    def update_task(self, task_id: str, content: str = None, status: TaskStatus = None, result: str = None):
    """Updates a task's details."""
        updates = []
        params = []
        
        if content:
            updates.append("content = ?")
            params.append(content)
        if status:
            updates.append("status = ?")
            params.append(status)
        if result:
            updates.append("result = ?")
            params.append(result)
            
        if not updates: return

        updates.append("updated_at = ?")
        params.append(datetime.datetime.utcnow())
        params.append(task_id)

        sql = f"UPDATE tasks SET {', '.join(updates)} WHERE id = ?"
        
        with self._get_conn() as conn:
            conn.execute(sql, params)
        log.info(f"Updated task {task_id}")

    # --- Tree Reconstruction ---

    @service_endpoint(
    inputs={"list_id": "str"},
    outputs={"tree": "Dict[str, Any]"},
    description="Fetches a list and reconstructs the full hierarchy of tasks.",
    tags=["tasks", "read"],
    side_effects=["db:read"]
    )
    def get_full_tree(self, list_id: str) -> Dict[str, Any]:
    """
    Fetches a list and reconstructs the full hierarchy of tasks.
    """
        with self._get_conn() as conn:
# 1. Get List Info
            list_row = conn.execute("SELECT * FROM task_lists WHERE id = ?", (list_id,)).fetchone()
            if not list_row: return None
            
            # 2. Get All Tasks
            task_rows = conn.execute("SELECT * FROM tasks WHERE list_id = ?", (list_id,)).fetchall()
            
        # 3. Build Adjacency Map
        tasks_by_id = {}
        for r in task_rows:
            t = dict(r)
            t['sub_tasks'] = [] # Prepare children container
            tasks_by_id[t['id']] = t

        # 4. Link Parents and Children
        root_tasks = []
        for t_id, task in tasks_by_id.items():
            parent_id = task['parent_id']
            if parent_id and parent_id in tasks_by_id:
                tasks_by_id[parent_id]['sub_tasks'].append(task)
            else:
                root_tasks.append(task)

        return {
            "id": list_row['id'],
            "name": list_row['name'],
            "tasks": root_tasks
        }

    @service_endpoint(
    inputs={"list_id": "str"},
    outputs={},
    description="Deletes a task list and all its tasks.",
    tags=["tasks", "delete"],
    side_effects=["db:write"]
    )
    def delete_list(self, list_id: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM task_lists WHERE id = ?", (list_id,))
        log.info(f"Deleted list {list_id}")
# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    vault = TasklistVaultMS()
    print("Service ready:", vault)
    
    # 1. Create a Plan
    plan_id = vault.create_list("System Upgrade Plan")
    
    # 2. Add Root Tasks
    t1 = vault.add_task(plan_id, "Backup Database")
    t2 = vault.add_task(plan_id, "Update Server")
    
    # 3. Add Sub-Tasks
    t2_1 = vault.add_task(plan_id, "Stop Services", parent_id=t2)
    t2_2 = vault.add_task(plan_id, "Run Installer", parent_id=t2)
    
    # 4. Update Status
    vault.update_task(t1, status="Complete", result="Backup saved to /tmp/bk.tar")
    vault.update_task(t2_1, status="Running")
    
    # 5. Render Tree
    tree = vault.get_full_tree(plan_id)
    print(f"\n--- {tree['name']} ---")
    
    def print_node(node, indent=0):
        status_icon = "‚úì" if node['status'] == 'Complete' else "‚óã"
        print(f"{'  '*indent}{status_icon} {node['content']} [{node['status']}]")
        for child in node['sub_tasks']:
            print_node(child, indent + 1)

    for task in tree['tasks']:
        print_node(task)
        
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__TelemetryServiceMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TelemetryServiceMS
ENTRY_POINT: __TelemetryServiceMS.py
DEPENDENCIES: None
"""

import logging
import queue
import time
from base_service import BaseService
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="TelemetryServiceMS",
    version="1.0.0",
    description="The Nervous System: Watches the thread-safe LogQueue and updates GUI components with real-time status.",
    tags=["utility", "logging", "telemetry"],
    capabilities=["log-redirection", "real-time-updates"]
)
class TelemetryServiceMS(BaseService):
    """
    The Nervous System.
    Watches the thread-safe LogQueue and updates the GUI Panels.
    """
    def __init__(self, root, panels):
        super().__init__("TelemetryServiceMS")
        self.root = root
        self.panels = panels
        self.log_queue = queue.Queue()
        self.start_time = time.time()
        self._heartbeat_count = 0
        
        # We set up the global logging hook HERE, inside the service
        self._setup_logging_hook()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "queue_depth": "int"},
        description="Standardized health check to verify the operational state of the telemetry pipeline.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the TelemetryServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "queue_depth": self.log_queue.qsize()
        }

    def _setup_logging_hook(self):
        """Redirects Python's standard logging to our Queue."""
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        
        # Create our custom handler that feeds the queue
        q_handler = QueueHandler(self.log_queue)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
        q_handler.setFormatter(formatter)
        logger.addHandler(q_handler)

    @service_endpoint(
        inputs={},
        outputs={},
        description="Initiates the telemetry service and begins the asynchronous GUI log-polling loop.",
        tags=["lifecycle", "event-loop"],
        mode="async"
    )
    def start(self):
        """Begins the GUI update loop."""
        self.log_info("Telemetry Service starting...")
        self._poll_queue()

    @service_endpoint(
        inputs={},
        outputs={"alive": "bool", "heartbeat": "int"},
        description="Verifies that the GUI polling loop is actively processing the log queue.",
        tags=["diagnostic", "heartbeat"]
    )
    def ping(self) -> Dict[str, Any]:
        """Allows an agent to verify the pulse of the UI loop."""
        return {"alive": True, "heartbeat": self._heartbeat_count}

    def _poll_queue(self):
        """The heartbeat that drains the queue into the GUI."""
        self._heartbeat_count += 1
        try:
            while True:
                record = self.log_queue.get_nowait()
                msg = f"[{record.levelname}] {record.message}" # Removed \n because .log() adds it
                
                # Update the GUI
                self.panels.log(msg)
                
        except queue.Empty:
            pass
        finally:
            # Check again in 100ms
            self.root.after(100, self._poll_queue)

# Helper Class for the Queue
class QueueHandler(logging.Handler):
    def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.format(record)
        self.log_queue.put(record)

    if __name__ == "__main__":
    # Mock objects for independent test
    class MockRoot: 
        def after(self, ms, func): print(f"Loop scheduled for {ms}ms")
    class MockPanels:
        def log(self, msg): print(f"UI LOG: {msg}")
    
    svc = TelemetryServiceMS(MockRoot(), MockPanels())
    print("Service ready:", svc._service_info["name"])
    svc.log_info("Internal test message")
    svc._poll_queue()




--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__TextChunkerMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TextChunkerMS
ENTRY_POINT: __TextChunkerMS.py
DEPENDENCIES: None
"""

from typing import Any, Dict, List, Optional, Tuple
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="TextChunker",
version="1.0.0",
description="Splits text into chunks using various strategies (chars, lines).",
tags=["chunking", "nlp", "rag"],
capabilities=["compute"]
)
class TextChunkerMS:
    """
The Butcher: A unified service for splitting text into digestible chunks
for RAG (Retrieval Augmented Generation).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@staticmethod
@service_endpoint(
inputs={"text": "str", "chunk_size": "int", "chunk_overlap": "int"},
outputs={"chunks": "List[str]"},
description="Standard sliding window split by character count.",
tags=["chunking", "chars"],
side_effects=[]
)
    def chunk_by_chars(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """
        Standard Sliding Window. Best for prose/documentation.
        Splits purely by character count.
        """
        if chunk_size <= 0: raise ValueError("chunk_size must be positive")
        
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            # Advance start, backing up by overlap
            start += chunk_size - chunk_overlap
            
        return chunks

    @staticmethod
    @service_endpoint(
    inputs={"text": "str", "max_lines": "int", "max_chars": "int"},
    outputs={"chunks": "List[Dict]"},
    description="Line-preserving chunker, best for code.",
    tags=["chunking", "lines", "code"],
    side_effects=[]
    )
    def chunk_by_lines(text: str, max_lines: int = 200, max_chars: int = 4000) -> List[Dict[str, Any]]:
    """
    Line-Preserving Chunker. Best for Code.
    Respects line boundaries and returns metadata about line numbers.
    """
        lines = text.splitlines()
        chunks = []
        start = 0
        
        while start < len(lines):
            end = min(start + max_lines, len(lines))
            chunk_str = "\n".join(lines[start:end])
            
            # If too big, shrink window (back off)
            while len(chunk_str) > max_chars and end > start + 1:
                end -= 1
                chunk_str = "\n".join(lines[start:end])
            
            chunks.append({
                "text": chunk_str,
                "start_line": start + 1,
                "end_line": end
            })
            start = end
            
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = TextChunkerMS()
print("Service ready:", chunker)
    
    # 1. Prose Test
    print("--- Prose Chunking ---")
    lorem = "A" * 100 # 100 chars
    result = chunker.chunk_by_chars(lorem, chunk_size=40, chunk_overlap=10)
for i, c in enumerate(result):
        print(f"Chunk {i}: len={len(c)}")

    # 2. Code Test
    print("\n--- Code Chunking ---")
    code = "\n".join([f"print('Line {i}')" for i in range(1, 10)])
    # Force splits small for testing
    result_code = chunker.chunk_by_lines(code, max_lines=3, max_chars=100)
    for i, c in enumerate(result_code):
        print(f"Chunk {i}: Lines {c['start_line']}-{c['end_line']}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__ThoughtStreamMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ThoughtStreamMS
ENTRY_POINT: __ThoughtStreamMS.py
DEPENDENCIES: None
"""

import tkinter as tk
from tkinter import ttk
import datetime
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="ThoughtStream",
version="1.0.0",
description="A UI widget for displaying a stream of AI thoughts/logs.",
tags=["ui", "stream", "logs", "widget"],
capabilities=["ui:gui"]
)
class ThoughtStreamMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
        
        # Header
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        # The Stream Area (Canvas allows for custom drawing like sparklines)
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340) # Fixed width like React
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    @service_endpoint(
    inputs={"filename": "str", "chunk_id": "int", "content": "str", "vector_preview": "List[float]", "color": "str"},
    outputs={},
    description="Adds a new thought bubble to the visual stream.",
    tags=["ui", "update"],
    side_effects=["ui:update"]
    )
    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
    """
    Mimics the 'InspectorFrame' from your React code.
    """
# Bubble Container
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        # Header: File + Timestamp
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        header_lbl = tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", 
                              fg="#007ACC", bg="#1a1a25", font=("Consolas", 8))
        header_lbl.pack(anchor="w", padx=5, pady=2)
        
        # Content Snippet
        snippet = content[:400] + "..." if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", 
                               font=("Consolas", 8), justify="left", wraplength=300)
        content_lbl.pack(fill="x", padx=5, pady=2)
        
        # Vector Sparkline (The Custom Draw)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        
        bar_w = w / len(vector) if len(vector) > 0 else 0
        
        for i, val in enumerate(vector):
            # Normalize -1..1 to 0..1 for height
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            
            # Draw bar
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")

# --- Usage Example ---
if __name__ == "__main__":
    root = tk.Tk()
    root.geometry("400x600")
    
    stream = ThoughtStreamMS({"parent": root})
    print("Service ready:", stream)
    stream.pack(fill="both", expand=True)
    
    # Simulate an incoming "Microservice" event
    import random
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("ExplorerView.tsx", 1, "import React from 'react'...", fake_vector, "#FF00FF")
    
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__TkinterUniButtonMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterUniButtonMS
ENTRY_POINT: __TkinterUniButtonMS.py
DEPENDENCIES: None
"""

## Locking Dual Button - Instructions
# TO USE:
# In your main app file
# from components import UnifiedButtonGroup # assuming you saved the TkinterUniButtonMS there
# 
# def my_validation_logic():
#     # do pandas stuff, etc
#     pass
# 
# def my_apply_logic():
#    # do database stuff
#     pass
# 
# Drop the button group into your GUI
# my_buttons = UnifiedButtonGroup(
#     parent=my_frame, 
#     on_validate=my_validation_logic, 
#     on_apply=my_apply_logic
# )
# my_buttons.pack()
## 

import tkinter as tk
from dataclasses import dataclass, field
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

@dataclass TkinterUniButtonMS ButtonConfig:
    text: str
    command: callable
    bg_color: str
    active_bg_color: str
    fg_color: str = "#FFFFFF"

@dataclass
TkinterUniButtonMS LinkConfig:
    """Configuration for the 'Linked' state (The Trap)"""
    trap_bg: str = "#7C3AED"    # Deep Purple
    btn_bg: str = "#8B5CF6"     # Lighter Purple
    text_color: str = "#FFFFFF"

@service_metadata(
name="LockingDualBtn",
version="1.0.0",
description="A unified button group (Left/Right/Link) where linking merges the actions.",
tags=["ui", "widget", "button"],
capabilities=["ui:gui"]
)
TkinterUniButtonMS LockingDualBtnMS(tk.Frame):
"""
A generic button group that can merge ANY two actions.
Pass the visual/functional definitions in via the config objects.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
        
self.left_cfg = self.config.get("left_btn")
self.right_cfg = self.config.get("right_btn")
self.link_cfg = self.config.get("link_config") or LinkConfig()
        
        self.is_linked = False
        self.default_bg = parent.cget("bg") # Fallback to parent background

        self._setup_ui()
        self._update_state()

    def _setup_ui(self):
        self.config(padx=4, pady=4)
        
        common_style = {"relief": "flat", "font": ("Segoe UI", 10, "bold"), "bd": 0, "cursor": "hand2"}

        # 1. Left Button (Generic)
        self.btn_left = tk.Button(self, command=lambda: self._execute("left"), **common_style)
        self.btn_left.pack(side="left", fill="y", padx=(0, 2))

        # 2. Link Toggle (The Chain)
        self.btn_link = tk.Button(self, text="&", width=3, command=self._toggle_link, **common_style)
        self.btn_link.pack(side="left", fill="y", padx=(0, 2))

        # 3. Right Button (Generic)
        self.btn_right = tk.Button(self, command=lambda: self._execute("right"), **common_style)
        self.btn_right.pack(side="left", fill="y")

    def _toggle_link(self):
        self.is_linked = not self.is_linked
        self._update_state()

    def _update_state(self):
        if self.is_linked:
            # --- LINKED STATE (The Trap) ---
            self.config(bg=self.link_cfg.trap_bg)
            
            # Both buttons look identical in the "Trap"
            for btn in (self.btn_left, self.btn_right, self.btn_link):
                btn.config(bg=self.link_cfg.btn_bg, fg=self.link_cfg.text_color, activebackground=self.link_cfg.trap_bg)
            
            # Keep original text
            self.btn_left.config(text=self.left_cfg.text)
            self.btn_right.config(text=self.right_cfg.text)

        else:
            # --- INDEPENDENT STATE ---
            try: self.config(bg=self.default_bg)
            except: self.config(bg="#f0f0f0") 

            # Restore Left Button
            self.btn_left.config(
                text=self.left_cfg.text, 
                bg=self.left_cfg.bg_color, 
                fg=self.left_cfg.fg_color,
                activebackground=self.left_cfg.active_bg_color
            )

            # Restore Right Button
            self.btn_right.config(
                text=self.right_cfg.text, 
                bg=self.right_cfg.bg_color, 
                fg=self.right_cfg.fg_color,
                activebackground=self.right_cfg.active_bg_color
            )

            # Restore Link Button (Neutral Gray)
            self.btn_link.config(bg="#E5E7EB", fg="#374151", activebackground="#D1D5DB")

    def _execute(self, source):
        if self.is_linked:
            # Chain them: Left then Right
            self.left_cfg.command()
            self.right_cfg.command()
        else:
        if source == "left": self.left_cfg.command()
        elif source == "right": self.right_cfg.command()

        if __name__ == "__main__":
        root = tk.Tk()
        btn1 = ButtonConfig("Save", lambda: print("Save"), "#444", "#555")
        btn2 = ButtonConfig("Run", lambda: print("Run"), "#444", "#555")
        svc = LockingDualBtnMS({"parent": root, "left_btn": btn1, "right_btn": btn2})
        print("Service ready:", svc)
        svc.pack(pady=20)
        root.mainloop()

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__TreeMapperMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TreeMapperMS
ENTRY_POINT: __TreeMapperMS.py
DEPENDENCIES: None
"""

import os
from pathlib import Path
from typing import Any, Dict, List, Set, Optional
from microservice_std_lib import service_metadata, service_endpoint
import datetime

# ==============================================================================
# USER CONFIGURATION: DEFAULT EXCLUSIONS
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    '.venv', 'env', 'venv', 'dist', 'build', '.DS_Store'
}
# ==============================================================================

@service_metadata(
name="TreeMapper",
version="1.0.0",
description="Generates ASCII-art style directory maps of the file system.",
tags=["filesystem", "map", "visualization"],
capabilities=["filesystem:read"]
)
class TreeMapperMS:
    """
The Cartographer: Generates ASCII-art style directory maps.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"root_path": "str", "additional_exclusions": "Set[str]", "use_default_exclusions": "bool"},
outputs={"tree_map": "str"},
description="Generates an ASCII tree map of the directory.",
tags=["filesystem", "visualization"],
side_effects=["filesystem:read"]
)
def generate_tree(self, 
                      root_path: str, 
                      additional_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> str:
        
        start_path = Path(root_path).resolve()
        if not start_path.exists(): return f"Error: Path '{root_path}' does not exist."

        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_EXCLUDES)
        if additional_exclusions:
            exclusions.update(additional_exclusions)

        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lines = [
            f"Project Map: {start_path.name}",
            f"Generated: {timestamp}",
            "-" * 40,
            f"üìÅ {start_path.name}/"
        ]

        self._walk(start_path, "", lines, exclusions)
        return "\n".join(lines)

    def _walk(self, directory: Path, prefix: str, lines: List[str], exclusions: Set[str]):
        try:
            children = sorted(
                [p for p in directory.iterdir() if p.name not in exclusions],
                key=lambda x: (x.is_file(), x.name.lower())
            )
        except PermissionError:
            lines.append(f"{prefix}‚îî‚îÄ‚îÄ üö´ [Permission Denied]")
            return

        count = len(children)
        for index, path in enumerate(children):
            is_last = (index == count - 1)
            connector = "‚îî‚îÄ‚îÄ " if is_last else "‚îú‚îÄ‚îÄ "
            
            if path.is_dir():
                lines.append(f"{prefix}{connector}üìÅ {path.name}/")
                extension = "    " if is_last else "‚îÇ   "
                self._walk(path, prefix + extension, lines, exclusions)
            else:
                lines.append(f"{prefix}{connector}üìÑ {path.name}")

if __name__ == "__main__":
svc = TreeMapperMS()
print("Service ready:", svc)

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__VectorFactoryMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _VectorFactoryMS
ENTRY_POINT: __VectorFactoryMS.py
DEPENDENCIES: pip install chromadb faiss-cpu numpy
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pip install chromadb faiss-cpu numpy"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _VectorFactoryMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import os
import uuid
import logging
import shutil
from typing import List, Dict, Any, Optional, Protocol, Union
from pathlib import Path
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("VectorFactory")
# ==============================================================================

# --- Interface Definition ---

class VectorFactoryMS(Protocol):
    """The contract that all vector backends must fulfill."""
    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]) -> None:
        ...
    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        ...
    def count(self) -> int:
        ...
    def clear(self) -> None:
        ...

# --- Implementation 1: FAISS (Local, Fast, RAM-heavy) ---

class FaissVectorStore:
    def __init__(self, index_path: str, dimension: int):
        import numpy as np
        import faiss # Lazy import
        self.np = np
        self.faiss = faiss
        
        self.index_path = index_path
        self.dim = dimension
        self.metadata_store = []
        
        # Load or Create
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
            # Load metadata (simple JSON sidecar for this implementation)
            meta_path = index_path + ".meta.json"
            if os.path.exists(meta_path):
                import json
                with open(meta_path, 'r') as f:
                    self.metadata_store = json.load(f)
        else:
            self.index = faiss.IndexFlatL2(dimension)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        
        vecs = self.np.array(embeddings).astype("float32")
        self.index.add(vecs)
        self.metadata_store.extend(metadatas)
        self._save()

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        if self.index.ntotal == 0: return []
        
        q_vec = self.np.array([query_vector]).astype("float32")
        distances, indices = self.index.search(q_vec, k)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and idx < len(self.metadata_store):
                entry = self.metadata_store[idx].copy()
                entry['score'] = float(dist) # FAISS returns L2 distance (lower is better)
                results.append(entry)
        return results

    def count(self) -> int:
        return self.index.ntotal

    def clear(self):
        self.index.reset()
        self.metadata_store = []
        self._save()

    def _save(self):
        self.faiss.write_index(self.index, self.index_path)
        import json
        with open(self.index_path + ".meta.json", 'w') as f:
            json.dump(self.metadata_store, f)

# --- Implementation 2: ChromaDB (Persistent, Feature-rich) ---

class ChromaVectorStore:
    def __init__(self, persist_dir: str, collection_name: str):
        import chromadb # Lazy import
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(collection_name)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        # Chroma requires unique IDs
        ids = [str(uuid.uuid4()) for _ in embeddings]
        
        # Ensure metadata is flat (Chroma limitation on nested dicts)
        clean_metas = [{k: str(v) if isinstance(v, (list, dict)) else v for k, v in m.items()} for m in metadatas]
        
        # Chroma expects 'documents' usually, but we handle logic upstream. 
        # We pass empty strings for 'documents' if purely vector-based, 
        # or map content from metadata if available.
        docs = [m.get("content", "") for m in metadatas]

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            metadatas=clean_metas,
            documents=docs
        )

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        results = self.collection.query(
            query_embeddings=[query_vector],
            n_results=k
        )
        
        output = []
        if not results['ids']: return []

        # Unpack Chroma's columnar response format
        for i in range(len(results['ids'][0])):
            entry = results['metadatas'][0][i].copy()
            entry['score'] = results['distances'][0][i]
            entry['id'] = results['ids'][0][i]
            output.append(entry)
        return output

    def count(self) -> int:
        return self.collection.count()

    def clear(self):
        # Chroma doesn't have a truncate command, so we delete the collection
        name = self.collection.name
        self.client.delete_collection(name)
        self.collection = self.client.get_or_create_collection(name)

# --- The Factory ---

@service_metadata(
name="VectorFactory",
version="1.0.0",
description="Factory for creating VectorFactoryMS instances (FAISS, Chroma).",
tags=["vector", "factory", "db"],
capabilities=["filesystem:read", "filesystem:write"]
)
class VectorFactoryMS:
    """
The Switchboard: Returns the appropriate VectorFactoryMS implementation
based on configuration.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"backend": "str", "config": "Dict"},
outputs={"store": "VectorFactoryMS"},
description="Creates and returns a configured VectorFactoryMS instance.",
tags=["vector", "create"],
side_effects=[]
)
def create(self, backend: str, config: Dict[str, Any]) -> VectorFactoryMS:
"""
:param backend: 'faiss' or 'chroma'
        :param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)
        """
        log.info(f"Initializing Vector Store: {backend.upper()}")
        
        if backend == "faiss":
            path = config.get("path", "vector_index.bin")
            dim = config.get("dim", 384)
            return FaissVectorStore(path, dim)
            
        elif backend == "chroma":
            path = config.get("path", "./chroma_db")
            name = config.get("collection", "default_collection")
            return ChromaVectorStore(path, name)
            
        else:
            raise ValueError(f"Unknown backend: {backend}")

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing VectorFactoryMS ---")
    
    # 1. Mock Data (dim=4 for simplicity)
    mock_vec = [0.1, 0.2, 0.3, 0.4]
    mock_meta = {"text": "Hello World", "source": "test"}
    
    # 2. Test FAISS
    print("\n[Testing FAISS]")
    factory = VectorFactoryMS()
    print("Service ready:", factory)
    try:
    faiss_store = factory.create("faiss", {"path": "test_faiss.index", "dim": 4})
        faiss_store.add([mock_vec], [mock_meta])
        print(f"Count: {faiss_store.count()}")
        res = faiss_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("test_faiss.index"): os.remove("test_faiss.index")
        if os.path.exists("test_faiss.index.meta.json"): os.remove("test_faiss.index.meta.json")
    except ImportError:
        print("Skipping FAISS test (library not installed)")
# 3. Test Chroma
print("\n[Testing Chroma]")
try:
    chroma_store = factory.create("chroma", {"path": "./test_chroma_db", "collection": "test_col"})
    chroma_store.add([mock_vec], [mock_meta])
    print(f"Count: {chroma_store.count()}")
    res = chroma_store.search(mock_vec, 1)
    print(f"Search Result: {res[0]['text']}")
    # Cleanup
    if os.path.exists("./test_chroma_db"): shutil.rmtree("./test_chroma_db")
except ImportError:
    print("Skipping Chroma test (library not installed)")
except Exception as e:
    print(f"Chroma Error: {e}")

--------------------------------------------------------------------------------
FILE: _TaskORCHESTRATOR\_sandbox_test_directory\microservices_test_sample\__WebScraperMS.py.txt
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _WebScraperMS
ENTRY_POINT: __WebScraperMS.py
DEPENDENCIES: pip install httpx readability-lxml
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pip install httpx readability-lxml"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _WebScraperMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import httpx
import logging
import asyncio
from typing import Optional, Dict, Any, List
from readability import Document
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
TIMEOUT_SECONDS = 15.0

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("WebScraper")
# ==============================================================================

@service_metadata(
name="WebScraper",
version="1.0.0",
description="Fetches URLs and extracts main content using Readability (stripping ads/nav).",
tags=["scraper", "web", "readability"],
capabilities=["network:outbound", "compute"]
)
class WebScraperMS:
    """
The Reader: Fetches URLs and extracts the main content using Readability.
Strips ads, navbars, and boilerplate to return clean text for LLMs.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.headers = {"User-Agent": USER_AGENT}

    @service_endpoint(
    inputs={"url": "str"},
    outputs={"data": "Dict[str, Any]"},
    description="Fetches and cleans a URL.",
    tags=["scraper", "read"],
    side_effects=["network:outbound"]
    )
    def scrape(self, url: str) -> Dict[str, Any]:
    """
    Synchronous wrapper for fetching and cleaning a URL.
    Returns: {
            "url": str,
            "title": str,
            "content": str (The main body text),
            "html": str (The raw HTML of the main content area)
        }
        """
return asyncio.run(self._scrape_async(url))

    async def _scrape_async(self, url: str) -> Dict[str, Any]:
        log.info(f"Fetching: {url}")
        
        async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=TIMEOUT_SECONDS) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                log.error(f"HTTP Error {e.response.status_code}: {e}")
                raise
            except httpx.RequestError as e:
                log.error(f"Request failed: {e}")
                raise

        # Parse with Readability
        try:
            doc = Document(response.text)
            title = doc.title()
            # Summary() returns the HTML of the main content area
            clean_html = doc.summary() 
            
            # Convert HTML content to plain text for the LLM
            # (Simple strip tags implementation, for better results use BeautifulSoup)
            clean_text = self._strip_tags(clean_html)
            
            log.info(f"Successfully scraped '{title}' ({len(clean_text)} chars)")
            
            return {
                "url": url,
                "title": title,
                "content": clean_text,
                "html": clean_html
            }
        except Exception as e:
            log.error(f"Parsing failed: {e}")
            raise

    def _strip_tags(self, html: str) -> str:
def _strip_tags(self, html: str) -> str:
        """
        Removes HTML tags to leave only the readable text.
        """
        import re
        # Remove scripts and styles
        html = re.sub(r'<(script|style).*?>.*?</\1>', '', html, flags=re.DOTALL)
        # Remove tags
        text = re.sub(r'<[^>]+>', ' ', html)
        # Collapse whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# --- Independent Test Block ---
if __name__ == "__main__":
    scraper = WebScraperMS()
    print("Service ready:", scraper)
    
    # Test URL (Example: Python's PEP 8)
    target_url = "https://peps.python.org/pep-0008/"
    
    print(f"--- Scraping {target_url} ---")
    try:
        data = scraper.scrape(target_url)
        print(f"\nTitle: {data['title']}")
        print(f"Content Preview:\n{data['content'][:500]}...")
        print(f"\nTotal Length: {len(data['content'])} characters")
    except Exception as e:
        print(f"Scrape failed: {e}")

--------------------------------------------------------------------------------
FILE: _TempServerMAKER\.gitignore
--------------------------------------------------------------------------------
Python

pycache/
*.pyc
*.pyo
*.pyd
.egg
.build/
.dist/
.eggs/
.pytest_cache/
.mypy_cache/
Envs

.venv/
venv/
Editors

.vscode/
.idea/
Logs & artifacts

_logs/
*.log
Application-generated files
Ignore reports generated by the web UI's export buttons

file_dump_export.html
ai_report.txt
OS

.DS_Store
Thumbs.db

--------------------------------------------------------------------------------
FILE: _TempServerMAKER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _TempServerMAKER\README.md
--------------------------------------------------------------------------------
Ôªøüñ•Ô∏è T E M P S E R V E R M A K E R ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

TempServerMAKER is a simple, standalone tool that serves a local project directory as an interactive web page. It's designed for securely feeding your project's source code to Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) systems.

It runs as a portable Python application with a graphical user interface to manage the server, making it easy to run on Windows, macOS, and Linux without external dependencies.

‚ú® FEATURES ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    Desktop GUI Control: A simple Tkinter-based GUI to easily select the project folder, set the port, and start/stop/restart the server.

    Interactive Web UI: Serves a clean, modern web interface with a collapsible file tree, tabbed file viewer, and in-browser AST (Abstract Syntax Tree) generation for Python files.

    Advanced Exporting: Export the entire project from the web UI in multiple formats:

        AI Report (.txt): A single, comprehensive text file with all file contents.

        Project Codebase Log (JSONL): A detailed machine-readable log of all files.

        AST Tree Log (JSONL): A complete AST dump for all Python files.

    Client-Server Architecture: Separates the Python backend (app.py) from the frontend (HTML, CSS, JS), making the code easy to maintain and customize.

    No Installation Needed: Runs on any system with a standard Python 3 (with Tkinter) installation.

‚öôÔ∏è HOW TO USE ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ñ∫ On Windows ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    Place the _Temp-Server-Maker/ folder (containing the start_app.py script) inside your project directory.

    Open the _Temp-Server-Maker/ folder and double-click the start_app.py file to start the server.

    The desktop GUI will open. Click "Start" and your web browser will launch automatically.

    When you are finished, click "Quit" in the GUI or close the terminal window.

‚ñ∫ On macOS or Linux (Ubuntu, etc.) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ First-Time Setup (One Time Only): Before running the script for the first time, you need to make it executable.

    Open your Terminal application.

    Navigate into the _Temp-Server-Maker/ folder.

    Run the following command and press Enter:
    Bash

    chmod +x start_app.py

Running the Application:

    Open a terminal in the _Temp-Server-Maker/ folder.

    Run the application by typing:
    Bash

    ./start_app.py

    The desktop GUI will open. Click "Start" and your web browser will launch automatically.

    To stop the server, click "Quit" in the GUI or go back to the terminal and press Ctrl+C.

üí° HOW IT WORKS ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê This application is built with Python's standard http.server and tkinter libraries. It operates on a client-server model:

    Back-End (_src/app.py): A Python script that runs the Tkinter GUI and acts as a web server. When started, it scans the project directory, serves the frontend files, and provides a simple API for project data and exports.

    Front-End (_src/index.html, _src/style.css, _src/index.js): A modern, single-page web application that runs in your browser. It fetches the file data from the Python backend and dynamically renders the interactive UI.
--------------------------------------------------------------------------------
FILE: _TempServerMAKER\requirements.txt
--------------------------------------------------------------------------------
# Standard Library dependencies:
# os, sys, json, http.server, socket, threading, webbrowser, ast, pathlib
#
# No external pip packages required for core functionality.
--------------------------------------------------------------------------------
FILE: _TempServerMAKER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo Setting up System Thinker Environment...
python -m venv .venv
call .venv\Scripts\activate
pip install -r requirements.txt
echo Setup Complete. Run via: python -m src.app
pause
--------------------------------------------------------------------------------
FILE: _TempServerMAKER\src\app.py
--------------------------------------------------------------------------------
from __future__ import annotations
import argparse, http.server, json, mimetypes, os, socket, socketserver, sys, threading, time, webbrowser, hashlib, ast
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse, parse_qs, unquote

try:
    import tkinter as tk
    import tkinter.filedialog as fd
except Exception:
    tk = None
    fd = None

# ------------------------------ Utilities ------------------------------ #
class QuietTCPServer(socketserver.TCPServer):
    allow_reuse_address = True

def pick_free_port(host: str = "127.0.0.1") -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind((host, 0))
        return s.getsockname()[1]

def now_iso() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%S%z")

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

# ------------------------------ JSONL Report Builder (From app_OG.py) ------------------------------ #

def build_project_codebase_log(root: Path, max_bytes: int = 0, include_binaries: bool = False) -> str:
    """
    Assemble a JSONL stream (as a single string) with:
      - meta
      - file_tree section (dir/file entries)
      - files section (per-file headers + content)
    """
    lines = []
    lines.append(_jsonl({
        "type": "meta",
        "root": str(root),
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "format": "jsonl",
        "sections": ["file_tree","files"],
    }))

    # file tree
    lines.append(_jsonl({"type": "section", "name": "file_tree"}))
    for p in _iter_all_paths(root):
        lines.append(_jsonl({
            "type": "dir" if p.is_dir() else "file",
            "path": _rel(root, p),
        }))

    # files
    lines.append(_jsonl({"type": "section", "name": "files"}))
    for p in _iter_all_paths(root):
        if p.is_dir():
            continue
        rel = _rel(root, p)
        lang = _guess_lang(p)
        try:
            raw = p.read_bytes()
        except Exception as e:
            lines.append(_jsonl({"type": "file_error", "path": rel, "error": str(e)}))
            continue

        if lang == "binary" and not include_binaries:
            try:
                st = p.stat()
                lines.append(_jsonl({
                    "type": "file_header",
                    "path": rel,
                    "size": st.st_size,
                    "sha256": _sha256_bytes(raw),
                    "language": lang,
                    "skipped": "binary"
                }))
            except Exception:
                lines.append(_jsonl({"type": "file_header", "path": rel, "language": lang, "skipped": "binary"}))
            continue

        text = raw.decode("utf-8", errors="replace")
        truncated = False
        if max_bytes and len(text.encode("utf-8")) > max_bytes:
            # rough truncation by characters (OK for logs)
            text = text[:max_bytes]
            truncated = True

        try:
            st = p.stat()
            size = st.st_size
        except Exception:
            size = len(raw)

        lines.append(_jsonl({
            "type": "file",
            "path": rel,
            "size": size,
            "sha256": _sha256_bytes(raw),
            "language": lang,
            "truncated": truncated,
            "content": text
        }))

    return "".join(lines)


def build_ast_tree_log(root: Path) -> str:
    """
    Assemble a JSONL stream (as a single string) for *.py files:
      - meta
      - ast_file header per file
      - ast_node records (flat walk)
    """
    lines = []
    lines.append(_jsonl({
        "type": "meta",
        "root": str(root),
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "format": "jsonl",
        "scope": "*.py only",
    }))

    for p in _iter_all_paths(root):
        if p.is_dir() or p.suffix != ".py":
            continue
        rel = _rel(root, p)
        try:
            src = p.read_text(encoding="utf-8", errors="replace")
        except Exception as e:
            lines.append(_jsonl({"type": "ast_error", "path": rel, "error": str(e)}))
            continue

        lines.append(_jsonl({"type": "ast_file", "path": rel}))
        for item in _iter_ast_nodes(src, rel):
            lines.append(_jsonl(item))

    return "".join(lines)


def _iter_all_paths(root: Path):
    for p in sorted(root.rglob("*")):
        # skip junky dirs if you like:
        if any(part in {".git", "__pycache__", ".venv", "venv", "node_modules"} for part in p.parts):
            continue
        yield p

def _rel(root: Path, p: Path) -> str:
    return str(p.relative_to(root)).replace("\\", "/")

def _sha256_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()

def _guess_lang(path: Path) -> str:
    mt = (mimetypes.guess_type(str(path))[0] or "")
    if path.suffix == ".py": return "python"
    if path.suffix in {".js", ".mjs", ".cjs"}: return "javascript"
    if path.suffix == ".ts": return "typescript"
    if path.suffix in {".json"}: return "json"
    if path.suffix in {".css"}: return "css"
    if path.suffix in {".html", ".htm"}: return "html"
    if mt.startswith("text/"): return "text"
    return "binary"

def _iter_ast_nodes(py_source: str, relpath: str):
    try:
        tree = ast.parse(py_source)
    except Exception as e:
        yield {"type": "ast_error", "path": relpath, "error": str(e)}
        return
    for node in ast.walk(tree):
        # core shape that‚Äôs useful yet compact
        item = {
            "type": "ast_node",
            "path": relpath,
            "node": type(node).__name__,
        }
        # common fields (best-effort)
        for attr in ("name", "id", "arg", "attr"):
            if hasattr(node, attr):
                item["name"] = getattr(node, attr)
                break
        if hasattr(node, "lineno"):
            item["lineno"] = getattr(node, "lineno")
        if hasattr(node, "col_offset"):
            item["col"] = getattr(node, "col_offset")
        if hasattr(node, "end_lineno"):
            item["end_lineno"] = getattr(node, "end_lineno")
        yield item

def _jsonl(obj: dict) -> str:
    return json.dumps(obj, ensure_ascii=False, separators=(",", ":")) + "\n"

# ------------------------------ Built-in HTML (blue theme renderer) ------------------------------ #
DEFAULT_INDEX_HTML = """<!doctype html>
<html>
<head>
  <meta charset=\"utf-8\" />
  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />
  <title>Temp Server Maker</title>
  <style>
    :root { --bg:#0b1220; --panel:#0f1b31; --panel-2:#132240; --text:#e8f1ff; --muted:#a7c1ff; --accent:#4da3ff; --accent-2:#72b6ff; --border:#21406e; --chip:#0d1a33; }
    html,body{height:100%}
    body{margin:0;background:var(--bg);color:var(--text);font:14px/1.5 system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,Arial}
    .topbar{display:flex;align-items:center;gap:.5rem;padding:.75rem 1rem;background:linear-gradient(180deg,#0e1a31,#0a1325);position:sticky;top:0;z-index:2;border-bottom:1px solid var(--border)}
    .title{font-weight:700;letter-spacing:.3px;margin-right:auto}
    .btn{background:var(--panel);color:var(--text);border:1px solid var(--border);padding:.45rem .7rem;border-radius:10px;cursor:pointer}
    .btn:hover{background:var(--panel-2);border-color:var(--accent)}
    .btn.primary{background:#12407e;border-color:#2f6eb9}
    .btn.primary:hover{background:#15519f}
    .grid{display:grid;grid-template-columns:320px 1fr;height:calc(100vh - 52px)}
    .pane{overflow:auto}
    .left{background:var(--panel);border-right:1px solid var(--border)}
    .right{background:var(--panel-2)}
    .section{padding:.75rem 1rem;border-bottom:1px solid var(--border)}
    .meta{display:grid;grid-template-columns:repeat(2,minmax(0,1fr));gap:.35rem .75rem;font-size:13px;color:var(--muted)}
    .chip{display:inline-block;background:var(--chip);border:1px solid var(--border);border-radius:999px;padding:.15rem .5rem;margin-right:.35rem}
    .tree{padding:.5rem 0 1rem 0}
    .node{user-select:none}
    .entry{display:flex;align-items:center;gap:.5rem;padding:.25rem .5rem;cursor:pointer;border-radius:8px}
    .entry:hover{background:rgba(114,182,255,.08)}
    .twist{width:1rem;text-align:center;font-weight:700;color:var(--accent-2)}
    .folder{color:var(--accent)}
    .file{color:#cbd9f3}
    .viewer{padding:1rem 1.25rem}
    .path{font-weight:700;margin-bottom:.25rem;color:#d9e6ff}
    .meta-line{font-size:12px;color:var(--muted);margin-bottom:.75rem}
    pre{white-space:pre-wrap;word-wrap:break-word;background:#0a1529;border:1px solid var(--border);border-radius:10px;padding:.75rem}
  </style>
</head>
<body>
  <div class=\"topbar\"> <div class=\"title\">Temp Server Maker</div>
    <button class=\"btn\" id=\"btn-refresh\">Refresh</button>
    <a class=\"btn\" id=\"btn-export\" href=\"#\">Export JSONL</a>
    <button class=\"btn\" id=\"btn-copy\">Copy Snapshot</button>
    <button class=\"btn primary\" id=\"btn-open\">Open in New Tab</button>
  </div>
  <div class=\"grid\">
    <div class=\"pane left\">
      <div class=\"section\" id=\"meta\"></div>
      <div class=\"section\"><div class=\"chip\" id=\"count\"></div><div class=\"chip\" id=\"bytes\"></div></div>
      <div class=\"pane tree\" id=\"tree\"></div>
    </div>
    <div class=\"pane right viewer\" id=\"viewer\"><div style=\"opacity:.7\">Select a file to preview its contents.</div></div>
  </div>
  <script id=\"meta-json\" type=\"application/json\"></script>
  <script id=\"files-json\" type=\"application/json\"></script>
  <script>
    function $(id){return document.getElementById(id)}
    function jget(id){const el=$(id);try{return JSON.parse(el.textContent||'{}')}catch{return{}}}
    const meta=jget('meta-json');
    const files=jget('files-json');
    const metaEl=$('meta');
    metaEl.innerHTML=`<div class=meta><div><b>Root:</b> ${meta.root||'?'} </div><div><b>Generated:</b> ${meta.generated_at||'?'}</div></div>`;
    $('count').textContent = `files: ${meta.count ?? meta.file_count ?? (files?.length || 0)}`;
    $('bytes').textContent = `bytes: ${new Intl.NumberFormat().format(meta.total_bytes || 0)}`;

    const exportUrl='/__api__/report/project-codebase-log?max_bytes=0';
    $('btn-export').href=exportUrl;
    $('btn-open').onclick=()=>window.open('/', '_blank');
    $('btn-refresh').onclick=async()=>{try{await fetch('/__api__/refresh',{method:'POST'});location.reload()}catch{location.reload()}}
    $('btn-copy').onclick=async()=>{try{const r=await fetch(exportUrl);const t=await r.text();await navigator.clipboard.writeText(t);$('btn-copy').textContent='Copied!';setTimeout(()=>$('btn-copy').textContent='Copy Snapshot',1200)}catch(e){alert('Copy failed: '+e)}}

    function buildTree(paths){const root={name:'',children:new Map(),files:[]};for(const f of paths){const parts=f.path.split('/');let cur=root;for(let i=0;i<parts.length-1;i++){const seg=parts[i];if(!cur.children.has(seg))cur.children.set(seg,{name:seg,children:new Map(),files:[]});cur=cur.children.get(seg)}cur.files.push(f)}return root}
    function el(tag,cls,text){const e=document.createElement(tag);if(cls)e.className=cls;if(text!=null)e.textContent=text;return e}
    function renderTree(container,node,depth=0){for(const [name,child] of [...node.children.entries()].sort()){const row=el('div','node');const entry=el('div','entry');const twist=el('div','twist','‚ñ∏');const label=el('div','folder',name);entry.style.paddingLeft=(depth*12+6)+'px';entry.append(twist,label);const body=el('div');body.style.display='none';entry.onclick=()=>{const open=body.style.display==='none';body.style.display=open?'block':'none';twist.textContent=open?'‚ñæ':'‚ñ∏'};row.append(entry,body);container.append(row);renderTree(body,child,depth+1)}for(const f of node.files.sort((a,b)=>a.path.localeCompare(b.path))){const row=el('div','node');const entry=el('div','entry');entry.style.paddingLeft=(depth*12+24)+'px';const dot=el('div','twist','‚Ä¢');const label=el('div','file',f.path.split('/').pop());entry.append(dot,label);entry.onclick=()=>showFile(f);row.append(entry);container.append(row)}}

    async function loadChunk(relPath, offset){const v=$('viewer');const limit=200000;const res=await fetch(`/__api__/file?path=${encodeURIComponent(relPath)}&offset=${offset}&limit=${limit}`);const j=await res.json();if(!j.ok){v.append(el('div',null,j.error||'Preview failed.'));return}if(offset===0){v.innerHTML='';v.append(el('div','path',j.path));v.append(el('div','meta-line',`size=${j.size} mime=${j.mime||'?'} (${j.chunk_start}-${j.chunk_end})`))}if(typeof j.text==='string'){const pre=el('pre');pre.textContent=j.text;v.append(pre)}else{v.append(el('div',null,'Binary file ‚Äî no text preview.'))}if(j.more){const btn=el('button','btn','Load more‚Ä¶');btn.onclick=()=>loadChunk(j.path,j.chunk_end);v.append(btn)}}

    function showFile(f){const v=$('viewer');v.innerHTML='';v.append(el('div','path',f.path));v.append(el('div','meta-line',`size=${f.size} mime=${f.mime||'?'}${f.truncated?' (truncated)':''}`));if(typeof f.text==='string'){v.append(el('pre',null,f.text));if(f.truncated){const loadMore=el('button','btn','Load more‚Ä¶');loadMore.onclick=()=>{loadChunk(f.path,(f.text||'').length)};v.append(loadMore)}}else{loadChunk(f.path,0)}}

    const treeRoot=buildTree(files||[]);renderTree($('tree'),treeRoot);
  </script>
</body>
</html>"""

# ------------------------------ Core Application ------------------------- #
class App:
    MAX_TEXT_BYTES = 400_000  # inline preview cap

    def __init__(self, directory: Path, host: str, port: int,
                 open_browser: bool, keep_index: bool,
                 headless: bool, write_report: bool) -> None:
        self.root_dir = Path(directory).resolve()
        self.host = host
        self.port = port
        self.open_browser = open_browser
        self.keep_index = keep_index  # if True and no index.html, write DEFAULT_INDEX_HTML to disk
        self.headless = headless
        self.write_report_flag = write_report
        self.httpd: QuietTCPServer | None = None
        self.thread: threading.Thread | None = None
        self.url: str = ""
        self.template_path = self.root_dir / "index.html"
        self.logs_dir = self.root_dir / "_logs" / "_temp-server"
        ensure_dir(self.logs_dir)
        self.log_path = self.logs_dir / f"server_{int(time.time())}.log"
        self.report_path = self.logs_dir / "ai_report.txt"

    def set_root_dir(self, new_root: Path) -> None:
        new_root = Path(new_root).resolve()
        if not new_root.is_dir():
            raise FileNotFoundError(f"directory does not exist: {new_root}")
        self.root_dir = new_root
        self.template_path = self.root_dir / "index.html"
        self.logs_dir = self.root_dir / "_logs" / "_temp-server"
        ensure_dir(self.logs_dir)
        self.log_path = self.logs_dir / f"server_{int(time.time())}.log"
        self.report_path = self.logs_dir / "ai_report.txt"
        self._log(f"Root changed to: {self.root_dir}")

    def _file_record(self, p: Path) -> dict:
        rel = str(p.relative_to(self.root_dir))
        size = p.stat().st_size
        mtype, _ = mimetypes.guess_type(rel)
        mtype = mtype or "application/octet-stream"
        rec: dict[str, object] = {"path": rel, "size": size, "mime": mtype}
        # Text sniff with bounded preview; no reliance on mimetype
        try:
            with p.open('rb') as f:
                chunk = f.read(self.MAX_TEXT_BYTES + 1)
            is_text_like = b"\x00" not in chunk[:4096]
            if is_text_like:
                preview = chunk[: self.MAX_TEXT_BYTES]
                try:
                    rec["text"] = preview.decode("utf-8")
                except UnicodeDecodeError:
                    rec["text"] = preview.decode("latin-1", errors="replace")
                if size > self.MAX_TEXT_BYTES or len(chunk) > self.MAX_TEXT_BYTES:
                    rec["truncated"] = True
            else:
                rec["binary"] = True
        except Exception:
            rec["binary"] = True
        return rec

    def _parse_ast(self, p: Path) -> list:
        try:
            tree = ast.parse(p.read_text(encoding="utf-8", errors="replace"))
            nodes = []
            for node in ast.walk(tree):
                nodes.append({
                    "type": node.__class__.__name__,
                    "lineno": getattr(node, "lineno", None),
                    "col_offset": getattr(node, "col_offset", None),
                    "end_lineno": getattr(node, "end_lineno", None),
                    "end_col_offset": getattr(node, "end_col_offset", None),
                    "fields": {field: getattr(node, field, None).__class__.__name__ for field in node._fields}
                })
            return nodes
        except Exception as e:
            return [{"error": str(e)}]

    def _gather_files(self) -> list[Path]:
        files: list[Path] = []
        for p in self.root_dir.rglob("*"):
            if not p.is_file():
                continue
            if any(part.startswith('.') for part in p.parts) or self.logs_dir in p.parents or p.name == "app.py" or p.name == "app_merged.py":
                continue
            files.append(p)
        files.sort()
        return files

    def _load_template(self) -> str:
        # Look for assets relative to this script (app.py)
        assets_dir = Path(__file__).parent / "assets"
        index_path = assets_dir / "index.html"
        
        if index_path.exists():
            html = index_path.read_text(encoding="utf-8")
            
            # Inline CSS
            css_path = assets_dir / "style.css"
            if css_path.exists():
                css_content = css_path.read_text(encoding="utf-8")
                html = html.replace('<link rel="stylesheet" href="style.css">', f'<style>{css_content}</style>')
            
            # Inline JS
            js_path = assets_dir / "index.js"
            if js_path.exists():
                js_content = js_path.read_text(encoding="utf-8")
                html = html.replace('<script src="index.js"></script>', f'<script>{js_content}</script>')
                
            return html

        # Fallback to built-in if src/assets/index.html is missing
        self._log("assets/index.html not found ‚Äî serving built-in template.")
        return DEFAULT_INDEX_HTML

    def generate_populated_html(self) -> str:
        files = [self._file_record(p) for p in self._gather_files()]
        meta = {"generated_at": now_iso(), "root": str(self.root_dir), "count": len(files), "total_bytes": sum(f.get("size", 0) for f in files)}
        def safe_json(obj: object) -> str:
            return json.dumps(obj, ensure_ascii=False).replace("</", "<\/").replace("<\\/", "<\\/")
        template_content = self._load_template()
        populated_html = template_content.replace('<script id="meta-json" type="application/json"></script>', f'<script id="meta-json" type="application/json">{safe_json(meta)}</script>')
        populated_html = populated_html.replace('<script id="files-json" type="application/json"></script>', f'<script id="files-json" type="application/json">{safe_json(files)}</script>')
        return populated_html

    def write_ai_report(self) -> None:
        if not self.write_report_flag:
            return
        files = [self._file_record(p) for p in self._gather_files()]
        meta = {"generated_at": now_iso(), "root": str(self.root_dir), "count": len(files), "total_bytes": sum(f.get("size", 0) for f in files)}
        lines = [json.dumps(meta, ensure_ascii=False)]
        for f in files:
            lines += ["\n" + "=" * 80, f"FILE: {f['path']}", "-" * 80, f.get("text", "[binary or omitted]") if isinstance(f.get("text"), str) else "[binary or omitted]"]
        self.report_path.write_text("\n".join(lines), encoding="utf-8")

    def refresh(self) -> None:
        self.write_ai_report()
        self._log("Refreshed AI report")

    # --------------------------- Server ----------------------------- #
    def _make_server(self) -> tuple[QuietTCPServer, str]:
        # NOTE:
        # Do NOT os.chdir() here.
        # - It changes process-global CWD (can break other tools running in the same process)
        # - It's unnecessary because the handler is already configured with directory=...
        port = self.port if self.port != 0 else pick_free_port(self.host)
        # If we auto-picked a port (self.port was 0), lock in the actual bound port.
        self.port = port
        app_ref = self

        class Handler(http.server.SimpleHTTPRequestHandler):
            def _set_cors(self):
                self.send_header('Access-Control-Allow-Origin', '*')
                self.send_header('Access-Control-Allow-Methods', 'GET,POST,OPTIONS')
                self.send_header('Access-Control-Allow-Headers', 'Content-Type')

            def _send_json(self, obj: object, code: int = 200):
                data = json.dumps(obj, ensure_ascii=False).encode('utf-8')
                self.send_response(code)
                self._set_cors()
                self.send_header('Content-Type', 'application/json; charset=utf-8')
                self.send_header('Content-Length', str(len(data)))
                self.end_headers()
                self.wfile.write(data)

            def _send_text(self, text: str, code: int = 200, ctype: str = 'text/plain; charset=utf-8'):
                b = text.encode('utf-8', errors='replace')
                self.send_response(code)
                self._set_cors()
                self.send_header('Content-Type', ctype)
                self.send_header('Content-Length', str(len(b)))
                self.end_headers()
                self.wfile.write(b)

            def do_OPTIONS(self):
                self.send_response(204)
                self._set_cors()
                self.end_headers()

            def do_GET(self):
                if self.path.startswith('/__api__/status'):
                    tpl_exists = app_ref.template_path.exists()
                    return self._send_json({
                        "ok": True,
                        "root": str(app_ref.root_dir),
                        "host": app_ref.host,
                        "port": app_ref.port,
                        "url": app_ref.url,
                        "template_path": str(app_ref.template_path),
                        "template_exists": tpl_exists,
                        "keep_index": app_ref.keep_index,
                    })

                if self.path.startswith('/__api__/'):
                    # Project Codebase Log (JSONL) - From app_OG.py
                    if self.path.startswith('/__api__/report/project-codebase-log'):
                        q = parse_qs(urlparse(self.path).query)
                        max_bytes = int(q.get("max_bytes", ["0"])[0] or "0")
                        include_binaries = q.get("include_binaries", ["0"])[0] == "1"

                        text = build_project_codebase_log(app_ref.root_dir.resolve(), max_bytes=max_bytes, include_binaries=include_binaries)
                        return self._send_text(text, 200, 'text/plain; charset=utf-8')

                    # AST Tree Log (JSONL) - From app_OG.py
                    if self.path.startswith('/__api__/report/ast-tree-log'):
                        text = build_ast_tree_log(app_ref.root_dir.resolve())
                        return self._send_text(text, 200, 'text/plain; charset=utf-8')

                    # AST Endpoint - From app_OG.py
                    if self.path.startswith('/__api__/ast'):
                        q = parse_qs(urlparse(self.path).query)
                        raw = q.get("path", [""])[0]
                        rel = unquote(raw)

                        # force relative path under the served root
                        abs_p = (app_ref.root_dir / rel).resolve()
                        root = app_ref.root_dir.resolve()
                        try:
                            abs_p.relative_to(root)
                        except ValueError:
                             return self._send_json({"error": "path outside root"}, 400)

                        if abs_p.suffix != ".py":
                            return self._send_json({"error": "Only .py files are supported"}, 400)
                        if not abs_p.is_file():
                            return self._send_json({"error": "File not found"}, 404)

                        ast_data = app_ref._parse_ast(abs_p)
                        return self._send_json(ast_data)

                    # Chunked File Loader - From app_newUI.py
                    if self.path.startswith('/__api__/file'):
                        q = parse_qs(urlparse(self.path).query)
                        rel = (q.get('path', [''])[0] or '').strip()
                        try:
                            offset = int(q.get('offset', ['0'])[0] or '0')
                            limit = int(q.get('limit', [str(app_ref.MAX_TEXT_BYTES)])[0])
                            if limit <= 0 or limit > 2_000_000:
                                limit = app_ref.MAX_TEXT_BYTES
                        except Exception:
                            offset, limit = 0, app_ref.MAX_TEXT_BYTES
                        if not rel:
                            return self._send_json({"ok": False, "error": "missing path"}, 400)
                        abs_path = (app_ref.root_dir / rel).resolve()
                        try:
                            abs_path.relative_to(app_ref.root_dir)
                        except Exception:
                            return self._send_json({"ok": False, "error": "path outside root"}, 400)
                        if not abs_path.exists() or not abs_path.is_file():
                            return self._send_json({"ok": False, "error": "not a file"}, 404)
                        total = abs_path.stat().st_size
                        mtype, _ = mimetypes.guess_type(abs_path.name)
                        mtype = mtype or 'application/octet-stream'
                        with abs_path.open('rb') as f:
                            f.seek(max(0, offset))
                            chunk = f.read(max(0, limit))
                        is_text_like = b"\x00" not in chunk[:4096]
                        payload = {
                            "ok": True,
                            "path": rel,
                            "size": total,
                            "mime": mtype,
                            "chunk_start": max(0, offset),
                            "chunk_end": max(0, offset) + len(chunk),
                            "more": (max(0, offset) + len(chunk)) < total,
                        }
                        if is_text_like:
                            try:
                                payload["text"] = chunk.decode('utf-8')
                            except UnicodeDecodeError:
                                payload["text"] = chunk.decode('latin-1', errors='replace')
                        return self._send_json(payload)
                    
                    # Fallback for other /__api__/ calls
                    return self._send_json({"error": "API endpoint not found"}, 404)

                if self.path == '/' or self.path == '/index.html':
                    html_content = app_ref.generate_populated_html()
                    return self._send_text(html_content, ctype='text/html; charset=utf-8')

                # Use SimpleHTTPRequestHandler's default file serving
                # (which now correctly uses the 'directory' kwarg)
                return super().do_GET()

            def do_POST(self):
                if self.path.startswith('/__api__/refresh'):
                    app_ref.refresh()
                    return self._send_json({"ok": True, "url": app_ref.url, "root": str(app_ref.root_dir)})
                if self.path.startswith('/__api__/shutdown'):
                    self._send_json({"ok": True})
                    threading.Thread(target=app_ref.shutdown, daemon=True).start()
                    return
                return super().do_POST()

        httpd = QuietTCPServer((self.host, port), Handler)
        url = f"http://{self.host}:{port}/"
        return httpd, url

    def start(self) -> None:
        self.write_ai_report()
        self.httpd, self.url = self._make_server()
        self.thread = threading.Thread(target=self.httpd.serve_forever, daemon=True)
        self.thread.start()
        self._log(f"Serving {self.root_dir} at {self.url}")
        if self.open_browser:
            webbrowser.open(self.url)

    def shutdown(self) -> None:
        if self.httpd:
            self.httpd.shutdown()
            self.httpd.server_close()
            self.httpd = None
        if self.thread:
            self.thread.join(timeout=1.0)
        self._log("Server stopped")

    def _log(self, msg: str) -> None:
        line = f"[{now_iso()}] {msg}\n"
        if sys.stdout:
            sys.stdout.write(line)
            sys.stdout.flush()
        try:
            with open(self.log_path, "a", encoding="utf-8") as f:
                f.write(line)
        except Exception:
            pass

    def run_headless(self) -> int:
        try:
            self.start()
            while self.thread and self.thread.is_alive():
                time.sleep(0.2)
        except KeyboardInterrupt:
            print("\n[info] Keyboard interrupt received, shutting down.")
        finally:
            self.shutdown()
        return 0

    def run_gui(self) -> int:
        if tk is None:
            print("[error] Tkinter is not available on this system.")
            return 1
        
        # Store the original CWD
        original_cwd = Path.cwd()
        
        root = tk.Tk()
        root.title("Temp Server Maker")
        status_var = tk.StringVar(value=f"Directory: {self.root_dir}")
        tk.Label(root, textvariable=status_var, anchor='w', padx=10, pady=5).pack(fill='x')

        dir_row = tk.Frame(root, padx=10, pady=5); dir_row.pack(fill='x')
        tk.Label(dir_row, text="Root:", width=6, anchor='w').pack(side='left')
        dir_var = tk.StringVar(value=str(self.root_dir))
        dir_entry = tk.Entry(dir_row, textvariable=dir_var); dir_entry.pack(side='left', fill='x', expand=True, padx=(0,8))
        def choose_dir():
            path = fd.askdirectory(initialdir=dir_var.get() or str(self.root_dir))
            if not path:
                return
            was_running = self.httpd is not None
            if was_running:
                self.shutdown()
            try:
                self.set_root_dir(Path(path))
            except Exception as e:
                status_var.set(f"[error] {e}")
                if was_running: # Try to restart in the old dir
                    try: self.start()
                    except Exception: pass
                return
            dir_var.set(str(self.root_dir))
            status_var.set(f"Directory: {self.root_dir}")
            if was_running:
                try: self.start()
                except Exception as e: status_var.set(f"[error] Restart failed: {e}")
        tk.Button(dir_row, text="Choose Folder‚Ä¶", command=choose_dir).pack(side='left')

        net_row = tk.Frame(root, padx=10, pady=5); net_row.pack(fill='x')
        auto_port_var = tk.BooleanVar(value=(self.port == 0))
        def toggle_port_state():
            if auto_port_var.get(): port_entry.configure(state='disabled')
            else: port_entry.configure(state='normal')
        tk.Checkbutton(net_row, text="Auto port", variable=auto_port_var, command=toggle_port_state).pack(side='left')
        tk.Label(net_row, text="Port:", padx=6).pack(side='left')
        port_var = tk.StringVar(value=str(self.port or 8000))
        port_entry = tk.Entry(net_row, width=8, textvariable=port_var); port_entry.pack(side='left')
        toggle_port_state()

        ctrls = tk.Frame(root, padx=10, pady=5); ctrls.pack(fill='x')
        def start_server():
            try:
                self.port = 0 if auto_port_var.get() else int(port_var.get())
                if not (0 <= self.port <= 65535):
                    raise ValueError("port out of range")
            except Exception:
                status_var.set("[error] Invalid port")
                return
            typed_root = Path(os.path.expanduser(dir_var.get() or "."))
            if typed_root.resolve() != self.root_dir:
                try:
                    self.set_root_dir(typed_root)
                except Exception as e:
                    status_var.set(f"[error] {e}")
                    return
            if self.httpd:
                self.shutdown()
            try:
                self.start()
                status_var.set(f"Server running at {self.url}")
            except Exception as e:
                status_var.set(f"[error] Start failed: {e}")
                
        def stop_server():
            self.shutdown(); status_var.set("Server stopped.")
        def restart_server():
            stop_server(); start_server()
        def open_browser_now():
            if self.httpd and self.url: webbrowser.open(self.url)
            else: status_var.set("Server not running.")
        def quit_app():
            try: self.shutdown()
            finally: root.destroy()
            
        root.protocol("WM_DELETE_WINDOW", quit_app) # Handle window close button
            
        for text_label, cmd in (("Start", start_server),("Stop", stop_server),("Restart", restart_server),("Open", open_browser_now),("Quit", quit_app)):
            tk.Button(ctrls, text=text_label, command=cmd).pack(side='left', padx=(0,6))
        
        try:
            root.mainloop()
        except KeyboardInterrupt:
            print("\n[info] GUI interrupted, shutting down.")
            quit_app()
        finally:
            # Restore original CWD on exit
            try: os.chdir(original_cwd)
            except Exception: pass
            
        return 0

# ------------------------------ CLI ------------------------------ #

def parse_args(argv: list[str] | None = None):
    p = argparse.ArgumentParser(description='Serve a folder quickly with a minimal UI')
    p.add_argument('-d', '--directory', default='.', help='Directory to serve (default: .)')
    p.add_argument('--host', default='127.0.0.1', help='Host/IP to bind (default: 127.0.0.1)')
    p.add_argument('-p', '--port', type=int, default=8000, help='Port to bind (0 for auto)')
    p.add_argument('--open', dest='open_browser', action='store_true', help='Open browser on start')
    p.add_argument('--no-open', dest='open_browser', action='store_false', help='Do not open browser')
    p.set_defaults(open_browser=True)
    p.add_argument('--no-gui', dest='no_gui', action='store_true', help='Run headless (no Tk)')
    p.add_argument('--keep-index', action='store_true', help='If no index.html exists, write built-in template to disk')
    p.add_argument('--report', action='store_true', help='Also write ai_report.txt to _logs/_temp-server/')
    p.add_argument('--keep-file', action='store_true', help='Keep generated files on exit (deprecated)')
    return p.parse_args(argv)

def main(argv: list[str] | None = None) -> int:
    args = parse_args(argv)
    
    # Store CWD at startup
    original_cwd = Path.cwd()
    
    try:
        directory = (Path(__file__).resolve().parent if args.directory == '.' else Path(os.path.expanduser(args.directory)).resolve())
        if not directory.is_dir():
            print(f"[error] directory does not exist: {directory}")
            return 2
        
        app = App(directory=directory, host=args.host, port=args.port, open_browser=args.open_browser, keep_index=args.keep_index, headless=args.no_gui, write_report=args.report)
        
        return app.run_headless() if app.headless else app.run_gui()

    finally:
        # Ensure CWD is restored even on headless exit
        try: os.chdir(original_cwd)
        except Exception: pass


if __name__ == "__main__":
    raise SystemExit(main())



--------------------------------------------------------------------------------
FILE: _TempServerMAKER\src\app_OG.py
--------------------------------------------------------------------------------
from __future__ import annotations
import argparse, contextlib, http.server, json, mimetypes, os, socket, socketserver, sys, threading, time, webbrowser, ast, hashlib
from datetime import datetime
from pathlib import Path
from urllib.parse import urlparse, parse_qs, unquote


try:
    import tkinter as tk
    import tkinter.filedialog as fd
except Exception:
    tk = None
    fd = None

# ------------------------------ Utilities ------------------------------ #
class QuietTCPServer(socketserver.TCPServer):
    allow_reuse_address = True

def pick_free_port(host: str = "127.0.0.1") -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind((host, 0))
        return s.getsockname()[1]

def now_iso() -> str:
    return time.strftime("%Y-%m-%dT%H:%M:%S%z")

def ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)

# ---------- helpers ----------
def build_project_codebase_log(root: Path, max_bytes: int = 0, include_binaries: bool = False) -> str:
    """
    Assemble a JSONL stream (as a single string) with:
      - meta
      - file_tree section (dir/file entries)
      - files section (per-file headers + content)
    """
    lines = []
    lines.append(_jsonl({
        "type": "meta",
        "root": str(root),
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "format": "jsonl",
        "sections": ["file_tree","files"],
    }))

    # file tree
    lines.append(_jsonl({"type": "section", "name": "file_tree"}))
    for p in _iter_all_paths(root):
        lines.append(_jsonl({
            "type": "dir" if p.is_dir() else "file",
            "path": _rel(root, p),
        }))

    # files
    lines.append(_jsonl({"type": "section", "name": "files"}))
    for p in _iter_all_paths(root):
        if p.is_dir():
            continue
        rel = _rel(root, p)
        lang = _guess_lang(p)
        try:
            raw = p.read_bytes()
        except Exception as e:
            lines.append(_jsonl({"type": "file_error", "path": rel, "error": str(e)}))
            continue

        if lang == "binary" and not include_binaries:
            try:
                st = p.stat()
                lines.append(_jsonl({
                    "type": "file_header",
                    "path": rel,
                    "size": st.st_size,
                    "sha256": _sha256_bytes(raw),
                    "language": lang,
                    "skipped": "binary"
                }))
            except Exception:
                lines.append(_jsonl({"type": "file_header", "path": rel, "language": lang, "skipped": "binary"}))
            continue

        text = raw.decode("utf-8", errors="replace")
        truncated = False
        if max_bytes and len(text.encode("utf-8")) > max_bytes:
            # rough truncation by characters (OK for logs)
            text = text[:max_bytes]
            truncated = True

        try:
            st = p.stat()
            size = st.st_size
        except Exception:
            size = len(raw)

        lines.append(_jsonl({
            "type": "file",
            "path": rel,
            "size": size,
            "sha256": _sha256_bytes(raw),
            "language": lang,
            "truncated": truncated,
            "content": text
        }))

    return "".join(lines)


def build_ast_tree_log(root: Path) -> str:
    """
    Assemble a JSONL stream (as a single string) for *.py files:
      - meta
      - ast_file header per file
      - ast_node records (flat walk)
    """
    lines = []
    lines.append(_jsonl({
        "type": "meta",
        "root": str(root),
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "format": "jsonl",
        "scope": "*.py only",
    }))

    for p in _iter_all_paths(root):
        if p.is_dir() or p.suffix != ".py":
            continue
        rel = _rel(root, p)
        try:
            src = p.read_text(encoding="utf-8", errors="replace")
        except Exception as e:
            lines.append(_jsonl({"type": "ast_error", "path": rel, "error": str(e)}))
            continue

        lines.append(_jsonl({"type": "ast_file", "path": rel}))
        for item in _iter_ast_nodes(src, rel):
            lines.append(_jsonl(item))

    return "".join(lines)


def _iter_all_paths(root: Path):
    for p in sorted(root.rglob("*")):
        # skip junky dirs if you like:
        if any(part in {".git", "__pycache__", ".venv", "venv", "node_modules"} for part in p.parts):
            continue
        yield p

def _rel(root: Path, p: Path) -> str:
    return str(p.relative_to(root)).replace("\\", "/")

def _sha256_bytes(b: bytes) -> str:
    h = hashlib.sha256()
    h.update(b)
    return h.hexdigest()

def _guess_lang(path: Path) -> str:
    mt = (mimetypes.guess_type(str(path))[0] or "")
    if path.suffix == ".py": return "python"
    if path.suffix in {".js", ".mjs", ".cjs"}: return "javascript"
    if path.suffix == ".ts": return "typescript"
    if path.suffix in {".json"}: return "json"
    if path.suffix in {".css"}: return "css"
    if path.suffix in {".html", ".htm"}: return "html"
    if mt.startswith("text/"): return "text"
    return "binary"

def _iter_ast_nodes(py_source: str, relpath: str):
    try:
        tree = ast.parse(py_source)
    except Exception as e:
        yield {"type": "ast_error", "path": relpath, "error": str(e)}
        return
    for node in ast.walk(tree):
        # core shape that‚Äôs useful yet compact
        item = {
            "type": "ast_node",
            "path": relpath,
            "node": type(node).__name__,
        }
        # common fields (best-effort)
        for attr in ("name", "id", "arg", "attr"):
            if hasattr(node, attr):
                item["name"] = getattr(node, attr)
                break
        if hasattr(node, "lineno"):
            item["lineno"] = getattr(node, "lineno")
        if hasattr(node, "col_offset"):
            item["col"] = getattr(node, "col_offset")
        if hasattr(node, "end_lineno"):
            item["end_lineno"] = getattr(node, "end_lineno")
        yield item

def _jsonl(obj: dict) -> str:
    return json.dumps(obj, ensure_ascii=False, separators=(",", ":")) + "\n"

# ---------------------------- Core Application ------------------------- #
class App:
    MAX_TEXT_BYTES = 400_000

    def __init__(self, directory: Path, host: str, port: int,
                 open_browser: bool, keep_index: bool,
                 headless: bool, write_report: bool) -> None:
        self.root_dir = Path(directory).resolve()
        self.host = host
        self.port = port
        self.open_browser = open_browser
        self.keep_index = keep_index
        self.headless = headless
        self.write_report_flag = write_report
        self.httpd: QuietTCPServer | None = None
        self.thread: threading.Thread | None = None
        self.url: str = ""
        self.template_path = self.root_dir / "index.html"
        self.logs_dir = self.root_dir / "_logs" / "_temp-server"
        ensure_dir(self.logs_dir)
        self.log_path = self.logs_dir / f"server_{int(time.time())}.log"
        self.report_path = self.logs_dir / "ai_report.txt"

    def _gather_files(self) -> list[Path]:
        files: list[Path] = []
        for p in self.root_dir.rglob("*"):
            if not p.is_file():
                continue
            if any(part.startswith('.') for part in p.parts) or self.logs_dir in p.parents or p.name == "app.py":
                continue
            files.append(p)
        files.sort()
        return files

    def _file_record(self, p: Path) -> dict:
        rel = str(p.relative_to(self.root_dir))
        size = p.stat().st_size
        mtype, _ = mimetypes.guess_type(rel)
        mtype = mtype or "application/octet-stream"
        rec: dict[str, object] = {"path": rel, "size": size, "mime": mtype}
        if (mtype.startswith("text/") or size <= self.MAX_TEXT_BYTES):
            try:
                data = p.read_bytes()
                if b"\x00" not in data[:4096]:
                    rec["text"] = data[:self.MAX_TEXT_BYTES].decode("utf-8", errors="replace")
            except Exception:
                pass
        return rec
    
    def _parse_ast(self, p: Path) -> list:
        try:
            tree = ast.parse(p.read_text(encoding="utf-8", errors="replace"))
            nodes = []
            for node in ast.walk(tree):
                nodes.append({
                    "type": node.__class__.__name__,
                    "lineno": getattr(node, "lineno", None),
                    "col_offset": getattr(node, "col_offset", None),
                    "end_lineno": getattr(node, "end_lineno", None),
                    "end_col_offset": getattr(node, "end_col_offset", None),
                    "fields": {field: getattr(node, field, None).__class__.__name__ for field in node._fields}
                })
            return nodes
        except Exception as e:
            return [{"error": str(e)}]



    def generate_populated_html(self) -> str:
        if not self.template_path.exists():
            return "<html><body><h1>Error</h1><p>index.html not found in the root directory.</p></body></html>"
        files = [self._file_record(p) for p in self._gather_files()]
        meta = {"generated_at": now_iso(), "root": str(self.root_dir), "file_count": len(files), "total_bytes": sum(f.get("size", 0) for f in files)}
        def safe_json(obj: object) -> str:
            return json.dumps(obj, ensure_ascii=False).replace("</", "<\\/")
        template_content = self.template_path.read_text(encoding="utf-8")
        populated_html = template_content.replace('<script id="meta-json" type="application/json"></script>', f'<script id="meta-json" type="application/json">{safe_json(meta)}</script>')
        populated_html = populated_html.replace('<script id="files-json" type="application/json"></script>', f'<script id="files-json" type="application/json">{safe_json(files)}</script>')
        return populated_html

    def write_ai_report(self) -> None:
        if not self.write_report_flag:
            return
        files = [self._file_record(p) for p in self._gather_files()]
        meta = {"generated_at": now_iso(), "root": str(self.root_dir), "file_count": len(files), "total_bytes": sum(f.get("size", 0) for f in files)}
        lines = [json.dumps(meta, ensure_ascii=False)]
        for f in files:
            lines += ["\n" + "=" * 80, f"FILE: {f['path']}", "-" * 80, f.get("text") if isinstance(f.get("text"), str) else "[binary or omitted]"]
        self.report_path.write_text("\n".join(lines), encoding="utf-8")

    def refresh(self) -> None:
        self.write_ai_report()
        self._log("Refreshed AI report")

    def _make_server(self) -> tuple[QuietTCPServer, str]:
        os.chdir(self.root_dir)
        port = self.port if self.port != 0 else pick_free_port(self.host)
        app_ref = self

        class Handler(http.server.SimpleHTTPRequestHandler):
            def _set_cors(self):
                self.send_header('Access-Control-Allow-Origin', '*')
                self.send_header('Access-Control-Allow-Methods', 'GET,POST,OPTIONS')
                self.send_header('Access-Control-Allow-Headers', 'Content-Type')

            def _send_json(self, obj: object, code: int = 200):
                data = json.dumps(obj, ensure_ascii=False).encode('utf-8')
                self.send_response(code)
                self._set_cors()
                self.send_header('Content-Type', 'application/json; charset=utf-8')
                self.send_header('Content-Length', str(len(data)))
                self.end_headers()
                self.wfile.write(data)

            def _send_text(self, text: str, code: int = 200, ctype: str = 'text/plain; charset=utf-8'):
                b = text.encode('utf-8')
                self.send_response(code)
                self._set_cors()
                self.send_header('Content-Type', ctype)
                self.send_header('Content-Length', str(len(b)))
                self.end_headers()
                self.wfile.write(b)

            def do_OPTIONS(self):
                self.send_response(204)
                self._set_cors()
                self.end_headers()

            def do_POST(self):
                if self.path.startswith('/__api__/refresh'):
                    app_ref.refresh()
                    return self._send_json({"ok": True, "url": app_ref.url, "root": str(app_ref.root_dir)})
                if self.path.startswith('/__api__/shutdown'):
                    self._send_json({"ok": True})
                    threading.Thread(target=app_ref.shutdown, daemon=True).start()
                    return
                return super().do_POST()

            def do_GET(self):
                if self.path.startswith('/__api__/'):
                    # Project Codebase Log (JSONL)
                    if self.path.startswith('/__api__/report/project-codebase-log'):
                        # parse query
                        q = parse_qs(urlparse(self.path).query)
                        max_bytes = int(q.get("max_bytes", ["0"])[0] or "0")
                        include_binaries = q.get("include_binaries", ["0"])[0] == "1"

                        text = build_project_codebase_log(app_ref.root_dir.resolve(), max_bytes=max_bytes, include_binaries=include_binaries)
                        # served as plain text; client names the file
                        return self._send_text(text, 200, 'text/plain; charset=utf-8')

                    # AST Tree Log (JSONL)
                    if self.path.startswith('/__api__/report/ast-tree-log'):
                        text = build_ast_tree_log(app_ref.root_dir.resolve())
                        return self._send_text(text, 200, 'text/plain; charset=utf-8')

                    # AST Endpoint
                    if self.path.startswith('/__api__/ast'):
                        q = parse_qs(urlparse(self.path).query)
                        raw = q.get("path", [""])[0]
                        rel = unquote(raw)

                        # force relative path under the served root
                        abs_p = (app_ref.root_dir / rel).resolve()
                        root = app_ref.root_dir.resolve()
                        if not str(abs_p).startswith(str(root) + os.sep) and abs_p != root:
                            return self._send_json({"error": "path outside root"}, 400)

                        if abs_p.suffix != ".py":
                            return self._send_json({"error": "Only .py files are supported"}, 400)
                        if not abs_p.is_file():
                            return self._send_json({"error": "File not found"}, 404)

                        ast_data = app_ref._parse_ast(abs_p)
                        return self._send_json(ast_data)


                    if self.path == '/__api__/ping':
                        return self._send_json({"ok": True, "time": now_iso()})
                    if self.path == '/__api__/meta':
                        files = [app_ref._file_record(p) for p in app_ref._gather_files()]
                        meta = {"generated_at": now_iso(), "root": str(app_ref.root_dir), "file_count": len(files), "total_bytes": sum(f.get('size', 0) for f in files)}
                        return self._send_json(meta)
                    if self.path == '/__api__/files':
                        files = [app_ref._file_record(p) for p in app_ref._gather_files()]
                        return self._send_json(files)
                    return self._send_json({"error": "not found"}, 404)

                if self.path == '/' or self.path == '/index.html':
                    html_content = app_ref.generate_populated_html()
                    return self._send_text(html_content, ctype='text/html; charset=utf-8')

                return super().do_GET()

        httpd = QuietTCPServer((self.host, port), Handler)
        url = f"http://{self.host}:{port}/"
        return httpd, url

    def start(self) -> None:
        self.write_ai_report()
        self.httpd, self.url = self._make_server()
        self.thread = threading.Thread(target=self.httpd.serve_forever, daemon=True)
        self.thread.start()
        self._log(f"Serving {self.root_dir} at {self.url}")
        if self.open_browser:
            webbrowser.open(self.url)

    def shutdown(self) -> None:
        if self.httpd:
            self.httpd.shutdown()
            self.httpd.server_close()
            self.httpd = None
        self._log("Server stopped")

    def _log(self, msg: str) -> None:
        line = f"[{now_iso()}] {msg}\n"
        sys.stdout.write(line)
        sys.stdout.flush()
        with open(self.log_path, "a", encoding="utf-8") as f:
            f.write(line)

    def run_headless(self) -> int:
        self.start()
        print(f"[info] Serving {self.root_dir} at {self.url}")
        try:
            while self.thread and self.thread.is_alive():
                time.sleep(0.2)
        except KeyboardInterrupt:
            print("\n[info] Keyboard interrupt received, shutting down.")
        finally:
            self.shutdown()
        return 0

    def run_gui(self) -> int:
        if tk is None:
            print("[error] Tkinter is not available on this system.")
            return 1
        root = tk.Tk()
        root.title("Temp Server Maker")
        status_var = tk.StringVar(value=f"Directory: {self.root_dir}")
        tk.Label(root, textvariable=status_var, anchor='w', padx=10, pady=5).pack(fill='x')
        ctrls = tk.Frame(root, padx=10, pady=5)
        ctrls.pack(fill='x')
        def start_server():
            print("Server is starting...")
            self.start()
            status_var.set(f"Server running at {self.url}")
        def stop_server():
            self.shutdown()
            status_var.set("Server stopped.")
        for text, cmd in (("Start", start_server), ("Stop", stop_server), ("Open", lambda: webbrowser.open(self.url)), ("Quit", root.destroy)):
            tk.Button(ctrls, text=text, command=cmd).pack(side='left')
        root.mainloop()
        return 0

def parse_args(argv: list[str] | None = None) -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Temp Server Maker ‚Äî simple HTTP server with rich HTML index")
    p.add_argument('-d', '--directory', default='.', help='Directory to serve (default: .)')
    p.add_argument('--host', default='127.0.0.1', help='Host/IP to bind (default: 127.0.0.1)')
    p.add_argument('-p', '--port', type=int, default=8000, help='Port (0 = random; default: 8000)')
    p.add_argument('--open', action='store_true', help='Open default browser to the server URL')
    p.add_argument('--no-gui', action='store_true', help='Run headless (no Tk window)')
    p.add_argument('--report', action='store_true', help='Also write ai_report.txt to _logs/_temp-server/')
    p.add_argument('--keep-file', action='store_true', help='Keep generated files on exit (deprecated)')
    return p.parse_args(argv)

def main(argv: list[str] | None = None) -> int:
    args = parse_args(argv)
    directory = (Path(__file__).resolve().parent if args.directory == '.' else Path(os.path.expanduser(args.directory)).resolve())
    if not directory.is_dir():
        print(f"[error] directory does not exist: {directory}")
        return 2
    app = App(directory=directory, host=args.host, port=args.port, open_browser=args.open, headless=args.no_gui, write_report=args.report, keep_index=args.keep_file)
    return app.run_headless() if app.headless else app.run_gui()

if __name__ == "__main__":
    raise SystemExit(main())
--------------------------------------------------------------------------------
FILE: _TempServerMAKER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TempServerMAKER\src\assets\index.html
--------------------------------------------------------------------------------
<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Index</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="header">
    <div class="title">Index</div>
    <div class="spacer"></div>
    <input id="search" class="search" placeholder="Filter files‚Ä¶" style="max-width:320px"/>
    <button id="expand" class="btn">Expand All</button>
    <button id="collapse" class="btn">Collapse All</button>
    <button id="export-html" class="btn">Export HTML</button>
    <button id="export-report" class="btn">Export AI Report</button>
    <button id="btn-export-codebase-log" class="btn">Export Project Codebase Log</button>
    <button id="btn-export-ast-log" class="btn">Export AST Tree Log</button>
  </div>
  <div class="main">
    <aside class="sidebar"><div class="tree"></div></aside>
    <section class="content"></section>
  </div>
  <script id="meta-json" type="application/json"></script>
  <script id="files-json" type="application/json"></script>
  <script src="index.js"></script>
</body>
</html>

--------------------------------------------------------------------------------
FILE: _TempServerMAKER\src\assets\index.js
--------------------------------------------------------------------------------
document.addEventListener("DOMContentLoaded", () => {
    // --- DATA & DOM ---
    const META = JSON.parse(document.getElementById("meta-json").textContent);
    const FILES = JSON.parse(document.getElementById("files-json").textContent);
    const contentEl = document.querySelector(".content");
    const treeEl = document.querySelector(".tree");

    document.getElementById('btn-export-codebase-log')
        ?.addEventListener('click', async () => {
            try { await fetchAndDownload('/__api__/report/project-codebase-log', 'project-codebase-log.txt'); }
            catch (e) { alert('Export failed: ' + e.message); }
        });

        document.getElementById('btn-export-ast-log')
        ?.addEventListener('click', async () => {
            try { await fetchAndDownload('/__api__/report/ast-tree-log', 'ast-tree-log.txt'); }
            catch (e) { alert('Export failed: ' + e.message); }
        });


    // --- HELPERS ---
    const humanBytes = (num) => {
        const units = ['B', 'KB', 'MB', 'GB'];
        let i = 0;
        while (num >= 1024 && i < units.length - 1) {
            num /= 1024;
            i++;
        }
        return `${num.toFixed(i ? 1 : 0)} ${units[i]}`;
    };
    const escapeHtml = (str) => str.replace(/[&<>]/g, (char) => ({
        '&': '&amp;', '<': '&lt;', '>': '&gt;'
    })[char]);
    const path_to_id = (path) => `card_${path.replace(/[^a-zA-Z0-9_-]/g, '_')}`;

    // --- LOGIC ---

    function makeTree(files) {
        const root = {};
        for (const file of files) {
            let currentNode = root;
            const parts = file.path.split('/');
            for (let i = 0; i < parts.length; i++) {
                const part = parts[i];
                currentNode.children = currentNode.children || {};
                currentNode.children[part] = currentNode.children[part] || {};
                currentNode = currentNode.children[part];
                if (i === parts.length - 1) {
                    currentNode.file = file;
                }
            }
        }
        return root;
    }

    function renderTree(node) {
        const ul = document.createElement('ul');
        const entries = Object.keys(node.children || {}).sort();
        for (const key of entries) {
            const childNode = node.children[key];
            const li = document.createElement('li');

            if (childNode.file) {
                const a = document.createElement('a');
                const cardId = path_to_id(childNode.file.path);
                a.href = `#${cardId}`;
                a.textContent = key;
                a.className = 'file';
                a.onclick = (e) => {
                    e.preventDefault();
                    const targetEl = document.getElementById(cardId);
                    if (targetEl) {
                        targetEl.scrollIntoView({ behavior: 'smooth', block: 'start' });
                        targetEl.style.outline = '1px solid var(--accent)';
                        setTimeout(() => {
                            targetEl.style.outline = 'none';
                        }, 1200);
                    }
                };
                li.appendChild(a);
            } else {
                const div = document.createElement('div');
                div.textContent = key;
                div.className = 'folder';
                const nestedUl = renderTree(childNode);
                nestedUl.hidden = true;
                div.onclick = () => {
                    div.classList.toggle('open');
                    nestedUl.hidden = !nestedUl.hidden;
                };
                li.appendChild(div);
                li.appendChild(nestedUl);
            }
            ul.appendChild(li);
        }
        return ul;
    }

    function renderAllCards() {
        contentEl.innerHTML = '';
        const overviewCard = `<div class="card"><h3>Overview</h3><div class="body"><div class="meta"><div><b>Root:</b> <code>${META.root}</code></div><div><b>Generated:</b> ${new Date(META.generated_at).toLocaleString()}</div><div><b>Files:</b> ${META.file_count}</div><div><b>Total:</b> ${humanBytes(META.total_bytes)}</div></div></div></div>`;
        contentEl.insertAdjacentHTML('beforeend', overviewCard);
        FILES.forEach(file => {
            const cardId = path_to_id(file.path);
            const isPythonFile = file.path.endsWith('.py');
            const tabs = isPythonFile ? `<div class="tab-container"><button class="tab-btn active" data-tab="source">Source Code</button><button class="tab-btn" data-tab="ast" data-file="${file.path}">AST</button></div>` : '';
            
            const fileCard = `<div class="card" id="${cardId}"><h3>${file.path}</h3>${tabs}<div class="body"><div class="meta"><div><b>Size:</b> ${humanBytes(file.size)}</div><div><b>MIME:</b> ${file.mime}</div></div><div class="tab-content source-content active">${typeof file.text === 'string'
                ? `<details><summary>Contents (${humanBytes(file.text.length)})</summary><pre>${escapeHtml(file.text)}</pre></details>`
                : `<div class="small">(binary or too large to preview)</div>`}</div><div class="tab-content ast-content" data-file="${file.path}">Loading AST...</div></div></div>`;
            contentEl.insertAdjacentHTML('beforeend', fileCard);
        });
        
        // Add event listeners for the new tabs
        document.querySelectorAll('.tab-btn[data-tab="ast"]').forEach(btn => {
            btn.addEventListener('click', async (e) => {
                const filePath = e.target.getAttribute('data-file');
                const cardBody = e.target.closest('.card').querySelector('.body');
                const astContent = cardBody.querySelector('.ast-content');

                // Toggle active class on buttons
                cardBody.querySelectorAll('.tab-btn').forEach(b => b.classList.remove('active'));
                e.target.classList.add('active');

                // Toggle active class on content
                cardBody.querySelector('.tab-content.active').classList.remove('active');
                astContent.classList.add('active');

                // Fetch AST if not already loaded
                if (astContent.dataset.loaded !== 'true') {
                    try {
                        const response = await fetch(`/__api__/ast?path=${encodeURIComponent(filePath)}`);
                        const astData = await response.json();
                        astContent.innerHTML = renderAST(astData);
                        astContent.dataset.loaded = 'true';
                    } catch (error) {
                        astContent.innerHTML = `<div class="small">Error loading AST: ${error.message}</div>`;
                    }
                }
            });
        });
        document.querySelectorAll('.tab-btn[data-tab="source"]').forEach(btn => {
            btn.addEventListener('click', (e) => {
                const cardBody = e.target.closest('.card').querySelector('.body');
                // Toggle active class on buttons
                cardBody.querySelectorAll('.tab-btn').forEach(b => b.classList.remove('active'));
                e.target.classList.add('active');
                // Toggle active class on content
                cardBody.querySelectorAll('.tab-content').forEach(c => c.classList.remove('active'));
                cardBody.querySelector('.source-content').classList.add('active');
            });
        });
    }

    function renderAST(nodes) {
        if (!Array.isArray(nodes) || nodes.length === 0) {
            return `<div class="small">No AST data available.</div>`;
        }

        const astTree = document.createElement('ul');
        astTree.classList.add('ast-tree');

        // Simple rendering for now; a more robust renderer would handle nested structures.
        nodes.forEach(node => {
            const li = document.createElement('li');
            li.innerHTML = `<span class="ast-node">${node.type}</span> <span class="ast-field">(${node.lineno}:${node.col_offset})</span>`;
            astTree.appendChild(li);
        });

        return astTree.outerHTML;
    }

    // --- NEW EXPORT FUNCTIONS ---

    /**
     * Creates a downloadable self-contained HTML file of the current page.
     */
    function exportHtml() {
        const cleanHtml = `<!DOCTYPE html>\n${document.documentElement.outerHTML}`;
        const blob = new Blob([cleanHtml], {
            type: 'text/html'
        });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = 'file_dump_export.html';
        document.body.appendChild(a); // Required for Firefox
        a.click();
        document.body.removeChild(a);
        setTimeout(() => URL.revokeObjectURL(url), 100);
    }

    /**
     * Creates a downloadable AI-friendly text report of all file contents.
     */
    function exportAiReport() {
        const lines = [JSON.stringify(META, null, 2)];
        for (const file of FILES) {
            lines.push('\n' + '='.repeat(80));
            lines.push(`FILE: ${file.path}`);
            lines.push('-'.repeat(80));
            lines.push(typeof file.text === 'string' ? file.text : '[binary or omitted]');
        }
        const reportText = lines.join('\n');
        const blob = new Blob([reportText], {
            type: 'text/plain'
        });
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url;
        a.download = 'ai_report.txt';
        document.body.appendChild(a); // Required for Firefox
        a.click();
        document.body.removeChild(a);
        setTimeout(() => URL.revokeObjectURL(url), 100);
    }

    function downloadBlob(filename, blob) {
        const url = URL.createObjectURL(blob);
        const a = document.createElement('a');
        a.href = url; a.download = filename; a.click();
        URL.revokeObjectURL(url);
        }

        async function fetchAndDownload(path, filename) {
        const res = await fetch(path);
        if (!res.ok) throw new Error(`HTTP ${res.status}`);
        const blob = await res.blob();
        downloadBlob(filename, blob);
        }

    // --- EVENT LISTENERS ---
    document.getElementById('expand').onclick = () => document.querySelectorAll('details').forEach(d => d.open = true);
    document.getElementById('collapse').onclick = () => document.querySelectorAll('details').forEach(d => d.open = false);
    document.getElementById('export-html').onclick = exportHtml;
    document.getElementById('export-report').onclick = exportAiReport;
    document.getElementById('search').addEventListener('input', (e) => {
        const query = e.target.value.toLowerCase();
        document.querySelectorAll('.card').forEach(card => {
            const h3 = card.querySelector('h3');
            if (!h3 || h3.textContent === 'Overview') return;
            card.style.display = h3.textContent.toLowerCase().includes(query) ? '' : 'none';
        });
    });

    // --- INITIALIZE ---
    document.title = `Index of ${META.root}`;
    document.querySelector('.title').textContent = `Index of ${META.root}`;
    renderAllCards();
    treeEl.innerHTML = '';
    const fileTree = makeTree(FILES);
    treeEl.appendChild(renderTree(fileTree));
});
--------------------------------------------------------------------------------
FILE: _TempServerMAKER\src\assets\style.css
--------------------------------------------------------------------------------
:root {
  --bg: #0b0d10;
  --panel: #12161b;
  --muted: #8aa0b5;
  --text: #e8eef5;
  --accent: #6cb2ff;
  --border: #233241;
}

* {
  box-sizing: border-box;
}

html, body {
  height: 100%;
}

body {
  margin: 0;
  background: var(--bg);
  color: var(--text);
  font: 14px/1.4 system-ui, Segoe UI, Roboto, Arial;
}

.header {
  display: flex;
  gap: 12px;
  align-items: center;
  padding: 10px 14px;
  border-bottom: 1px solid var(--border);
  background: var(--panel);
  position: sticky;
  top: 0;
  z-index: 10;
}

.header .title {
  font-weight: 600;
}

.header .spacer {
  flex: 1;
}

.btn {
  background: #18212a;
  color: var(--text);
  border: 1px solid var(--border);
  padding: 6px 10px;
  border-radius: 8px;
  cursor: pointer;
}

.btn:hover {
  border-color: var(--accent);
}

.main {
  display: grid;
  grid-template-columns: 320px 1fr;
  min-height: calc(100vh - 48px);
}

.sidebar {
  border-right: 1px solid var(--border);
  background: #0f141a;
  padding: 10px;
  overflow: auto;
}

.content {
  padding: 12px;
  overflow: auto;
}

/* File Tree */
.tree ul {
  list-style: none;
  margin: 0;
  padding-left: 14px;
}

.tree li {
  margin: 2px 0;
}

.tree .folder {
  cursor: pointer;
}

.tree .folder::before {
  content: '‚ñ∏ ';
  color: var(--muted);
}

.tree .folder.open::before {
  content: '‚ñæ ';
}

/* Tabbed Content */
.tab-container {
  display: flex;
  margin-bottom: 12px;
  border-bottom: 1px solid var(--border);
}

.tab-btn {
  padding: 8px 16px;
  cursor: pointer;
  background: transparent;
  border: none;
  border-bottom: 2px solid transparent;
  color: var(--muted);
  font-weight: 600;
  transition: all 0.2s ease;
}

.tab-btn:hover {
  color: var(--text);
}

.tab-btn.active {
  color: var(--text);
  border-color: var(--accent);
}

.tab-content {
  display: none;
}

.tab-content.active {
  display: block;
}

/* Cards & Content */
.card {
  border: 1px solid var(--border);
  border-radius: 10px;
  background: var(--panel);
  margin: 0 0 12px;
}

.card h3 {
  margin: 0;
  padding: 10px;
  border-bottom: 1px solid var(--border);
  font-size: 13px;
  color: var(--muted);
}

.card .body {
  padding: 10px;
}

.meta {
  display: flex;
  gap: 14px;
  flex-wrap: wrap;
  color: var(--muted);
}

details > summary {
  cursor: pointer;
  user-select: none;
}

pre {
  white-space: pre-wrap;
  word-wrap: break-word;
  background: #0b1117;
  padding: 10px;
  border-radius: 8px;
  border: 1px solid var(--border);
}

.small {
  font-size: 12px;
  color: var(--muted);
}

.search {
  width: 100%;
  padding: 6px 8px;
  border-radius: 8px;
  border: 1px solid var(--border);
  background: #0b1117;
  color: var(--text);
  margin: 0 0 10px;
}

/* AST Tree View */
.ast-tree ul {
  list-style-type: none;
  padding-left: 1.5em;
  position: relative;
}

.ast-tree li {
  margin: 0.5em 0;
  line-height: 1.5;
}

.ast-tree li::before {
  content: '‚Ä¢';
  color: var(--accent);
  display: inline-block;
  width: 1em;
  margin-left: -1em;
}

.ast-node {
  font-weight: bold;
}

.ast-field {
  color: var(--muted);
  font-style: italic;
  font-weight: normal;
}
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\injector_prompt.txt
--------------------------------------------------------------------------------
You are a strict Python Code Repair Agent.
TASK: You have a BROKEN file (Origin) and a TEMPLATE file (Boilerplate).
You must output a single, runnable Python file.

STRICT RULES:
1. HEADERS: The output file MUST start with the metadata block ("""SERVICE_NAME...""") found in the Origin file.
2. IMPORTS: AGGRESSIVE IMPORT PRESERVATION. You MUST include ALL imports from the Origin file. Do not remove imports even if they look unused. If the code uses 'mp.', you MUST have 'import multiprocessing as mp'.
3. CLASS SCOPE: The output must define the class (e.g., 'class IsoProcessMS(BaseService):'). 
   - All methods (def __init__, def execute) MUST be indented (4 spaces) so they are INSIDE the class.
   - Do NOT define methods at the root level.
4. LOGIC INJECTION: Paste the Origin logic into the 'execute' method. 
   - The method definition is at 4 spaces.
   - The method BODY is at 8 spaces.
5. SELF-CORRECTION: Before outputting, check:
   - Did I delete 'import multiprocessing'? -> Put it back.
   - Is 'def execute' at column 0? -> Indent it to column 4.
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\injector_prompt_v2.txt
--------------------------------------------------------------------------------
You are a Code Extraction Engine.
OBJECTIVE: Analyze the ORIGIN python file and extract 3 specific components.
OUTPUT FORMAT: Wrap content in XML tags. Do not output the whole file.

<IMPORTS>
(Paste all import statements here. Include 'import multiprocessing as mp' if present.)
</IMPORTS>

<HELPERS>
(Paste any global helper functions starting with '_' here, e.g., 'def _isolated_worker...'. Copy them EXACTLY.)
</HELPERS>

<LOGIC>
(Refactor the main execution logic to fit inside a class method. Indent it by 8 spaces. Use 'self.config' if needed.)
</LOGIC>

SYSTEM NOTE: Do not chat. Only output these three tagged blocks.
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\README.md
--------------------------------------------------------------------------------
# _LogicINJECTOR

A local AI tool to inject raw Python logic into standardized microservice boilerplates.

## Current Architecture ("Rewrite Strategy")
- **Engine:** Python (Tkinter) + Ollama
- **Model:** `qwen2.5-coder:7b` (Recommended)
- **Method:** Sends Origin + Boilerplate to LLM and asks it to merge them while fixing bugs.

## How to Use
1. Run `python LogicInjector.py`.
2. Select your `Origin Logic` file (the raw script).
3. Select your `Boilerplate` file (the class structure).
4. Click **INJECT & SAVE**.
5. **Review the Output:** Check the bottom of the file to ensure the `if __name__` block is indented correctly.

## Roadmap / Future Improvements
### The "Mad Libs" Architecture (Planned)
The current method relies on the LLM to rewrite the *entire* file, which risks dropping imports or helper functions.
**Future Plan:**
1. **Templating:** Convert Boilerplate into a template string with tags (e.g., `<IMPORTS>`, `<LOGIC>`).
2. **Extraction:** Ask LLM *only* to extract the logic chunks, not rewrite the whole file.
3. **Assembly:** Python script inserts the chunks into the safe template.
   - *Benefit:* 100% guarantee that class structure and helpers are never deleted.
   - *Benefit:* Allows use of smaller/faster models (3B).
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\requirements.txt
--------------------------------------------------------------------------------
# Standard Library dependencies only:
# tkinter, argparse, json, ast, threading, os, sys
#
# No external pip packages required.
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\src\app.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import json
import urllib.request
import urllib.error
import os
import threading
import re
import ast

class LogicInjector:
    def __init__(self, root):
        self.root = root
        self.root.title("_LogicINJECTOR (Mad Libs Architecture)")
        self.root.geometry("750x700")
        
        # Variables
        self.origin_path = tk.StringVar()
        self.boiler_path = tk.StringVar()
        self.output_folder = tk.StringVar()
        self.output_name = tk.StringVar()
        self.selected_model = tk.StringVar()

        # Check/Create External Prompt
        self.prompt_file = "injector_prompt.txt"
        self.ensure_prompt_file()

        # UI Layout
        self.create_widgets()
        
        # Load models in background
        threading.Thread(target=self.fetch_models, daemon=True).start()

    def clean_logic_with_ast(self, dirty_code):
            """
            Uses Python's AST to find the 'execute' function inside the AI's output
            and extract ONLY its body lines, preserving comments.
            """
            try:
                # 1. Parse the dirty code into a Tree
                tree = ast.parse(dirty_code)
                
                target_node = None
                # 2. Walk the tree to find the 'execute' function definition
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef) and node.name == 'execute':
                        target_node = node
                        break
                
                # 3. If found, slice the original string using line numbers
                if target_node and target_node.body:
                    # Get start and end lines of the BODY (skipping the 'def execute' line)
                    start_line = target_node.body[0].lineno - 1
                    end_line = target_node.body[-1].end_lineno
                    
                    lines = dirty_code.splitlines()
                    # Extract the block
                    body_lines = lines[start_line:end_line]
                    
                    # 4. Auto-Dedent the block
                    # (Find the indentation of the first line and remove it from all lines)
                    if body_lines:
                        first_line = body_lines[0]
                        indent_len = len(first_line) - len(first_line.lstrip())
                        cleaned_lines = [line[indent_len:] if len(line) >= indent_len else line for line in body_lines]
                        return "\n".join(cleaned_lines)
                
                # If no 'execute' function found, maybe the AI respected us? Return as is.
                return dirty_code

            except Exception as e:
                self.log(f"AST Warning: Could not parse AI output ({e}). Using raw.")
                return dirty_code

    def ensure_prompt_file(self):
        """Creates the Extraction Prompt (Mad Libs Style)."""
        # Note: We overwrite the old prompt because the strategy has changed.
        prompt_content = (
            "You are a Code Extraction Engine.\n"
            "OBJECTIVE: Analyze the ORIGIN python file and extract 3 specific components.\n"
            "OUTPUT FORMAT: Wrap content in XML tags. Do not output the whole file.\n\n"
            "<IMPORTS>\n(Paste all import statements here. Include 'import multiprocessing as mp' if present.)\n</IMPORTS>\n\n"
            "<HELPERS>\n(Paste any global helper functions starting with '_' here, e.g., 'def _isolated_worker...'. Copy them EXACTLY.)\n</HELPERS>\n\n"
            "<LOGIC>\n(Refactor the main execution logic to fit inside a class method. Indent it by 8 spaces. Use 'self.config' if needed.)\n</LOGIC>\n\n"
            "SYSTEM NOTE: Do not chat. Only output these three tagged blocks."
        )
        with open(self.prompt_file, "w", encoding="utf-8") as f:
            f.write(prompt_content)

    def create_widgets(self):
        # ... (Standard UI boilerplate) ...
        paddings = {'padx': 10, 'pady': (5, 0)}
        
        tk.Label(self.root, text="1. Origin Logic File:").pack(anchor="w", **paddings)
        frame1 = tk.Frame(self.root); frame1.pack(fill="x", padx=10)
        tk.Entry(frame1, textvariable=self.origin_path).pack(side="left", fill="x", expand=True)
        tk.Button(frame1, text="Browse", command=lambda: self.pick_file(self.origin_path)).pack(side="right")

        tk.Label(self.root, text="2. Boilerplate File (Must have '# [INJECT LOGIC HERE]'):").pack(anchor="w", **paddings)
        frame2 = tk.Frame(self.root); frame2.pack(fill="x", padx=10)
        tk.Entry(frame2, textvariable=self.boiler_path).pack(side="left", fill="x", expand=True)
        tk.Button(frame2, text="Browse", command=lambda: self.pick_file(self.boiler_path)).pack(side="right")

        tk.Label(self.root, text="3. Ollama Model:").pack(anchor="w", **paddings)
        self.model_combo = ttk.Combobox(self.root, textvariable=self.selected_model, state="readonly")
        self.model_combo.pack(fill="x", padx=10)

        tk.Label(self.root, text="4. Output Folder:").pack(anchor="w", **paddings)
        frame4 = tk.Frame(self.root); frame4.pack(fill="x", padx=10)
        tk.Entry(frame4, textvariable=self.output_folder).pack(side="left", fill="x", expand=True)
        tk.Button(frame4, text="Browse", command=lambda: self.pick_folder(self.output_folder)).pack(side="right")

        tk.Label(self.root, text="5. Output Filename:").pack(anchor="w", **paddings)
        tk.Entry(self.root, textvariable=self.output_name).pack(fill="x", padx=10)

        # START BUTTON
        self.btn_run = tk.Button(self.root, text="EXTRACT & ASSEMBLE", command=self.start_thread, bg="#dddddd", height=2)
        self.btn_run.pack(fill="x", padx=10, pady=15)

        # LOGGING WINDOW
        tk.Label(self.root, text="Process Log:").pack(anchor="w", padx=10)
        self.log_window = scrolledtext.ScrolledText(self.root, height=15, state='disabled', font=("Consolas", 9))
        self.log_window.pack(fill="both", expand=True, padx=10, pady=5)

    def log(self, message):
        def _append():
            self.log_window.config(state='normal')
            self.log_window.insert(tk.END, message + "\n")
            self.log_window.see(tk.END)
            self.log_window.config(state='disabled')
        self.root.after(0, _append)

    def pick_file(self, var):
        f = filedialog.askopenfilename()
        if f: var.set(f)
    
    def pick_folder(self, var):
        f = filedialog.askdirectory()
        if f: var.set(f)

    def fetch_models(self):
        try:
            with urllib.request.urlopen("http://localhost:11434/api/tags") as r:
                data = json.loads(r.read().decode())
                models = [m['name'] for m in data['models']]
                self.root.after(0, lambda: self.model_combo.config(values=models))
                if models: self.root.after(0, lambda: self.model_combo.current(0))
        except:
            self.log("Error: Could not fetch models.")

    def start_thread(self):
        if not all([self.origin_path.get(), self.boiler_path.get(), self.output_folder.get(), self.output_name.get()]):
            messagebox.showerror("Error", "All fields required.")
            return
        
        self.btn_run.config(state="disabled", text="Working...")
        self.log_window.config(state='normal'); self.log_window.delete(1.0, tk.END); self.log_window.config(state='disabled')
        threading.Thread(target=self.run_injection, daemon=True).start()

    def run_injection(self):
        try:
            self.log("--- Starting Mad Libs Assembly ---")
            
            # 1. READ FILES (With strict error handling for encoding)
            self.log("Reading files...")
            with open(self.origin_path.get(), 'r', encoding='utf-8', errors='ignore') as f: 
                origin_content = f.read()
            with open(self.boiler_path.get(), 'r', encoding='utf-8', errors='ignore') as f: 
                boiler_content = f.read()
            with open(self.prompt_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read()

            # 2. ASK OLLAMA TO FILL THE TAGS
            user_prompt = f"==== ORIGIN CODE ====\n{origin_content}"
            
            payload = {
                "model": self.selected_model.get(),
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {"temperature": 0.1, "num_ctx": 8192} 
            }

            self.log(f"Requesting Extraction from {self.selected_model.get()}...")
            
            req = urllib.request.Request("http://localhost:11434/api/chat", 
                                         data=json.dumps(payload).encode('utf-8'), 
                                         headers={'Content-Type': 'application/json'})
            
            with urllib.request.urlopen(req) as response:
                result = json.loads(response.read().decode('utf-8'))
                ai_output = result['message']['content']
                self.log("Received AI response. Parsing tags...")

            # 3. PARSE TAGS (REGEX)
            imports = re.search(r"<IMPORTS>(.*?)</IMPORTS>", ai_output, re.DOTALL)
            helpers = re.search(r"<HELPERS>(.*?)</HELPERS>", ai_output, re.DOTALL)
            logic   = re.search(r"<LOGIC>(.*?)</LOGIC>", ai_output, re.DOTALL)

            imports_code = imports.group(1).strip() if imports else ""
            helpers_code = helpers.group(1).strip() if helpers else ""
            
            # --- AST CLEANER ---
            if logic:
                raw_logic = logic.group(1)
                
                # 1. Run the AST surgery (Stripping wrappers + auto-dedenting)
                cleaned_logic = self.clean_logic_with_ast(raw_logic)
                
                # 2. Force 8-space indentation for the final Boilerplate insertion
                logic_lines = cleaned_logic.splitlines()
                indented_lines = ["        " + line for line in logic_lines]
                logic_code = "\n".join(indented_lines)
            else:
                logic_code = "        # NO LOGIC DETECTED"

            self.log(f"Extracted:\n- Imports: {len(imports_code)} chars\n- Helpers: {len(helpers_code)} chars\n- Logic: {len(logic_code)} chars")

            # 4. ASSEMBLE THE MAD LIB
            # A. Inject Imports at the very top
            final_code = f"{imports_code}\n\n{boiler_content}"

            # B. Inject Logic (Find the placeholder)
            if "# [INJECT LOGIC HERE]" in final_code:
                final_code = final_code.replace("# [INJECT LOGIC HERE]", logic_code)
            else:
                self.log("WARNING: Placeholder '# [INJECT LOGIC HERE]' not found. Appending logic to end (unsafe).")
                final_code += f"\n\n# ORPHANED LOGIC:\n{logic_code}"

            # C. Inject Helpers (Before the class definition is usually best, but top level works too)
            # We will insert them right after the imports/metadata block
            # A simple trick: Insert them before "class "
            if "class " in final_code:
                parts = final_code.split("class ", 1)
                final_code = f"{parts[0]}\n\n# --- HELPERS ---\n{helpers_code}\n\nclass {parts[1]}"
            else:
                final_code += f"\n\n{helpers_code}"

            # 5. SAVE
            final_path = os.path.join(self.output_folder.get(), self.output_name.get())
            with open(final_path, 'w', encoding='utf-8') as f:
                f.write(final_code)
            
            self.log(f"SUCCESS: Assembled file saved to {final_path}")
            self.root.after(0, lambda: messagebox.showinfo("Success", "Mad Libs Assembly Complete!"))

        except Exception as e:
            self.log(f"CRITICAL ERROR: {str(e)}")
            print(e)
        finally:
            self.root.after(0, lambda: self.btn_run.config(state="normal", text="EXTRACT & ASSEMBLE"))

if __name__ == "__main__":
    root = tk.Tk()
    app = LogicInjector(root)
    root.mainloop()
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\src\New folder\app.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext
import json
import urllib.request
import urllib.error
import os
import threading
import re
import ast


class LogicInjector:
    """
    A simple Tkinter GUI that allows a user to select a source Python file
    (the "origin"), a boilerplate class file, and an output location.  The tool
    then asks a language model to extract imports, helper functions and core
    logic from the origin file and reassembles them into the boilerplate.  This
    implementation closely follows the behaviour described in the original
    project but also displays the raw response from the model in the log window.
    """

    def __init__(self, root):
        self.root = root
        self.root.title("_LogicINJECTOR (Mad Libs Architecture)")
        self.root.geometry("750x700")

        # Variables to hold user selections
        self.origin_path = tk.StringVar()
        self.boiler_path = tk.StringVar()
        self.output_folder = tk.StringVar()
        self.output_name = tk.StringVar()
        self.selected_model = tk.StringVar()

        # Check/Create External Prompt
        # The prompt file tells the LLM how to perform extraction.  If it
        # doesn't exist or the format changes this method writes a fresh copy.
        self.prompt_file = "injector_prompt.txt"
        self.ensure_prompt_file()

        # Build out the interface
        self.create_widgets()

        # Fetch available models from a local Ollama instance in the background
        threading.Thread(target=self.fetch_models, daemon=True).start()

    def clean_logic_with_ast(self, dirty_code):
        """
        Uses Python's `ast` module to find an `execute` function in the text
        returned by the model and extract only its body lines while preserving
        comments.  The AST module is part of the standard library and can parse
        Python source into an abstract syntax tree, which we then traverse to
        locate the function definition„Äê899441087435170‚Ä†L74-L84„Äë.

        Parameters
        ----------
        dirty_code : str
            A string containing Python code that presumably wraps the logic we
            are interested in.  The function attempts to strip away the
            surrounding function definition and dedent the body so it can be
            re‚Äëindented into the boilerplate.

        Returns
        -------
        str
            The dedented body of the `execute` function if found; otherwise
            returns the original input.
        """
        try:
            # Parse the returned code into an AST tree
            tree = ast.parse(dirty_code)
            target_node = None

            # Walk the tree to find the first FunctionDef named 'execute'
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef) and node.name == 'execute':
                    target_node = node
                    break

            # If the function definition is found, slice out its body using
            # line numbers.  ast nodes record their source location in
            # lineno/end_lineno attributes.
            if target_node and target_node.body:
                start_line = target_node.body[0].lineno - 1
                end_line = target_node.body[-1].end_lineno
                lines = dirty_code.splitlines()
                body_lines = lines[start_line:end_line]

                # Auto‚Äëdedent: remove common indentation from the block
                if body_lines:
                    first_line = body_lines[0]
                    indent_len = len(first_line) - len(first_line.lstrip())
                    cleaned_lines = [
                        line[indent_len:] if len(line) >= indent_len else line
                        for line in body_lines
                    ]
                    return "\n".join(cleaned_lines)

            # If no execute function is found, return the original payload
            return dirty_code

        except Exception as e:
            # Fall back to returning the raw text if parsing fails
            self.log(f"AST Warning: Could not parse AI output ({e}). Using raw.")
            return dirty_code

    def ensure_prompt_file(self):
        """
        Create the extraction prompt file if it does not exist or overwrite it
        with the current template.  This prompt instructs the language model
        to output XML tagged sections for imports, helper functions and logic.
        """
        prompt_content = (
            "You are a Code Extraction Engine.\n"
            "OBJECTIVE: Analyze the ORIGIN python file and extract 3 specific components.\n"
            "OUTPUT FORMAT: Wrap content in XML tags. Do not output the whole file.\n\n"
            "<IMPORTS>\n(Paste all import statements here. Include 'import multiprocessing as mp' if present.)\n</IMPORTS>\n\n"
            "<HELPERS>\n(Paste any global helper functions starting with '_' here, e.g., 'def _isolated_worker...'. Copy them EXACTLY.)\n</HELPERS>\n\n"
            "<LOGIC>\n(Refactor the main execution logic to fit inside a class method. Indent it by 8 spaces. Use 'self.config' if needed.)\n</LOGIC>\n\n"
            "SYSTEM NOTE: Do not chat. Only output these three tagged blocks."
        )
        with open(self.prompt_file, "w", encoding="utf-8") as f:
            f.write(prompt_content)

    def create_widgets(self):
        """Build the Tkinter widgets that make up the user interface."""
        paddings = {'padx': 10, 'pady': (5, 0)}

        tk.Label(self.root, text="1. Origin Logic File:").pack(anchor="w", **paddings)
        frame1 = tk.Frame(self.root); frame1.pack(fill="x", padx=10)
        tk.Entry(frame1, textvariable=self.origin_path).pack(side="left", fill="x", expand=True)
        tk.Button(frame1, text="Browse", command=lambda: self.pick_file(self.origin_path)).pack(side="right")

        tk.Label(self.root, text="2. Boilerplate File (Must have '# [INJECT LOGIC HERE]'):").pack(anchor="w", **paddings)
        frame2 = tk.Frame(self.root); frame2.pack(fill="x", padx=10)
        tk.Entry(frame2, textvariable=self.boiler_path).pack(side="left", fill="x", expand=True)
        tk.Button(frame2, text="Browse", command=lambda: self.pick_file(self.boiler_path)).pack(side="right")

        tk.Label(self.root, text="3. Ollama Model:").pack(anchor="w", **paddings)
        self.model_combo = ttk.Combobox(self.root, textvariable=self.selected_model, state="readonly")
        self.model_combo.pack(fill="x", padx=10)

        tk.Label(self.root, text="4. Output Folder:").pack(anchor="w", **paddings)
        frame4 = tk.Frame(self.root); frame4.pack(fill="x", padx=10)
        tk.Entry(frame4, textvariable=self.output_folder).pack(side="left", fill="x", expand=True)
        tk.Button(frame4, text="Browse", command=lambda: self.pick_folder(self.output_folder)).pack(side="right")

        tk.Label(self.root, text="5. Output Filename:").pack(anchor="w", **paddings)
        tk.Entry(self.root, textvariable=self.output_name).pack(fill="x", padx=10)

        # START BUTTON
        self.btn_run = tk.Button(self.root, text="EXTRACT & ASSEMBLE", command=self.start_thread, bg="#dddddd", height=2)
        self.btn_run.pack(fill="x", padx=10, pady=15)

        # LOGGING WINDOW
        tk.Label(self.root, text="Process Log:").pack(anchor="w", padx=10)
        self.log_window = scrolledtext.ScrolledText(self.root, height=15, state='disabled', font=("Consolas", 9))
        self.log_window.pack(fill="both", expand=True, padx=10, pady=5)

    def log(self, message):
        """Append a message to the log window in a thread‚Äësafe manner."""
        def _append():
            self.log_window.config(state='normal')
            self.log_window.insert(tk.END, message + "\n")
            self.log_window.see(tk.END)
            self.log_window.config(state='disabled')
        self.root.after(0, _append)

    def pick_file(self, var):
        """Open a file selection dialog and set the provided StringVar."""
        f = filedialog.askopenfilename()
        if f:
            var.set(f)

    def pick_folder(self, var):
        """Open a directory selection dialog and set the provided StringVar."""
        f = filedialog.askdirectory()
        if f:
            var.set(f)

    def fetch_models(self):
        """
        Fetch a list of available models from a local Ollama instance.  Updates
        the model_combo drop‚Äëdown asynchronously so the UI doesn't block.
        """
        try:
            with urllib.request.urlopen("http://localhost:11434/api/tags") as r:
                data = json.loads(r.read().decode())
                models = [m['name'] for m in data['models']]
                self.root.after(0, lambda: self.model_combo.config(values=models))
                if models:
                    self.root.after(0, lambda: self.model_combo.current(0))
        except Exception:
            self.log("Error: Could not fetch models.")

    def start_thread(self):
        """
        Validate the user inputs and spawn a background thread to run the
        injection procedure.  This prevents the GUI from freezing while the
        network request is in progress.
        """
        if not all([
            self.origin_path.get(),
            self.boiler_path.get(),
            self.output_folder.get(),
            self.output_name.get(),
        ]):
            messagebox.showerror("Error", "All fields required.")
            return
        self.btn_run.config(state="disabled", text="Working...")
        self.log_window.config(state='normal')
        self.log_window.delete(1.0, tk.END)
        self.log_window.config(state='disabled')
        threading.Thread(target=self.run_injection, daemon=True).start()

    def run_injection(self):
        """
        Perform the extraction and assembly.  This method reads the origin and
        boilerplate files, calls the language model to perform extraction,
        cleans and indents the returned logic with AST, inserts imports,
        helpers and logic into the boilerplate, then writes out the final
        assembled microservice.  The raw AI response is logged so the user can
        inspect what the model produced.
        """
        try:
            self.log("--- Starting Mad Libs Assembly ---")
            # 1. READ FILES (with strict error handling for encoding)
            self.log("Reading files...")
            with open(self.origin_path.get(), 'r', encoding='utf-8', errors='ignore') as f:
                origin_content = f.read()
            with open(self.boiler_path.get(), 'r', encoding='utf-8', errors='ignore') as f:
                boiler_content = f.read()
            with open(self.prompt_file, 'r', encoding='utf-8') as f:
                system_prompt = f.read()

            # 2. ASK OLLAMA TO FILL THE TAGS
            user_prompt = f"==== ORIGIN CODE ====\n{origin_content}"
            payload = {
                "model": self.selected_model.get(),
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                "stream": False,
                "options": {"temperature": 0.1, "num_ctx": 8192},
            }
            self.log(f"Requesting Extraction from {self.selected_model.get()}...")
            req = urllib.request.Request(
                "http://localhost:11434/api/chat",
                data=json.dumps(payload).encode('utf-8'),
                headers={'Content-Type': 'application/json'},
            )
            with urllib.request.urlopen(req) as response:
                result = json.loads(response.read().decode('utf-8'))
                ai_output = result['message']['content']
                # Log the raw AI output so the user can watch what the model generated
                self.log("--- AI Response ---")
                self.log(ai_output)
                self.log("--- End of AI Response ---")

            # 3. PARSE TAGS (REGEX)
            imports = re.search(r"<IMPORTS>(.*?)</IMPORTS>", ai_output, re.DOTALL)
            helpers = re.search(r"<HELPERS>(.*?)</HELPERS>", ai_output, re.DOTALL)
            logic = re.search(r"<LOGIC>(.*?)</LOGIC>", ai_output, re.DOTALL)

            imports_code = imports.group(1).strip() if imports else ""
            helpers_code = helpers.group(1).strip() if helpers else ""
            # Use AST to strip any wrapper function around the logic and dedent
            if logic:
                raw_logic = logic.group(1)
                cleaned_logic = self.clean_logic_with_ast(raw_logic)
                logic_lines = cleaned_logic.splitlines()
                indented_lines = ["        " + line for line in logic_lines]
                logic_code = "\n".join(indented_lines)
            else:
                logic_code = "        # NO LOGIC DETECTED"
            self.log(
                f"Extracted:\n- Imports: {len(imports_code)} chars\n"
                f"- Helpers: {len(helpers_code)} chars\n"
                f"- Logic: {len(logic_code)} chars"
            )

            # 4. ASSEMBLE THE MAD LIB
            # A. Inject Imports at the very top
            final_code = f"{imports_code}\n\n{boiler_content}"
            # B. Inject Logic (find the placeholder)
            if "# [INJECT LOGIC HERE]" in final_code:
                final_code = final_code.replace("# [INJECT LOGIC HERE]", logic_code)
            else:
                self.log(
                    "WARNING: Placeholder '# [INJECT LOGIC HERE]' not found. "
                    "Appending logic to end (unsafe)."
                )
                final_code += f"\n\n# ORPHANED LOGIC:\n{logic_code}"
            # C. Inject Helpers (insert before the first class definition if possible)
            if "class " in final_code:
                parts = final_code.split("class ", 1)
                final_code = (
                    f"{parts[0]}\n\n# --- HELPERS ---\n{helpers_code}\n\nclass {parts[1]}"
                )
            else:
                final_code += f"\n\n{helpers_code}"
            # 5. SAVE
            final_path = os.path.join(self.output_folder.get(), self.output_name.get())
            with open(final_path, 'w', encoding='utf-8') as f:
                f.write(final_code)
            self.log(f"SUCCESS: Assembled file saved to {final_path}")
            self.root.after(
                0, lambda: messagebox.showinfo("Success", "Mad Libs Assembly Complete!")
            )
        except Exception as e:
            self.log(f"CRITICAL ERROR: {str(e)}")
            print(e)
        finally:
            # Re‚Äëenable the start button regardless of success or failure
            self.root.after(0, lambda: self.btn_run.config(state="normal", text="EXTRACT & ASSEMBLE"))


if __name__ == "__main__":
    # Entry point for running the GUI directly.  Note: Running Tkinter in
    # headless environments may raise exceptions.  When run in a local desktop
    # environment this will open a window where you can choose files and run
    # the extraction.
    root = tk.Tk()
    app = LogicInjector(root)
    root.mainloop()
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\src\New folder\convert_script.py
--------------------------------------------------------------------------------
"""
Utility for converting a plain Python script into a microservice based on a
boilerplate template.  The conversion extracts import statements, helper
functions (those whose names begin with an underscore), and the remaining
execution logic from the origin script.  It then injects these pieces into a
predefined boilerplate class, placing the logic inside the ``execute`` method
and helper functions at the top of the file.

This script does not require a language model.  It demonstrates the "Mad Libs"
approach described in the project README: assemble a microservice from
components without rewriting the class structure.  To run it, provide the
origin script and the boilerplate template on the command line, for example::

    python convert_script.py origin.py _boilerplates/microservice_boiler_plate.py output.py

"""

from __future__ import annotations

import ast
import os
import sys
from pathlib import Path
from typing import List, Tuple


def extract_sections(source_code: str) -> Tuple[str, str, str]:
    """
    Parse a Python source file and extract import statements, helper functions,
    and the remaining logic.

    Parameters
    ----------
    source_code : str
        The text of the origin Python file.

    Returns
    -------
    tuple[str, str, str]
        A tuple containing three strings: ``imports``, ``helpers`` and
        ``logic``.  Each string preserves the original formatting except
        indentation adjustments for the logic section.
    """
    lines = source_code.splitlines()
    imports: List[str] = []
    helpers: List[str] = []
    logic: List[str] = []

    # Use AST to identify top‚Äëlevel helper functions and their line numbers
    try:
        tree = ast.parse(source_code)
        helper_spans: List[Tuple[int, int]] = []
        for node in tree.body:
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                # Record import lines by line numbers (1‚Äëbased) so we can skip
                # them later when constructing the logic section
                imports.append("\n".join(lines[node.lineno - 1 : node.end_lineno]))
            elif isinstance(node, ast.FunctionDef) and node.name.startswith("_"):
                # Capture helper function source lines
                helper_spans.append((node.lineno - 1, node.end_lineno))
        # Merge helper spans to avoid duplicates
        helper_lines: List[str] = []
        for start, end in helper_spans:
            helper_lines.extend(lines[start:end])
        helpers = helper_lines

        # Determine which line numbers belong to imports, helpers and the module
        # docstring to exclude.  The docstring, if present, is represented as
        # the first statement in the module body that is an Expr whose value is
        # a Constant string.  We derive its span via lineno/end_lineno.
        excluded: set[int] = set()
        for node in tree.body:
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                excluded.update(range(node.lineno - 1, node.end_lineno))
        for start, end in helper_spans:
            excluded.update(range(start, end))
        # Exclude module docstring lines
        if (
            tree.body
            and isinstance(tree.body[0], ast.Expr)
            and isinstance(getattr(tree.body[0], "value", None), ast.Constant)
            and isinstance(tree.body[0].value.value, str)
        ):
            doc_node = tree.body[0]
            excluded.update(range(doc_node.lineno - 1, doc_node.end_lineno))
        # Everything else (except excluded lines) goes into logic
        for idx, line in enumerate(lines):
            if idx in excluded:
                continue
            logic.append(line)
    except Exception:
        # Fallback: naive extraction ‚Äì take lines starting with 'import' as imports
        for line in lines:
            if line.startswith("import ") or line.startswith("from "):
                imports.append(line)
            elif line.lstrip().startswith("def _"):
                helpers.append(line)
            else:
                logic.append(line)
    # Deduplicate import lines and join helpers
    import_block = "\n".join(imports)
    helpers_block = "\n".join(helpers).rstrip()
    logic_block = "\n".join(logic).rstrip()
    return import_block, helpers_block, logic_block


def indent_logic(logic: str, indent: int = 8) -> str:
    """Indent every line of the logic block by a fixed number of spaces."""
    indentation = " " * indent
    if not logic:
        return ""  # nothing to indent
    return "\n".join(f"{indentation}{line}" if line.strip() != "" else "" for line in logic.splitlines())


def assemble_microservice(
    imports: str,
    helpers: str,
    logic: str,
    boilerplate: str,
) -> str:
    """
    Assemble the final microservice code by combining imports, boilerplate,
    helpers, and logic.  Logic is injected into the location marked with
    ``# [INJECT LOGIC HERE]``.  Helpers are inserted just before the first
    class definition in the boilerplate.  Imports appear at the very top.
    """
    # Prepend imports
    final_code = f"{imports}\n\n{boilerplate}" if imports else boilerplate
    # Insert logic
    indented_logic = indent_logic(logic)
    if "# [INJECT LOGIC HERE]" in final_code:
        final_code = final_code.replace("# [INJECT LOGIC HERE]", indented_logic or "        pass")
    else:
        final_code += f"\n\n# ORPHANED LOGIC:\n{indented_logic}"
    # Insert helpers
    if helpers and "class " in final_code:
        parts = final_code.split("class ", 1)
        final_code = f"{parts[0]}\n\n# --- HELPERS ---\n{helpers}\n\nclass {parts[1]}"
    elif helpers:
        final_code += f"\n\n{helpers}"
    return final_code


def main(argv: List[str]) -> None:
    if len(argv) != 3:
        print(
            "Usage: python convert_script.py <origin.py> <boilerplate.py> <output.py>",
            file=sys.stderr,
        )
        sys.exit(1)
    origin_path, boiler_path, output_path = argv
    origin_code = Path(origin_path).read_text(encoding="utf-8", errors="ignore")
    boiler_code = Path(boiler_path).read_text(encoding="utf-8", errors="ignore")
    imports, helpers, logic = extract_sections(origin_code)
    final_code = assemble_microservice(imports, helpers, logic, boiler_code)
    Path(output_path).write_text(final_code, encoding="utf-8")
    print(f"Converted {origin_path} -> {output_path}")


if __name__ == "__main__":
    main(sys.argv[1:])
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\src\New folder\example_origin.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: ExampleService

This script demonstrates a simple origin that calculates square roots of
numbers and prints the total.  It contains imports, a helper function and
some main logic.  The convert_script will extract these components and
assemble a microservice class for reuse.
"""

import math
import multiprocessing as mp
import json


def _helper(value: int) -> float:
    """Return the square root of ``value`` using the math module."""
    return math.sqrt(value)


def main() -> None:
    """Compute the sum of square roots for numbers 0 through 4 and print it."""
    total = 0.0
    for i in range(5):
        total += _helper(i)
    print("Sum of square roots:", total)


if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\src\New folder\example_service_ms.py
--------------------------------------------------------------------------------
import math
import multiprocessing as mp
import json

"""
This module defines a skeletal microservice class.  A separate process will
inject imports, helper functions and logic into the marked sections of this
template to produce a fully fledged microservice.  The decorators imported
from ``microservice_std_lib`` are no‚Äëops if that library is not available.  A
standard logger is configured in the constructor to aid with debugging.
"""

from typing import Dict, Any, Optional
import logging

# Import decorators from the microservice standard library.  If the package
# doesn't exist in the local environment our stub in ``microservice_std_lib.py``
# will be used instead.
from microservice_std_lib import service_metadata, service_endpoint

# NOTE: The [Mad Libs] Script will inject Imports and Helper Functions above this line automatically.

@service_metadata(
    name="YourServiceName",
    version="1.0.0",
    description="Briefly describe the 'Purpose' here.",
    tags=["category", "utility"],
    capabilities=["filesystem:read"],  # Optional: what it actually touches
)


# --- HELPERS ---
def _helper(value: int) -> float:
    """Return the square root of ``value`` using the math module."""
    return math.sqrt(value)

class YourServiceMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        # Persist configuration and set a default timeout
        self.config = config or {}
        self.timeout = self.config.get("timeout_seconds", 60)

        # Setup a standard logger named after the class.  Only configure the
        # root logging system once to avoid duplicate handlers.
        self.log = logging.getLogger(self.__class__.__name__)
        if not self.log.handlers:
            logging.basicConfig(level=logging.INFO)

    @service_endpoint(
        inputs={"param1": "str", "param2": "int"},
        outputs={"result": "str"},
        description="Detailed description of what this specific method does.",
        tags=["action"],
        side_effects=["filesystem:write"],
    )
    def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        """
        Primary entry point for the microservice.  This method will be
        populated with user logic by the extraction tool.  It accepts an
        arbitrary payload and optional runtime configuration and should return
        the computed result.
        """
        




        def main() -> None:
            """Compute the sum of square roots for numbers 0 through 4 and print it."""
            total = 0.0
            for i in range(5):
                total += _helper(i)
            print("Sum of square roots:", total)


        if __name__ == "__main__":
            main()
        return {}


if __name__ == "__main__":
    # Standard independent test block for the catalogue
    svc = YourServiceMS()
    print("Service ready:", svc)
    # Add a print test of your logic here if needed
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\src\New folder\microservice_boiler_plate.py
--------------------------------------------------------------------------------
"""
This module defines a skeletal microservice class.  A separate process will
inject imports, helper functions and logic into the marked sections of this
template to produce a fully fledged microservice.  The decorators imported
from ``microservice_std_lib`` are no‚Äëops if that library is not available.  A
standard logger is configured in the constructor to aid with debugging.
"""

from typing import Dict, Any, Optional
import logging

# Import decorators from the microservice standard library.  If the package
# doesn't exist in the local environment our stub in ``microservice_std_lib.py``
# will be used instead.
from microservice_std_lib import service_metadata, service_endpoint

# NOTE: The [Mad Libs] Script will inject Imports and Helper Functions above this line automatically.

@service_metadata(
    name="YourServiceName",
    version="1.0.0",
    description="Briefly describe the 'Purpose' here.",
    tags=["category", "utility"],
    capabilities=["filesystem:read"],  # Optional: what it actually touches
)
class YourServiceMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        # Persist configuration and set a default timeout
        self.config = config or {}
        self.timeout = self.config.get("timeout_seconds", 60)

        # Setup a standard logger named after the class.  Only configure the
        # root logging system once to avoid duplicate handlers.
        self.log = logging.getLogger(self.__class__.__name__)
        if not self.log.handlers:
            logging.basicConfig(level=logging.INFO)

    @service_endpoint(
        inputs={"param1": "str", "param2": "int"},
        outputs={"result": "str"},
        description="Detailed description of what this specific method does.",
        tags=["action"],
        side_effects=["filesystem:write"],
    )
    def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        """
        Primary entry point for the microservice.  This method will be
        populated with user logic by the extraction tool.  It accepts an
        arbitrary payload and optional runtime configuration and should return
        the computed result.
        """
        # [INJECT LOGIC HERE]
        return {}


if __name__ == "__main__":
    # Standard independent test block for the catalogue
    svc = YourServiceMS()
    print("Service ready:", svc)
    # Add a print test of your logic here if needed
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\src\New folder\microservice_std_lib.py
--------------------------------------------------------------------------------
"""
Minimal stub of the ``microservice_std_lib`` package used by the microservice
boilerplate.  In environments where the real library is unavailable this stub
provides decorator definitions for ``service_metadata`` and ``service_endpoint``
that simply attach the provided metadata to the class or function.  This
allows generated microservices to run without import errors while still
carrying descriptive annotations.
"""

from typing import Callable, Any, Dict


def service_metadata(**metadata: Any) -> Callable[[type], type]:
    """
    Class decorator that attaches metadata to the decorated class.  When used
    with the real ``microservice_std_lib`` this would likely register the
    service with a catalogue or apply additional behaviours.  Here we simply
    store the metadata on the class for introspection.
    """
    def decorator(cls: type) -> type:
        setattr(cls, "_service_metadata", metadata)
        return cls

    return decorator


def service_endpoint(**metadata: Any) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """
    Function decorator that attaches endpoint metadata to the decorated
    function.  This mimics the behaviour of the real library by storing
    configuration on the callable object.
    """
    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        setattr(func, "_endpoint_metadata", metadata)
        return func

    return decorator


__all__ = ["service_metadata", "service_endpoint"]
--------------------------------------------------------------------------------
FILE: _TestTRANSLATOR\_boilerplates\microservice_boiler_plate.py
--------------------------------------------------------------------------------
from microservice_std_lib import service_metadata, service_endpoint
from typing import Dict, Any, Optional

# NOTE: The [Mad Libs] Script will inject Imports and Helper Functions above this line automatically.

@service_metadata(
    name="YourServiceName",
    version="1.0.0",
    description="Briefly describe the 'Purpose' here.",
    tags=["category", "utility"],
    capabilities=["filesystem:read"] # Optional: what it actually touches
)
class YourServiceMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.timeout = self.config.get("timeout_seconds", 60)
        
        # Setup standard logger
        self.log = logging.getLogger(self.__class__.__name__)
        if not self.log.handlers:
            logging.basicConfig(level=logging.INFO)

    @service_endpoint(
        inputs={"param1": "str", "param2": "int"},
        outputs={"result": "str"},
        description="Detailed description of what this specific method does.",
        tags=["action"],
        side_effects=["filesystem:write"] 
    )
    def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        # [INJECT LOGIC HERE]
        return {}

if __name__ == "__main__":
    # Standard independent test block for the catalogue
    svc = YourServiceMS()
    print("Service ready:", svc)
    # Add a print test of your logic here
--------------------------------------------------------------------------------
FILE: _TextTOUCHER\.ragforge.json
--------------------------------------------------------------------------------
{
  ".": true,
  "_logs": false,
  "_logs/_BoilerPlatePythonTEMPLATE_filedump.txt": false,
  "_logs/_BoilerPlatePythonTEMPLATE_project_folder_tree.txt": false,
  "_logs/_project_mapper_config.json": false,
  "_logs/AI_AGENT_BOILERPLATE_INSTRUCTIONS.md": false,
  "src": true,
  "src/__init__.py": true,
  "src/app.py": true,
  "LICENSE.md": true,
  "README.md": true,
  "requirements.txt": true,
  "setup_env.bat": true
}
--------------------------------------------------------------------------------
FILE: _TextTOUCHER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _TextTOUCHER\README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TextTOUCHER\requirements.txt
--------------------------------------------------------------------------------
# This project relies only on the Python Standard Library.
# No external pip packages are required.

# Core Dependencies:
# - tkinter (Included with standard Python installations)
# - os
# - datetime
# - argparse

# Note for Linux users:
# If tkinter is missing, run: sudo apt-get install python3-tk
--------------------------------------------------------------------------------
FILE: _TextTOUCHER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _TextTOUCHER\src\app.py
--------------------------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
== Quick Text Generator ==

A Tkinter-based utility for quickly generating text files.
Now supports custom/no extensions via the "(None)" dropdown option.
"""

import sys
import os
import argparse
import tkinter as tk
from tkinter import filedialog, messagebox
from tkinter import ttk
from datetime import datetime

# ==========================================
#           USER CONFIGURATION
# ==========================================
# Customize your app startup defaults here:
CONFIG = {
    "WINDOW_WIDTH":  600,
    "WINDOW_HEIGHT": 600,
    "APP_TITLE":     "Quick Text Generator",
    "DEFAULT_EXT":   ".txt",
    "FONT_PATH":     ("Segoe UI", 9),       # Font for the path label
    "FONT_INPUT":    ("Consolas", 10),      # Font for the content editing
}
# ==========================================

class TextFileGenerator:
    """
    Main application logic for the Text File Generator.
    """
    def __init__(self, root):
        self.root = root
        self.root.title(CONFIG["APP_TITLE"])
        
        # Apply Configured Geometry
        geom_str = f"{CONFIG['WINDOW_WIDTH']}x{CONFIG['WINDOW_HEIGHT']}"
        self.root.geometry(geom_str)

        # --- Variables ---
        self.selected_folder_path = tk.StringVar(value="")
        self.use_timestamp = tk.BooleanVar(value=False)
        self.file_extension = tk.StringVar(value=CONFIG["DEFAULT_EXT"])

        # --- UI Layout ---
        self._build_ui()
        
        # UX Polish: Focus the filename entry immediately
        self.ent_filename.focus_set()

    def _build_ui(self):
        """Constructs the visual elements of the application."""
        
        # 1. Path Display (Top Bar)
        # We use a LabelFrame or just a Frame with a border to make it pop
        path_frame = tk.Frame(self.root, bg="#f0f0f0", pady=5, padx=10, relief="groove", bd=1)
        path_frame.pack(fill="x", side="top")

        tk.Label(path_frame, text="Save Path:", bg="#f0f0f0", fg="#666666").pack(side="left")
        
        self.lbl_path = tk.Label(
            path_frame, 
            text="No folder selected (Click üìÇ below)", 
            fg="red", 
            bg="#f0f0f0",
            font=CONFIG["FONT_PATH"],
            anchor="w"
        )
        self.lbl_path.pack(side="left", fill="x", expand=True, padx=(5, 0))

        # 2. Main Controls Container
        # Holds the inputs and the content area
        main_body = tk.Frame(self.root, padx=15, pady=15)
        main_body.pack(fill="both", expand=True)

        # -- Row A: [Folder Btn] [Filename Input] [Extension] --
        input_row = tk.Frame(main_body)
        input_row.pack(fill="x", pady=(0, 10))

        # Folder Button (Square, Icon-like)
        # Using Unicode üìÇ to keep it single-file without needing .png assets
        btn_folder = tk.Button(
            input_row, 
            text="üìÇ", 
            font=("Arial", 12),
            width=3, 
            command=self.select_folder,
            cursor="hand2"
        )
        btn_folder.pack(side="left", padx=(0, 10))

        # Filename Entry
        tk.Label(input_row, text="Name:").pack(side="left")
        self.ent_filename = tk.Entry(input_row, font=("Segoe UI", 10))
        self.ent_filename.pack(side="left", fill="x", expand=True, padx=(5, 5))

        # Extension Dropdown
        extensions = [".txt", ".py", ".md", ".json", ".csv", ".log", ".bat", ".sh", ".yaml", " (None)"]
        self.combo_ext = ttk.Combobox(
            input_row, 
            values=extensions, 
            textvariable=self.file_extension, 
            width=8,
            state="readonly"
        )
        self.combo_ext.pack(side="right")

        # -- Row B: Content Area --
        tk.Label(main_body, text="File Content:").pack(anchor="w", pady=(5, 0))
        
        # Frame for text + scrollbar
        text_frame = tk.Frame(main_body)
        text_frame.pack(fill="both", expand=True, pady=(2, 10))

        scrollbar = tk.Scrollbar(text_frame)
        scrollbar.pack(side="right", fill="y")

        self.txt_content = tk.Text(
            text_frame, 
            font=CONFIG["FONT_INPUT"],
            undo=True, # Polish: Allow Ctrl+Z
            yscrollcommand=scrollbar.set
        )
        self.txt_content.pack(fill="both", expand=True)
        scrollbar.config(command=self.txt_content.yview)

        # -- Row C: Footer (Timestamp + Save) --
        footer_frame = tk.Frame(main_body)
        footer_frame.pack(fill="x")

        chk_timestamp = tk.Checkbutton(
            footer_frame, 
            text="Append Date/Time to filename", 
            variable=self.use_timestamp
        )
        chk_timestamp.pack(side="left")

        # Save Button
        self.btn_save = tk.Button(
            footer_frame, 
            text="SAVE FILE", 
            state="disabled", 
            bg="#dddddd", 
            font=("Segoe UI", 9, "bold"),
            command=self.save_file,
            cursor="arrow"
        )
        self.btn_save.pack(side="right", padx=(10, 0), ipadx=20)


    def select_folder(self):
        folder_selected = filedialog.askdirectory()
        if folder_selected:
            self.selected_folder_path.set(folder_selected)
            # Update path label color/text
            self.lbl_path.config(text=folder_selected, fg="#0055aa") # Professional Blue
            # Enable save button with a visual cue (Greenish tint if supported, or standard)
            self.btn_save.config(state="normal", bg="#e1e1e1", cursor="hand2") 
        else:
            if not self.selected_folder_path.get():
                self.lbl_path.config(text="No folder selected (Click üìÇ below)", fg="red")
                self.btn_save.config(state="disabled", bg="#dddddd", cursor="arrow")

    def save_file(self):
        raw_name = self.ent_filename.get().strip()
        content = self.txt_content.get("1.0", tk.END)
        path = self.selected_folder_path.get()
        default_ext = self.file_extension.get()

        if not raw_name:
            messagebox.showwarning("Missing Data", "Please enter a file name.")
            self.ent_filename.focus_set()
            return

        # --- EXTENSION LOGIC ---
        if default_ext == " (None)":
            default_ext = ""

        base_name, user_ext = os.path.splitext(raw_name)
        final_ext = user_ext if user_ext else default_ext
        
        # --- TIMESTAMP LOGIC ---
        if self.use_timestamp.get():
            timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            filename = f"{base_name}_{timestamp}{final_ext}"
        else:
            filename = f"{base_name}{final_ext}"

        full_path = os.path.join(path, filename)

        # --- OVERWRITE PROTECTION ---
        if os.path.exists(full_path):
            confirm = messagebox.askyesno(
                "Confirm Overwrite", 
                f"The file '{filename}' already exists.\n\nDo you want to overwrite it?"
            )
            if not confirm:
                return 

        try:
            # Using newline='' handles line endings better across OSs
            with open(full_path, "w", encoding="utf-8", newline='') as f:
                f.write(content)
            
            messagebox.showinfo("Success", f"File Saved:\n{full_path}")
            
            # Reset logic
            self.ent_filename.delete(0, tk.END)
            self.txt_content.delete("1.0", tk.END)
            self.ent_filename.focus_set() # Jump back to name for next file
            
        except Exception as e:
            messagebox.showerror("Error", f"Could not save file:\n{e}")

# 4. CLI ENTRY POINT
def main():
    parser = argparse.ArgumentParser(description="Launch the Quick Text Generator GUI.")
    parser.add_argument("-v", "--verbose", action="store_true", help="Print status.")
    args = parser.parse_args()

    if args.verbose:
        print("Initializing Quick Text Generator...", file=sys.stderr)

    try:
        root = tk.Tk()
        app = TextFileGenerator(root)
        root.mainloop()
        sys.exit(0)
    except Exception as e:
        print(f"\nError: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: _TextTOUCHER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TokenizingPATCHER\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _TokenizingPATCHER\README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _TokenizingPATCHER\requirements.txt
--------------------------------------------------------------------------------
tk>=0.1.0
--------------------------------------------------------------------------------
FILE: _TokenizingPATCHER\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _TokenizingPATCHER\src\app.py
--------------------------------------------------------------------------------
import sys
import argparse
import tkinter as tk
from tkinter import ttk, filedialog, scrolledtext
import os
import json
import re
import datetime
import difflib
from dataclasses import dataclass, field

def get_asset_path(filename: str) -> str:
    """Resolves path to the assets directory relative to this script."""
    # src/app.py -> project_root/assets/filename
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    return os.path.join(base_dir, 'assets', filename)

# ==============================================================================
# INTERNAL UI HELPERS (Dependency Replacement)
# ==============================================================================

@dataclass
class ButtonConfig:
    text: str
    command: callable
    bg_color: str
    active_bg_color: str
    fg_color: str = "#FFFFFF"

@dataclass
class LinkConfig:
    """Configuration for the 'Linked' state (The Trap)"""
    trap_bg: str = "#7C3AED"    # Deep Purple
    btn_bg: str = "#8B5CF6"     # Lighter Purple
    text_color: str = "#FFFFFF"

class LocalUnifiedButtonGroup(tk.Frame):
    """
    The robust 'UnifiedButtonGroup' that supports linking actions (The Trap).
    """
    def __init__(self, parent, left_btn: ButtonConfig, right_btn: ButtonConfig, link_config: LinkConfig = None, **kwargs):
        super().__init__(parent, **kwargs)
        
        self.left_cfg = left_btn
        self.right_cfg = right_btn
        self.link_cfg = link_config or LinkConfig()
        
        self.is_linked = False
        # Try to grab parent bg, default to dark theme if failing
        try: 
            self.default_bg = parent.cget("bg")
        except: 
            self.default_bg = "#0f172a"

        self._setup_ui()
        self._update_state()

    def _setup_ui(self):
        self.config(padx=4, pady=4)
        
        # Base style for buttons
        common_style = {"relief": "flat", "font": ("Segoe UI", 9, "bold"), "bd": 0, "cursor": "hand2", "padx": 15, "pady": 5}
        link_style = {"relief": "flat", "font": ("Segoe UI", 10, "bold"), "bd": 0, "cursor": "hand2"}

        # 1. Left Button
        self.btn_left = tk.Button(self, command=lambda: self._execute("left"), **common_style)
        self.btn_left.pack(side="left", fill="y", padx=(0, 2))

        # 2. Link Toggle (The Chain)
        self.btn_link = tk.Button(self, text="&", width=3, command=self._toggle_link, **link_style)
        self.btn_link.pack(side="left", fill="y", padx=(0, 2))

        # 3. Right Button
        self.btn_right = tk.Button(self, command=lambda: self._execute("right"), **common_style)
        self.btn_right.pack(side="left", fill="y")

    def _toggle_link(self):
        self.is_linked = not self.is_linked
        self._update_state()

    def _update_state(self):
        if self.is_linked:
            # --- LINKED STATE (The Trap) ---
            self.config(bg=self.link_cfg.trap_bg)
            
            # Both buttons look identical in the "Trap"
            for btn in (self.btn_left, self.btn_right, self.btn_link):
                btn.config(bg=self.link_cfg.btn_bg, fg=self.link_cfg.text_color, activebackground=self.link_cfg.trap_bg)
            
            # Keep original text
            self.btn_left.config(text=self.left_cfg.text)
            self.btn_right.config(text=self.right_cfg.text)

        else:
            # --- INDEPENDENT STATE ---
            self.config(bg=self.default_bg)

            # Restore Left Button
            self.btn_left.config(
                text=self.left_cfg.text, 
                bg=self.left_cfg.bg_color, 
                fg=self.left_cfg.fg_color,
                activebackground=self.left_cfg.active_bg_color
            )

            # Restore Right Button
            self.btn_right.config(
                text=self.right_cfg.text, 
                bg=self.right_cfg.bg_color, 
                fg=self.left_cfg.fg_color,
                activebackground=self.right_cfg.active_bg_color
            )

            # Restore Link Button (Neutral Gray/Dark)
            self.btn_link.config(bg="#334155", fg="#94a3b8", activebackground="#475569")

    def _execute(self, source):
        if self.is_linked:
            # Chain them: Left then Right
            self.left_cfg.command()
            self.right_cfg.command()
        else:
            if source == "left": self.left_cfg.command()
            elif source == "right": self.right_cfg.command()

# ==============================================================================
# CORE LOGIC (Headless/Shared)
# ==============================================================================

class PatchError(Exception):
    pass

class StructuredLine:
    """Represents a single line split into indent + content + trailing whitespace."""
    __slots__ = ["indent", "content", "trailing", "original"]

    def __init__(self, line: str):
        self.original = line
        # Capture leading whitespace, core content, and trailing whitespace
        m = re.match(r"(^[ \t]*)(.*?)([ \t]*$)", line, re.DOTALL)
        if m:
            self.indent, self.content, self.trailing = m.group(1), m.group(2), m.group(3)
        else:
            self.indent, self.content, self.trailing = "", line, ""

    def reconstruct(self) -> str:
        return f"{self.indent}{self.content}{self.trailing}"

def tokenize_text(text: str):
    """Tokenize the raw file into StructuredLine objects and detect newline style."""
    if "\r\n" in text:
        newline = "\r\n"
    elif "\n" in text:
        newline = "\n"
    else:
        newline = "\n"

    raw_lines = text.splitlines()
    lines = [StructuredLine(l) for l in raw_lines]
    return lines, newline

def locate_hunk(file_lines, search_lines, floating=False):
    """Locate the hunk's search_lines inside file_lines."""
    if not search_lines:
        return []

    matches = []
    max_start = len(file_lines) - len(search_lines)
    for start in range(max_start + 1):
        ok = True
        for i, s in enumerate(search_lines):
            f = file_lines[start + i]
            if floating:
                # Compare logical content only
                if f.content != s.content:
                    ok = False
                    break
            else:
                # Compare fully reconstructed lines
                if f.reconstruct() != s.reconstruct():
                    ok = False
                    break
        if ok:
            matches.append(start)

    return matches

def apply_patch_text(original_text: str, patch_obj: dict, global_force_indent: bool = False) -> str:
    """Apply a patch schema instance to original_text and return the new text."""
    if not isinstance(patch_obj, dict) or "hunks" not in patch_obj:
        raise PatchError("Patch must be a dict with a 'hunks' list.")

    hunks = patch_obj.get("hunks", [])
    if not isinstance(hunks, list):
        raise PatchError("'hunks' must be a list.")

    file_lines, newline = tokenize_text(original_text)

    # First pass: compute all applications (start/end/replacements)
    applications = []
    for idx, hunk in enumerate(hunks, start=1):
        search_block = hunk.get("search_block")
        replace_block = hunk.get("replace_block")
        use_patch_indent = hunk.get("use_patch_indent", global_force_indent)

        if search_block is None or replace_block is None:
            raise PatchError(f"Hunk {idx}: Missing 'search_block' or 'replace_block'.")

        s_lines = [StructuredLine(l) for l in search_block.splitlines()]
        r_lines = [StructuredLine(l) for l in replace_block.splitlines()]

        # 1. Strict match
        matches = locate_hunk(file_lines, s_lines, floating=False)
        # 2. Fallback: content-only match
        if not matches:
            matches = locate_hunk(file_lines, s_lines, floating=True)

        if not matches:
            raise PatchError(f"Hunk {idx}: Search block not found.")
        if len(matches) > 1:
            raise PatchError(f"Hunk {idx}: Ambiguous match ({len(matches)} found).")

        start = matches[0]
        applications.append(
            {
                "start": start,
                "end": start + len(s_lines),
                "replace_lines": r_lines,
                "use_patch_indent": bool(use_patch_indent),
                "id": idx,
            }
        )

    # Collision check: ensure no overlapping edit ranges
    applications.sort(key=lambda a: a["start"])
    for i in range(len(applications) - 1):
        if applications[i]["end"] > applications[i + 1]["start"]:
            raise PatchError(
                f"Hunks {applications[i]['id']} and {applications[i+1]['id']} overlap in the target file."
            )

    # Apply from bottom up
    for app in reversed(applications):
        start = app["start"]
        end = app["end"]
        r_lines = app["replace_lines"]
        use_patch_indent = app["use_patch_indent"]

        base_indent = ""
        # Get the indentation of the anchor point in the FILE
        if 0 <= start < len(file_lines):
            base_indent = file_lines[start].indent

        # Get the indentation of the anchor point in the PATCH (First non-empty line)
        patch_base_indent = ""
        for rl in r_lines:
            if rl.content.strip():
                patch_base_indent = rl.indent
                break

        final_block = []
        for rl in r_lines:
            # If we are strictly using patch indent, do nothing.
            # Otherwise, calculate relative indentation.
            if not use_patch_indent:
                if rl.content.strip():
                    # 1. Remove the patch's baseline indent from this line
                    #    (Careful: this assumes rl.indent starts with patch_base_indent)
                    if rl.indent.startswith(patch_base_indent):
                        relative_indent = rl.indent[len(patch_base_indent):]
                    else:
                        # Fallback: if patch is weirdly dedented, keep original
                        relative_indent = rl.indent
                    
                    # 2. Add the file's base indent + the relative indent
                    rl.indent = base_indent + relative_indent
                
                # OPTIONAL: Handle empty lines (copy base indent or leave empty?)
                # Usually leaving them empty (just \n) is safer for git/linting.

            final_block.append(rl)

        file_lines[start:end] = final_block

    return newline.join([l.reconstruct() for l in file_lines])


# ==============================================================================
# GUI MODE (Default / Showcase)
# ==============================================================================

class App:
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("_TokenizingPATCHER v4.3 [System Ecosystem]")
        self.root.geometry("1100x850")
        self.root.configure(bg="#0f172a")
        
        # Load Icon
        try:
            icon_path = get_asset_path(os.path.join('icons', 'tokenizing-patcher.png'))
            if os.path.exists(icon_path):
                img = tk.PhotoImage(file=icon_path)
                self.root.iconphoto(False, img)
        except Exception as e:
            print(f"Warning: Could not load icon: {e}")

        self.loaded_filepath = None

        # State variables
        self.version_enabled_var = tk.BooleanVar(value=False)
        self.version_suffix_var = tk.StringVar(value="_v1.0")
        self.force_indent_var = tk.BooleanVar(value=False)
        self.is_blinking = False

        # Validation / diff preview state
        self.validation_preview_text = None
        self.validation_valid = False
        self.diff_view_var = tk.BooleanVar(value=False)

        self.setup_styles()
        self.build_ui()

    def setup_styles(self):
        style = ttk.Style()
        style.theme_use("clam")
        style.configure(
            "TButton",
            padding=6,
            relief="flat",
            background="#334155",
            foreground="white",
        )
        style.map("TButton", background=[("active", "#475569")])

        self.colors = {
            "bg": "#0f172a",
            "panel_bg": "#1e293b",
            "text": "#e2e8f0",
            "accent": "#6366f1",
            "success": "#22c55e",
            "error": "#ef4444",
            "working": "#facc15",
            "log_bg": "#020617",
            "log_fg": "#94a3b8",
        }

    def build_ui(self):
        # --- Top Toolbar ---
        toolbar = tk.Frame(self.root, bg=self.colors["bg"])
        toolbar.pack(fill="x", padx=15, pady=10)

        # File operations
        tk.Button(
            toolbar,
            text="üìÇ Load File",
            command=self.load_file,
            bg="#334155",
            fg="white",
            relief="flat",
        ).pack(side="left", padx=(0, 5))

        tk.Button(
            toolbar,
            text="üíæ Save Result",
            command=self.save_file,
            bg="#334155",
            fg="white",
            relief="flat",
        ).pack(side="left", padx=5)

        # Versioning controls
        v_frame = tk.Frame(toolbar, bg=self.colors["bg"])
        v_frame.pack(side="left", padx=15)

        chk_ver = tk.Checkbutton(
            v_frame,
            text="Version",
            variable=self.version_enabled_var,
            bg=self.colors["bg"],
            fg="#cbd5e1",
            selectcolor=self.colors["bg"],
            activebackground=self.colors["bg"],
            activeforeground="white",
        )
        chk_ver.pack(side="left")

        tk.Label(
            v_frame,
            text="Suffix:",
            bg=self.colors["bg"],
            fg="#64748b",
        ).pack(side="left", padx=(10, 5))

        tk.Entry(
            v_frame,
            textvariable=self.version_suffix_var,
            width=10,
        ).pack(side="left")

        # --- Main Paned Layout ---
        paned = tk.PanedWindow(
            self.root,
            orient="horizontal",
            sashrelief="raised",
            bg=self.colors["bg"],
        )
        paned.pack(fill="both", expand=True, padx=15, pady=5)

        # LEFT PANE
        left_frame = tk.Frame(paned, bg=self.colors["panel_bg"])
        l_hdr = tk.Frame(left_frame, bg=self.colors["panel_bg"])
        l_hdr.pack(fill="x", padx=5, pady=5)

        self.lbl_left_title = tk.Label(
            l_hdr,
            text="TARGET SOURCE CODE",
            fg="#94a3b8",
            bg=self.colors["panel_bg"],
            font=("Segoe UI", 8, "bold"),
        )
        self.lbl_left_title.pack(side="left")

        self.txt_file = scrolledtext.ScrolledText(
            left_frame,
            bg=self.colors["panel_bg"],
            fg=self.colors["text"],
            insertbackground="white",
            borderwidth=0,
        )
        self.txt_file.pack(fill="both", expand=True)

        paned.add(left_frame)

        # RIGHT PANE
        right_frame = tk.Frame(paned, bg=self.colors["panel_bg"])
        r_hdr = tk.Frame(right_frame, bg=self.colors["panel_bg"])
        r_hdr.pack(fill="x", padx=5, pady=5)

        self.lbl_patch_title = tk.Label(
            r_hdr,
            text="Patch: UNVALIDATED",
            fg="#94a3b8",
            bg=self.colors["panel_bg"],
            font=("Segoe UI", 8, "bold"),
        )
        self.lbl_patch_title.pack(side="left")

        tk.Button(
            r_hdr,
            text="üìã Schema",
            command=self.copy_schema_to_clipboard,
            bg="#334155",
            fg="white",
            font=("Segoe UI", 8),
            relief="flat",
        ).pack(side="right", padx=5)

        self.txt_patch = scrolledtext.ScrolledText(
            right_frame,
            bg="#020617",
            fg="#cbd5e1",
            insertbackground="white",
            borderwidth=0,
        )
        self.txt_patch.pack(fill="both", expand=True)
        self.txt_patch.insert("1.0", self.get_schema_template())

        paned.add(right_frame)

        # --- Action Footer ---
        footer = tk.Frame(self.root, bg=self.colors["bg"])
        footer.pack(fill="x", padx=15, pady=10)

        chk_indent = tk.Checkbutton(
            footer,
            text="Force Patch Indentation (Strict Whitespace)",
            variable=self.force_indent_var,
            bg=self.colors["bg"],
            fg="#cbd5e1",
            selectcolor=self.colors["bg"],
            activebackground=self.colors["bg"],
            activeforeground="white",
        )
        chk_indent.pack(side="left")

        chk_diff = tk.Checkbutton(
            footer,
            text="Show Diff Preview",
            variable=self.diff_view_var,
            bg=self.colors["bg"],
            fg="#cbd5e1",
            selectcolor=self.colors["bg"],
            activebackground=self.colors["bg"],
            activeforeground="white",
            command=self.on_diff_toggle,
        )
        chk_diff.pack(side="left", padx=(10, 0))

        # Unified Validate / Apply group
        btn_val_config = ButtonConfig(
            text="Validate",
            command=self.validate_patch,
            bg_color="#10B981",  # Emerald Green
            active_bg_color="#059669"
        )

        btn_apply_config = ButtonConfig(
            text="Apply",
            command=self.apply_patch,
            bg_color="#2563EB",  # Royal Blue
            active_bg_color="#1D4ED8"
        )

        # Using the Local replacement here
        self.button_group = LocalUnifiedButtonGroup(
            parent=footer,
            left_btn=btn_val_config,
            right_btn=btn_apply_config
        )
        self.button_group.pack(side="right")

        # --- Log Window ---
        self.debug_out = scrolledtext.ScrolledText(
            self.root,
            height=6,
            bg=self.colors["log_bg"],
            fg=self.colors["log_fg"],
            insertbackground="white",
            borderwidth=0,
            font=("Consolas", 9),
        )
        self.debug_out.pack(fill="x", padx=15, pady=(0, 5))

        # --- Status Bar (Blinking) ---
        self.status_bar = tk.Label(
            self.root,
            text="Ready",
            bg=self.colors["bg"],
            fg=self.colors["success"],
            font=("Consolas", 10),
            anchor="w",
            padx=15,            pady=5,
        )
        self.status_bar.pack(fill="x", side="bottom")

    # --- Status / Blink Helpers ---

    def set_status(self, msg, state="info"):
        colors = {
            "info": self.colors["success"],
            "error": self.colors["error"],
            "working": self.colors["working"],
        }
        color = colors.get(state, self.colors["success"])

        # 1. Status label text
        self.status_bar.config(text=msg, fg=color)
        if state == "working":
            self.start_blink()
        else:
            self.stop_blink()

        # 2. Log window
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        prefix = "ERROR: " if state == "error" else ""
        log_entry = f"[{timestamp}] {prefix}{msg}\n"
        self.debug_out.insert(tk.END, log_entry)
        self.debug_out.see(tk.END)

    def start_blink(self):
        self.is_blinking = True
        self._blink_loop()

    def stop_blink(self):
        self.is_blinking = False
        # Reset to working color when stopping from blink
        try:
            self.status_bar.config(fg=self.colors["success"])
        except Exception:
            pass

    def _blink_loop(self):
        if not self.is_blinking:
            return
        current_fg = self.status_bar.cget("foreground")
        next_fg = self.colors["bg"] if current_fg == self.colors["working"] else self.colors["working"]
        self.status_bar.config(fg=next_fg)
        self.root.after(600, self._blink_loop)

    # --- File Logic & Diff Preview ---

    def load_file(self):
        path = filedialog.askopenfilename()
        if not path:
            return
        try:
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
            self.txt_file.delete("1.0", tk.END)
            self.txt_file.insert("1.0", content)
            self.loaded_filepath = path

            # Reset panel titles
            self.lbl_left_title.config(text="TARGET SOURCE CODE")
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")

            # Reset validation / diff state
            self.validation_preview_text = None
            self.validation_valid = False
            self.diff_view_var.set(False)

            self.set_status(f"Loaded: {path}", "info")
        except Exception as e:
            self.set_status(f"Error loading file: {e}", "error")

    def save_file(self):
        orig = self.loaded_filepath
        if orig and self.version_enabled_var.get():
            d, f = os.path.split(orig)
            base, ext = os.path.splitext(f)
            suffix = self.version_suffix_var.get()
            if suffix and not suffix.startswith("_"):
                suffix = "_" + suffix
            filename = f"{base}{suffix}{ext}"
            path = os.path.join(d, filename)
        elif orig:
            path = orig
        else:
            path = filedialog.asksaveasfilename(defaultextension=".txt")

        if not path:
            return

        try:
            with open(path, "w", encoding="utf-8") as f:
                f.write(self.txt_file.get("1.0", tk.END))
            self.set_status(f"Saved to: {os.path.basename(path)}", "info")
        except Exception as e:
            self.set_status(f"Save failed: {e}", "error")

    def get_schema_template(self):
        return (
            "{\n"
            "  \"hunks\": [\n"
            "    {\n"
            "      \"description\": \"Short human description\",\n"
            "      \"search_block\": \"exact text to find\\n(can span multiple lines)\",\n"
            "      \"replace_block\": \"replacement text\\n(same or different length)\",\n"
            "      \"use_patch_indent\": false\n"
            "    }\n"
            "  ]\n"
            "}\n"
        )

    def copy_schema_to_clipboard(self):
        self.root.clipboard_clear()
        self.root.clipboard_append(self.get_schema_template())
        self.set_status("Schema copied to clipboard.", "info")

    def _show_diff_view(self, original_text: str, preview_text: str):
        orig_lines = original_text.splitlines()
        new_lines = preview_text.splitlines()
        diff_lines = difflib.unified_diff(
            orig_lines,
            new_lines,
            fromfile="original",
            tofile="preview",
            lineterm="",
        )
        diff_text = "\n".join(diff_lines) or "(No differences)\n"
        self.txt_file.delete("1.0", tk.END)
        self.txt_file.insert("1.0", diff_text)

    def on_diff_toggle(self):
        # Only meaningful if we have a valid preview
        if not self.validation_valid or not self.validation_preview_text:
            return

        current_source = self.txt_file.get("1.0", tk.END)
        # If diff is being turned on, show diff between current source and preview
        if self.diff_view_var.get():
            self._show_diff_view(current_source, self.validation_preview_text)
        else:
            # Restore plain source view (just show current source as-is)
            self.txt_file.delete("1.0", tk.END)
            self.txt_file.insert("1.0", current_source)

    # --- Validate / Apply ---

    def validate_patch(self):
        original_text = self.txt_file.get("1.0", tk.END)
        patch_text = self.txt_patch.get("1.0", tk.END)
        force = self.force_indent_var.get()

        try:
            patch_obj = json.loads(patch_text)
            preview_text = apply_patch_text(
                original_text,
                patch_obj,
                global_force_indent=force,
            )

            self.validation_preview_text = preview_text
            self.validation_valid = True

            self.lbl_patch_title.config(text="Patch: VALIDATED (DRY RUN)")
            self.set_status("Validation succeeded (dry run).", "info")

            if self.diff_view_var.get():
                self._show_diff_view(original_text, preview_text)
            else:
                # Ensure left panel shows the original text
                self.txt_file.delete("1.0", tk.END)
                self.txt_file.insert("1.0", original_text)

        except json.JSONDecodeError:
            self.validation_preview_text = None
            self.validation_valid = False
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status("Error: Invalid JSON. Check formatting.", "error")

        except PatchError as e:
            self.validation_preview_text = None
            self.validation_valid = False
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status(f"Patch Error during validation: {e}", "error")

        except Exception as e:
            self.validation_preview_text = None
            self.validation_valid = False
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status(
                f"System Error during validation: {e}",
                "error",
            )

    def apply_patch(self):
        target_text = self.txt_file.get("1.0", tk.END)
        patch_text = self.txt_patch.get("1.0", tk.END)
        force = self.force_indent_var.get()

        try:
            if self.validation_valid and self.validation_preview_text:
                new_text = self.validation_preview_text
            else:
                patch_obj = json.loads(patch_text)
                new_text = apply_patch_text(
                    target_text,
                    patch_obj,
                    global_force_indent=force,
                )

            self.txt_file.delete("1.0", tk.END)
            self.txt_file.insert("1.0", new_text)

            # Clear validation/diff state after commit
            self.validation_preview_text = None
            self.validation_valid = False
            self.diff_view_var.set(False)

            self.lbl_left_title.config(text="PATCHED SOURCE CODE")
            self.lbl_patch_title.config(text="Patch: VALIDATED & APPLIED")
            self.set_status("Success: Patch Applied.", "info")

        except json.JSONDecodeError:
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status("Error: Invalid JSON.", "error")

        except PatchError as e:
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status(f"Patch Error: {e}", "error")

        except Exception as e:
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status(f"System Error: {e}", "error")

    def start(self):
        self.root.mainloop()

def run_gui():
    print("Launching GUI Mode...")
    app = App()
    app.start()

# ==============================================================================
# CLI MODE (Utility)
# ==============================================================================

def run_cli():
    """
    Command Line Interface Entry Point.
    """
    parser = argparse.ArgumentParser(description="_TokenizingPATCHER CLI")
    parser.add_argument("target", help="Path to the target source file")
    parser.add_argument("patch", help="Path to the JSON patch file")
    parser.add_argument("--output", "-o", help="Path to save the result (defaults to print stdout)")
    parser.add_argument("--force-indent", action="store_true", help="Force patch indentation")
    parser.add_argument("--dry-run", action="store_true", help="Validate only, do not write")
    
    args = parser.parse_args()
    
    # 1. Read Target
    try:
        with open(args.target, "r", encoding="utf-8") as f:
            target_text = f.read()
    except FileNotFoundError:
        print(f"Error: Target file not found: {args.target}")
        sys.exit(1)

    # 2. Read Patch
    try:
        with open(args.patch, "r", encoding="utf-8") as f:
            patch_obj = json.load(f)
    except FileNotFoundError:
        print(f"Error: Patch file not found: {args.patch}")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"Error: Patch file is not valid JSON.")
        sys.exit(1)

    # 3. Apply
    try:
        new_text = apply_patch_text(target_text, patch_obj, global_force_indent=args.force_indent)
        
        if args.dry_run:
            print("Dry Run Successful. Patch applies cleanly.")
        else:
            if args.output:
                with open(args.output, "w", encoding="utf-8") as f:
                    f.write(new_text)
                print(f"Success: Patched file written to {args.output}")
            else:
                print(new_text)
                
    except PatchError as e:
        print(f"Patch Failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"Unexpected Error: {e}")
        sys.exit(1)

# ==============================================================================
# HYBRID ENTRY POINT
# ==============================================================================

def main():
    if len(sys.argv) > 1:
        run_cli()
    else:
        run_gui()

if __name__ == "__main__":
    main()


--------------------------------------------------------------------------------
FILE: _TokenizingPATCHER\src\Patcher-app-python-code-insde.txt
--------------------------------------------------------------------------------
import sys
import argparse
import tkinter as tk
from tkinter import ttk, filedialog, scrolledtext
import os
import json
import re
import datetime
import difflib
from dataclasses import dataclass, field

def get_asset_path(filename: str) -> str:
    """Resolves path to the assets directory relative to this script."""
    # src/app.py -> project_root/assets/filename
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    return os.path.join(base_dir, 'assets', filename)

# ==============================================================================
# INTERNAL UI HELPERS (Dependency Replacement)
# ==============================================================================

@dataclass
class ButtonConfig:
    text: str
    command: callable
    bg_color: str
    active_bg_color: str
    fg_color: str = "#FFFFFF"

@dataclass
class LinkConfig:
    """Configuration for the 'Linked' state (The Trap)"""
    trap_bg: str = "#7C3AED"    # Deep Purple
    btn_bg: str = "#8B5CF6"     # Lighter Purple
    text_color: str = "#FFFFFF"

class LocalUnifiedButtonGroup(tk.Frame):
    """
    The robust 'UnifiedButtonGroup' that supports linking actions (The Trap).
    """
    def __init__(self, parent, left_btn: ButtonConfig, right_btn: ButtonConfig, link_config: LinkConfig = None, **kwargs):
        super().__init__(parent, **kwargs)
        
        self.left_cfg = left_btn
        self.right_cfg = right_btn
        self.link_cfg = link_config or LinkConfig()
        
        self.is_linked = False
        # Try to grab parent bg, default to dark theme if failing
        try: 
            self.default_bg = parent.cget("bg")
        except: 
            self.default_bg = "#0f172a"

        self._setup_ui()
        self._update_state()

    def _setup_ui(self):
        self.config(padx=4, pady=4)
        
        # Base style for buttons
        common_style = {"relief": "flat", "font": ("Segoe UI", 9, "bold"), "bd": 0, "cursor": "hand2", "padx": 15, "pady": 5}
        link_style = {"relief": "flat", "font": ("Segoe UI", 10, "bold"), "bd": 0, "cursor": "hand2"}

        # 1. Left Button
        self.btn_left = tk.Button(self, command=lambda: self._execute("left"), **common_style)
        self.btn_left.pack(side="left", fill="y", padx=(0, 2))

        # 2. Link Toggle (The Chain)
        self.btn_link = tk.Button(self, text="&", width=3, command=self._toggle_link, **link_style)
        self.btn_link.pack(side="left", fill="y", padx=(0, 2))

        # 3. Right Button
        self.btn_right = tk.Button(self, command=lambda: self._execute("right"), **common_style)
        self.btn_right.pack(side="left", fill="y")

    def _toggle_link(self):
        self.is_linked = not self.is_linked
        self._update_state()

    def _update_state(self):
        if self.is_linked:
            # --- LINKED STATE (The Trap) ---
            self.config(bg=self.link_cfg.trap_bg)
            
            # Both buttons look identical in the "Trap"
            for btn in (self.btn_left, self.btn_right, self.btn_link):
                btn.config(bg=self.link_cfg.btn_bg, fg=self.link_cfg.text_color, activebackground=self.link_cfg.trap_bg)
            
            # Keep original text
            self.btn_left.config(text=self.left_cfg.text)
            self.btn_right.config(text=self.right_cfg.text)

        else:
            # --- INDEPENDENT STATE ---
            self.config(bg=self.default_bg)

            # Restore Left Button
            self.btn_left.config(
                text=self.left_cfg.text, 
                bg=self.left_cfg.bg_color, 
                fg=self.left_cfg.fg_color,
                activebackground=self.left_cfg.active_bg_color
            )

            # Restore Right Button
            self.btn_right.config(
                text=self.right_cfg.text, 
                bg=self.right_cfg.bg_color, 
                fg=self.left_cfg.fg_color,
                activebackground=self.right_cfg.active_bg_color
            )

            # Restore Link Button (Neutral Gray/Dark)
            self.btn_link.config(bg="#334155", fg="#94a3b8", activebackground="#475569")

    def _execute(self, source):
        if self.is_linked:
            # Chain them: Left then Right
            self.left_cfg.command()
            self.right_cfg.command()
        else:
            if source == "left": self.left_cfg.command()
            elif source == "right": self.right_cfg.command()

# ==============================================================================
# CORE LOGIC (Headless/Shared)
# ==============================================================================

class PatchError(Exception):
    pass

class StructuredLine:
    """Represents a single line split into indent + content + trailing whitespace."""
    __slots__ = ["indent", "content", "trailing", "original"]

    def __init__(self, line: str):
        self.original = line
        # Capture leading whitespace, core content, and trailing whitespace
        m = re.match(r"(^[ \t]*)(.*?)([ \t]*$)", line, re.DOTALL)
        if m:
            self.indent, self.content, self.trailing = m.group(1), m.group(2), m.group(3)
        else:
            self.indent, self.content, self.trailing = "", line, ""

    def reconstruct(self) -> str:
        return f"{self.indent}{self.content}{self.trailing}"

def tokenize_text(text: str):
    """Tokenize the raw file into StructuredLine objects and detect newline style."""
    if "\r\n" in text:
        newline = "\r\n"
    elif "\n" in text:
        newline = "\n"
    else:
        newline = "\n"

    raw_lines = text.splitlines()
    lines = [StructuredLine(l) for l in raw_lines]
    return lines, newline

def locate_hunk(file_lines, search_lines, floating=False):
    """Locate the hunk's search_lines inside file_lines."""
    if not search_lines:
        return []

    matches = []
    max_start = len(file_lines) - len(search_lines)
    for start in range(max_start + 1):
        ok = True
        for i, s in enumerate(search_lines):
            f = file_lines[start + i]
            if floating:
                # Compare logical content only
                if f.content != s.content:
                    ok = False
                    break
            else:
                # Compare fully reconstructed lines
                if f.reconstruct() != s.reconstruct():
                    ok = False
                    break
        if ok:
            matches.append(start)

    return matches

def apply_patch_text(original_text: str, patch_obj: dict, global_force_indent: bool = False) -> str:
    """Apply a patch schema instance to original_text and return the new text."""
    if not isinstance(patch_obj, dict) or "hunks" not in patch_obj:
        raise PatchError("Patch must be a dict with a 'hunks' list.")

    hunks = patch_obj.get("hunks", [])
    if not isinstance(hunks, list):
        raise PatchError("'hunks' must be a list.")

    file_lines, newline = tokenize_text(original_text)

    # First pass: compute all applications (start/end/replacements)
    applications = []
    for idx, hunk in enumerate(hunks, start=1):
        search_block = hunk.get("search_block")
        replace_block = hunk.get("replace_block")
        use_patch_indent = hunk.get("use_patch_indent", global_force_indent)

        if search_block is None or replace_block is None:
            raise PatchError(f"Hunk {idx}: Missing 'search_block' or 'replace_block'.")

        s_lines = [StructuredLine(l) for l in search_block.splitlines()]
        r_lines = [StructuredLine(l) for l in replace_block.splitlines()]

        # 1. Strict match
        matches = locate_hunk(file_lines, s_lines, floating=False)
        # 2. Fallback: content-only match
        if not matches:
            matches = locate_hunk(file_lines, s_lines, floating=True)

        if not matches:
            raise PatchError(f"Hunk {idx}: Search block not found.")
        if len(matches) > 1:
            raise PatchError(f"Hunk {idx}: Ambiguous match ({len(matches)} found).")

        start = matches[0]
        applications.append(
            {
                "start": start,
                "end": start + len(s_lines),
                "replace_lines": r_lines,
                "use_patch_indent": bool(use_patch_indent),
                "id": idx,
            }
        )

    # Collision check: ensure no overlapping edit ranges
    applications.sort(key=lambda a: a["start"])
    for i in range(len(applications) - 1):
        if applications[i]["end"] > applications[i + 1]["start"]:
            raise PatchError(
                f"Hunks {applications[i]['id']} and {applications[i+1]['id']} overlap in the target file."
            )

    # Apply from bottom up
    for app in reversed(applications):
        start = app["start"]
        end = app["end"]
        r_lines = app["replace_lines"]
        use_patch_indent = app["use_patch_indent"]

        base_indent = ""
        # Get the indentation of the anchor point in the FILE
        if 0 <= start < len(file_lines):
            base_indent = file_lines[start].indent

        # Get the indentation of the anchor point in the PATCH (First non-empty line)
        patch_base_indent = ""
        for rl in r_lines:
            if rl.content.strip():
                patch_base_indent = rl.indent
                break

        final_block = []
        for rl in r_lines:
            # If we are strictly using patch indent, do nothing.
            # Otherwise, calculate relative indentation.
            if not use_patch_indent:
                if rl.content.strip():
                    # 1. Remove the patch's baseline indent from this line
                    #    (Careful: this assumes rl.indent starts with patch_base_indent)
                    if rl.indent.startswith(patch_base_indent):
                        relative_indent = rl.indent[len(patch_base_indent):]
                    else:
                        # Fallback: if patch is weirdly dedented, keep original
                        relative_indent = rl.indent
                    
                    # 2. Add the file's base indent + the relative indent
                    rl.indent = base_indent + relative_indent
                
                # OPTIONAL: Handle empty lines (copy base indent or leave empty?)
                # Usually leaving them empty (just \n) is safer for git/linting.

            final_block.append(rl)

        file_lines[start:end] = final_block

    return newline.join([l.reconstruct() for l in file_lines])


# ==============================================================================
# GUI MODE (Default / Showcase)
# ==============================================================================

class App:
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("_TokenizingPATCHER v4.3 [System Ecosystem]")
        self.root.geometry("1100x850")
        self.root.configure(bg="#0f172a")
        
        # Load Icon
        try:
            icon_path = get_asset_path(os.path.join('icons', 'tokenizing-patcher.png'))
            if os.path.exists(icon_path):
                img = tk.PhotoImage(file=icon_path)
                self.root.iconphoto(False, img)
        except Exception as e:
            print(f"Warning: Could not load icon: {e}")

        self.loaded_filepath = None

        # State variables
        self.version_enabled_var = tk.BooleanVar(value=False)
        self.version_suffix_var = tk.StringVar(value="_v1.0")
        self.force_indent_var = tk.BooleanVar(value=False)
        self.is_blinking = False

        # Validation / diff preview state
        self.validation_preview_text = None
        self.validation_valid = False
        self.diff_view_var = tk.BooleanVar(value=False)

        self.setup_styles()
        self.build_ui()

    def setup_styles(self):
        style = ttk.Style()
        style.theme_use("clam")
        style.configure(
            "TButton",
            padding=6,
            relief="flat",
            background="#334155",
            foreground="white",
        )
        style.map("TButton", background=[("active", "#475569")])

        self.colors = {
            "bg": "#0f172a",
            "panel_bg": "#1e293b",
            "text": "#e2e8f0",
            "accent": "#6366f1",
            "success": "#22c55e",
            "error": "#ef4444",
            "working": "#facc15",
            "log_bg": "#020617",
            "log_fg": "#94a3b8",
        }

    def build_ui(self):
        # --- Top Toolbar ---
        toolbar = tk.Frame(self.root, bg=self.colors["bg"])
        toolbar.pack(fill="x", padx=15, pady=10)

        # File operations
        tk.Button(
            toolbar,
            text="üìÇ Load File",
            command=self.load_file,
            bg="#334155",
            fg="white",
            relief="flat",
        ).pack(side="left", padx=(0, 5))

        tk.Button(
            toolbar,
            text="üíæ Save Result",
            command=self.save_file,
            bg="#334155",
            fg="white",
            relief="flat",
        ).pack(side="left", padx=5)

        # Versioning controls
        v_frame = tk.Frame(toolbar, bg=self.colors["bg"])
        v_frame.pack(side="left", padx=15)

        chk_ver = tk.Checkbutton(
            v_frame,
            text="Version",
            variable=self.version_enabled_var,
            bg=self.colors["bg"],
            fg="#cbd5e1",
            selectcolor=self.colors["bg"],
            activebackground=self.colors["bg"],
            activeforeground="white",
        )
        chk_ver.pack(side="left")

        tk.Label(
            v_frame,
            text="Suffix:",
            bg=self.colors["bg"],
            fg="#64748b",
        ).pack(side="left", padx=(10, 5))

        tk.Entry(
            v_frame,
            textvariable=self.version_suffix_var,
            width=10,
        ).pack(side="left")

        # --- Main Paned Layout ---
        paned = tk.PanedWindow(
            self.root,
            orient="horizontal",
            sashrelief="raised",
            bg=self.colors["bg"],
        )
        paned.pack(fill="both", expand=True, padx=15, pady=5)

        # LEFT PANE
        left_frame = tk.Frame(paned, bg=self.colors["panel_bg"])
        l_hdr = tk.Frame(left_frame, bg=self.colors["panel_bg"])
        l_hdr.pack(fill="x", padx=5, pady=5)

        self.lbl_left_title = tk.Label(
            l_hdr,
            text="TARGET SOURCE CODE",
            fg="#94a3b8",
            bg=self.colors["panel_bg"],
            font=("Segoe UI", 8, "bold"),
        )
        self.lbl_left_title.pack(side="left")

        self.txt_file = scrolledtext.ScrolledText(
            left_frame,
            bg=self.colors["panel_bg"],
            fg=self.colors["text"],
            insertbackground="white",
            borderwidth=0,
        )
        self.txt_file.pack(fill="both", expand=True)

        paned.add(left_frame)

        # RIGHT PANE
        right_frame = tk.Frame(paned, bg=self.colors["panel_bg"])
        r_hdr = tk.Frame(right_frame, bg=self.colors["panel_bg"])
        r_hdr.pack(fill="x", padx=5, pady=5)

        self.lbl_patch_title = tk.Label(
            r_hdr,
            text="Patch: UNVALIDATED",
            fg="#94a3b8",
            bg=self.colors["panel_bg"],
            font=("Segoe UI", 8, "bold"),
        )
        self.lbl_patch_title.pack(side="left")

        tk.Button(
            r_hdr,
            text="üìã Schema",
            command=self.copy_schema_to_clipboard,
            bg="#334155",
            fg="white",
            font=("Segoe UI", 8),
            relief="flat",
        ).pack(side="right", padx=5)

        self.txt_patch = scrolledtext.ScrolledText(
            right_frame,
            bg="#020617",
            fg="#cbd5e1",
            insertbackground="white",
            borderwidth=0,
        )
        self.txt_patch.pack(fill="both", expand=True)
        self.txt_patch.insert("1.0", self.get_schema_template())

        paned.add(right_frame)

        # --- Action Footer ---
        footer = tk.Frame(self.root, bg=self.colors["bg"])
        footer.pack(fill="x", padx=15, pady=10)

        chk_indent = tk.Checkbutton(
            footer,
            text="Force Patch Indentation (Strict Whitespace)",
            variable=self.force_indent_var,
            bg=self.colors["bg"],
            fg="#cbd5e1",
            selectcolor=self.colors["bg"],
            activebackground=self.colors["bg"],
            activeforeground="white",
        )
        chk_indent.pack(side="left")

        chk_diff = tk.Checkbutton(
            footer,
            text="Show Diff Preview",
            variable=self.diff_view_var,
            bg=self.colors["bg"],
            fg="#cbd5e1",
            selectcolor=self.colors["bg"],
            activebackground=self.colors["bg"],
            activeforeground="white",
            command=self.on_diff_toggle,
        )
        chk_diff.pack(side="left", padx=(10, 0))

        # Unified Validate / Apply group
        btn_val_config = ButtonConfig(
            text="Validate",
            command=self.validate_patch,
            bg_color="#10B981",  # Emerald Green
            active_bg_color="#059669"
        )

        btn_apply_config = ButtonConfig(
            text="Apply",
            command=self.apply_patch,
            bg_color="#2563EB",  # Royal Blue
            active_bg_color="#1D4ED8"
        )

        # Using the Local replacement here
        self.button_group = LocalUnifiedButtonGroup(
            parent=footer,
            left_btn=btn_val_config,
            right_btn=btn_apply_config
        )
        self.button_group.pack(side="right")

        # --- Log Window ---
        self.debug_out = scrolledtext.ScrolledText(
            self.root,
            height=6,
            bg=self.colors["log_bg"],
            fg=self.colors["log_fg"],
            insertbackground="white",
            borderwidth=0,
            font=("Consolas", 9),
        )
        self.debug_out.pack(fill="x", padx=15, pady=(0, 5))

        # --- Status Bar (Blinking) ---
        self.status_bar = tk.Label(
            self.root,
            text="Ready",
            bg=self.colors["bg"],
            fg=self.colors["success"],
            font=("Consolas", 10),
            anchor="w",
            padx=15,            pady=5,
        )
        self.status_bar.pack(fill="x", side="bottom")

    # --- Status / Blink Helpers ---

    def set_status(self, msg, state="info"):
        colors = {
            "info": self.colors["success"],
            "error": self.colors["error"],
            "working": self.colors["working"],
        }
        color = colors.get(state, self.colors["success"])

        # 1. Status label text
        self.status_bar.config(text=msg, fg=color)
        if state == "working":
            self.start_blink()
        else:
            self.stop_blink()

        # 2. Log window
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        prefix = "ERROR: " if state == "error" else ""
        log_entry = f"[{timestamp}] {prefix}{msg}\n"
        self.debug_out.insert(tk.END, log_entry)
        self.debug_out.see(tk.END)

    def start_blink(self):
        self.is_blinking = True
        self._blink_loop()

    def stop_blink(self):
        self.is_blinking = False
        # Reset to working color when stopping from blink
        try:
            self.status_bar.config(fg=self.colors["success"])
        except Exception:
            pass

    def _blink_loop(self):
        if not self.is_blinking:
            return
        current_fg = self.status_bar.cget("foreground")
        next_fg = self.colors["bg"] if current_fg == self.colors["working"] else self.colors["working"]
        self.status_bar.config(fg=next_fg)
        self.root.after(600, self._blink_loop)

    # --- File Logic & Diff Preview ---

    def load_file(self):
        path = filedialog.askopenfilename()
        if not path:
            return
        try:
            with open(path, "r", encoding="utf-8") as f:
                content = f.read()
            self.txt_file.delete("1.0", tk.END)
            self.txt_file.insert("1.0", content)
            self.loaded_filepath = path

            # Reset panel titles
            self.lbl_left_title.config(text="TARGET SOURCE CODE")
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")

            # Reset validation / diff state
            self.validation_preview_text = None
            self.validation_valid = False
            self.diff_view_var.set(False)

            self.set_status(f"Loaded: {path}", "info")
        except Exception as e:
            self.set_status(f"Error loading file: {e}", "error")

    def save_file(self):
        orig = self.loaded_filepath
        if orig and self.version_enabled_var.get():
            d, f = os.path.split(orig)
            base, ext = os.path.splitext(f)
            suffix = self.version_suffix_var.get()
            if suffix and not suffix.startswith("_"):
                suffix = "_" + suffix
            filename = f"{base}{suffix}{ext}"
            path = os.path.join(d, filename)
        elif orig:
            path = orig
        else:
            path = filedialog.asksaveasfilename(defaultextension=".txt")

        if not path:
            return

        try:
            with open(path, "w", encoding="utf-8") as f:
                f.write(self.txt_file.get("1.0", tk.END))
            self.set_status(f"Saved to: {os.path.basename(path)}", "info")
        except Exception as e:
            self.set_status(f"Save failed: {e}", "error")

    def get_schema_template(self):
        return (
            "{\n"
            "  \"hunks\": [\n"
            "    {\n"
            "      \"description\": \"Short human description\",\n"
            "      \"search_block\": \"exact text to find\\n(can span multiple lines)\",\n"
            "      \"replace_block\": \"replacement text\\n(same or different length)\",\n"
            "      \"use_patch_indent\": false\n"
            "    }\n"
            "  ]\n"
            "}\n"
        )

    def copy_schema_to_clipboard(self):
        self.root.clipboard_clear()
        self.root.clipboard_append(self.get_schema_template())
        self.set_status("Schema copied to clipboard.", "info")

    def _show_diff_view(self, original_text: str, preview_text: str):
        orig_lines = original_text.splitlines()
        new_lines = preview_text.splitlines()
        diff_lines = difflib.unified_diff(
            orig_lines,
            new_lines,
            fromfile="original",
            tofile="preview",
            lineterm="",
        )
        diff_text = "\n".join(diff_lines) or "(No differences)\n"
        self.txt_file.delete("1.0", tk.END)
        self.txt_file.insert("1.0", diff_text)

    def on_diff_toggle(self):
        # Only meaningful if we have a valid preview
        if not self.validation_valid or not self.validation_preview_text:
            return

        current_source = self.txt_file.get("1.0", tk.END)
        # If diff is being turned on, show diff between current source and preview
        if self.diff_view_var.get():
            self._show_diff_view(current_source, self.validation_preview_text)
        else:
            # Restore plain source view (just show current source as-is)
            self.txt_file.delete("1.0", tk.END)
            self.txt_file.insert("1.0", current_source)

    # --- Validate / Apply ---

    def validate_patch(self):
        original_text = self.txt_file.get("1.0", tk.END)
        patch_text = self.txt_patch.get("1.0", tk.END)
        force = self.force_indent_var.get()

        try:
            patch_obj = json.loads(patch_text)
            preview_text = apply_patch_text(
                original_text,
                patch_obj,
                global_force_indent=force,
            )

            self.validation_preview_text = preview_text
            self.validation_valid = True

            self.lbl_patch_title.config(text="Patch: VALIDATED (DRY RUN)")
            self.set_status("Validation succeeded (dry run).", "info")

            if self.diff_view_var.get():
                self._show_diff_view(original_text, preview_text)
            else:
                # Ensure left panel shows the original text
                self.txt_file.delete("1.0", tk.END)
                self.txt_file.insert("1.0", original_text)

        except json.JSONDecodeError:
            self.validation_preview_text = None
            self.validation_valid = False
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status("Error: Invalid JSON. Check formatting.", "error")

        except PatchError as e:
            self.validation_preview_text = None
            self.validation_valid = False
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status(f"Patch Error during validation: {e}", "error")

        except Exception as e:
            self.validation_preview_text = None
            self.validation_valid = False
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status(
                f"System Error during validation: {e}",
                "error",
            )

    def apply_patch(self):
        target_text = self.txt_file.get("1.0", tk.END)
        patch_text = self.txt_patch.get("1.0", tk.END)
        force = self.force_indent_var.get()

        try:
            if self.validation_valid and self.validation_preview_text:
                new_text = self.validation_preview_text
            else:
                patch_obj = json.loads(patch_text)
                new_text = apply_patch_text(
                    target_text,
                    patch_obj,
                    global_force_indent=force,
                )

            self.txt_file.delete("1.0", tk.END)
            self.txt_file.insert("1.0", new_text)

            # Clear validation/diff state after commit
            self.validation_preview_text = None
            self.validation_valid = False
            self.diff_view_var.set(False)

            self.lbl_left_title.config(text="PATCHED SOURCE CODE")
            self.lbl_patch_title.config(text="Patch: VALIDATED & APPLIED")
            self.set_status("Success: Patch Applied.", "info")

        except json.JSONDecodeError:
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status("Error: Invalid JSON.", "error")

        except PatchError as e:
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status(f"Patch Error: {e}", "error")

        except Exception as e:
            self.lbl_patch_title.config(text="Patch: UNVALIDATED")
            self.set_status(f"System Error: {e}", "error")

    def start(self):
        self.root.mainloop()

def run_gui():
    print("Launching GUI Mode...")
    app = App()
    app.start()

# ==============================================================================
# CLI MODE (Utility)
# ==============================================================================

def run_cli():
    """
    Command Line Interface Entry Point.
    """
    parser = argparse.ArgumentParser(description="_TokenizingPATCHER CLI")
    parser.add_argument("target", help="Path to the target source file")
    parser.add_argument("patch", help="Path to the JSON patch file")
    parser.add_argument("--output", "-o", help="Path to save the result (defaults to print stdout)")
    parser.add_argument("--force-indent", action="store_true", help="Force patch indentation")
    parser.add_argument("--dry-run", action="store_true", help="Validate only, do not write")
    
    args = parser.parse_args()
    
    # 1. Read Target
    try:
        with open(args.target, "r", encoding="utf-8") as f:
            target_text = f.read()
    except FileNotFoundError:
        print(f"Error: Target file not found: {args.target}")
        sys.exit(1)

    # 2. Read Patch
    try:
        with open(args.patch, "r", encoding="utf-8") as f:
            patch_obj = json.load(f)
    except FileNotFoundError:
        print(f"Error: Patch file not found: {args.patch}")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"Error: Patch file is not valid JSON.")
        sys.exit(1)

    # 3. Apply
    try:
        new_text = apply_patch_text(target_text, patch_obj, global_force_indent=args.force_indent)
        
        if args.dry_run:
            print("Dry Run Successful. Patch applies cleanly.")
        else:
            if args.output:
                with open(args.output, "w", encoding="utf-8") as f:
                    f.write(new_text)
                print(f"Success: Patched file written to {args.output}")
            else:
                print(new_text)
                
    except PatchError as e:
        print(f"Patch Failed: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"Unexpected Error: {e}")
        sys.exit(1)

# ==============================================================================
# HYBRID ENTRY POINT
# ==============================================================================

def main():
    if len(sys.argv) > 1:
        run_cli()
    else:
        run_gui()

if __name__ == "__main__":
    main()


--------------------------------------------------------------------------------
FILE: _TokenizingPATCHER\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\requirements.txt
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\src\app.pyw
--------------------------------------------------------------------------------
import json
import os
import shutil
import subprocess
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional

import tkinter as tk
from tkinter import ttk, messagebox, simpledialog

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Path to your Microservice Library
MICROSERVICE_LIB_PATH = Path(r"C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_MicroserviceLIBRARY")

# ---------- Data model ----------

@dataclass
class AppConfig:
    name: str
    folder: Path              # absolute
    python_cmd: Optional[str] = None  # interpreter or "py"
    env: Dict[str, str] = field(default_factory=dict)

    @property
    def has_src_app(self) -> bool:
        return (self.folder / "src" / "app.py").is_file()

    def resolve_python(self) -> List[str]:
        """
        Determine the Python command list to run for this app.
        Priority:
        1. Explicit python_cmd (absolute or relative to folder, or just 'py').
        2. .venv inside app folder.
        3. 'py' (on Windows) or sys.executable elsewhere.
        """
        # 1. Explicit config
        if self.python_cmd:
            cmd = self.python_cmd
            if os.path.sep in cmd or "/" in cmd:
                python_path = (self.folder / cmd).resolve()
                return [str(python_path)]
            else:
                return [cmd]

        # 2. Local venv
        win_candidate = self.folder / ".venv" / "Scripts" / "pythonw.exe"
        win_fallback = self.folder / ".venv" / "Scripts" / "python.exe"
        nix_candidate = self.folder / ".venv" / "bin" / "python"

        if win_candidate.is_file():
            return [str(win_candidate.resolve())]
        if win_fallback.is_file():
            return [str(win_fallback.resolve())]
        if nix_candidate.is_file():
            return [str(nix_candidate.resolve())]

        # 3. Fallback to system
        if os.name == "nt":
            return ["pyw"]
        return [sys.executable]


# ---------- Config discovery ----------

# Go up 3 levels: src -> _UsefulHelperScriptsMENU -> _UsefulHelperSCRIPTS (Root)
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
CONFIG_FILE = ROOT_DIR / "helper_apps.json"


def load_config_file() -> Dict[str, AppConfig]:
    configs: Dict[str, AppConfig] = {}
    if not CONFIG_FILE.is_file():
        return configs

    try:
        raw = json.load(CONFIG_FILE.open("r", encoding="utf-8"))
    except Exception as e:
        print(f"[WARN] Failed to load {CONFIG_FILE}: {e}")
        return configs

    for entry in raw:
        folder = ROOT_DIR / entry["folder"]
        name = entry.get("name", folder.name)
        python_cmd = entry.get("python")
        env = entry.get("env", {})

        cfg = AppConfig(
            name=name,
            folder=folder,
            python_cmd=python_cmd,
            env=env,
        )
        configs[str(folder.resolve())] = cfg

    return configs


def discover_apps() -> List[AppConfig]:
    configs_by_folder = load_config_file()
    apps: Dict[str, AppConfig] = {}

    # 1. From config
    for folder_key, cfg in configs_by_folder.items():
        apps[folder_key] = cfg

    # 2. Auto-discover
    for child in ROOT_DIR.iterdir():
        if not child.is_dir():
            continue
        candidate = child / "src" / "app.py"
        if candidate.is_file():
            key = str(child.resolve())
            if key not in apps:
                apps[key] = AppConfig(name=child.name, folder=child)

    return list(apps.values())


# ---------- Launcher logic ----------

def launch_app(app_cfg: AppConfig):
    if not app_cfg.has_src_app:
        messagebox.showerror(
            "Missing app.py",
            f"Could not find src/app.py in:\n{app_cfg.folder}",
        )
        return

    python_cmd = app_cfg.resolve_python()
    cmd = python_cmd + ["-m", "src.app"]
    env = os.environ.copy()
    env.update(app_cfg.env)

    try:
        if os.name == "nt":
            subprocess.Popen(
                cmd,
                cwd=str(app_cfg.folder),
                env=env,
            )
        else:
            subprocess.Popen(
                cmd,
                cwd=str(app_cfg.folder),
                env=env,
            )
    except Exception as e:
        messagebox.showerror(
            "Launch failed",
            f"Failed to launch {app_cfg.name}:\n\n{e}",
        )

# ==============================================================================
# NEW: MICROSERVICE SELECTOR MODAL
# ==============================================================================
class MicroserviceSelector(tk.Toplevel):
    def __init__(self, parent):
        super().__init__(parent)
        self.title("Select Microservices to Inject")
        self.geometry("600x600")
        self.confirmed = False
        self.selected_files = [] # List of Path objects
        
        self._build_ui()
        self.transient(parent)
        self.grab_set()
        
    def _build_ui(self):
        # 1. Header
        lbl = ttk.Label(self, text="Select capabilities to add to your new app:", font=("Segoe UI", 10, "bold"))
        lbl.pack(pady=10)
        
        # 2. Scrollable Checkbox List
        frame_list = ttk.Frame(self)
        frame_list.pack(fill="both", expand=True, padx=10, pady=5)
        
        canvas = tk.Canvas(frame_list, bg="#f0f0f0")
        scrollbar = ttk.Scrollbar(frame_list, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)

        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        canvas.pack(side="left", fill="both", expand=True)
        scrollbar.pack(side="right", fill="y")
        
        # 3. Populate
        self.check_vars = {} # name -> BooleanVar
        
        if MICROSERVICE_LIB_PATH.exists():
            files = sorted([f for f in MICROSERVICE_LIB_PATH.glob("*MS.py")])
            for f in files:
                var = tk.BooleanVar()
                cb = ttk.Checkbutton(scrollable_frame, text=f.name, variable=var)
                cb.pack(anchor="w", padx=5, pady=2)
                self.check_vars[f] = var
        else:
            ttk.Label(scrollable_frame, text=f"Library not found at:\n{MICROSERVICE_LIB_PATH}").pack()

        # 4. Buttons
        btn_frame = ttk.Frame(self)
        btn_frame.pack(fill="x", pady=10, padx=10)
        
        ttk.Button(btn_frame, text="Create App", command=self._on_confirm).pack(side="right", padx=5)
        ttk.Button(btn_frame, text="Cancel", command=self.destroy).pack(side="right")
        
    def _on_confirm(self):
        self.selected_files = [f for f, var in self.check_vars.items() if var.get()]
        self.confirmed = True
        self.destroy()

# ---------- Tkinter UI ----------

class AppLauncherUI:
    def __init__(self, root: tk.Tk, apps: List[AppConfig]):
        self.root = root
        self.apps = sorted(apps, key=lambda a: a.name.lower())
        self.app_by_name = {a.name: a for a in self.apps}

        self.root.title("Useful Helper Apps Launcher")
        self.root.geometry("800x500") # Slightly wider

        self._build_widgets()

    def _build_widgets(self):
        main_frame = ttk.Frame(self.root, padding=10)
        main_frame.pack(fill=tk.BOTH, expand=True)

        # Left: app list
        left_frame = ttk.Frame(main_frame)
        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=False)

        ttk.Label(left_frame, text="Available Apps").pack(anchor="w")

        self.app_listbox = tk.Listbox(left_frame, height=20, width=40)
        self.app_listbox.pack(fill=tk.BOTH, expand=True)
        self.app_listbox.bind("<Double-1>", self._on_double_click)

        for app in self.apps:
            suffix = "" if app.has_src_app else " (missing src/app.py)"
            self.app_listbox.insert(tk.END, f"{app.name}{suffix}")

        # Right: details + launch
        right_frame = ttk.Frame(main_frame, padding=(10, 0))
        right_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        self.details_text = tk.Text(
            right_frame, height=10, wrap="word", state="disabled"
        )
        self.details_text.pack(fill=tk.BOTH, expand=True)

        btn_frame = ttk.Frame(right_frame)
        btn_frame.pack(fill=tk.X, pady=(10, 0))

        self.launch_button = ttk.Button(
            btn_frame, text="Launch", command=self._on_launch_clicked
        )
        self.launch_button.pack(side=tk.LEFT)

        self.create_button = ttk.Button(
            btn_frame, text="Create New App...", command=self._on_create_clicked
        )
        self.create_button.pack(side=tk.LEFT, padx=(5, 0))

        self.refresh_button = ttk.Button(
            btn_frame, text="Refresh", command=self._on_refresh_clicked
        )
        self.refresh_button.pack(side=tk.LEFT, padx=(5, 0))

        # --- New Tools (Right Aligned) ---
        self.btn_ps = ttk.Button(
            btn_frame, text="PS", width=3, command=self._on_open_powershell
        )
        self.btn_ps.pack(side=tk.RIGHT, padx=(5, 0))

        self.btn_cmd = ttk.Button(
            btn_frame, text="CMD", width=4, command=self._on_open_cmd
        )
        self.btn_cmd.pack(side=tk.RIGHT, padx=(5, 0))

        self.btn_explore = ttk.Button(
            btn_frame, text="Folder", command=self._on_open_folder
        )
        self.btn_explore.pack(side=tk.RIGHT, padx=(5, 0))

        self.app_listbox.bind("<<ListboxSelect>>", self._on_select)

    def _on_create_clicked(self):
        # 1. Ask for Name
        name = simpledialog.askstring("New App", "Enter name for new app (Folder Name):")
        if not name: return
        
        safe_name = "".join(c for c in name if c.isalnum() or c in ('_', '-')).strip()
        if not safe_name:
            messagebox.showerror("Error", "Invalid name.")
            return

        target_dir = ROOT_DIR / safe_name
        if target_dir.exists():
            messagebox.showerror("Error", f"Folder '{safe_name}' already exists.")
            return

        # 2. Ask for Microservices
        selector = MicroserviceSelector(self.root)
        self.root.wait_window(selector)
        
        if not selector.confirmed:
            return # User cancelled

        # 3. Create
        try:
            self._write_boilerplate(target_dir, selector.selected_files)
            self._on_refresh_clicked()
            messagebox.showinfo("Success", f"Created {safe_name} with {len(selector.selected_files)} services.")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to create app: {e}")

    def _write_boilerplate(self, root_path: Path, services: List[Path] = None):
        # A. Copy Template if exists
        launcher_folder = Path(__file__).resolve().parent.parent
        template_source = launcher_folder / "_BoilerPlatePythonTEMPLATE"

        if template_source.is_dir():
            try:
                shutil.copytree(template_source, root_path, dirs_exist_ok=True)
                print(f"[Info] Cloned template from {template_source}")
            except Exception as e:
                messagebox.showerror("Template Error", f"Failed to copy template:\n{e}")
                return
        else:
            # Fallback structure
            root_path.mkdir(parents=True, exist_ok=True)
            (root_path / "src").mkdir(exist_ok=True)
            (root_path / "requirements.txt").touch()
            (root_path / "src" / "__init__.py").touch()
            
            # Basic app.py fallback
            with (root_path / "src" / "app.py").open("w", encoding="utf-8") as f:
                f.write("def main():\n    print('Hello World')\n\nif __name__ == '__main__':\n    main()")

        # B. Inject Microservices
        if services:
            ms_dir = root_path / "src" / "microservices"
            ms_dir.mkdir(exist_ok=True)
            
            # 1. Copy microservice_std_lib.py (Required dependency)
            std_lib = MICROSERVICE_LIB_PATH / "microservice_std_lib.py"
            if std_lib.exists():
                shutil.copy2(std_lib, ms_dir / "microservice_std_lib.py")
            
            # 2. Copy Selected Files
            for svc_path in services:
                shutil.copy2(svc_path, ms_dir / svc_path.name)
            
            # 3. Generate a 'Smart' app.py that imports them
            self._overwrite_app_py_with_imports(root_path, services)

    def _overwrite_app_py_with_imports(self, root_path: Path, services: List[Path]):
        """Overwrites src/app.py with a version that imports the selected services."""
        app_py = root_path / "src" / "app.py"
        
        imports = []
        inits = []
        
        for svc in services:
            # Filename: _AuthMS.py -> Class: AuthMS (Assuming convention)
            module_name = svc.stem # _AuthMS
            class_name = svc.stem.replace("_", "") # AuthMS
            
            # Correction: Your files define classes like 'AuthMS' inside '_AuthMS.py'
            # But wait, some might be '_TkinterAppShellMS' -> 'TkinterAppShellMS'
            # Let's assume class name matches filename without underscore for now, 
            # or just import the module to be safe.
            
            # Logic: from src.microservices._AuthMS import AuthMS
            # Note: We need to handle the underscore correctly. 
            # If file is _AuthMS.py, class is usually AuthMS.
            clean_class_name = module_name[1:] if module_name.startswith("_") else module_name
            
            imports.append(f"from src.microservices.{module_name} import {clean_class_name}")
            inits.append(f"    # {clean_class_name} initialized")
            inits.append(f"    {clean_class_name.lower()} = {clean_class_name}()")
            inits.append(f"    print('Service Loaded:', {clean_class_name.lower()})")

        content = [
            "import sys",
            "import os",
            "# Add src to path so imports work cleanly",
            "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))",
            "",
            "# --- Microservice Imports ---"
        ]
        content.extend(imports)
        content.append("")
        content.append("def main():")
        content.append("    print('--- Booting Microservice App ---')")
        content.extend(inits)
        content.append("    print('--- System Ready ---')")
        content.append("")
        content.append("if __name__ == '__main__':")
        content.append("    main()")
        
        with open(app_py, "w", encoding="utf-8") as f:
            f.write("\n".join(content))

    def _on_refresh_clicked(self):
        self.apps = sorted(discover_apps(), key=lambda a: a.name.lower())
        self.app_by_name = {a.name: a for a in self.apps}
        self.app_listbox.delete(0, tk.END)
        for app in self.apps:
            suffix = "" if app.has_src_app else " (missing src/app.py)"
            self.app_listbox.insert(tk.END, f"{app.name}{suffix}")
        
        self.details_text.config(state="normal")
        self.details_text.delete("1.0", tk.END)
        self.details_text.insert("1.0", "Refreshed app list.")
        self.details_text.config(state="disabled")

    def _on_open_folder(self):
        app = self._get_selected_app()
        if app and app.folder.is_dir():
            if os.name == "nt":
                os.startfile(app.folder)
            else:
                subprocess.Popen(["xdg-open", str(app.folder)])

    def _on_open_cmd(self):
        app = self._get_selected_app()
        if app and app.folder.is_dir():
            if os.name == "nt":
                subprocess.Popen(["start", "cmd"], shell=True, cwd=app.folder)

    def _on_open_powershell(self):
        app = self._get_selected_app()
        if app and app.folder.is_dir():
            if os.name == "nt":
                subprocess.Popen(["start", "powershell"], shell=True, cwd=app.folder)

    def _get_selected_app(self) -> Optional[AppConfig]:
        selection = self.app_listbox.curselection()
        if not selection:
            return None
        idx = selection[0]
        name_with_suffix = self.app_listbox.get(idx)
        name = name_with_suffix.split(" (missing")[0]
        return self.app_by_name.get(name)

    def _on_select(self, event=None):
        app = self._get_selected_app()
        if not app: return
        self._update_details(app)

    def _update_details(self, app: AppConfig):
        folder_display = str(app.folder)
        python_cmd = " ".join(app.resolve_python())
        has_app = "Yes" if app.has_src_app else "No"
        env_lines = "\n".join([f"  {k}={v}" for k, v in app.env.items()]) or "  (none)"

        text = (
            f"Name: {app.name}\n"
            f"Folder: {folder_display}\n"
            f"Has src/app.py: {has_app}\n"
            f"Python command: {python_cmd}\n"
            f"Extra env vars:\n{env_lines}\n"
        )
        self.details_text.config(state="normal")
        self.details_text.delete("1.0", tk.END)
        self.details_text.insert("1.0", text)
        self.details_text.config(state="disabled")

    def _on_launch_clicked(self):
        app = self._get_selected_app()
        if not app:
            messagebox.showinfo("No selection", "Please select an app to launch.")
            return
        launch_app(app)

    def _on_double_click(self, event=None):
        self._on_launch_clicked()


def main():
    apps = discover_apps()
    root = tk.Tk()
    if not apps:
        messagebox.showinfo("No Apps Found", "No apps found. Create one!")
    AppLauncherUI(root, apps)
    root.mainloop()


if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\src\app_OG_worksGREAT_KEEP.pyw
--------------------------------------------------------------------------------
import json
import os
import shutil
import subprocess
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional

import tkinter as tk
from tkinter import ttk, messagebox, simpledialog


# ---------- Data model ----------

@dataclass
class AppConfig:
    name: str
    folder: Path              # absolute
    python_cmd: Optional[str] = None  # interpreter or "py"
    env: Dict[str, str] = field(default_factory=dict)

    @property
    def has_src_app(self) -> bool:
        return (self.folder / "src" / "app.py").is_file()

    def resolve_python(self) -> List[str]:
        """
        Determine the Python command list to run for this app.
        Priority:
        1. Explicit python_cmd (absolute or relative to folder, or just 'py').
        2. .venv inside app folder.
        3. 'py' (on Windows) or sys.executable elsewhere.
        """
        # 1. Explicit config
        if self.python_cmd:
            cmd = self.python_cmd
            # If relative path, resolve to inside app folder
            if os.path.sep in cmd or "/" in cmd:
                python_path = (self.folder / cmd).resolve()
                return [str(python_path)]
            else:
                # e.g., "py" or "python"
                return [cmd]

        # 2. Local venv
        win_candidate = self.folder / ".venv" / "Scripts" / "pythonw.exe"
        win_fallback = self.folder / ".venv" / "Scripts" / "python.exe"
        nix_candidate = self.folder / ".venv" / "bin" / "python"

        if win_candidate.is_file():
            return [str(win_candidate.resolve())]
        if win_fallback.is_file():
            return [str(win_fallback.resolve())]
        if nix_candidate.is_file():
            return [str(nix_candidate.resolve())]

        # 3. Fallback to system
        if os.name == "nt":
            return ["pyw"]
        return [sys.executable]


# ---------- Config discovery ----------

# Go up 3 levels: src -> _UsefulHelperScriptsMENU -> _UsefulHelperSCRIPTS (Root)
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
CONFIG_FILE = ROOT_DIR / "helper_apps.json"


def load_config_file() -> Dict[str, AppConfig]:
    """
    Load helper_apps.json if it exists.
    Returns a mapping of folder (resolved) -> AppConfig
    """
    configs: Dict[str, AppConfig] = {}

    if not CONFIG_FILE.is_file():
        return configs

    try:
        raw = json.load(CONFIG_FILE.open("r", encoding="utf-8"))
    except Exception as e:
        print(f"[WARN] Failed to load {CONFIG_FILE}: {e}")
        return configs

    for entry in raw:
        folder = ROOT_DIR / entry["folder"]
        name = entry.get("name", folder.name)
        python_cmd = entry.get("python")
        env = entry.get("env", {})

        cfg = AppConfig(
            name=name,
            folder=folder,
            python_cmd=python_cmd,
            env=env,
        )
        configs[str(folder.resolve())] = cfg

    return configs


def discover_apps() -> List[AppConfig]:
    """
    Discover app folders that contain src/app.py.
    Merge with config entries when present.
    """
    configs_by_folder = load_config_file()
    apps: Dict[str, AppConfig] = {}

    # 1. From config (even if src/app.py missing, we keep but mark missing)
    for folder_key, cfg in configs_by_folder.items():
        apps[folder_key] = cfg

    # 2. Auto-discover any dir with src/app.py (one level deep by default)
    for child in ROOT_DIR.iterdir():
        if not child.is_dir():
            continue
        candidate = child / "src" / "app.py"
        if candidate.is_file():
            key = str(child.resolve())
            if key not in apps:
                apps[key] = AppConfig(name=child.name, folder=child)

    # Filter out ones that truly have no src/app.py AND weren‚Äôt meant to be virtual
    # (We keep them though, but you can choose to hide them instead).
    return list(apps.values())


# ---------- Launcher logic ----------

def launch_app(app_cfg: AppConfig):
    if not app_cfg.has_src_app:
        messagebox.showerror(
            "Missing app.py",
            f"Could not find src/app.py in:\n{app_cfg.folder}",
        )
        return

    python_cmd = app_cfg.resolve_python()

    cmd = python_cmd + ["-m", "src.app"]

    env = os.environ.copy()
    env.update(app_cfg.env)

    try:
        if os.name == "nt":
            # Spawn in a new console on Windows
            subprocess.Popen(
                cmd,
                cwd=str(app_cfg.folder),
                env=env,
                # creationflags=subprocess.CREATE_NEW_CONSOLE,
            )
        else:
            # On Linux/macOS, normal Popen is usually fine
            subprocess.Popen(
                cmd,
                cwd=str(app_cfg.folder),
                env=env,
            )
    except Exception as e:
        messagebox.showerror(
            "Launch failed",
            f"Failed to launch {app_cfg.name}:\n\n{e}",
        )


# ---------- Tkinter UI ----------

class AppLauncherUI:
    def __init__(self, root: tk.Tk, apps: List[AppConfig]):
        self.root = root
        # Fix: Ensure apps are sorted safely even if empty
        self.apps = sorted(apps, key=lambda a: a.name.lower())
        self.app_by_name = {a.name: a for a in self.apps}

        self.root.title("Useful Helper Apps Launcher")
        self.root.geometry("600x400")

        self._build_widgets()

    def _build_widgets(self):
        # FIX: Added proper indentation here
        main_frame = ttk.Frame(self.root, padding=10)
        main_frame.pack(fill=tk.BOTH, expand=True)

        # Left: app list
        left_frame = ttk.Frame(main_frame)
        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=False)

        ttk.Label(left_frame, text="Available Apps").pack(anchor="w")

        self.app_listbox = tk.Listbox(left_frame, height=20)
        self.app_listbox.pack(fill=tk.BOTH, expand=True)
        self.app_listbox.bind("<Double-1>", self._on_double_click)

        for app in self.apps:
            suffix = "" if app.has_src_app else " (missing src/app.py)"
            self.app_listbox.insert(tk.END, f"{app.name}{suffix}")

        # Right: details + launch
        right_frame = ttk.Frame(main_frame, padding=(10, 0))
        right_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)

        self.details_text = tk.Text(
            right_frame, height=10, wrap="word", state="disabled"
        )
        self.details_text.pack(fill=tk.BOTH, expand=True)

        btn_frame = ttk.Frame(right_frame)
        btn_frame.pack(fill=tk.X, pady=(10, 0))

        self.launch_button = ttk.Button(
            btn_frame, text="Launch Selected App", command=self._on_launch_clicked
        )
        self.launch_button.pack(side=tk.LEFT)

        self.create_button = ttk.Button(
            btn_frame, text="Create New", command=self._on_create_clicked
        )
        self.create_button.pack(side=tk.LEFT, padx=(5, 0))

        self.refresh_button = ttk.Button(
            btn_frame, text="Refresh Apps", command=self._on_refresh_clicked
        )
        self.refresh_button.pack(side=tk.LEFT, padx=(5, 0))

        # --- New Tools (Right Aligned) ---
        self.btn_ps = ttk.Button(
            btn_frame, text="PS", width=3, command=self._on_open_powershell
        )
        self.btn_ps.pack(side=tk.RIGHT, padx=(5, 0))

        self.btn_cmd = ttk.Button(
            btn_frame, text="CMD", width=4, command=self._on_open_cmd
        )
        self.btn_cmd.pack(side=tk.RIGHT, padx=(5, 0))

        self.btn_explore = ttk.Button(
            btn_frame, text="Folder", command=self._on_open_folder
        )
        self.btn_explore.pack(side=tk.RIGHT, padx=(5, 0))

        self.app_listbox.bind("<<ListboxSelect>>", self._on_select)

    def _on_create_clicked(self):
        name = simpledialog.askstring("New App", "Enter name for new app (Folder Name):")
        if not name:
            return
        
        # Basic sanitization
        safe_name = "".join(c for c in name if c.isalnum() or c in ('_', '-')).strip()
        if not safe_name:
            messagebox.showerror("Error", "Invalid name.")
            return

        target_dir = ROOT_DIR / safe_name
        if target_dir.exists():
            messagebox.showerror("Error", f"Folder '{safe_name}' already exists.")
            return

        try:
            self._write_boilerplate(target_dir)
            self._on_refresh_clicked()
            messagebox.showinfo("Success", f"Created {safe_name}")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to create app: {e}")

    def _write_boilerplate(self, root_path: Path):
        # Define the master template path
        launcher_folder = Path(__file__).resolve().parent.parent
        template_source = launcher_folder / "_BoilerPlatePythonTEMPLATE"

        if template_source.is_dir():
            # OPTION A: Copy from your existing template folder
            # shutil.copytree requires the destination to NOT exist, but we created 
            # root_path logic earlier. So we copy contents manually or use dirs_exist_ok.
            try:
                shutil.copytree(template_source, root_path, dirs_exist_ok=True)
                print(f"[Info] Cloned template from {template_source}")
                return
            except Exception as e:
                messagebox.showerror("Template Error", f"Failed to copy template:\n{e}")
                return
        
        # OPTION B: Fallback (If template folder is missing)
        # This ensures the launcher still works even if you move the template folder.
        messagebox.showwarning("Template Missing", 
            f"Could not find '{template_source.name}'.\nUsing minimal fallback.")
        
        root_path.mkdir(parents=True, exist_ok=True)
        (root_path / "src").mkdir(exist_ok=True)
        (root_path / "requirements.txt").touch()
        (root_path / "src" / "__init__.py").touch()
        
        # Minimal app.py so it runs
        with (root_path / "src" / "app.py").open("w", encoding="utf-8") as f:
            f.write("def main():\n    print('Template folder missing! This is a fallback.')\n\nif __name__ == '__main__':\n    main()")

    def _on_refresh_clicked(self):
        """Re-discover apps and refresh the listbox/details without restarting the launcher."""
        # FIX: Added proper indentation here
        self.apps = sorted(discover_apps(), key=lambda a: a.name.lower())
        self.app_by_name = {a.name: a for a in self.apps}

        # Repopulate listbox
        self.app_listbox.delete(0, tk.END)
        for app in self.apps:
            suffix = "" if app.has_src_app else " (missing src/app.py)"
            self.app_listbox.insert(tk.END, f"{app.name}{suffix}")

        # Reset details panel
        self.details_text.config(state="normal")
        self.details_text.delete("1.0", tk.END)
        self.details_text.insert(
            "1.0",
            "Refreshed app list.\n\nSelect an app to see details.",
        )
        self.details_text.config(state="disabled")

    def _on_open_folder(self):
        app = self._get_selected_app()
        if app and app.folder.is_dir():
            if os.name == "nt":
                os.startfile(app.folder)
            else:
                # Linux/Mac fallback
                subprocess.Popen(["xdg-open", str(app.folder)])

    def _on_open_cmd(self):
        app = self._get_selected_app()
        if app and app.folder.is_dir():
            if os.name == "nt":
                subprocess.Popen(["start", "cmd"], shell=True, cwd=app.folder)

    def _on_open_powershell(self):
        app = self._get_selected_app()
        if app and app.folder.is_dir():
            if os.name == "nt":
                subprocess.Popen(["start", "powershell"], shell=True, cwd=app.folder)

    def _get_selected_app(self) -> Optional[AppConfig]:
        selection = self.app_listbox.curselection()
        if not selection:
            return None
        idx = selection[0]
        name_with_suffix = self.app_listbox.get(idx)
        name = name_with_suffix.split(" (missing")[0]
        return self.app_by_name.get(name)

    def _on_select(self, event=None):
        app = self._get_selected_app()
        if not app:
            return
        self._update_details(app)

    def _update_details(self, app: AppConfig):
        folder_display = str(app.folder)
        python_cmd = " ".join(app.resolve_python())
        has_app = "Yes" if app.has_src_app else "No"

        env_lines = "\n".join(
            [f"  {k}={v}" for k, v in app.env.items()]
        ) or "  (none)"

        text = (
            f"Name: {app.name}\n"
            f"Folder: {folder_display}\n"
            f"Has src/app.py: {has_app}\n"
            f"Python command: {python_cmd}\n"
            f"Extra env vars:\n{env_lines}\n"
        )

        self.details_text.config(state="normal")
        self.details_text.delete("1.0", tk.END)
        self.details_text.insert("1.0", text)
        self.details_text.config(state="disabled")

    def _on_launch_clicked(self):
        app = self._get_selected_app()
        if not app:
            messagebox.showinfo("No selection", "Please select an app to launch.")
            return
        launch_app(app)

    def _on_double_click(self, event=None):
        self._on_launch_clicked()


def main():
    apps = discover_apps()
    
    # FIX: Initialize root immediately
    root = tk.Tk()
    
    # FIX: If no apps, just show a message or launch empty. 
    # Do NOT return silently.
    if not apps:
        # Option A: Show error then open empty UI
        messagebox.showinfo(
            "No Apps Found", 
            "No apps were found in subfolders.\n\nThe launcher will open empty. "
            "Add folders with 'src/app.py' and click Refresh."
        )
        # Or Option B: Just pass empty list to UI (which is what we do below)

    AppLauncherUI(root, apps)
    root.mainloop()


if __name__ == "__main__":
    main()






--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\_BoilerPlatePythonTEMPLATE\LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\_BoilerPlatePythonTEMPLATE\README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\_BoilerPlatePythonTEMPLATE\requirements.txt
--------------------------------------------------------------------------------
# Standard Library dependencies only:
# tkinter, argparse, json, ast, threading, os, sys
#
# No external pip packages required.
--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\_BoilerPlatePythonTEMPLATE\setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\_BoilerPlatePythonTEMPLATE\src\app.py
--------------------------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
== Generic Python Module/CLI Boilerplate ==

This is a generic template for a Python file that can be:
1.  Imported as a module by other scripts (e.g., `import generic_module`).
2.  Run as a standalone command-line script (e.g., `$ python generic_module.py --input data.txt`).

How to use this template:
1.  Rename this file to match your new tool (e.g., `my_data_processor.py`).
2.  Update this docstring to describe what your tool does.
3.  Fill in the "CORE FUNCTIONALITY" section with your app's logic.
4.  Go to the `main()` function to define your CLI arguments.
5.  In `main()`, add the code to call your core functions using the parsed arguments.
"""

# 1. IMPORTS
# Standard library imports
import sys
import os
import argparse  # For parsing command-line arguments

# Third-party imports (if any)
# e.g., import requests

# Local/application imports (if any)
# e.g., from . import my_other_module


# 2. CONSTANTS
# TODO: Define any constants your application needs.
SOME_DEFAULT_SETTING = "default_value"


# 3. CORE FUNCTIONALITY (The "Importable" Module)
#
# These functions make up the "core logic" of your application.
# They can be imported and used by other Python scripts.
# They should be self-contained and not rely on command-line arguments.

def core_logic_function(data: any, setting: str = SOME_DEFAULT_SETTING) -> any:
    """
    TODO: Replace this with your main logic function.
    
    This function should perform the primary task of your module.
    
    Args:
        data (any): The input data to process.
        setting (str, optional): An example of an optional setting.
                                 Defaults to SOME_DEFAULT_SETTING.

    Returns:
        any: The processed data.
    """
    print(f"[Core Logic] Processing data with setting: {setting}")
    
    # --- TODO: Your actual logic goes here ---
    # Example:
    try:
        processed_data = f"Processed data: {str(data).upper()}"
        print("[Core Logic] Processing complete.")
        return processed_data
    except Exception as e:
        print(f"[Core Logic] Error during processing: {e}", file=sys.stderr)
        # Re-raise the exception to be handled by the caller
        raise


def helper_function(value: int) -> str:
    """
    TODO: Add any helper functions your core logic needs.
    
    This is an example of a helper that might be called by
    core_logic_function or also be importable.
    
    Args:
        value (int): An input value.

    Returns:
        str: A formatted string.
    """
    return f"Helper processed value: {value * 2}"


# 4. CLI (Command-Line Interface) LOGIC
#
# This code only runs when the script is executed directly.
# It should handle parsing arguments and calling the core functions.

def main():
    """
    Main function to run the script from the command line.
    
    It parses arguments, calls core functions, and handles CLI-specific
    input/output and error handling.
    """
    
    # --- Argument Parsing ---
    # Set up the argument parser
    # TODO: Update the description to match your tool.
    parser = argparse.ArgumentParser(
        description="A generic CLI tool. TODO: Describe your tool here.",
        epilog="Example: python generic_module.py my_input.txt -o my_output.txt -v"
    )
    
    # --- TODO: Define your arguments ---
    
    # Example of a required positional argument
    parser.add_argument(
        "input_path",  # The name of the argument
        type=str,
        help="TODO: Describe this required input (e.g., path to an input file)."
    )
    
    # Example of an optional argument (e.g., -o or --output)
    parser.add_argument(
        "-o", "--output",
        type=str,
        default=None,  # Default to None if not provided
        help="TODO: Describe this optional argument (e.g., path to an output file)."
    )
    
    # Example of a "flag" argument (stores True if present)
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",  # This makes it a boolean flag
        help="Enable verbose output."
    )
    
    # Parse the arguments from the command line (e.g., sys.argv)
    args = parser.parse_args()

    # --- Main Application Flow ---
    
    # Use the 'verbose' flag to control print statements
    if args.verbose:
        print("Verbose mode enabled.", file=sys.stderr)
        print(f"Arguments received: {args}", file=sys.stderr)

    try:
        # 1. Load data (CLI-specific task)
        #    TODO: Replace this with your actual data loading
        if args.verbose:
            print(f"Loading data from {args.input_path}...", file=sys.stderr)
        # This is just an example. You'd likely load a file here.
        input_data = f"Content of {args.input_path}" 

        # 2. Call core logic (the "importable" part)
        if args.verbose:
            print("Calling core logic...", file=sys.stderr)
        
        # Here we pass the CLI arguments to the core function
        processed_data = core_logic_function(input_data)
        
        # 3. Handle output (CLI-specific task)
        if args.output:
            # Save to a file
            if args.verbose:
                print(f"Saving processed data to {args.output}...", file=sys.stderr)
            # TODO: Add file-saving logic here
            # with open(args.output, 'w') as f:
            #     f.write(processed_data)
            print(f"Success: Output saved to {args.output}")
        else:
            # Print to standard output
            if args.verbose:
                print("Printing processed data to stdout:", file=sys.stderr)
            print(processed_data)
        
        # Exit with a success code
        sys.exit(0)

    except FileNotFoundError as e:
        print(f"\nError: Input file not found.", file=sys.stderr)
        print(f"Details: {e}", file=sys.stderr)
        sys.exit(1) # Exit with a non-zero error code
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
        sys.exit(1)


# This "magic" line is the key to the whole pattern:
#
# - If you run `python generic_module.py ...`, Python sets
#   __name__ = "__main__", and the main() function is called.
#
# - If you `import generic_module` in another script, __name__
#   is "generic_module", so this block is SKIPPED.
#
if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: _UsefulHelperScriptsMENU\_BoilerPlatePythonTEMPLATE\src\__init__.py
--------------------------------------------------------------------------------
