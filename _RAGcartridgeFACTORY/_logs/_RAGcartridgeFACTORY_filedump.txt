Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_RAGcartridgeFACTORY


--------------------------------------------------------------------------------
FILE: LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: requirements.txt
--------------------------------------------------------------------------------
# Standard Library dependencies only:
# tkinter, argparse, json, ast, threading, os, sys
#
# No external pip packages required.
--------------------------------------------------------------------------------
FILE: setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: src\app.py
--------------------------------------------------------------------------------
import sys
import os
# Add src to path so imports work cleanly
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# --- Microservice Imports ---
from src.microservices._PythonChunkerMS import PythonChunkerMS
from src.microservices._NeuralGraphEngineMS import NeuralGraphEngineMS
from src.microservices._RefineryServiceMS import RefineryServiceMS
from src.microservices._TasklistVaultMS import TasklistVaultMS
from src.microservices._ThoughtStreamMS import ThoughtStreamMS
from src.microservices._LogViewMS import LogViewMS
from src.microservices._LexicalSearchMS import LexicalSearchMS
from src.microservices._ScoutMS import ScoutMS
from src.microservices._ContentExtractorMS import ContentExtractorMS
from src.microservices._ChunkingRouterMS import ChunkingRouterMS
from src.microservices._TkinterThemeManagerMS import TkinterThemeManagerMS
from src.microservices._TkinterAppShellMS import TkinterAppShellMS
from src.microservices._TkinterSmartExplorerMS import TkinterSmartExplorerMS
from src.microservices._NeuralGraphViewerMS import NeuralGraphViewerMS
from src.microservices._IntakeServiceMS import IntakeServiceMS
from src.microservices._ServiceRegistryMS import ServiceRegistryMS
from src.microservices._TelemetryServiceMS import TelemetryServiceMS
from src.microservices._RegexWeaverMS import RegexWeaverMS
from src.microservices._SearchEngineMS import SearchEngineMS
from src.microservices._NeuralServiceMS import NeuralServiceMS
from src.microservices._CartridgeServiceMS import CartridgeServiceMS
from src.microservices._TreeMapperMS import TreeMapperMS
from src.microservices._ContextAggregatorMS import ContextAggregatorMS

def main():
    print('--- Booting Microservice App ---')
    # PythonChunkerMS initialized
    pythonchunkerms = PythonChunkerMS()
    print('Service Loaded:', pythonchunkerms)
    # NeuralGraphEngineMS initialized
    neuralgraphenginems = NeuralGraphEngineMS()
    print('Service Loaded:', neuralgraphenginems)
    # RefineryServiceMS initialized
    refineryservicems = RefineryServiceMS()
    print('Service Loaded:', refineryservicems)
    # TasklistVaultMS initialized
    tasklistvaultms = TasklistVaultMS()
    print('Service Loaded:', tasklistvaultms)
    # ThoughtStreamMS initialized
    thoughtstreamms = ThoughtStreamMS()
    print('Service Loaded:', thoughtstreamms)
    # LogViewMS initialized
    logviewms = LogViewMS()
    print('Service Loaded:', logviewms)
    # LexicalSearchMS initialized
    lexicalsearchms = LexicalSearchMS()
    print('Service Loaded:', lexicalsearchms)
    # ScoutMS initialized
    scoutms = ScoutMS()
    print('Service Loaded:', scoutms)
    # ContentExtractorMS initialized
    contentextractorms = ContentExtractorMS()
    print('Service Loaded:', contentextractorms)
    # ChunkingRouterMS initialized
    chunkingrouterms = ChunkingRouterMS()
    print('Service Loaded:', chunkingrouterms)
    # TkinterThemeManagerMS initialized
    tkinterthememanagerms = TkinterThemeManagerMS()
    print('Service Loaded:', tkinterthememanagerms)
    # TkinterAppShellMS initialized
    tkinterappshellms = TkinterAppShellMS()
    print('Service Loaded:', tkinterappshellms)
    # TkinterSmartExplorerMS initialized
    tkintersmartexplorerms = TkinterSmartExplorerMS()
    print('Service Loaded:', tkintersmartexplorerms)
    # NeuralGraphViewerMS initialized
    neuralgraphviewerms = NeuralGraphViewerMS()
    print('Service Loaded:', neuralgraphviewerms)
    # IntakeServiceMS initialized
    intakeservicems = IntakeServiceMS()
    print('Service Loaded:', intakeservicems)
    # ServiceRegistryMS initialized
    serviceregistryms = ServiceRegistryMS()
    print('Service Loaded:', serviceregistryms)
    # TelemetryServiceMS initialized
    telemetryservicems = TelemetryServiceMS()
    print('Service Loaded:', telemetryservicems)
    # RegexWeaverMS initialized
    regexweaverms = RegexWeaverMS()
    print('Service Loaded:', regexweaverms)
    # SearchEngineMS initialized
    searchenginems = SearchEngineMS()
    print('Service Loaded:', searchenginems)
    # NeuralServiceMS initialized
    neuralservicems = NeuralServiceMS()
    print('Service Loaded:', neuralservicems)
    # CartridgeServiceMS initialized
    cartridgeservicems = CartridgeServiceMS()
    print('Service Loaded:', cartridgeservicems)
    # TreeMapperMS initialized
    treemapperms = TreeMapperMS()
    print('Service Loaded:', treemapperms)
    # ContextAggregatorMS initialized
    contextaggregatorms = ContextAggregatorMS()
    print('Service Loaded:', contextaggregatorms)
    print('--- System Ready ---')

if __name__ == '__main__':
    main()
--------------------------------------------------------------------------------
FILE: src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: src\microservices\microservice_std_lib.py
--------------------------------------------------------------------------------
"""
LIBRARY: Microservice Standard Lib
VERSION: 2.0.0
ROLE: Provides decorators for tagging Python classes as AI-discoverable services.
"""

import functools
import inspect
from typing import Dict, List, Any, Optional, Type

# ==============================================================================
# DECORATORS (The "Writer" Tools)
# ==============================================================================

def service_metadata(name: str, version: str, description: str, tags: List[str], capabilities: List[str] = None, dependencies: List[str] = None, side_effects: List[str] = None):
    """
    Class Decorator.
    Labels a Microservice class with high-level metadata for the Catalog.
    """
    def decorator(cls):
        cls._is_microservice = True
        cls._service_info = {
            "name": name,
            "version": version,
            "description": description,
            "tags": tags,
            "capabilities": capabilities or [],
            "dependencies": dependencies or [],
            "side_effects": side_effects or []
        }
        return cls
    return decorator

def service_endpoint(inputs: Dict[str, str], outputs: Dict[str, str], description: str, tags: List[str] = None, side_effects: List[str] = None, mode: str = "sync"):
    """
    Method Decorator.
    Defines the 'Socket' that the AI Architect can plug into.
    
    :param inputs: Dict of {arg_name: type_string} (e.g. {"query": "str"})
    :param outputs: Dict of {return_name: type_string} (e.g. {"results": "List[Dict]"})
    :param description: What this specific function does.
    :param tags: Keywords for searching (e.g. ["search", "read-only"])
    :param side_effects: List of impact types (e.g. ["network:outbound", "disk:write"])
    :param mode: 'sync', 'async', or 'ui_event'
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        
        # Attach metadata to the function object itself
        wrapper._endpoint_info = {
            "name": func.__name__,
            "inputs": inputs,
            "outputs": outputs,
            "description": description,
            "tags": tags or [],
            "side_effects": side_effects or [],
            "mode": mode
        }
        return wrapper
    return decorator

# ==============================================================================
# INTROSPECTION (The "Reader" Tools)
# ==============================================================================

def extract_service_schema(service_cls: Type) -> Dict[str, Any]:
    """
    Scans a decorated Service Class and returns a JSON-serializable schema 
    of its metadata and all its exposed endpoints.
    
    This is what the AI Agent uses to 'read' the manual.
    """
    if not getattr(service_cls, "_is_microservice", False):
        raise ValueError(f"Class {service_cls.__name__} is not decorated with @service_metadata")

    schema = {
        "meta": getattr(service_cls, "_service_info", {}),
        "endpoints": []
    }

    # Inspect all methods of the class
    for name, method in inspect.getmembers(service_cls, predicate=inspect.isfunction):
        # Unwrap decorators if necessary to find our tags
        # (Though usually the wrapper has the tag attached)
        endpoint_info = getattr(method, "_endpoint_info", None)
        
        if endpoint_info:
            schema["endpoints"].append(endpoint_info)

    return schema

--------------------------------------------------------------------------------
FILE: src\microservices\_CartridgeServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CartridgeServiceMS
ENTRY_POINT: _CartridgeServiceMS.py
DEPENDENCIES: None
"""

import datetime
import json
import os
import sqlite3
import struct
import time
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional

# Try to import sqlite-vec (pip install sqlite-vec)
try:
    import sqlite_vec
except ImportError:
    sqlite_vec = None

from microservice_std_lib import service_metadata, service_endpoint, BaseService

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="CartridgeServiceMS",
    version="1.1.0",
    description="The Source of Truth. Manages the Unified Neural Cartridge Format (UNCF v1.0).",
    tags=["storage", "database", "RAG"],
    capabilities=["sqlite", "vector-search", "graph-storage"],
    dependencies=["sqlite3", "json", "uuid"],
    side_effects=["filesystem:read", "filesystem:write"]
)
class CartridgeServiceMS(BaseService):
    """
    The Source of Truth.
    Manages the Unified Neural Cartridge Format (UNCF v1.0).
    """
    
    SCHEMA_VERSION = "uncf_v1.0"

    def __init__(self, db_path: str):
        super().__init__("CartridgeServiceMS")
        self.db_path = Path(db_path)
        self._init_db()

    def _get_conn(self):
        # Set generous timeout (60s) for multi-threaded Ingest/Refinery contention
        conn = sqlite3.connect(self.db_path, timeout=60.0)
        if sqlite_vec:
            try:
                conn.enable_load_extension(True)
                sqlite_vec.load(conn)
                conn.enable_load_extension(False)
            except Exception as e:
                self.log_error(f"Failed to load sqlite-vec: {e}")
        return conn

    def get_vector_dim(self) -> int:
        """Retrieves the expected vector dimension from the manifest spec."""
        spec = self.get_manifest("embedding_spec") or {}
        if isinstance(spec, str):
            try: spec = json.loads(spec)
            except: spec = {}
        return int(spec.get("dim", 0))

    def _init_db(self):
        """Initializes the standard Schema."""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = self._get_conn()
        cursor = conn.cursor()
        
        # Enable WAL Mode: Allows concurrent Readers (Refinery) & Writers (Ingest)
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL")
        
        # 1. Manifest (The Boot Sector)
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")
        
        # 1.5 Directories (The VFS Index)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS directories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT UNIQUE NOT NULL,
                parent_path TEXT,
                metadata TEXT DEFAULT '{}'
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_dir_parent ON directories(parent_path)")

        # 2. Files (The Content Store)
        # Supports Text AND Binary (blob_data)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT NOT NULL,       -- Portable path (e.g. "src/main.py")
                origin_path TEXT,             -- Provenance (e.g. "C:/Users/...")
                origin_type TEXT,             -- 'filesystem', 'web', 'github'
                content TEXT,                 -- Text content (UTF-8)
                blob_data BLOB,               -- Binary content (Images, PDFs)
                mime_type TEXT,
                status TEXT DEFAULT 'RAW',    -- RAW, REFINED, ERROR, SKIPPED
                metadata TEXT DEFAULT '{}',   -- JSON tags, summaries
                last_updated TIMESTAMP
            )
        """)
        # Index for fast lookups by VFS path
        cursor.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_vfs ON files(vfs_path)")

        # 3. Chunks (The Vector Store)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_id INTEGER,
                chunk_index INTEGER,
                content TEXT,
                embedding BLOB,
                name TEXT,
                type TEXT,
                start_line INTEGER,
                end_line INTEGER,
                FOREIGN KEY(file_id) REFERENCES files(id)
            )
        """)

        # 3.5 Vector Index (sqlite-vec)
        if sqlite_vec:
            dim = self.get_vector_dim()
            if dim > 0:
                try:
                    cursor.execute(f"CREATE VIRTUAL TABLE IF NOT EXISTS vec_items USING vec0(embedding float[{dim}])")
                except Exception as e:
                    self.log_error(f"Vector Table Init Error: {e}")
            else:
                self.log_info("Vector table creation deferred: No dimensions found in manifest yet.")

        # 4. Graph Topology (The Neural Wiring)
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, relation TEXT, weight REAL)")

        # 5. Validation Logs
        cursor.execute("CREATE TABLE IF NOT EXISTS logs (timestamp REAL, level TEXT, message TEXT, context TEXT)")
        
        conn.commit()
        conn.close()
        
        # Initialize standard keys if new
        self.initialize_manifest()

    def initialize_manifest(self):
        """Populates the boot sector with strict RagFORGE Cartridge Schema (UNCF) v1.1."""
        if not self.get_manifest("cartridge_id"):
            now = datetime.datetime.utcnow().isoformat()

            # 1. Identity & Versioning
            self.set_manifest("schema_name", "ragforge_cartridge")
            self.set_manifest("schema_version", "1.1.0")
            self.set_manifest("cartridge_id", str(uuid.uuid4()))
            self.set_manifest("created_at_utc", now)
            self.set_manifest("created_by_app", "RagFORGE")

            # 2. Provenance / Sources
            self.set_manifest("sources", [])
            self.set_manifest("source_policies", {
                "binary_policy": "Extract Text",
                "web_depth": 0
            })

            # 3. Specs (Defaults - updated by RefineryService._stamp_specs)
            self.set_manifest("embedding_spec", {
                "provider": "unknown",
                "model": "pending_init",
                "dim": 0,
                "dtype": "unknown",
                "distance": "unknown"
            })
            self.set_manifest("chunking_spec", {
                "strategy": "semantic_hybrid",
                "python_ast": True,
                "generic_window": 1500
            })

            # 4. VFS + Content Stats (populated/updated over time)
            self.set_manifest("vfs", {
                "root_label": "",
                "directories": {"count": 0},
                "files": {
                    "count": 0,
                    "by_origin_type": {},
                    "by_mime": {}
                },
                "index_built": False
            })
            self.set_manifest("content_stats", {
                "chunks": {"count": 0},
                "vector_index": {
                    "enabled": False,
                    "table": "vec_items",
                    "backend": "sqlite-vec",
                    "dims": 0,
                    "status": "unknown"
                },
                "graph": {
                    "nodes": 0,
                    "edges": 0
                }
            })

            # 5. Capabilities Contract (what an agent can assume exists / how to navigate)
            self.set_manifest("capabilities", {
                "tables": {
                    "manifest": True,
                    "directories": True,
                    "files": True,
                    "chunks": True,
                    "vec_items": True,
                    "graph_nodes": True,
                    "graph_edges": True,
                    "logs": True
                },
                "navigation": {
                    "vfs_path": "files.vfs_path",
                    "directory_index": "directories.vfs_path",
                    "list_files_query": "SELECT vfs_path, mime_type, origin_type, status FROM files ORDER BY vfs_path",
                    "list_directories_query": "SELECT vfs_path, parent_path FROM directories ORDER BY vfs_path"
                },
                "retrieval": {
                    "raw_file_content_query": "SELECT content, blob_data, mime_type FROM files WHERE vfs_path=?",
                    "chunks_by_file_query": "SELECT chunk_index, name, type, start_line, end_line, content FROM chunks WHERE file_id=? ORDER BY chunk_index",
                    "vector_search": "sqlite-vec on vec_items if available"
                },
                "python_helper_api": {
                    "note": "Optional convenience layer for agents running inside Python. For non-Python consumers, use the SQL queries above.",
                    "methods": [
                        "CartridgeServiceMS.get_status_flags",
                        "CartridgeServiceMS.list_files",
                        "CartridgeServiceMS.list_directories",
                        "CartridgeServiceMS.get_file_record",
                        "CartridgeServiceMS.get_directory_tree",
                        "CartridgeServiceMS.get_status_summary",
                        "CartridgeServiceMS.add_node",
                        "CartridgeServiceMS.add_edge",
                        "CartridgeServiceMS.search_embeddings"
                    ]
                }
            })

            # 6. Status & Health
            self.set_manifest("cartridge_health", "FRESH")
            self.set_manifest("ingest_complete", False)
            self.set_manifest("refine_complete", False)
            self.set_manifest("last_ingest_at_utc", "")
            self.set_manifest("last_refine_at_utc", "")
            self.set_manifest("last_error", "")
            self.set_manifest("locks", {
                "write_lock_expected": False,
                "notes": "If DB locks occur, consider batching writes and shorter-lived connections."
            })

    def set_manifest(self, key: str, value: Any):
        """Upsert metadata key."""
        conn = self._get_conn()
        val_str = json.dumps(value) if isinstance(value, (dict, list)) else str(value)
        conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", (key, val_str))
        conn.commit()
        conn.close()

    def get_manifest(self, key: str) -> Optional[str]:
        """Retrieve metadata key."""
        conn = self._get_conn()
        row = conn.execute("SELECT value FROM manifest WHERE key=?", (key,)).fetchone()
        conn.close()
        return row[0] if row else None

    def validate_cartridge(self) -> Dict[str, Any]:
        """Quality Control: Checks if the cartridge is Agent-Safe."""
        report = {"valid": True, "health": "OK", "errors": []}
        
        # 1. Check Required Keys
        required = [
            "schema_name", "schema_version", "cartridge_id",
            "created_at_utc", "created_by_app", "embedding_spec",
            "chunking_spec", "capabilities"
        ]
        for key in required:
            if not self.get_manifest(key):
                report["valid"] = False
                report["errors"].append(f"Missing Manifest Key: {key}")
        
        # 2. Check Vector Index Presence
        conn = self._get_conn()
        vec_enabled = False
        vec_status = "unknown"
        try:
            conn.execute("SELECT count(*) FROM vec_items").fetchone()
            vec_enabled = True
            vec_status = "available"
        except Exception:
            vec_enabled = False
            vec_status = "unavailable"
            report["errors"].append("Vector Index (vec_items) missing or not loaded.")
            report["health"] = "WARN_NO_VECTORS"
        finally:
            conn.close()

        # Update manifest content_stats.vector_index to reflect truth
        try:
            content_stats = self.get_manifest("content_stats") or {}
            if isinstance(content_stats, str):
                 try: content_stats = json.loads(content_stats)
                 except: content_stats = {}
            
            vec = content_stats.get("vector_index", {}) if isinstance(content_stats, dict) else {}

            embed_spec = self.get_manifest("embedding_spec") or {}
            if isinstance(embed_spec, str):
                try: embed_spec = json.loads(embed_spec)
                except: embed_spec = {}
            
            spec_dim = 0
            if isinstance(embed_spec, dict):
                spec_dim = int(embed_spec.get("dim", 0) or 0)

            vec["enabled"] = bool(vec_enabled)
            vec["status"] = vec_status
            if spec_dim > 0:
                vec["dims"] = spec_dim

            if "table" not in vec:
                vec["table"] = "vec_items"
            if "backend" not in vec:
                vec["backend"] = "sqlite-vec"

            content_stats["vector_index"] = vec
            self.set_manifest("content_stats", content_stats)
        except Exception as e:
            report["errors"].append(f"Failed to stamp vector_index status into manifest: {e}")
            report["health"] = "WARN_MANIFEST_STAMP_FAIL"
            
        return report

    def store_file(self, vfs_path: str, origin_path: str, content: str = None, blob: bytes = None, mime_type: str = "text/plain", origin_type: str = "filesystem"):
        """
        The Universal Input Method. 
        Stores raw data. If file exists, updates it and resets status to 'RAW' for re-refining.
        """
        conn = self._get_conn()
        try:
            conn.execute("""
                INSERT OR REPLACE INTO files 
                (vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, 'RAW', ?)
            """, (vfs_path, origin_path, origin_type, content, blob, mime_type, time.time()))
            conn.commit()
            return True
        except Exception as e:
            self.log_error(f"DB Store Error ({vfs_path}): {e}")
            return False
        finally:
            conn.close()

    def get_pending_files(self, limit: int = 10) -> List[Dict]:
        """Fetches files waiting for the Refinery."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        rows = conn.execute("SELECT * FROM files WHERE status = 'RAW' LIMIT ?", (limit,)).fetchall()
        conn.close()
        return [dict(row) for row in rows]

    def update_status(self, file_id: int, status: str, metadata: dict = None):
        conn = self._get_conn()
        if metadata:
            conn.execute("UPDATE files SET status = ?, metadata = ? WHERE id = ?", 
                         (status, json.dumps(metadata), file_id))
        else:
            conn.execute("UPDATE files SET status = ? WHERE id = ?", (status, file_id))
        conn.commit()
        conn.close()

    def ensure_directory(self, vfs_path: str):
        """Idempotent insert for VFS directories."""
        if not vfs_path: return
        parent = os.path.dirname(vfs_path).replace("\\", "/")
        if parent == vfs_path: parent = "" # Root case
        
        conn = self._get_conn()
        try:
            conn.execute("INSERT OR IGNORE INTO directories (vfs_path, parent_path) VALUES (?, ?)", (vfs_path, parent))
            conn.commit()
        except: pass
        finally:
            conn.close()

    # --- Agent-Friendly Helpers (No raw SQL required) ---
    def _coerce_bool(self, v: Any) -> bool:
        """Best-effort conversion for manifest values stored as strings."""
        if v is None:
            return False
        if isinstance(v, bool):
            return v
        s = str(v).strip().lower()
        return s in ("1", "true", "yes", "y", "on")

    @service_endpoint(
        inputs={},
        outputs={"ingest_complete": "bool", "refine_complete": "bool", "cartridge_health": "str"},
        description="Returns key manifest status flags (ingest/refine status and health) in a single call.",
        tags=["status", "health"]
    )
    def get_status_flags(self) -> Dict[str, Any]:
        """Returns key manifest status flags in a single call."""
        ingest_complete = self._coerce_bool(self.get_manifest("ingest_complete"))
        refine_complete = self._coerce_bool(self.get_manifest("refine_complete"))
        health = self.get_manifest("cartridge_health") or "UNKNOWN"
        return {
            "ingest_complete": ingest_complete,
            "refine_complete": refine_complete,
            "cartridge_health": health,
            "schema_name": self.get_manifest("schema_name") or "",
            "schema_version": self.get_manifest("schema_version") or "",
            "cartridge_id": self.get_manifest("cartridge_id") or ""
        }

    def list_files(self, prefix: str = "", status: Optional[str] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Enumerate files in the cartridge (optionally filtered by VFS prefix and/or status)."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            sql = "SELECT id, vfs_path, origin_path, origin_type, mime_type, status, last_updated, metadata FROM files"
            clauses = []
            params = []

            if prefix:
                clauses.append("vfs_path LIKE ?")
                params.append(prefix.rstrip("/") + "/%")

            if status:
                clauses.append("status = ?")
                params.append(status)

            if clauses:
                sql += " WHERE " + " AND ".join(clauses)

            sql += " ORDER BY vfs_path"

            if limit is not None:
                sql += " LIMIT ?"
                params.append(int(limit))

            rows = conn.execute(sql, tuple(params)).fetchall()
            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    def get_file_record(self, vfs_path: str) -> Optional[Dict[str, Any]]:
        """Fetch a single file record by VFS path."""
        if not vfs_path:
            return None
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            row = conn.execute(
                "SELECT id, vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, metadata, last_updated FROM files WHERE vfs_path = ?",
                (vfs_path,)
            ).fetchone()
            if not row:
                return None
            d = dict(row)
            try:
                d["metadata"] = json.loads(d.get("metadata") or "{}")
            except Exception:
                d["metadata"] = {}
            return d
        finally:
            conn.close()

    def list_directories(self, prefix: str = "") -> List[Dict[str, Any]]:
        """Enumerate directories in the cartridge VFS."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            if prefix:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories WHERE vfs_path LIKE ? ORDER BY vfs_path",
                    (prefix.rstrip("/") + "/%",)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories ORDER BY vfs_path"
                ).fetchall()

            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    @service_endpoint(
        inputs={"root": "str"},
        outputs={"tree": "dict"},
        description="Builds a nested directory tree structure for UI navigation or context mapping.",
        tags=["vfs", "navigation"]
    )
    def get_directory_tree(self, root: str = "") -> Dict[str, Any]:
        """Builds a nested directory tree starting at `root` ("" for full tree)."""
        dirs = self.list_directories(prefix=root) if root else self.list_directories()
        files = self.list_files(prefix=root) if root else self.list_files()

        # Tree nodes are dicts: {"_dirs": {name: node}, "_files": [file_records...]}
        def new_node():
            return {"_dirs": {}, "_files": []}

        tree = new_node()

        # Insert directories
        for d in dirs:
            path = (d.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            cur = tree
            for p in parts:
                cur = cur["_dirs"].setdefault(p, new_node())

        # Insert files
        for f in files:
            path = (f.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            fname = parts[-1]
            
            cur = tree
            for p in parts[:-1]:
                cur = cur["_dirs"].setdefault(p, new_node())
            # Store a light file record for tree browsing
            cur["_files"].append({
                "name": fname,
                "vfs_path": f.get("vfs_path"),
                "mime_type": f.get("mime_type"),
                "origin_type": f.get("origin_type"),
                "status": f.get("status")
            })

        return tree

    def get_status_summary(self) -> Dict[str, Any]:
        """Counts files by status and provides a quick cartridge overview."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            rows = conn.execute("SELECT status, COUNT(*) as n FROM files GROUP BY status").fetchall()
            by_status = {r["status"]: r["n"] for r in rows}

            dcnt = conn.execute("SELECT COUNT(*) FROM directories").fetchone()[0]
            fcnt = conn.execute("SELECT COUNT(*) FROM files").fetchone()[0]
            ccnt = conn.execute("SELECT COUNT(*) FROM chunks").fetchone()[0]
            ncnt = conn.execute("SELECT COUNT(*) FROM graph_nodes").fetchone()[0]
            ecnt = conn.execute("SELECT COUNT(*) FROM graph_edges").fetchone()[0]

            return {
                "directories": int(dcnt),
                "files": int(fcnt),
                "chunks": int(ccnt),
                "graph_nodes": int(ncnt),
                "graph_edges": int(ecnt),
                "files_by_status": by_status,
                "flags": self.get_status_flags()
            }
        finally:
            conn.close()

    # --- Graph Helpers ---
    def add_node(self, node_id: str, node_type: str, label: str, data: dict = None):
        conn = self._get_conn()
        conn.execute("INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json) VALUES (?, ?, ?, ?)",
                     (node_id, node_type, label, json.dumps(data or {})))
        conn.commit()
        conn.close()

    def add_edge(self, source: str, target: str, relation: str = "related", weight: float = 1.0):
        conn = self._get_conn()
        conn.execute("INSERT OR IGNORE INTO graph_edges (source, target, relation, weight) VALUES (?, ?, ?, ?)",
                     (source, target, relation, weight))
        conn.commit()
        conn.close()

    # --- Vector Search ---
    @service_endpoint(
        inputs={"query_vector": "list", "limit": "int"},
        outputs={"results": "list"},
        description="Performs semantic vector search using sqlite-vec against the cartridge chunks.",
        tags=["search", "vector"]
    )
    def search_embeddings(self, query_vector: List[float], limit: int = 5) -> List[Dict]:
        """Performs semantic search using sqlite-vec."""
        if not sqlite_vec or not query_vector:
            return []

        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        results = []
        
        try:
            # Pack vector to binary if needed, but sqlite-vec usually handles raw lists in parameterized queries
            # dependent on the binding. We'll pass binary for safety if using standard bindings,
            # but typically raw list works with the extension's adapters.
            # For now, we assume the extension handles the list->vector conversion.
            rows = conn.execute("""
                SELECT
                    rowid,
                    distance
                FROM vec_items
                WHERE embedding MATCH ?
                ORDER BY distance
                LIMIT ?
            """, (json.dumps(query_vector), limit)).fetchall()
            
            # Resolve back to chunks with VFS context
            for r in rows:
                chunk_id = r['rowid']
                # Join with files to get vfs_path
                query = """
                    SELECT c.*, f.vfs_path 
                    FROM chunks c 
                    JOIN files f ON c.file_id = f.id 
                    WHERE c.id=?
                """
                chunk = conn.execute(query, (chunk_id,)).fetchone()
                
                if chunk:
                    res = dict(chunk)
                    res['score'] = r['distance']
                    results.append(res)
                    
        except Exception as e:
            self.log_error(f"Vector Search Error: {e}")
        finally:
            conn.close()
            
        return results

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    import tempfile
    
    with tempfile.TemporaryDirectory() as tmp_dir:
        db_file = os.path.join(tmp_dir, "test_cartridge.db")
        print(f"Initializing service at: {db_file}")
        
        svc = CartridgeServiceMS(db_file)
        print(f"Service Ready: {svc}")
        
        # Simple test: check manifest
        status = svc.get_status_flags()
        print(f"Initial Status: {status}")
--------------------------------------------------------------------------------
FILE: src\microservices\_ChunkingRouterMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ChunkingRouterMS
ENTRY_POINT: _ChunkingRouterMS.py
DEPENDENCIES: None
"""

import re
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint, BaseService

# Attempt to import the specialist. If missing, we will fallback or warn.
try:
    from _PythonChunkerMS import PythonChunkerMS, CodeChunk
except ImportError:
    # Fallback mock for standalone testing if dependency is missing
    class CodeChunk:
        def __init__(self, name, type, content, start_line, end_line):
            self.name = name; self.type = type; self.content = content
            self.start_line = start_line; self.end_line = end_line
        def __repr__(self): return f"<CodeChunk {self.name}>"
    
    class PythonChunkerMS:
        def chunk(self, text): return [CodeChunk("mock", "text", text, 0, 0)]

# ==============================================================================
# SERVICE DEFINITION
# ==============================================================================
@service_metadata(
    name="ChunkingRouterMS",
    version="1.1.0",
    description="The Dispatcher: Routes files to specialized chunkers based on extension (AST for Python, Recursive for Prose).",
    tags=["orchestration", "chunking", "nlp"],
    capabilities=["routing", "text-processing"],
    dependencies=["re"],
    side_effects=[]
)
class ChunkingRouterMS(BaseService):
    """
    The Editor: A 'Recursive' text splitter.
    It respects the natural structure of text (Paragraphs -> Sentences -> Words)
    rather than just hacking it apart by character count.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        super().__init__("ChunkingRouterMS")
        self.config = config or {}
        self.python_specialist = PythonChunkerMS()
        # Separators for the Prose Specialist logic
        self.separators = ["\n\n", "\n", "(?<=[.?!])\s+", " ", ""]

    @service_endpoint(
        inputs={"text": "str", "filename": "str", "max_size": "int", "overlap": "int"},
        outputs={"chunks": "list"},
        description="Routes text to the appropriate specialist. Returns a list of CodeChunk objects or raw strings.",
        tags=["routing", "chunking"]
    )
    def chunk_file(self, text: str, filename: str, max_size: int = 1000, overlap: int = 100) -> List[Any]:
        """
        Extension-aware router.
        """
        if filename.endswith(".py"):
            return self.python_specialist.chunk(text)
        
        # Fallback to the internal Prose Specialist (Recursive Splitter)
        raw_chunks = self._recursive_split(text, self.separators, max_size, overlap)
        
        # Standardize output for the Refinery: Wrap prose in CodeChunk objects
        return [
            CodeChunk(
                name=f"prose_chunk_{i}", 
                type="text", 
                content=c, 
                start_line=0, 
                end_line=0
            ) for i, c in enumerate(raw_chunks)
        ]

    # ==========================================================================
    # INTERNAL LOGIC (The Prose Specialist)
    # ==========================================================================
    def _recursive_split(self, text: str, separators: List[str], max_size: int, overlap: int) -> List[str]:
        final_chunks = []
        
        # 1. Base Case: If the text fits, return it
        if len(text) <= max_size:
            return [text]
        
        # 2. Edge Case: No more separators, forced hard split
        if not separators:
            return self._hard_split(text, max_size, overlap)

        # 3. Recursive Step: Try to split by the current separator
        current_sep = separators[0]
        next_separators = separators[1:]
        
        # Regex split to keep delimiters if possible (logic varies by regex complexity)
        # For simple string splits like \n\n, we just split.
        if len(current_sep) > 1 and "(" in current_sep: 
            # It's a regex lookbehind (sentence splitter), use re.split
            splits = re.split(current_sep, text)
        else:
            splits = text.split(current_sep)

        # Now we have a list of smaller pieces. We need to merge them back together
        # until they fill the 'max_size' bucket, then start a new bucket.
        current_doc = []
        current_length = 0
        
        for split in splits:
            if not split: continue
            
            # If a single split is STILL too big, recurse deeper on it
            if len(split) > max_size:
                # If we have stuff in the buffer, flush it first
                if current_doc:
                    final_chunks.append(current_sep.join(current_doc))
                    current_doc = []
                    current_length = 0
                
                # Recurse on the big chunk using the NEXT separator
                sub_chunks = self._recursive_split(split, next_separators, max_size, overlap)
                final_chunks.extend(sub_chunks)
                continue

            # Check if adding this split would overflow
            if current_length + len(split) + len(current_sep) > max_size:
                # Flush the current buffer
                doc_text = current_sep.join(current_doc)
                final_chunks.append(doc_text)
                
                # For simplicity in recursion, we start fresh with the current split.
                current_doc = [split]
                current_length = len(split)
            else:
                # Add to buffer
                current_doc.append(split)
                current_length += len(split) + len(current_sep)

        # Flush remaining
        if current_doc:
            final_chunks.append(current_sep.join(current_doc))

        return final_chunks

    def _hard_split(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Last resort: naive character sliding window."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

# ==============================================================================
# SELF-TEST / RUNNER
# ==============================================================================
if __name__ == "__main__":
    chunker = ChunkingRouterMS()
    print(f"Service ready: {chunker}")
    
    # Example: A technical document with structure
    doc = """
    # Intro to AI
    Artificial Intelligence is great. It helps us code.
    
    ## How it works
    1. Ingestion: Reading data.
    2. Processing: Thinking about data.
    
    This is a very long paragraph that effectively serves as a stress test for the sentence splitter. It should hopefully not break in the middle of a thought! We want to keep sentences whole.
    """
    
    print("--- Testing Smart Chunking (Max 60 chars) ---")
    # We set max_size very small to force it to use the sentence/word splitters
    chunks = chunker.chunk_file(doc, "test.md", max_size=60, overlap=0)
    
    for i, c in enumerate(chunks):
        print(f"[{i}] {repr(c)}")
--------------------------------------------------------------------------------
FILE: src\microservices\_ContentExtractorMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ContentExtractorMS
ENTRY_POINT: _ContentExtractorMS.py
DEPENDENCIES: None
"""

import io
import re
import time
from typing import Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# Configuration for the Graph Mapper
@service_metadata(
    name="ContentExtractorMS",
    version="1.0.0",
    description="The Decoder: A specialist service for extracting clean text from complex formats like PDF and HTML.",
    tags=["utility", "extraction", "nlp"],
    capabilities=["pdf-to-text", "html-cleaning"],
    dependencies=["pypdf", "beautifulsoup4"],
    side_effects=["filesystem:read"]
)
class ContentExtractorMS:
    """
    The Decoder.
    A standalone utility microservice that separates the concern of 
    document parsing from ingestion logic.
    """
    
    def __init__(self):
        self.start_time = time.time()
        
        # Lazy load imports to prevent service crash if dependencies are missing
        self._pdf_ready = False
        try:
            from pypdf import PdfReader
            self._pdf_ready = True
        except ImportError:
            pass
            
        self._html_ready = False
        try:
            from bs4 import BeautifulSoup
            self._html_ready = True
        except ImportError:
            pass

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "pdf_support": "bool", "html_support": "bool"},
        description="Health check to verify which extraction backends are installed.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status and library availability."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "pdf_support": self._pdf_ready,
            "html_support": self._html_ready
        }

    @service_endpoint(
        inputs={"blob": "bytes", "mime_type": "str"},
        outputs={"text": "str"},
        description="Unified entry point for text extraction. Routes to the correct parser based on mime_type.",
        tags=["processing", "extraction"]
    )
    def extract_text(self, blob: bytes, mime_type: str) -> str:
        """
        Main routing logic for extraction. 
         logic is internalized here.
        """
        if "pdf" in mime_type.lower():
            return self._extract_pdf(blob)
        elif "html" in mime_type.lower():
            # Decode bytes to string for HTML parser
            try:
                html_content = blob.decode('utf-8', errors='ignore')
                return self._extract_html(html_content)
            except:
                return ""
        return ""

    def _extract_pdf(self, file_bytes: bytes) -> str:
        """Extracts text from a PDF blob using pypdf. [cite: 96-97]"""
        if not self._pdf_ready:
            return ""
        
        from pypdf import PdfReader
        text_content = []
        try:
            stream = io.BytesIO(file_bytes)
            reader = PdfReader(stream)
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text_content.append(extracted)
            return "\n".join(text_content)
        except Exception as e:
            return f"PDF Extraction Error: {e}"

    def _extract_html(self, html_content: str) -> str:
        """Cleans HTML to raw text using BeautifulSoup. [cite: 98-99]"""
        if not self._html_ready:
            return self._strip_tags_regex(html_content)
        
        from bs4 import BeautifulSoup
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for script in soup(["script", "style", "meta", "noscript"]):
                script.decompose()
                
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            return '\n'.join(chunk for chunk in chunks if chunk)
        except Exception:
            return self._strip_tags_regex(html_content)

    def _strip_tags_regex(self, html: str) -> str:
        """Fallback if BS4 is missing. [cite: 100]"""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', html)

if __name__ == "__main__":
    svc = ContentExtractorMS()
    print("Service ready:", svc._service_info["name"])
    print("Health:", svc.get_health())

--------------------------------------------------------------------------------
FILE: src\microservices\_ContextAggregatorMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ContextAggregatorMS
ENTRY_POINT: _ContextAggregatorMS.py
DEPENDENCIES: None
"""

import os
import fnmatch
import datetime
import logging
from pathlib import Path
from typing import Set, Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
# Extensions known to be binary/non-text (Images, Archives, Executables)
DEFAULT_BINARY_EXTENSIONS = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".mp4", ".mkv", ".avi", ".mov", ".webm",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".exe", ".dll", ".so",
    ".db", ".sqlite", ".mdb", ".pyc", ".pyo", ".class", ".jar", ".wasm"
}

# Folders to ignore by default
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", ".env", 
    "dist", "build", "coverage", ".idea", ".vscode"
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("ContextAggregator")
# ==============================================================================

@service_metadata(
    name="ContextAggregator",
    version="1.0.0",
    description="Flattens a project folder into a single readable text file.",
    tags=["filesystem", "context", "compilation"],
    capabilities=["filesystem:read", "filesystem:write"],
    dependencies=["os", "fnmatch", "datetime"],
    side_effects=["filesystem:read", "filesystem:write"]
)
class ContextAggregatorMS:
    """
    The Context Builder: Flattens a project folder into a single readable text file.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        max_file_size_mb = self.config.get("max_file_size_mb", 1)
        self.max_file_size_bytes = max_file_size_mb * 1024 * 1024

    @service_endpoint(
        inputs={"root_path": "str", "output_file": "str", "extra_exclusions": "Set[str]", "use_default_exclusions": "bool"},
        outputs={"file_count": "int"},
        description="Aggregates project files into a single text dump.",
        tags=["filesystem", "dump"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def aggregate(self, 
                  root_path: str, 
                  output_file: str, 
                  extra_exclusions: Optional[Set[str]] = None,
                  use_default_exclusions: bool = True) -> int:
        
        project_root = Path(root_path).resolve()
        out_path = Path(output_file).resolve()
        
        # Build Exclusions
        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_IGNORE_DIRS)
        if extra_exclusions:
            exclusions.update(extra_exclusions)

        # Build Binary List
        binary_exts = DEFAULT_BINARY_EXTENSIONS.copy()
        
        file_count = 0
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.write(f"File Dump from Project: {project_root.name}\nGenerated: {timestamp}\n{'='*60}\n\n")

                for root, dirs, files in os.walk(project_root):
                    # In-place filtering of directories
                    dirs[:] = [d for d in dirs if d not in exclusions]
                    
                    for filename in files:
                        if self._should_exclude(filename, exclusions): continue

                        file_path = Path(root) / filename
                        if file_path.resolve() == out_path: continue

                        if self._is_safe_to_dump(file_path, binary_exts):
                            self._write_file_content(out_f, file_path, project_root)
                            file_count += 1                            
        except IOError as e:
            log.error(f"Error writing dump: {e}")
            
        return file_count

    def _should_exclude(self, filename: str, exclusions: Set[str]) -> bool:
        return any(fnmatch.fnmatch(filename, pattern) for pattern in exclusions)

    def _is_safe_to_dump(self, file_path: Path, binary_exts: Set[str]) -> bool:
        if "".join(file_path.suffixes).lower() in binary_exts: return False
        try:
            if file_path.stat().st_size > self.max_file_size_bytes: return False
            with open(file_path, 'rb') as f:
                if b'\0' in f.read(1024): return False
        except (IOError, OSError): return False
        return True

    def _write_file_content(self, out_f, file_path: Path, project_root: Path):
        relative_path = file_path.relative_to(project_root)
        header = f"\n{'-'*20} FILE: {relative_path} {'-'*20}\n"
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as in_f:
                out_f.write(header + in_f.read() + f"\n{'-'*60}\n")
        except Exception as e:
            out_f.write(f"\n[Error reading file: {e}]\n")

if __name__ == "__main__":
    svc = ContextAggregatorMS()
    print("Service ready:", svc)
--------------------------------------------------------------------------------
FILE: src\microservices\_IntakeServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IntakeServiceMS
ENTRY_POINT: _IntakeServiceMS.py
DEPENDENCIES: None
"""

import os
import mimetypes
import requests
import fnmatch
import json
from pathlib import Path
from typing import Dict, Set, List, Any
from base_service import BaseService
from _CartridgeServiceMS import CartridgeServiceMS
from _ScannerMS import ScannerMS
import document_utils
from microservice_std_lib import service_metadata, service_endpoint

# Optional import for Web
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

@service_metadata(
    name="IntakeServiceMS",
    version="1.2.0",
    description="The Vacuum: Handles two-phase ingestion by scanning sources and processing selected paths into the cartridge.",
    tags=["ingestion", "scanner", "vfs"],
    capabilities=["filesystem:read", "web:crawl"],
    dependencies=["bs4", "requests"],
    side_effects=["filesystem:read", "cartridge:write"]
)
class IntakeServiceMS(BaseService):
    """
    The Vacuum. 
    Now supports two-phase ingestion:
    1. Scan -> Build Tree (with .gitignore respect)
    2. Ingest -> Process selected paths
    """

    DEFAULT_IGNORE_DIRS = {
        '.git', '__pycache__', 'node_modules', 'venv', '.venv', 'env', '.env', 
        '.idea', '.vscode', 'dist', 'build', 'target', 'bin', 'obj', 
        '__cartridge__'
    }
    
    DEFAULT_IGNORE_EXTS = {
        '.pyc', '.pyd', '.exe', '.dll', '.so', '.db', '.sqlite', '.sqlite3', 
        '.bin', '.iso', '.img', '.zip', '.tar', '.gz', '.7z', '.jpg', '.png'
    }


    def __init__(self, cartridge: CartridgeService):
        super().__init__("IntakeServiceMS")
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "cartridge_connected": "bool"},
        description="Standardized health check to verify service status and cartridge connectivity.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the IntakeServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "cartridge_connected": self.cartridge is not None
        }
        self.cartridge = cartridge
        self.ignore_patterns: Set[str] = set()

    def ingest_source(self, source_path: str) -> Dict[str, int]:
        """Headless/CLI Entry point: Scans and Ingests in one go."""
        self.cartridge.initialize_manifest()
        
        # Update Manifest source info
        self.cartridge.set_manifest("source_root", source_path)
        
        is_web = source_path.startswith("http")
        self.cartridge.set_manifest("source_type", "web_root" if is_web else "filesystem_dir")

        scanner = ScoutMS()
        tree_node = scanner.scan_directory(source_path, web_depth=1 if is_web else 0)
        
        if not tree_node:
             return {"error": "Source not found"}

        # Flatten tree to list of paths
        files_to_ingest = scanner.flatten_tree(tree_node)
        self.cartridge.set_manifest("ingest_config", {"auto_flattened": True, "count": len(files_to_ingest)})
        
        return self.ingest_selected(files_to_ingest, source_path)

    # --- PHASE 1: SCANNING ---

    @service_endpoint(
        inputs={"root_path": "str", "web_depth": "int"},
        outputs={"tree": "dict"},
        description="Scans a local directory or URL to build a hierarchical tree structure of available files.",
        tags=["scan", "discovery"]
    )
    def scan_path(self, root_path: str, web_depth: int = 0) -> Dict[str, Any]:
        """
        Unified Scanner Interface.
        Delegates to ScoutMS for both Web and Local FS to ensure consistent node structure.
        """
        scanner = ScannerMS()

        # 1. Delegate to Scanner
        tree_root = scanner.scan_directory(root_path, web_depth=web_depth)
        if not tree_root: return None

        # 2. Apply Persistence / Checked State
        # (We only do this for FS usually, but we can try for web if we had it)
        if not root_path.startswith("http"):
            saved_config = self._load_persistence(os.path.abspath(root_path))
            self._apply_persistence(tree_root, saved_config)
    
        return tree_root

    def _apply_persistence(self, node: Dict, saved_config: Dict):
        """Recursively applies checked state from saved config."""
        if 'rel_path' in node and node['rel_path'] in saved_config:
            node['checked'] = saved_config[node['rel_path']]
        elif 'children' in node:
            # Default check all if no config? Or check logic from before?
            pass
    
        if 'children' in node:
            for child in node['children']:
                self._apply_persistence(child, saved_config)

    def _scan_recursive(self, current_path: str, root_path: str, saved_config: Dict) -> Dict:
        name = os.path.basename(current_path)
        is_dir = os.path.isdir(current_path)
        rel_path = os.path.relpath(current_path, root_path).replace("\\", "/")
        
        node = {
            'name': name,
            'path': current_path,
            'rel_path': rel_path,
            'type': 'dir' if is_dir else 'file',
            'children': [],
            'checked': True
        }

        # Determine Check State
        if saved_config and rel_path in saved_config:
            # Respect user persistence
            node['checked'] = saved_config[rel_path]
        elif self._is_ignored(name) or (not is_dir and self._is_binary_ext(name)):
            # Default to unchecked if ignored
            node['checked'] = False

        if is_dir:
            try:
                with os.scandir(current_path) as it:
                    entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                    for entry in entries:
                        child = self._scan_recursive(entry.path, root_path, saved_config)
                        node['children'].append(child)
            except PermissionError:
                pass
        
        return node

    # --- PHASE 2: INGESTION ---

    @service_endpoint(
        inputs={"file_list": "list", "root_path": "str"},
        outputs={"stats": "dict"},
        description="Processes a specific list of files into the cartridge storage, handling text extraction and VFS indexing.",
        tags=["ingest", "write"],
        side_effects=["cartridge:write"]
    )
    def ingest_selected(self, file_list: List[str], root_path: str) -> Dict[str, int]:
        """Ingests only the specific files passed in the list."""
        stats = {"added": 0, "skipped": 0, "errors": 0}
        
        for file_path in file_list:
            try:
                # Calculate VFS Path
                try:
                    vfs_path = os.path.relpath(file_path, root_path).replace("\\", "/")
                except ValueError:
                    vfs_path = os.path.basename(file_path)

                self._read_and_store(Path(file_path), vfs_path, "filesystem", stats)
            except Exception as e:
                self.log_error(f"Error ingesting {file_path}: {e}")
                stats["errors"] += 1
        
        # --- POST-INGESTION: Update Manifest ---
        self._rebuild_directory_index()
        
        return stats

    def _rebuild_directory_index(self):
        """
        Scans 'files' table and populates 'directories' table.
        This creates the navigable VFS structure.
        """
        self.log_info("Rebuilding VFS Directory Index...")
        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            seen_dirs = set()
            
            for r in rows:
                path = r[0]
                # Walk up the path to register all parents
                current = os.path.dirname(path).replace("\\", "/")
                while current and current != "." and current not in seen_dirs:
                    self.cartridge.ensure_directory(current)
                    seen_dirs.add(current)
                    current = os.path.dirname(current).replace("\\", "/")
            
        except Exception as e:
            self.log_error(f"Directory Index Error: {e}")
        finally:
            conn.close()

    # --- HELPERS ---

    def _load_persistence(self, root_path: str) -> Dict[str, bool]:
        """Loads config from DB Manifest (Portable) or fallback to local."""
        # 1. Try DB Manifest
        try:
            conn = self.cartridge._get_conn()
            row = conn.execute("SELECT value FROM manifest WHERE key='ingest_config'").fetchone()
            conn.close()
            if row:
                return json.loads(row[0])
        except: pass
        
        # 2. Fallback to local (Legacy)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        if os.path.exists(cfg_path):
            try:
                with open(cfg_path, 'r') as f: return json.load(f)
            except: pass
        return {}

    def save_persistence(self, root_path: str, checked_map: Dict[str, bool]):
        """Saves user selections into the Cartridge Manifest (Portable)."""
        # 1. Save to DB
        try:
            conn = self.cartridge._get_conn()
            conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", 
                         ("ingest_config", json.dumps(checked_map)))
            conn.commit()
            conn.close()
        except Exception as e:
            self.log_error(f"Failed to save persistence to DB: {e}")

        # 2. Save local backup (Optional, keeps scan state if DB is deleted)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        try:
            with open(cfg_path, 'w') as f: json.dump(checked_map, f, indent=2)
        except: pass

    def _load_gitignore(self, root_path: str):
        gitignore_path = os.path.join(root_path, '.gitignore')
        self.ignore_patterns = self.DEFAULT_IGNORE_DIRS.copy()
        if os.path.exists(gitignore_path):
            try:
                with open(gitignore_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            if line.endswith('/'): line = line[:-1]
                            self.ignore_patterns.add(line)
            except: pass

    def _is_ignored(self, name: str) -> bool:
        if name in self.ignore_patterns: return True
        for pattern in self.ignore_patterns:
            if fnmatch.fnmatch(name, pattern): return True
        return False

    def _is_binary_ext(self, name: str) -> bool:
        _, ext = os.path.splitext(name)
        return ext.lower() in self.DEFAULT_IGNORE_EXTS

    def _read_and_store(self, real_path: Path, vfs_path: str, origin_type: str, stats: Dict):
        mime_type, _ = mimetypes.guess_type(real_path)
        if not mime_type: mime_type = "application/octet-stream"
        
        content = None
        blob = None
        
        # 1. Try Binary Read First (Covers PDF/Images/Safe Read)
        try:
            with open(real_path, 'rb') as f:
                blob = f.read()
        except Exception as e:
            self.log_error(f"Read error {real_path}: {e}")
            stats["errors"] += 1
            return

        # 2. Text Extraction / Decoding Strategy
        lower_path = str(real_path).lower()
        
        if lower_path.endswith(".pdf"):
            # PDF: Extract text, keep blob
            content = document_utils.extract_text_from_pdf(blob)
            if not content: mime_type = "application/pdf" # Fallback if extraction fails
            
        elif lower_path.endswith(".html") or lower_path.endswith(".htm"):
            # HTML: Decode and Clean
            try:
                raw_text = blob.decode('utf-8', errors='ignore')
                content = document_utils.extract_text_from_html(raw_text)
            except: pass
            
        else:
            # Default: Try UTF-8 Decode
            try:
                content = blob.decode('utf-8')
            except UnicodeDecodeError:
                content = None # Leave as binary blob

        # 3. Store in Cartridge
        # If content is set, it will be chunked/indexed. If only blob, it's stored but skipped by refinery.
        success = self.cartridge.store_file(
            vfs_path, 
            str(real_path), 
            content=content, 
            blob=blob, 
            mime_type=mime_type, 
            origin_type=origin_type
        )
        
        if success: stats["added"] += 1
        else: stats["errors"] += 1

        if __name__ == "__main__":
            # Manual test setup requires a CartridgeService instance
            from _CartridgeServiceMS import CartridgeService
            mock_cartridge = CartridgeService(":memory:")
            svc = IntakeServiceMS(mock_cartridge)
            print("Service ready:", svc._service_info["name"])




--------------------------------------------------------------------------------
FILE: src\microservices\_LexicalSearchMS.py
--------------------------------------------------------------------------------
import sqlite3
import json
import os
from pathlib import Path
from typing import List, Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="LexicalSearch",
    version="1.0.0",
    description="Lightweight BM25 keyword search using SQLite FTS5 (No AI required).",
    tags=["search", "index", "sqlite"],
    capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class LexicalSearchMS:
    """
    The Librarian's Index: A lightweight, AI-free search engine.
    
    Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
    Ideal for environments where installing PyTorch/Transformers is impossible
    or overkill.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Ensure 'pathlib' is imported for this line to work
        default_db = str(Path(__file__).parent / "lexical_index.db")
        self.db_path = self.config.get("db_path", default_db)
        self._init_db()

    def _init_db(self):
        """
        Sets up the schema. 
        Uses Triggers to automatically keep the FTS index in sync with the main table.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1. Main Content Table (Stores the actual data)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                metadata TEXT  -- JSON blob for extra info (path, author, etc)
            );
        """)
        
        # 2. Virtual FTS Table (The Search Index)
        # content='documents' means it references the table above (saves space)
        cur.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                content,
                content='documents',
                content_rowid='rowid'  -- Internal SQLite mapping
            );
        """)

        # 3. Triggers (The "Magic" - Auto-sync index on Insert/Delete/Update)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_au AFTER UPDATE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        
        conn.commit()
        conn.close()

    @service_endpoint(
        inputs={"doc_id": "str", "text": "str", "metadata": "Dict"},
        outputs={},
        description="Adds or updates a document in the FTS index.",
        tags=["search", "write"],
        side_effects=["db:write"]
    )
    def add_document(self, doc_id: str, text: str, metadata: Optional[Dict[str, Any]] = None):
        """
        Adds or updates a document in the index.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        meta_json = json.dumps(metadata or {})
        
        # Upsert logic (Replace if ID exists)
        cur.execute("""
            INSERT OR REPLACE INTO documents (id, content, metadata)
            VALUES (?, ?, ?)
        """, (doc_id, text, meta_json))
        
        conn.commit()
        conn.close()

    @service_endpoint(
        inputs={"query": "str", "top_k": "int"},
        outputs={"results": "List[Dict]"},
        description="Performs a BM25 ranked keyword search.",
        tags=["search", "read"],
        side_effects=["db:read"]
    )
    def search(self, query: str, top_k: int = 20) -> List[Dict[str, Any]]:
        """
        Performs a BM25 Ranked Search.
        """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row # Allows dict-like access
        cur = conn.cursor()
        
        try:
            # The SQL Magic: 'bm25(documents_fts)' calculates relevance score
            sql = """
                SELECT 
                    d.id, 
                    d.content, 
                    d.metadata,
                    snippet(documents_fts, 0, '<b>', '</b>', '...', 15) as preview,
                    bm25(documents_fts) as score
                FROM documents_fts 
                JOIN documents d ON d.rowid = documents_fts.rowid
                WHERE documents_fts MATCH ? 
                ORDER BY score ASC
                LIMIT ?
            """
            # FTS5 query syntax: quotes typically help with special chars
            safe_query = f'"{query}"'
            rows = cur.execute(sql, (safe_query, top_k)).fetchall()
            
            results = []
            for r in rows:
                results.append({
                    "id": r['id'],
                    "score": round(r['score'], 4),
                    "preview": r['preview'], # FTS5 auto-generates snippets!
                    "metadata": json.loads(r['metadata']),
                    "full_content": r['content']
                })
            
            return results
            
        except sqlite3.OperationalError as e:
            # Usually happens if query syntax is bad (e.g. unmatched quotes)
            print(f"Search syntax error: {e}")
            return []
        finally:
            conn.close()


# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_name = "test_lexical.db"
    
    # 1. Init
    engine = LexicalSearchMS({"db_path": db_name})
    print("Service ready:", engine)
    
    # 2. Ingest Data
    print("Ingesting test data...")
    engine.add_document("doc1", "Python is a great language for data science.", {"category": "coding"})
    engine.add_document("doc2", "The snake python is a reptile found in jungles.", {"category": "biology"})
    engine.add_document("doc3", "Data science involves python, pandas, and SQL.", {"category": "coding"})
    
    # 3. Search
    query = "python data"
    print(f"\nSearching for: '{query}'")
    hits = engine.search(query)
        
    for hit in hits:
        print(f"[{hit['score']:.4f}] {hit['id']} ({hit['metadata']['category']})")
        print(f"   Preview: {hit['preview']}")
            
    # Cleanup
    if os.path.exists(db_name):
        os.remove(db_name)
--------------------------------------------------------------------------------
FILE: src\microservices\_LogViewMS.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import scrolledtext, filedialog
import queue
import logging
import datetime
from typing import Any, Dict, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# HELPER CLASS (Logging Handler)
# ==============================================================================

class QueueHandler(logging.Handler):
    """
    Sends log records to a thread-safe queue.
    Used to bridge the gap between Python's logging system and the Tkinter UI.
    """
    def __init__(self, log_queue: queue.Queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.log_queue.put(record)


# ==============================================================================
# MICROSERVICE CLASS (UI Widget)
# ==============================================================================

@service_metadata(
    name="LogView",
    version="1.0.0",
    description="A thread-safe log viewer widget for Tkinter.",
    tags=["ui", "logs", "widget"],
    capabilities=["ui:gui", "filesystem:write"]
)
class LogViewMS(tk.Frame):
    """
    The Console: A professional log viewer widget.
    Features:
    - Thread-safe (consumes from a Queue).
    - Message Consolidation ("Error occurred (x5)").
    - Level Filtering (Toggle INFO/DEBUG/ERROR).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        # Initialize tk.Frame
        super().__init__(parent)
        
        # Ensure we have a queue to pull from
        self.log_queue: queue.Queue = self.config.get("log_queue")
        if self.log_queue is None:
            # Fallback for safe instantiation if no queue provided
            self.log_queue = queue.Queue()

        # State for consolidation
        self.last_msg = None
        self.last_count = 0
        self.last_line_index = None

        self._build_ui()
        self._poll_queue()

    def _build_ui(self):
        # Toolbar
        toolbar = tk.Frame(self, bg="#2d2d2d", height=30)
        toolbar.pack(fill="x", side="top")
        
        # Filters
        self.filters = {
            "INFO": tk.BooleanVar(value=True),
            "DEBUG": tk.BooleanVar(value=True),
            "WARNING": tk.BooleanVar(value=True),
            "ERROR": tk.BooleanVar(value=True)
        }
        
        for level, var in self.filters.items():
            cb = tk.Checkbutton(
                toolbar, text=level, variable=var, 
                bg="#2d2d2d", fg="white", selectcolor="#444",
                activebackground="#2d2d2d", activeforeground="white"
            )
            cb.pack(side="left", padx=5)

        tk.Button(toolbar, text="Clear", command=self.clear, bg="#444", fg="white", relief="flat").pack(side="right", padx=5)
        tk.Button(toolbar, text="Save", command=self.save, bg="#444", fg="white", relief="flat").pack(side="right")

        # Text Area
        self.text = scrolledtext.ScrolledText(
            self, state="disabled", bg="#1e1e1e", fg="#d4d4d4", 
            font=("Consolas", 10), insertbackground="white"
        )
        self.text.pack(fill="both", expand=True)
        
        # Color Tags
        self.text.tag_config("INFO", foreground="#d4d4d4")
        self.text.tag_config("DEBUG", foreground="#569cd6")
        self.text.tag_config("WARNING", foreground="#ce9178")
        self.text.tag_config("ERROR", foreground="#f44747")
        self.text.tag_config("timestamp", foreground="#608b4e")

    def _poll_queue(self):
        """Pulls logs from the queue and updates UI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                self._display(record)
        except queue.Empty:
            pass
        finally:
            # Schedule the next poll in 100ms
            self.after(100, self._poll_queue)

    def _display(self, record):
        level = record.levelname
        # Skip if filter for this level is off
        if not self.filters.get(level, tk.BooleanVar(value=True)).get():
            return

        msg = record.getMessage()
        ts = datetime.datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        
        self.text.config(state="normal")
        
        # Basic display logic (Consolidation placeholder)
        if msg == self.last_msg:
            self.last_count += 1
            # In a full implementation, we would update the previous line here.
            # For this microservice, we append normally to ensure stability.
        else:
            self.last_msg = msg
            self.last_count = 1
        
        self.text.insert("end", f"[{ts}] ", "timestamp")
        self.text.insert("end", f"{msg}\n", level)
        self.text.see("end")
        self.text.config(state="disabled")

    @service_endpoint(
        inputs={},
        outputs={},
        description="Clears the log console.",
        tags=["ui", "logs"],
        side_effects=["ui:update"]
    )
    def clear(self):
        self.text.config(state="normal")
        self.text.delete("1.0", "end")
        self.text.config(state="disabled")

    @service_endpoint(
        inputs={},
        outputs={},
        description="Opens a dialog to save logs to a file.",
        tags=["ui", "filesystem"],
        side_effects=["filesystem:write", "ui:dialog"]
    )
    def save(self):
        path = filedialog.asksaveasfilename(defaultextension=".log", filetypes=[("Log Files", "*.log")])
        if path:
            try:
                with open(path, "w", encoding="utf-8") as f:
                    f.write(self.text.get("1.0", "end"))
            except Exception as e:
                print(f"Save failed: {e}")


# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Log View Test")
    root.geometry("600x400")
    
    # 1. Setup Queue
    q = queue.Queue()
    
    # 2. Setup Logger
    # Use the separated QueueHandler class
    logger = logging.getLogger("TestApp")
    logger.setLevel(logging.DEBUG)
    logger.addHandler(QueueHandler(q))
    
    # 3. Mount View
    # Pass the queue into the config
    log_view = LogViewMS({"parent": root, "log_queue": q})
    print("Service ready:", log_view)
    log_view.pack(fill="both", expand=True)
    
    # 4. Generate Logs
    def generate_noise():
        logger.info("System initializing...")
        logger.debug("Checking sensors...")
        logger.warning("Sensor 4 response slow.")
        logger.error("Connection failed!")
        # Keep generating noise to test the polling
        root.after(2000, generate_noise)
        
    generate_noise()
    root.mainloop()
--------------------------------------------------------------------------------
FILE: src\microservices\_NeuralGraphEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NeuralGraphEngineMS
ENTRY_POINT: _NeuralGraphEngineMS.py
DEPENDENCIES: None
"""

import pygame
import math
import random
import time

# Initialize font module globally once
pygame.font.init()

from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="NeuralGraphEngineMS",
    version="1.1.0",
    description="The Cartographer: A physics-driven rendering engine for visualizing complex neural relationships in a 2D force-directed graph.",
    tags=["visualization", "graph", "pygame"],
    capabilities=["force-directed-layout", "real-time-rendering"],
    dependencies=["pygame", "math", "random"],
    side_effects=["ui:update", "render:write"]
)
class NeuralGraphEngineMS(BaseService):
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        super().__init__("NeuralGraphEngineMS")
        self.width = width
        self.start_time = time.time()
        self.height = height
        self.bg_color = bg_color
        
        self.surface = pygame.Surface((width, height))
        
        # Camera
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction
        self.dragged_node_idx = None
        self.hovered_node_idx = None
        
        # Physics State
        self.settled = False

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "nodes": "int", "settled": "bool"},
        description="Standardized health check to verify the operational state of the graph renderer.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the NeuralGraphEngineMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "nodes": len(self.nodes),
            "settled": self.settled
        }

    def resize(self, width, height):
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    @service_endpoint(
        inputs={"nodes": "list", "links": "list"},
        outputs={},
        description="Injects new node and edge data into the engine and wakes up the physics simulation.",
        tags=["data", "update"],
        side_effects=["graph:write"]
    )
    def set_data(self, nodes, links):
        self.nodes = nodes
        self.links = links
        self.settled = False # Wake up physics on new data
        
        # 1. Build an ID map so we can find parents
        node_map = {node['id']: node for node in self.nodes}

        for n in self.nodes:
            # GNN Injection: Use pre-calculated layout if available
            if 'gnn_x' in n and 'gnn_y' in n:
                n['x'] = n['gnn_x'] * self.width
                n['y'] = n['gnn_y'] * self.height

            elif 'x' not in n:
                # SMART SPAWN: If I am a satellite, spawn near my planet
                parent_id = n.get('meta', {}).get('parent')
                if parent_id and parent_id in node_map and 'x' in node_map[parent_id]:
                    p = node_map[parent_id]
                    angle = random.random() * 6.28
                    dist = 30
                    n['x'] = p['x'] + math.cos(angle) * dist
                    n['y'] = p['y'] + math.sin(angle) * dist
                else:
                    # Random spawn for Files
                    n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                    n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Semantic Coloring
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # Blue
                n['_radius'] = 6
            elif n.get('type') == 'web':
                n['_color'] = (204, 0, 122) # Purple/Pink
                n['_radius'] = 7
            elif n.get('type') == 'chunk':
                n['_color'] = (100, 200, 100) # Satellite Green
                n['_radius'] = 3
            else:
                n['_color'] = (160, 32, 240) # Default
                n['_radius'] = 6

    # --- INPUT HANDLING ---
    
    def screen_to_world(self, sx, sy):
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def get_node_at(self, sx, sy):
        wx, wy = self.screen_to_world(sx, sy)
        for n in self.nodes:
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                return n
        return None

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                self.dragged_node_idx = i
                self.settled = False # Wake up physics
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
            self.settled = False
        else:
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            return prev_hover != self.hovered_node_idx

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    def highlight_nodes(self, node_ids):
        """Highlights specific nodes by ID."""
        for n in self.nodes:
            # 1. Reset to defaults
            if n.get('type') == 'file': 
                n['_color'] = (0, 122, 204)
                n['_radius'] = 6
            elif n.get('type') == 'web': 
                n['_color'] = (204, 0, 122)
                n['_radius'] = 7
            elif n.get('type') == 'chunk': 
                n['_color'] = (100, 200, 100)
                n['_radius'] = 3
            else: 
                n['_color'] = (160, 32, 240)
                n['_radius'] = 6
                
            # 2. Apply Highlight
            if n['id'] in node_ids:
                n['_color'] = (255, 255, 0) # Bright Yellow
                n['_radius'] = 12
                
        self.settled = False # Wake up physics

    # --- PHYSICS (Damped) ---

    @service_endpoint(
        inputs={},
        outputs={"settled": "bool"},
        description="Performs one iteration of the force-directed physics calculation.",
        tags=["physics", "lifecycle"],
        mode="async",
        side_effects=["graph:update"]
    )
    def step_physics(self):
        if not self.nodes or self.settled: return

        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.85 # Increased damping to settle faster
        
        cx, cy = self.width / 2, self.height / 2
        total_kinetic_energy = 0

        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue

            # LOD: Freeze satellites if zoomed out
            if self.zoom < 1.2 and a.get('type') == 'chunk':
                a['vx'] = 0
                a['vy'] = 0
                continue
            
            fx, fy = 0, 0
            
            # 1. Gravity (Center pull)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # 2. Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Performance opt: Ignore far away nodes
                if dist_sq > 25000: continue 

                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 3. Attraction (Links)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 4. Apply & Measure Energy
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']
            total_kinetic_energy += (abs(n['vx']) + abs(n['vy']))

        # 5. Sleep Threshold
        if total_kinetic_energy < 0.5:
            self.settled = True

    # --- RENDERING ---

    @service_endpoint(
        inputs={},
        outputs={"raw_data": "bytes"},
        description="Renders the current frame to a byte buffer for display in UI components.",
        tags=["render", "output"],
        side_effects=["ui:update", "render:write"]
    )
    def get_image_bytes(self):
        self.surface.fill(self.bg_color)
        
        cx, cy = self.width / 2, self.height / 2
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # Links
        for u, v in self.links:
            if self.zoom < 1.2:
                if self.nodes[u].get('type') == 'chunk' or self.nodes[v].get('type') == 'chunk':
                    continue

            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # Nodes
        for i, n in enumerate(self.nodes):
            # LOD: Hide chunks if zoomed out
            if self.zoom < 1.2 and n.get('type') == 'chunk':
                continue

            sx, sy = to_screen(n['x'], n['y'])
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20: continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')

        if __name__ == "__main__":
            # Manual test setup
            engine = NeuralGraphEngineMS(400, 300)
            print("Service ready:", engine._service_info['name'])
            test_nodes = [{'id': 'A', 'type': 'file', 'label': 'Node A'}, {'id': 'B', 'type': 'chunk', 'label': 'Node B'}]
            test_links = [(0, 1)]
            engine.set_data(test_nodes, test_links)
            engine.step_physics()
            print("Physics step completed.")






--------------------------------------------------------------------------------
FILE: src\microservices\_NeuralGraphViewerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NeuralGraphViewerMS
ENTRY_POINT: _NeuralGraphViewerMS.py
DEPENDENCIES: None
"""

import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import json
import os
from _NeuralGraphEngineMS import GraphRenderer
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="NeuralGraphViewerMS",
    version="1.0.0",
    description="The Lens: A Tkinter-based UI component that hosts the neural graph engine and provides search/highlighting overlays.",
    tags=["ui", "visualization", "tkinter"],
    capabilities=["graph-rendering", "search-highlighting"],
    dependencies=["tkinter", "PIL", "sqlite3", "json"],
    side_effects=["ui:update", "filesystem:read"]
)
class NeuralGraphViewerMS(BaseService, ttk.Frame):
    def __init__(self, parent):
        BaseService.__init__(self, "NeuralGraphViewerMS")
        ttk.Frame.__init__(self, parent)
        self.pack(fill="both", expand=True)
        
        # Search Overlay
        self.controls = tk.Frame(self, bg="#101018")
        self.controls.pack(fill="x", side="top", padx=5, pady=5)
        
        self.entry_search = tk.Entry(self.controls, bg="#252526", fg="white", insertbackground="white", font=("Consolas", 10))
        self.entry_search.pack(side="left", fill="x", expand=True, padx=(0, 5))
        self.entry_search.bind("<Return>", self.run_search)
        
        btn = tk.Button(self.controls, text="NEURAL TEST", command=self.run_search, bg="#007ACC", fg="white", relief="flat")
        btn.pack(side="right")

        # UI Container
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Services
        self.cartridge = None
        self.neural = None
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None 
        
        # Input State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        self.is_panning = False

        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<Double-Button-1>', self.on_double_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Start the Heartbeat
        self.animate()

    def bind_services(self, cartridge, neural):
        self.cartridge = cartridge
        self.neural = neural

    @service_endpoint(
        inputs={"event": "any"},
        outputs={},
        description="Triggers a neural search based on the entry field and highlights resulting nodes in the viewer.",
        tags=["ui-action", "search"],
        side_effects=["ui:update", "graph:highlight"]
    )
    def run_search(self, event=None):
        if not self.cartridge or not self.neural:
            return
            
        query = self.entry_search.get().strip()
        if not query: return
        
        # 1. Embed
        vec = self.neural.get_embedding(query)
        if not vec: return
        
        # 2. Search
        results = self.cartridge.search_embeddings(vec, limit=5)
        
        # 3. Resolve IDs for Graph
        # Graph Node ID format: "{vfs_path}::{chunk_name}"
        ids = set()
        for r in results:
            if 'vfs_path' in r and 'name' in r:
                ids.add(f"{r['vfs_path']}::{r['name']}")
                
        # 4. Highlight
        self.engine.highlight_nodes(ids)

    @service_endpoint(
        inputs={"db_path": "str"},
        outputs={},
        description="Loads graph nodes and edges from a Cartridge database and triggers the physics engine.",
        tags=["data-load", "sqlite"],
        side_effects=["filesystem:read"]
    )
    def load_from_db(self, db_path):
        """
        Loads graph data from SQLite.
        Does NOT block the UI. The physics engine will settle the nodes frame-by-frame.
        """
        if not os.path.exists(db_path): return
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # [cite_start]Fetch Nodes [cite: 198]
            db_nodes = cursor.execute("SELECT id, type, label, data_json FROM graph_nodes").fetchall()
            
            # [cite_start]Fetch Edges [cite: 198]
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
            
            conn.close()
        except Exception as e:
            print(f"Graph Load Error: {e}")
            return

        # Format for Engine
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label, raw_json = row
            meta = {}
            try:
                if raw_json: meta = json.loads(raw_json)
            except: pass
            
            id_to_index[node_id] = idx
            formatted_nodes.append({'id': node_id, 'type': n_type, 'label': label, 'meta': meta})

        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                formatted_links.append((id_to_index[src], id_to_index[tgt]))

        # Inject Data - The Physics Engine handles the "Explosion" logic internally
        self.engine.set_data(formatted_nodes, formatted_links)

    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_double_click(self, event):
        # Zoom in on the node we clicked
        hit_node = self.engine.get_node_at(event.x, event.y)
        if hit_node:
            # Center camera on node and zoom in
            self.engine.cam_x = hit_node['x']
            self.engine.cam_y = hit_node['y']
            self.engine.zoom = 2.0
            self.engine.settled = False

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        if hit:
            self.is_dragging_node = True
        else:
            self.is_panning = True

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False
        self.is_panning = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        elif self.is_panning:
            # Camera Pan
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)
        self.engine.settled = False # Wake up physics on zoom

    def on_windows_scroll(self, event):
        if event.delta > 0: self.on_zoom(1.1)
        else: self.on_zoom(0.9)

    @service_endpoint(
        inputs={},
        outputs={},
        description="The primary heartbeat loop that orchestrates frame-by-frame physics steps and UI blitting.",
        tags=["lifecycle", "rendering"],
        mode="async",
        side_effects=["ui:update", "render:write"]
    )
    def animate(self):
        """
        The Heartbeat Loop.
        Runs at ~30 FPS. Handles Physics + Rendering.
        """
        # 1. Step Physics (Micro-calculations)
        self.engine.step_physics()
        
        # 2. Render to Buffer
        raw_data = self.engine.get_image_bytes()
        
        # 3. Blit to Screen
        if raw_data:
            img = Image.frombytes('RGB', (self.engine.width, self.engine.height), raw_data)
            self.photo = ImageTk.PhotoImage(img)
            self.canvas_lbl.configure(image=self.photo)
        
        # 4. Loop
        self.after(30, self.animate)

        if __name__ == "__main__":
            root = tk.Tk()
            root.title("NeuralGraphViewerMS Test")
            view = NeuralGraphViewerMS(root)
            print("Service ready:", view._service_info['name'])
            # Note: Requires a valid DB and Pygame environment to fully render
            root.mainloop()



--------------------------------------------------------------------------------
FILE: src\microservices\_NeuralServiceMS.py
--------------------------------------------------------------------------------
import requests
import json
import concurrent.futures
import logging
from typing import Optional, Dict, Any, List

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================

OLLAMA_API_URL = "http://localhost:11434/api"
logger = logging.getLogger("NeuralService")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="NeuralService",
    version="1.0.0",
    description="The Brain Interface: Orchestrates local AI operations via Ollama.",
    tags=["ai", "neural", "inference", "ollama"],
    capabilities=["text-generation", "embeddings", "parallel-processing"]
)
class NeuralServiceMS:
    """
    The Brain Interface: Orchestrates local AI operations via Ollama for inference and embeddings.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.max_workers = self.config.get("max_workers", 4)
        
        # Default internal model config
        self.models = {
            "fast": "qwen2.5-coder:1.5b-cpu",
            "smart": "qwen2.5:3b-cpu",
            "embed": "mxbai-embed-large:latest-cpu"
        }
        # Override defaults if provided in initial config
        if "models" in self.config:
            self.models.update(self.config["models"])

    @service_endpoint(
        inputs={"fast_model": "str", "smart_model": "str", "embed_model": "str"},
        outputs={"status": "str"},
        description="Updates the active model configurations on the fly.",
        tags=["config", "write"],
        side_effects=["config:update"]
    )
    def update_models(self, fast_model: str, smart_model: str, embed_model: str) -> Dict[str, str]:
        """Called by the UI Settings Modal to change models on the fly."""
        self.models["fast"] = fast_model
        self.models["smart"] = smart_model
        self.models["embed"] = embed_model
        logger.info(f"Models Updated: Fast={fast_model}, Smart={smart_model}")
        return {"status": "success", "config": str(self.models)}

    @service_endpoint(
        inputs={},
        outputs={"models": "List[str]"},
        description="Fetches a list of available models from the local Ollama instance.",
        tags=["ai", "read"],
        side_effects=["network:read"]
    )
    def get_available_models(self) -> List[str]:
        """Fetches list from Ollama for the UI dropdown."""
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            if res.status_code == 200:
                return [m['name'] for m in res.json().get('models', [])]
        except Exception as e:
            logger.error(f"Failed to fetch models: {e}")
            return []
        return []

    @service_endpoint(
        inputs={},
        outputs={"is_alive": "bool"},
        description="Pings Ollama to verify connectivity.",
        tags=["health", "read"],
        side_effects=["network:read"]
    )
    def check_connection(self) -> bool:
        """Pings Ollama to see if it's alive."""
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except requests.RequestException:
            logger.error("Ollama connection failed. Is 'ollama serve' running?")
            return False

    @service_endpoint(
        inputs={"text": "str"},
        outputs={"embedding": "list"},
        description="Generates a vector embedding for the provided text.",
        tags=["nlp", "vector", "ai"],
        side_effects=["network:read"]
    )
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Generates a vector using the configured embedding model."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.models["embed"], "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
        return None

    @service_endpoint(
        inputs={"prompt": "str", "tier": "str", "format_json": "bool"},
        outputs={"response": "str"},
        description="Requests synchronous text generation from a local LLM.",
        tags=["llm", "inference"],
        side_effects=["network:read"]
    )
    def request_inference(self, prompt: str, tier: str = "fast", format_json: bool = False) -> str:
        """
        Synchronous inference request.
        tier: 'fast', 'smart', or other keys in self.models
        """
        model = self.models.get(tier, self.models["fast"])
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        if format_json:
            payload["format"] = "json"

        try:
            res = requests.post(f"{OLLAMA_API_URL}/generate", json=payload, timeout=60)
            if res.status_code == 200:
                return res.json().get("response", "").strip()
        except Exception as e:
            logger.error(f"Inference ({tier}) failed: {e}")
        return ""

    def process_parallel(self, items: List[Any], worker_func) -> List[Any]:
        """
        Helper to run a function across many items using a ThreadPool.
        Useful for batch ingestion.
        Note: Not exposed as an endpoint as it takes a function as an argument.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # worker_func should take a single item and return a result
            futures = {executor.submit(worker_func, item): item for item in items}
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    logger.error(f"Worker task failed: {e}")
        
        return results


# --- Independent Test Block ---
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    svc = NeuralServiceMS()
    print("Service ready:", svc)
    
    if svc.check_connection():
        print("Ollama Connection: OK")
        print(f"Models available: {svc.get_available_models()}")
        
        # Simple Inference Test
        print("Testing Inference (Fast Tier)...")
        response = svc.request_inference("Why is the sky blue? Answer in 1 sentence.")
        print(f"Response: {response}")
    else:
        print("Ollama Connection: FAILED (Is Ollama running?)")
--------------------------------------------------------------------------------
FILE: src\microservices\_PythonChunkerMS.py
--------------------------------------------------------------------------------
import ast
import time
from dataclasses import dataclass, asdict
from typing import List, Dict, Any

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# HELPER DATA MODEL
# ==============================================================================

@dataclass
class CodeChunk:
    name: str          # e.g., "class AuthMS" or "def login"
    type: str          # "class", "function", "text"
    content: str       # The raw source code of the chunk
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="PythonChunker",
    version="1.2.0",
    description="The Python Surgeon: Specialist in Abstract Syntax Tree (AST) parsing for Python source code.",
    tags=["chunking", "python", "ast"],
    capabilities=["python-ast"]
)
class PythonChunkerMS:
    """
    Specialized Python AST Chunker.
    Focuses exclusively on identifying classes and functions to preserve code logic.
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "specialty": "str"},
        description="Standardized health check for the Python specialist service.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the PythonChunkerMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "specialty": "python_ast"
        }

    @service_endpoint(
        inputs={"content": "str"},
        outputs={"chunks": "List[Dict]"},
        description="Primary entry point for high-fidelity Python-specific AST chunking.",
        tags=["processing", "python"]
    )
    def chunk(self, content: str) -> List[Dict[str, Any]]:
        """Parses Python source into semantic CodeChunks."""
        # Convert dataclasses to dicts for JSON serialization safety
        chunks = self._chunk_python(content)
        return [asdict(c) for c in chunks]

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                # end_lineno is available in Python 3.8+
                end = node.end_lineno if hasattr(node, "end_lineno") and node.end_lineno else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", 
                        type="function", 
                        content=text, 
                        start_line=s, 
                        end_line=e, 
                        docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"class {node.name}", 
                        type="class", 
                        content=text, 
                        start_line=s, 
                        end_line=e, 
                        docstring=doc
                    ))

            # Fallback: Return as single chunk if no structures found (e.g., flat script)
            if not chunks and source.strip():
                chunks.append(CodeChunk(
                    name="module_level", 
                    type="text", 
                    content=source, 
                    start_line=1, 
                    end_line=len(lines)
                ))
                
        except SyntaxError:
            # If the code is unparseable, return the whole block as a raw chunk
            chunks.append(CodeChunk(
                name="syntax_error_fallback", 
                type="text", 
                content=source, 
                start_line=1, 
                end_line=source.count('\n') + 1
            ))
            
        return chunks


if __name__ == "__main__":
    svc = PythonChunkerMS()
    print("Service ready:", svc)
    
    # Test on a Python snippet
    test_code = "class Test:\n    def run(self):\n        pass"
    results = svc.chunk(test_code)
    
    for c in results:
        print(f"[{c['type']}] {c['name']} (Lines {c['start_line']}-{c['end_line']})")
--------------------------------------------------------------------------------
FILE: src\microservices\_RefineryServiceMS.py
--------------------------------------------------------------------------------
import json
import re
import os
import time
import ast
import concurrent.futures
import logging
from typing import Dict, List, Any, Optional, Tuple

# Assume these are available in the local environment
try:
    from _CartridgeServiceMS import CartridgeServiceMS
    from _NeuralServiceMS import NeuralServiceMS
    from _ChunkingRouterMS import ChunkingRouterMS
except ImportError:
    # Fallbacks for static analysis or isolation
    CartridgeServiceMS = Any
    NeuralServiceMS = Any
    ChunkingRouterMS = Any

from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("RefineryService")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="RefineryService",
    version="1.1.0",
    description="The Night Shift: Processes 'RAW' files into semantic chunks and weaves them into a knowledge graph.",
    tags=["processing", "refinery", "graph", "RAG"],
    capabilities=["smart-chunking", "graph-weaving", "parallel-embedding"]
)
class RefineryServiceMS:
    """
    The Night Shift.
    Polls the DB for 'RAW' files and processes them into Chunks and Graph Nodes.

    Graph Enrichment:
    - Code: function/class nodes, resolved import edges when possible.
    - Docs: section/chapter nodes for long-form text (md/txt/rst).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        
        # Dependencies must be injected via config in this architecture
        self.cartridge = self.config.get("cartridge")
        self.neural = self.config.get("neural")
        
        # Instantiate internal router
        self.chunker = ChunkingRouterMS() if ChunkingRouterMS != Any else None
        
        self.start_time = time.time()
        
        # Import parsing / resolution
        self.import_pattern = re.compile(r"""(?:from|import)\s+([\w\.]+)|require\(['"]([\w\.\-/]+)['"]\)""")

        # Lightweight module/path index cache for resolving imports to VFS files
        self._module_index: Dict[str, str] = {}
        self._path_index: Dict[str, str] = {}
        self._index_built: bool = False

        # Simple section/chapter detection
        self._md_heading = re.compile(r"^(#{1,6})\s+(.+?)\s*$")
        self._chapter_heading = re.compile(r"^\s*(chapter|CHAPTER)\s+([0-9]+|[IVXLC]+)\b\s*[:\-]?\s*(.*)$")
        
        # Initial setup if dependencies exist
        if self.cartridge and self.neural:
            self._stamp_specs()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "cartridge_health": "str"},
        description="Standardized health check to verify the operational state of the Refinery service.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the RefineryServiceMS."""
        cart_status = "UNKNOWN"
        if self.cartridge:
            # Assuming cartridge has a status method or dict
            cart_status = "CONNECTED" 
        
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "cartridge_health": cart_status
        }

    def _stamp_specs(self):
        """Writes the active Neural/Chunker configuration to the Manifest."""
        try:
            # 1. Embedding Spec
            # We assume 1024 dim for mxbai-large, but ideally we'd probe it.
            # Access config from the neural service instance
            embed_model = getattr(self.neural, "models", {}).get("embed", "default")
            
            spec = {
                "provider": "ollama",
                "model": embed_model,
                "dim": 1024,  # Hardcoded for now based on mxbai-embed-large
                "dtype": "float32",
                "distance": "cosine"
            }
            if self.cartridge:
                self.cartridge.set_manifest("embedding_spec", spec)

        except Exception as e:
            logger.error(f"Failed to stamp specs: {e}")

    def _build_import_index(self):
        """Builds caches for resolving imports to VFS targets."""
        if self._index_built or not self.cartridge:
            return

        path_index: Dict[str, str] = {}
        module_index: Dict[str, str] = {}

        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            for (vp,) in rows:
                if not vp:
                    continue
                vfs_path = str(vp).replace("\\", "/")
                path_index[vfs_path] = vfs_path

                # Python module mapping
                if vfs_path.endswith(".py"):
                    mod = vfs_path[:-3].strip("/")
                    mod = mod.replace("/", ".")
                    if mod:
                        module_index[mod] = vfs_path

                    # If it's a package __init__.py, map the package name too
                    if vfs_path.endswith("/__init__.py"):
                        pkg = vfs_path[:-len("/__init__.py")].strip("/").replace("/", ".")
                        if pkg:
                            module_index[pkg] = vfs_path

        finally:
            conn.close()

        self._path_index = path_index
        self._module_index = module_index
        self._index_built = True

    @service_endpoint(
        inputs={"batch_size": "int"},
        outputs={"processed_count": "int"},
        description="Polls the database for files with 'RAW' status and processes them into chunks and graph nodes.",
        tags=["pipeline", "batch"],
        side_effects=["cartridge:write", "neural:inference"]
    )
    def process_pending(self, batch_size: int = 5) -> int:
        """Main loop. Returns number of files processed."""
        if not self.cartridge or not self.neural:
            logger.error("Refinery missing dependencies (Cartridge or Neural).")
            return 0

        pending = self.cartridge.get_pending_files(limit=batch_size)
        if not pending:
            return 0

        logger.info(f"Refining batch of {len(pending)} files...")

        for file_row in pending:
            self._refine_file(file_row)

        return len(pending)

    def _refine_file(self, row: Dict):
        file_id = row['id']
        vfs_path = row['vfs_path']
        content = row['content']

        # Skip binary files for now (unless we add OCR later)
        if not content:
            self.cartridge.update_status(file_id, "SKIPPED_BINARY")
            return

        try:
            # 1. Specialized Chunking via Router
            chunks = self.chunker.chunk_file(content, vfs_path)

            # 2. Vectorization & Storage
            chunk_texts = [c.content for c in chunks]

            # Buffer graph writes while DB transaction is open (prevents nested-writer locks)
            pending_nodes: List[Tuple[str, str, str, Dict[str, Any]]] = []
            pending_edges: List[Tuple[str, str, str, float]] = []

            # Use ThreadPool to embed in parallel (preserve order with map)
            max_workers = getattr(self.neural, "max_workers", 4)
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                vectors = list(executor.map(self.neural.get_embedding, chunk_texts))

            conn = self.cartridge._get_conn()
            try:
                cursor = conn.cursor()

                for i, chunk in enumerate(chunks):
                    vector = vectors[i]
                    vec_blob = json.dumps(vector).encode('utf-8') if vector else None

                    # Store Chunk
                    cursor.execute(
                        """
                        INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (file_id, i, chunk.content, vec_blob, chunk.name, chunk.type, chunk.start_line, chunk.end_line)
                    )

                    chunk_row_id = cursor.lastrowid

                    # Insert into Vector Index (if vector exists)
                    if vector:
                        try:
                            cursor.execute(
                                "INSERT INTO vec_items(rowid, embedding) VALUES (?, ?)",
                                (chunk_row_id, json.dumps(vector))
                            )
                        except Exception as ve:
                            logger.error(f"Vector Index Insert Failed: {ve}")

                    # Graph Node for Chunks (Functions/Classes)
                    if chunk.type in ['class', 'function']:
                        node_id = f"{vfs_path}::{chunk.name}"
                        pending_nodes.append(
                            (
                                node_id,
                                'chunk',
                                chunk.name,
                                {
                                    'parent': vfs_path,
                                    'file_id': file_id,
                                    'chunk_row_id': chunk_row_id,
                                    'chunk_type': chunk.type,
                                    'start_line': chunk.start_line,
                                    'end_line': chunk.end_line
                                }
                            )
                        )
                        pending_edges.append((node_id, vfs_path, "defined_in", 1.0))

                conn.commit()

            finally:
                conn.close()

            # 3. File Level Graph Node (after close)
            pending_nodes.append((vfs_path, 'file', vfs_path.split('/')[-1], {'path': vfs_path, 'file_id': file_id}))

            # 4. Section/Chapter Weaving (docs)
            self._weave_sections(vfs_path, content)

            # 5. Import Weaving (resolved when possible)
            self._weave_imports(vfs_path, content)

            # 6. Flush buffered graph writes
            for nid, ntype, label, data in pending_nodes:
                self.cartridge.add_node(nid, ntype, label, data)
            for src, tgt, rel, w in pending_edges:
                self.cartridge.add_edge(src, tgt, rel, w)

            # 7. Mark file refined
            self.cartridge.update_status(file_id, "REFINED")

        except Exception as e:
            logger.error(f"Refining failed for {vfs_path}: {e}")
            self.cartridge.update_status(file_id, "ERROR", {"error": str(e)})

    def _extract_imports_python(self, source_path: str, content: str) -> List[Tuple[str, int, int]]:
        """Returns list of (module_or_path, level, lineno)."""
        out: List[Tuple[str, int, int]] = []
        try:
            tree = ast.parse(content)
        except Exception:
            return out

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias and alias.name:
                        out.append((alias.name, 0, getattr(node, 'lineno', 0)))
            elif isinstance(node, ast.ImportFrom):
                level = int(getattr(node, 'level', 0) or 0)
                mod = getattr(node, 'module', None) or ""
                if mod:
                    out.append((mod, level, getattr(node, 'lineno', 0)))
                else:
                    # from . import x
                    for alias in node.names:
                        if alias and alias.name:
                            out.append((alias.name, level, getattr(node, 'lineno', 0)))
        return out

    def _resolve_python_import(self, source_path: str, module: str, level: int) -> List[str]:
        """Resolve a python import to possible VFS target paths."""
        self._build_import_index()

        # Absolute: try direct module mapping
        if level <= 0:
            if module in self._module_index:
                return [self._module_index[module]]
            return []

        # Relative: resolve from the source directory
        src_dir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        base_parts = src_dir.split("/") if src_dir else []

        # level=1 means "from .", so pop 0; level=2 means "from .." pop 1, etc.
        pops = max(level - 1, 0)
        if pops > 0 and pops <= len(base_parts):
            base_parts = base_parts[:-pops]

        rel_base = "/".join([p for p in base_parts if p])
        mod_path = module.replace(".", "/").strip("/")

        candidates: List[str] = []
        if rel_base:
            if mod_path:
                candidates.append(f"{rel_base}/{mod_path}.py")
                candidates.append(f"{rel_base}/{mod_path}/__init__.py")
            else:
                candidates.append(f"{rel_base}/__init__.py")
        else:
            if mod_path:
                candidates.append(f"{mod_path}.py")
                candidates.append(f"{mod_path}/__init__.py")

        return [c for c in candidates if c in self._path_index]

    def _resolve_js_like_import(self, source_path: str, imp: str) -> List[str]:
        """Resolve require('./x') / import ... from './x' to VFS candidates."""
        self._build_import_index()

        sdir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        raw = imp.strip().replace("\\", "/")

        # Only try to resolve relative-ish paths
        if not (raw.startswith(".") or raw.startswith("/")):
            return []

        # Normalize
        if raw.startswith("/"):
            rel = raw.lstrip("/")
        else:
            rel = os.path.normpath(os.path.join(sdir, raw)).replace("\\", "/").lstrip("./")

        ext_candidates = [rel]
        # Common extensions
        if not os.path.splitext(rel)[1]:
            ext_candidates.extend([rel + ".js", rel + ".ts", rel + ".json"]) 
            ext_candidates.extend([rel + "/index.js", rel + "/index.ts"]) 

        return [c for c in ext_candidates if c in self._path_index]

    def _weave_imports(self, source_path: str, content: str):
        """Scans content for imports and links them in the graph."""
        targets_resolved: List[str] = []

        # Python: use AST when possible
        if source_path.endswith(".py"):
            for mod, level, lineno in self._extract_imports_python(source_path, content):
                resolved = self._resolve_python_import(source_path, mod, level)
                if resolved:
                    for tgt in resolved:
                        self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                        targets_resolved.append(tgt)
                else:
                    self.cartridge.add_edge(source_path, mod, "imports_unresolved", 0.25)
            return

        # JS / generic: regex fallback
        for line in content.splitlines():
            match = self.import_pattern.search(line)
            if not match:
                continue

            imp = match.group(1) or match.group(2)
            if not imp:
                continue

            resolved = self._resolve_js_like_import(source_path, imp)
            if resolved:
                for tgt in resolved:
                    self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                    targets_resolved.append(tgt)
            else:
                self.cartridge.add_edge(source_path, imp, "imports_unresolved", 0.25)

    def _weave_sections(self, vfs_path: str, content: str):
        """Creates section/chapter nodes for long-form text and links them to the file node."""
        ext = os.path.splitext(vfs_path)[1].lower()
        if ext not in (".md", ".markdown", ".txt", ".rst"):
            return

        lines = content.splitlines()
        for idx, line in enumerate(lines):
            lineno = idx + 1

            m = self._md_heading.match(line)
            if m:
                hashes = m.group(1)
                title = (m.group(2) or "").strip()
                level = len(hashes)
                if title:
                    node_id = f"{vfs_path}::section::{lineno}:{title}"
                    self.cartridge.add_node(node_id, "section", title, {
                        "parent": vfs_path,
                        "level": level,
                        "line": lineno
                    })
                    self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)
                continue

            c = self._chapter_heading.match(line)
            if c:
                chap_num = (c.group(2) or "").strip()
                chap_title = (c.group(3) or "").strip()
                title = f"Chapter {chap_num}" + (f": {chap_title}" if chap_title else "")
                node_id = f"{vfs_path}::chapter::{lineno}:{chap_num}"
                self.cartridge.add_node(node_id, "section", title, {
                    "parent": vfs_path,
                    "level": 1,
                    "line": lineno
                })
                self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)


# --- Independent Test Block ---
if __name__ == "__main__":
    # Requires Cartridge and Neural services for testing
    try:
        from _CartridgeServiceMS import CartridgeServiceMS
        from _NeuralServiceMS import NeuralServiceMS
        
        print("Initializing Dependencies...")
        c = CartridgeServiceMS({"db_path": ":memory:"})
        n = NeuralServiceMS()
        
        # Inject dependencies via config
        svc = RefineryServiceMS({"cartridge": c, "neural": n})
        print("Service ready:", svc)
        print("Health Check:", svc.get_health())
        
    except ImportError:
        print("Dependencies not found. Run in project context.")
--------------------------------------------------------------------------------
FILE: src\microservices\_RegexWeaverMS.py
--------------------------------------------------------------------------------
import re
import logging
from typing import Any, Dict, List, Optional, Set

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: PATTERNS
# ==============================================================================
# Python: "import x", "from x import y"
PY_IMPORT = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')

# JS/TS: "import ... from 'x'", "require('x')"
JS_IMPORT = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

logger = logging.getLogger("RegexWeaver")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="RegexWeaver",
    version="1.0.0",
    description="Fault-tolerant dependency extractor using Regex.",
    tags=["parsing", "dependencies", "regex"],
    capabilities=["compute"]
)
class RegexWeaverMS:
    """
    The Weaver: A fault-tolerant dependency extractor.
    Uses Regex to find imports, making it faster and more permissive
    than AST parsers (works on broken code).
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"content": "str", "language": "str"},
        outputs={"dependencies": "List[str]"},
        description="Scans code content for import statements.",
        tags=["parsing", "dependencies"],
        side_effects=[]
    )
    def extract_dependencies(self, content: str, language: str) -> List[str]:
        """
        Scans code content for import statements.
        :param language: 'python' or 'javascript' (includes ts/jsx).
        """
        dependencies: Set[str] = set()
        lines = content.splitlines()
        
        pattern = PY_IMPORT if language == 'python' else JS_IMPORT
        
        for line in lines:
            # Skip comments roughly
            if line.strip().startswith(('#', '//')):
                continue
                
            if language == 'python':
                match = pattern.match(line)
            else:
                match = pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                # Clean up: "backend.database" -> "database"
                # We usually want the leaf name for simple linking
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                dependencies.add(clean_dep)
                
        return sorted(list(dependencies))


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging for test
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    
    weaver = RegexWeaverMS()
    print("Service ready:", weaver)
    
    # 1. Python Test
    py_code = """
    import os
    from backend.utils import helper
    # from commented.out import ignore_me
    import pandas as pd
    """
    print(f"Python Deps: {weaver.extract_dependencies(py_code, 'python')}")
    
    # 2. JS Test
    js_code = """
    import React from 'react';
    const utils = require('./lib/utils');
    // import hidden from 'hidden';
    """

    print(f"JS Deps:     {weaver.extract_dependencies(js_code, 'javascript')}")
--------------------------------------------------------------------------------
FILE: src\microservices\_ScoutMS.py
--------------------------------------------------------------------------------
import os
import time
import requests
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# Try imports for Web/PDF support
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

@service_metadata(
    name="Scout",
    version="1.0.0",
    description="The Scout: A depth-aware utility for recursively walking local file systems or crawling websites.",
    tags=["utility", "scanner", "crawler"],
    capabilities=["filesystem:read", "web:crawl"]
)
class ScoutMS:
    """
    The Scanner: Walks file systems OR crawls websites (Depth-Aware).
    """
    
    def __init__(self):
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage', 'site-packages'
        }
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', 
            '.zip', '.tar', '.gz', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }
        self.visited_urls = set()

    def is_binary(self, file_path: str) -> bool:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS: return True
        return False

    @service_endpoint(
        inputs={"root_path": "str", "web_depth": "int"},
        outputs={"tree": "dict"},
        description="Main entry point to perform a recursive scan of a directory or a web crawl.",
        tags=["discovery", "recursive"],
        side_effects=["filesystem:read", "network:read"]
    )
    def scan_directory(self, root_path: str, web_depth: int = 0) -> Optional[Dict[str, Any]]:
        """
        Main Entry Point.
        :param root_path: File path or URL.
        :param web_depth: How many links deep to crawl (0 = single page).
        """
        # 1. Web Crawl Mode
        if root_path.startswith("http://") or root_path.startswith("https://"):
            self.visited_urls.clear()
            return self._crawl_web_recursive(root_path, depth=web_depth, origin_domain=urlparse(root_path).netloc)

        # 2. Local File System Mode
        target = os.path.abspath(root_path)
        if not os.path.exists(target): return None
        
        if not os.path.isdir(target): 
            return self._create_node(target, is_dir=False)
            
        return self._scan_fs_recursive(target)

    # --- Web Logic ---
    def _crawl_web_recursive(self, url: str, depth: int, origin_domain: str) -> Dict[str, Any]:
        """
        Recursively fetches links.
        """
        # Generate a nice VFS path: web/domain/path
        parsed = urlparse(url)
        clean_path = parsed.path.strip("/")
        if not clean_path: clean_path = "index.html"
        rel_path = f"web/{parsed.netloc}/{clean_path}"

        node = {
            'name': url,
            'path': url,
            'rel_path': rel_path,
            'type': 'web',
            'children': [],
            'checked': True
        }
        
        if depth < 0 or url in self.visited_urls: return node
        self.visited_urls.add(url)

        if depth > 0 and BeautifulSoup:
            try:
                # Polite Delay
                time.sleep(0.1)
                resp = requests.get(url, timeout=5)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.content, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        parsed = urlparse(full_url)
                        
                        # Filter: Only same domain, valid schemes
                        if parsed.netloc == origin_domain and parsed.scheme in ['http', 'https']:
                            if full_url not in self.visited_urls:
                                child_node = self._crawl_web_recursive(full_url, depth - 1, origin_domain)
                                node['children'].append(child_node)
            except Exception as e:
                node['error'] = str(e)
                
        return node

    # --- File System Logic ---
    def _scan_fs_recursive(self, current_path: str, root_path: str = None) -> Dict[str, Any]:
        if root_path is None: root_path = current_path
        
        node = self._create_node(current_path, is_dir=True, root_path=root_path)
        node['children'] = []
        try:
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                for entry in entries:
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS: continue
                    if entry.name.startswith('.'): continue

                    if entry.is_dir():
                        child = self._scan_fs_recursive(entry.path, root_path=root_path)
                        if child: node['children'].append(child)
                    else:
                        node['children'].append(self._create_node(entry.path, is_dir=False, root_path=root_path))
        except PermissionError:
            node['error'] = "Access Denied"
        return node

    def _create_node(self, path: str, is_dir: bool, root_path: str = None) -> Dict[str, Any]:
        name = os.path.basename(path)
        # Calculate relative path for VFS
        rel_path = name
        if root_path:
            try:
                rel_path = os.path.relpath(path, root_path).replace("\\", "/")
            except ValueError:
                pass

        node = {
            'name': name, 
            'path': path, 
            'rel_path': rel_path,
            'type': 'folder' if is_dir else 'file', 
            'children': [],
            'checked': False
        }
        return node

    @service_endpoint(
        inputs={"tree_node": "dict"},
        outputs={"file_list": "list"},
        description="Flattens a hierarchical tree node structure into a simple list of paths.",
        tags=["utility", "processing"]
    )
    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        files = []
        if tree_node['type'] in ['file', 'web']:
            files.append(tree_node['path'])
        elif 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files


if __name__ == "__main__":
    svc = ScoutMS()
    print("Service ready:", svc)
    
    # Basic local test
    current_dir = os.path.dirname(os.path.abspath(__file__))
    tree = svc.scan_directory(current_dir)
    if tree:
        print(f"Scanned {len(svc.flatten_tree(tree))} files in current directory.")
--------------------------------------------------------------------------------
FILE: src\microservices\_SearchEngineMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import sqlite3
import json
import struct
import requests
import os
import logging
from typing import List, Dict, Any, Optional

# --- RUNTIME DEPENDENCY CHECK ---
REQUIRED = ["requests", "sqlite_vec"] # 'sqlite-vec' package name is often 'sqlite_vec' in pip/import
MISSING = []

for lib in REQUIRED:
    # Handle hyphenated package names for import check vs pip name
    import_name = lib.replace("-", "_")
    if importlib.util.find_spec(import_name) is None:
        MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _SearchEngineMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # We proceed so the class loads, but methods will likely fail.

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================

DEFAULT_OLLAMA_URL = "http://localhost:11434/api"
logger = logging.getLogger("SearchEngine")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="SearchEngine",
    version="1.0.0",
    description="The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching) on SQLite databases.",
    tags=["search", "vector", "hybrid", "rag"],
    capabilities=["db:sqlite", "network:outbound", "compute"]
)
class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    
    Architecture:
    1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
    2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
    3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.model_name = self.config.get("model_name", "phi3:mini-128k")
        self.ollama_url = self.config.get("ollama_url", DEFAULT_OLLAMA_URL)

    @service_endpoint(
        inputs={"db_path": "str", "query": "str", "limit": "int"},
        outputs={"results": "List[Dict]"},
        description="Main entry point. Returns a list of results sorted by relevance (RRF).",
        tags=["search", "query"],
        side_effects=["db:read", "network:outbound"]
    )
    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Main entry point. Returns a list of results sorted by relevance.
        """
        if not os.path.exists(db_path):
            logger.warning(f"Database not found at: {db_path}")
            return []

        conn = sqlite3.connect(db_path)
        # Enable sqlite-vec extension
        try:
            conn.enable_load_extension(True)
            import sqlite_vec
            sqlite_vec.load(conn)
        except Exception as e:
            logger.warning(f"Warning: sqlite_vec not loaded. Vector search may fail. Error: {e}")

        cursor = conn.cursor()

        # 1. Vectorize the User Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails
            logger.info("Vectorization failed. Falling back to keyword-only search.")
            conn.close()
            # Re-open connection is not strictly necessary for fallback logic, 
            # but we return early. Note: _keyword_search_only expects a cursor.
            # We reopen/reuse properly:
            return self._keyword_search_only(db_path, query, limit)

        # Pack vector for sqlite-vec (Float32 Little Endian)
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)

        # 2. HYBRID QUERY (The "Magic" SQL)
        # Note: This SQL assumes a specific schema ('knowledge_vectors', 'documents_fts', 'knowledge_chunks').
        # Ensure your database setup (e.g. Refinery/Librarian) matches these table names.
        sql = """
        WITH 
        vec_matches AS (
            SELECT rowid, distance,
            row_number() OVER (ORDER BY distance) as rank
            FROM knowledge_vectors
            WHERE embedding MATCH ? 
            AND k = 50
        ),
        fts_matches AS (
            SELECT rowid, rank as fts_score,
            row_number() OVER (ORDER BY rank) as rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT 50
        )
        SELECT 
            kc.file_path,
            kc.content,
            (
                -- RRF Formula: 1 / (k + rank)
                COALESCE(1.0 / (60 + v.rank), 0.0) +
                COALESCE(1.0 / (60 + f.rank), 0.0)
            ) as rrf_score
        FROM knowledge_chunks kc
        LEFT JOIN vec_matches v ON kc.id = v.rowid
        LEFT JOIN fts_matches f ON kc.id = f.rowid
        WHERE v.rowid IS NOT NULL OR f.rowid IS NOT NULL
        ORDER BY rrf_score DESC
        LIMIT ?;
        """

        try:
            # Escape quotes for FTS
            fts_query = f'"{query}"' 
            rows = cursor.execute(sql, (vec_bytes, fts_query, limit)).fetchall()
        except sqlite3.OperationalError as e:
            logger.error(f"Search Error (likely missing schema or sqlite-vec): {e}")
            return []
        finally:
            conn.close()

        results = []
        for r in rows:
            path, content, score = r
            snippet = self._extract_snippet(content, query)
            results.append({
                "path": path,
                "score": round(score, 4),
                "snippet": snippet,
                # "full_content": content # Optional: Uncomment if full content is needed
            })

        return results

    def _keyword_search_only(self, db_path: str, query: str, limit: int) -> List[Dict[str, Any]]:
        """Fallback if embeddings are offline."""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        sql = """
            SELECT file_path, content
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        try:
            rows = cursor.execute(sql, (f'"{query}"', limit)).fetchall()
            return [{
                "path": r[0], 
                "score": 0.0, 
                "snippet": self._extract_snippet(r[1], query)
            } for r in rows]
        except sqlite3.OperationalError as e:
            logger.error(f"Keyword Search Error: {e}")
            return []
        finally:
            conn.close()

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        """Call Ollama to get the vector for the search query."""
        try:
            res = requests.post(
                f"{self.ollama_url}/embeddings",
                json={"model": self.model_name, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            logger.error(f"Embedding request failed: {e}")
            return None
        return None

    def _extract_snippet(self, content: str, query: str) -> str:
        """Finds the best window of text around the keyword."""
        if not content:
            return ""
            
        lower_content = content.lower()
        parts = query.lower().split()
        lower_query = parts[0] if parts else "" 
        
        idx = lower_content.find(lower_query)
        if idx == -1:
            return content[:200].replace('\n', ' ') + "..."
            
        start = max(0, idx - 60)
        end = min(len(content), idx + 140)
        snippet = content[start:end].replace('\n', ' ')
        return f"...{snippet}..."


# --- Independent Test Block ---
if __name__ == "__main__":
    # Note: Requires a real DB path to work effectively
    print("Initializing Search Engine...")
    engine = SearchEngineMS({"model_name": "phi3:mini-128k"})
    print("Service ready:", engine)
    
    # Example usage:
    # results = engine.search("my_knowledge.db", "python error handling")
    # print(results)
--------------------------------------------------------------------------------
FILE: src\microservices\_ServiceRegistryMS.py
--------------------------------------------------------------------------------
import ast
import json
import uuid
import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logger = logging.getLogger("ServiceRegistry")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="ServiceRegistry",
    version="1.0.0",
    description="Scans a library of Python microservices and generates standardized JSON Service Tokens.",
    tags=["introspection", "registry", "parsing"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ServiceRegistryMS:
    """
    The Tokenizer (v2): Scans a library of Python microservices and generates
    standardized JSON 'Service Tokens'.
    Feature: Hybrid AST/Regex parsing for maximum robustness.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Default to current directory if not specified
        self.root = Path(self.config.get("root_path", ".")).resolve()
        self.registry = []

    @service_endpoint(
        inputs={"save_to": "str"},
        outputs={"registry": "List[Dict]"},
        description="Scans the file system for microservices and builds a registry.",
        tags=["introspection", "scan"],
        side_effects=["filesystem:read", "filesystem:write"]
    )
    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict[str, Any]]:
        logger.info(f"Scanning for microservices in: {self.root}")
        self.registry = [] # Reset registry
        
        # 1. Walk directories/files
        if self.root.exists():
            for item in self.root.iterdir():
                # Check for Service Folders (e.g. _AuthMS)
                if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                    self._process_folder(item)
                # Check for Service Files (e.g. __AuthMS.py)
                elif item.is_file() and item.name.startswith("_") and item.name.endswith("MS.py"):
                    token = self._tokenize_file(item)
                    if token:
                        self.registry.append(token)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            logger.info(f" Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            logger.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_folder(self, folder: Path):
        # Find the main .py file (usually matches folder name, or is the only .py file)
        candidates = list(folder.glob("*.py"))
        for file in candidates:
            # Usually entry points start with __ inside the folder
            if file.name.startswith("__") or len(candidates) == 1:
                token = self._tokenize_file(file)
                if token:
                    self.registry.append(token)
                    logger.info(f"  + Tokenized: {token['name']}")
                    break 

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source = f.read()
            
            # Attempt 1: Strict AST Parsing (The "Right" Way)
            try:
                return self._ast_parse(source, file_path)
            except Exception:
                # Attempt 2: Regex Fallback (The "Survival" Way)
                return self._regex_parse(source, file_path)
                
        except Exception as e:
            logger.warning(f"  - Failed to read {file_path.name}: {e}")
            return None

    def _ast_parse(self, source: str, file_path: Path):
        tree = ast.parse(source)
        target_class = None
        
        # Find class ending in 'MS'
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name.endswith("MS"):
                target_class = node
                break
        
        if not target_class: return None

        # Extract Metadata
        return self._build_token(
            name=target_class.name,
            doc=ast.get_docstring(target_class) or "",
            methods=[
                (n.name, [a.arg for a in n.args.args if a.arg != 'self'], ast.get_docstring(n) or "")
                for n in target_class.body if isinstance(n, ast.FunctionDef) and not n.name.startswith("_")
            ],
            deps=self._extract_ast_imports(tree),
            file_path=file_path
        )

    def _regex_parse(self, source: str, file_path: Path):
        # Find class definition
        class_match = re.search(r'class\s+(\w+MS)', source)
        if not class_match: return None
        name = class_match.group(1)
        
        # Find methods (def name(args):)
        methods = []
        for match in re.finditer(r'def\s+(\w+)\s*\((.*?)\):', source):
            m_name = match.group(1)
            if not m_name.startswith("_"):
                # Rough args parsing
                args = [a.strip().split(':')[0] for a in match.group(2).split(',') if a.strip() != 'self']
                methods.append((m_name, args, "Regex extracted"))
                
        return self._build_token(name, "Parsed via Regex", methods, [], file_path)

    def _build_token(self, name, doc, methods, deps, file_path):
        # Generate deterministic ID
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, name).hex[:8].upper()}"
        
        method_dict = {
            m_name: {"args": m_args, "doc": m_doc.strip()} 
            for m_name, m_args, m_doc in methods
        }
        
        try:
            rel_path = str(file_path.relative_to(self.root)).replace('\\', '/')
        except ValueError:
            rel_path = file_path.name

        return {
            "token_id": token_id,
            "name": name,
            "path": rel_path,
            "description": doc.strip(),
            "methods": method_dict,
            "dependencies": sorted(deps)
        }

    def _extract_ast_imports(self, tree):
        deps = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: deps.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: deps.add(node.module.split('.')[0])
        return list(deps)


if __name__ == "__main__":
    # Setup logging for independent test
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    
    svc = ServiceRegistryMS()
    print("Service ready:", svc)
    # Perform a test scan of the current directory
    svc.scan()
--------------------------------------------------------------------------------
FILE: src\microservices\_TasklistVaultMS.py
--------------------------------------------------------------------------------
import sqlite3
import uuid
import logging
import datetime
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "task_vault.db"
logger = logging.getLogger("TaskVault")

TaskStatus = Literal["Pending", "Running", "Complete", "Error", "Awaiting-Approval"]

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="TaskVault",
    version="1.0.0",
    description="Persistent SQLite engine for hierarchical task management.",
    tags=["tasks", "db", "project-management"],
    capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class TasklistVaultMS:
    """
    The Taskmaster: A persistent SQLite engine for hierarchical task management.
    Supports infinite nesting of sub-tasks and status tracking.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.db_path = self.config.get("db_path", DB_PATH)
        self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. Task Lists (The containers)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_lists (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP
                )
            """)
            # 2. Tasks (The items, supporting hierarchy via parent_id)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id TEXT PRIMARY KEY,
                    list_id TEXT NOT NULL,
                    parent_id TEXT,
                    content TEXT NOT NULL,
                    status TEXT DEFAULT 'Pending',
                    result TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    FOREIGN KEY(list_id) REFERENCES task_lists(id) ON DELETE CASCADE,
                    FOREIGN KEY(parent_id) REFERENCES tasks(id) ON DELETE CASCADE
                )
            """)

    # --- List Management ---

    @service_endpoint(
        inputs={"name": "str"},
        outputs={"list_id": "str"},
        description="Creates a new task list and returns its ID.",
        tags=["tasks", "create"],
        side_effects=["db:write"]
    )
    def create_list(self, name: str) -> str:
        """Creates a new task list and returns its ID."""
        list_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO task_lists (id, name, created_at) VALUES (?, ?, ?)",
                (list_id, name, now)
            )
        logger.info(f"Created Task List: '{name}' ({list_id})")
        return list_id

    @service_endpoint(
        inputs={},
        outputs={"lists": "List[Dict]"},
        description="Returns metadata for all task lists.",
        tags=["tasks", "read"],
        side_effects=["db:read"]
    )
    def get_lists(self) -> List[Dict[str, Any]]:
        """Returns metadata for all task lists."""
        with self._get_conn() as conn:
            rows = conn.execute("SELECT * FROM task_lists ORDER BY created_at DESC").fetchall()
            return [dict(r) for r in rows]

    # --- Task Management ---

    @service_endpoint(
        inputs={"list_id": "str", "content": "str", "parent_id": "Optional[str]"},
        outputs={"task_id": "str"},
        description="Adds a task (or sub-task) to a list.",
        tags=["tasks", "write"],
        side_effects=["db:write"]
    )
    def add_task(self, list_id: str, content: str, parent_id: Optional[str] = None) -> str:
        """Adds a task (or sub-task) to a list."""
        task_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                """INSERT INTO tasks (id, list_id, parent_id, content, status, created_at, updated_at) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (task_id, list_id, parent_id, content, "Pending", now, now)
            )
        return task_id

    @service_endpoint(
        inputs={"task_id": "str", "content": "str", "status": "str", "result": "str"},
        outputs={},
        description="Updates a task's details.",
        tags=["tasks", "update"],
        side_effects=["db:write"]
    )
    def update_task(self, task_id: str, content: str = None, status: TaskStatus = None, result: str = None):
        """Updates a task's details."""
        updates = []
        params = []
        
        if content:
            updates.append("content = ?")
            params.append(content)
        if status:
            updates.append("status = ?")
            params.append(status)
        if result:
            updates.append("result = ?")
            params.append(result)
            
        if not updates: return

        updates.append("updated_at = ?")
        params.append(datetime.datetime.utcnow())
        params.append(task_id)

        sql = f"UPDATE tasks SET {', '.join(updates)} WHERE id = ?"
        
        with self._get_conn() as conn:
            conn.execute(sql, params)
        logger.info(f"Updated task {task_id}")

    # --- Tree Reconstruction ---

    @service_endpoint(
        inputs={"list_id": "str"},
        outputs={"tree": "Dict[str, Any]"},
        description="Fetches a list and reconstructs the full hierarchy of tasks.",
        tags=["tasks", "read"],
        side_effects=["db:read"]
    )
    def get_full_tree(self, list_id: str) -> Dict[str, Any]:
        """
        Fetches a list and reconstructs the full hierarchy of tasks.
        """
        with self._get_conn() as conn:
            # 1. Get List Info
            list_row = conn.execute("SELECT * FROM task_lists WHERE id = ?", (list_id,)).fetchone()
            if not list_row: return {}
            
            # 2. Get All Tasks
            task_rows = conn.execute("SELECT * FROM tasks WHERE list_id = ?", (list_id,)).fetchall()
            
        # 3. Build Adjacency Map
        tasks_by_id = {}
        for r in task_rows:
            t = dict(r)
            t['sub_tasks'] = [] # Prepare children container
            tasks_by_id[t['id']] = t

        # 4. Link Parents and Children
        root_tasks = []
        for t_id, task in tasks_by_id.items():
            parent_id = task['parent_id']
            if parent_id and parent_id in tasks_by_id:
                tasks_by_id[parent_id]['sub_tasks'].append(task)
            else:
                root_tasks.append(task)

        return {
            "id": list_row['id'],
            "name": list_row['name'],
            "tasks": root_tasks
        }

    @service_endpoint(
        inputs={"list_id": "str"},
        outputs={},
        description="Deletes a task list and all its tasks.",
        tags=["tasks", "delete"],
        side_effects=["db:write"]
    )
    def delete_list(self, list_id: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM task_lists WHERE id = ?", (list_id,))
        logger.info(f"Deleted list {list_id}")


# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # Use a test DB
    test_db = Path("test_task_vault.db")
    if test_db.exists(): 
        os.remove(test_db)
    
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    vault = TasklistVaultMS({"db_path": test_db})
    print("Service ready:", vault)
    
    # 1. Create a Plan
    plan_id = vault.create_list("System Upgrade Plan")
    
    # 2. Add Root Tasks
    t1 = vault.add_task(plan_id, "Backup Database")
    t2 = vault.add_task(plan_id, "Update Server")
    
    # 3. Add Sub-Tasks
    t2_1 = vault.add_task(plan_id, "Stop Services", parent_id=t2)
    t2_2 = vault.add_task(plan_id, "Run Installer", parent_id=t2)
    
    # 4. Update Status
    vault.update_task(t1, status="Complete", result="Backup saved to /tmp/bk.tar")
    vault.update_task(t2_1, status="Running")
    
    # 5. Render Tree
    tree = vault.get_full_tree(plan_id)
    print(f"\n--- {tree.get('name')} ---")
    
    def print_node(node, indent=0):
        status_icon = "" if node['status'] == 'Complete' else ""
        print(f"{'  '*indent}{status_icon} {node['content']} [{node['status']}]")
        for child in node['sub_tasks']:
            print_node(child, indent + 1)

    if 'tasks' in tree:
        for task in tree['tasks']:
            print_node(task)
        
    # Cleanup
    if test_db.exists(): 
        os.remove(test_db)
--------------------------------------------------------------------------------
FILE: src\microservices\_TelemetryServiceMS.py
--------------------------------------------------------------------------------
import logging
import queue
import time
from typing import Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

logger = logging.getLogger("TelemetryService")

# ==============================================================================
# HELPER CLASS
# ==============================================================================

class QueueHandler(logging.Handler):
    """
    Custom logging handler that pushes log records into a thread-safe queue.
    """
    def __init__(self, log_queue: queue.Queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        # We format the record before putting it in the queue so the message field exists
        self.format(record)
        self.log_queue.put(record)

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="TelemetryService",
    version="1.0.0",
    description="The Nervous System: Watches the thread-safe LogQueue and updates GUI components with real-time status.",
    tags=["utility", "logging", "telemetry"],
    capabilities=["log-redirection", "real-time-updates"]
)
class TelemetryServiceMS:
    """
    The Nervous System.
    Watches the thread-safe LogQueue and updates the GUI Panels.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        
        # Dependencies injected via config
        self.root = self.config.get("root")
        self.panels = self.config.get("panels")
        
        self.log_queue = queue.Queue()
        self.start_time = time.time()
        self._heartbeat_count = 0
        
        # We set up the global logging hook HERE, inside the service
        self._setup_logging_hook()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "queue_depth": "int"},
        description="Standardized health check to verify the operational state of the telemetry pipeline.",
        tags=["diagnostic", "health"],
        side_effects=[]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the TelemetryServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "queue_depth": self.log_queue.qsize()
        }

    def _setup_logging_hook(self):
        """Redirects Python's standard logging to our Queue."""
        root_logger = logging.getLogger()
        root_logger.setLevel(logging.INFO)
        
        # Create our custom handler that feeds the queue
        q_handler = QueueHandler(self.log_queue)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
        q_handler.setFormatter(formatter)
        root_logger.addHandler(q_handler)

    @service_endpoint(
        inputs={},
        outputs={},
        description="Initiates the telemetry service and begins the asynchronous GUI log-polling loop.",
        tags=["lifecycle", "event-loop"],
        mode="async",
        side_effects=["ui:update"]
    )
    def start(self):
        """Begins the GUI update loop."""
        logger.info("Telemetry Service starting...")
        self._poll_queue()

    @service_endpoint(
        inputs={},
        outputs={"alive": "bool", "heartbeat": "int"},
        description="Verifies that the GUI polling loop is actively processing the log queue.",
        tags=["diagnostic", "heartbeat"],
        side_effects=[]
    )
    def ping(self) -> Dict[str, Any]:
        """Allows an agent to verify the pulse of the UI loop."""
        return {"alive": True, "heartbeat": self._heartbeat_count}

    def _poll_queue(self):
        """The heartbeat that drains the queue into the GUI."""
        if not self.root or not self.panels:
            return

        self._heartbeat_count += 1
        try:
            while True:
                record = self.log_queue.get_nowait()
                # record.message is populated by QueueHandler calling format()
                msg = f"[{record.levelname}] {record.message}" 
                
                # Update the GUI
                if hasattr(self.panels, 'log'):
                    self.panels.log(msg)
                
        except queue.Empty:
            pass
        finally:
            # Check again in 100ms
            if hasattr(self.root, 'after'):
                self.root.after(100, self._poll_queue)


# --- Independent Test Block ---
if __name__ == "__main__":
    # Mock objects for independent test
    class MockRoot: 
        def after(self, ms, func): 
            # Simulate a loop schedule
            pass

    class MockPanels:
        def log(self, msg): 
            print(f"[UI LOG]: {msg}")
    
    # Initialize
    svc = TelemetryServiceMS({"root": MockRoot(), "panels": MockPanels()})
    print("Service ready:", svc)
    
    # Generate a log
    logger.info("Internal test message")
    
    # Manually trigger one poll cycle to verify it picks up the log
    svc._poll_queue()
--------------------------------------------------------------------------------
FILE: src\microservices\_ThoughtStreamMS.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import datetime
from typing import Any, Dict, Optional, List

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="ThoughtStream",
    version="1.0.0",
    description="A UI widget for displaying a stream of AI thoughts/logs.",
    tags=["ui", "stream", "logs", "widget"],
    capabilities=["ui:gui"]
)
class ThoughtStreamMS(ttk.Frame):
    """
    The Neural Inspector: A UI widget for displaying a stream of AI thoughts/logs
    visualized as 'bubbles' with sparklines.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        # Initialize ttk.Frame
        super().__init__(parent)
        
        # Header
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        # The Stream Area (Canvas allows for custom drawing like sparklines)
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340) # Fixed width like React
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    @service_endpoint(
        inputs={"filename": "str", "chunk_id": "int", "content": "str", "vector_preview": "List[float]", "color": "str"},
        outputs={},
        description="Adds a new thought bubble to the visual stream.",
        tags=["ui", "update"],
        side_effects=["ui:update"]
    )
    def add_thought_bubble(self, filename: str, chunk_id: int, content: str, vector_preview: List[float], color: str):
        """
        Mimics the 'InspectorFrame' from your React code.
        """
        # Bubble Container
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        # Header: File + Timestamp
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        header_lbl = tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", 
                              fg="#007ACC", bg="#1a1a25", font=("Consolas", 8))
        header_lbl.pack(anchor="w", padx=5, pady=2)
        
        # Content Snippet
        snippet = content[:400] + "..." if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", 
                               font=("Consolas", 8), justify="left", wraplength=300)
        content_lbl.pack(fill="x", padx=5, pady=2)
        
        # Vector Sparkline (The Custom Draw)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector: List[float], color: str):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        
        if not vector:
            return

        bar_w = w / len(vector) if len(vector) > 0 else 0
        
        for i, val in enumerate(vector):
            # Normalize -1..1 to 0..1 for height
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            
            # Draw bar
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")


# --- Independent Test Block ---
if __name__ == "__main__":
    import random
    
    root = tk.Tk()
    root.title("Thought Stream Test")
    root.geometry("400x600")
    
    stream = ThoughtStreamMS({"parent": root})
    print("Service ready:", stream)
    stream.pack(fill="both", expand=True)
    
    # Simulate an incoming "Microservice" event
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("ExplorerView.tsx", 1, "import React from 'react'...", fake_vector, "#FF00FF")
    
    # Add another one for effect
    fake_vector_2 = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("Backend.py", 42, "def process_data(self): pass", fake_vector_2, "#00FF00")

    root.mainloop()
--------------------------------------------------------------------------------
FILE: src\microservices\_TkinterAppShellMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterAppShellMS
ENTRY_POINT: _TkinterAppShellMS.py
DEPENDENCIES: None
"""
import tkinter as tk
from tkinter import ttk
import logging
from typing import Dict, Any, Optional

from microservice_std_lib import service_metadata, service_endpoint

# Updated Import: Single Underscore + 'Tkinter' prefix
try:
    from _TkinterThemeManagerMS import TkinterThemeManagerMS
except ImportError:
    TkinterThemeManagerMS = None

logger = logging.getLogger("AppShell")

@service_metadata(
    name="TkinterAppShell",
    version="2.0.0",
    description="The Application Container. Manages the root window, main loop, and global layout.",
    tags=["ui", "core", "lifecycle"],
    capabilities=["ui:root", "ui:gui"]
)
class TkinterAppShellMS:
    """
    The Mother Ship.
    Owns the Tkinter Root. All other UI microservices dock into this.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.root = tk.Tk()
        self.root.withdraw() # Hide until launch
        
        # Load Theme (Inject dependency or create new)
        self.theme_svc = self.config.get("theme_manager")
        if not self.theme_svc and TkinterThemeManagerMS:
            self.theme_svc = TkinterThemeManagerMS()
            
        self.colors = self.theme_svc.get_theme() if self.theme_svc else {}
        self._configure_root()
        
    def _configure_root(self):
        self.root.title(self.config.get("title", "Microservice OS"))
        self.root.geometry(self.config.get("geometry", "1200x800"))
        
        # Apply Base Theme
        bg = self.colors.get('background', '#1e1e1e')
        self.root.configure(bg=bg)
        
        # Configure TTK Styles globally
        style = ttk.Style()
        style.theme_use('clam')
        
        # Standard Frames
        style.configure('TFrame', background=bg)
        style.configure('TLabel', background=bg, foreground=self.colors.get('foreground', '#ccc'))
        style.configure('TButton', background=self.colors.get('panel_bg', '#333'), foreground='white')
        
        # Main Container (Grid or Pack)
        self.main_container = tk.Frame(self.root, bg=bg)
        self.main_container.pack(fill="both", expand=True, padx=5, pady=5)

    @service_endpoint(
        inputs={},
        outputs={},
        description="Starts the GUI Main Loop.",
        tags=["lifecycle", "start"],
        mode="sync",
        side_effects=["ui:block"]
    )
    def launch(self):
        """Ignition sequence start."""
        self.root.deiconify()
        logger.info("AppShell Launched.")
        self.root.mainloop()

    @service_endpoint(
        inputs={},
        outputs={"container": "tk.Frame"},
        description="Returns the main content area for other services to dock into.",
        tags=["ui", "layout"]
    )
    def get_main_container(self):
        """Other services call this to know where to .pack() themselves."""
        return self.main_container

    @service_endpoint(
        inputs={},
        outputs={},
        description="Gracefully shuts down the application.",
        tags=["lifecycle", "stop"],
        side_effects=["ui:close"]
    )
    def shutdown(self):
        self.root.quit()

if __name__ == "__main__":
    shell = TkinterAppShellMS({"title": "Test Shell"})
    shell.launch()
--------------------------------------------------------------------------------
FILE: src\microservices\_TkinterSmartExplorerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterSmartExplorerMS
ENTRY_POINT: _TkinterSmartExplorerMS.py
DEPENDENCIES: None
"""
import tkinter as tk
from tkinter import ttk
from typing import Dict, Any, Optional, List

from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="TkinterSmartExplorer",
    version="1.0.0",
    description="A hierarchical tree viewer capable of displaying file systems or JSON data structures.",
    tags=["ui", "widget", "explorer"],
    capabilities=["ui:gui"]
)
class TkinterSmartExplorerMS(tk.Frame):
    """
    The Navigator.
    A TreeView widget that expects standard 'Node' dictionaries (name, type, children).
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        theme = self.config.get("theme", {})
        
        super().__init__(parent, bg=theme.get('panel_bg', '#252526'))
        
        # UI Setup
        self.tree = ttk.Treeview(self, show="tree headings", selectmode="browse")
        self.tree.heading("#0", text="Explorer", anchor="w")
        
        # Scrollbars
        vsb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        hsb = ttk.Scrollbar(self, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        
        self.tree.pack(side="left", fill="both", expand=True)
        vsb.pack(side="right", fill="y")
        
        # Icons (Unicode placeholders for simplicity)
        self.icons = {
            "folder": "",
            "file": "",
            "web": "",
            "unknown": ""
        }

    @service_endpoint(
        inputs={"data": "Dict"},
        outputs={},
        description="Populates the tree view with a nested dictionary structure (Standard 'Node' format).",
        tags=["ui", "update"],
        side_effects=["ui:update"]
    )
    def load_data(self, data: Dict[str, Any]):
        """
        Ingests a dictionary tree (like from _ScoutMS or _TreeMapperMS).
        """
        # Clear existing
        for item in self.tree.get_children():
            self.tree.delete(item)
            
        self._build_node("", data)

    def _build_node(self, parent_id, node_data):
        # Determine Icon
        ntype = node_data.get("type", "unknown")
        icon = self.icons.get(ntype, self.icons["unknown"])
        text = f"{icon} {node_data.get('name', '???')}"
        
        # Insert Item
        item_id = self.tree.insert(parent_id, "end", text=text, open=True)
        
        # Recursion
        for child in node_data.get("children", []):
            self._build_node(item_id, child)

if __name__ == "__main__":
    root = tk.Tk()
    explorer = TkinterSmartExplorerMS({"parent": root})
    explorer.pack(fill="both", expand=True)
    
    dummy_data = {
        "name": "Project Root",
        "type": "folder",
        "children": [
            {"name": "src", "type": "folder", "children": []},
            {"name": "README.md", "type": "file"}
        ]
    }
    
    explorer.load_data(dummy_data)
    root.mainloop()
--------------------------------------------------------------------------------
FILE: src\microservices\_TkinterThemeManagerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterThemeManagerMS
ENTRY_POINT: _TkinterThemeManagerMS.py
DEPENDENCIES: None
"""
from typing import Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# Default "Dark Modern" Theme
DEFAULT_THEME = {
    'background': '#1e1e1e',
    'foreground': '#d4d4d4',
    'panel_bg':   '#252526',
    'border':     '#3c3c3c',
    'accent':     '#007acc',
    'error':      '#f48771',
    'success':    '#89d185',
    'font_main':  ('Segoe UI', 10),
    'font_mono':  ('Consolas', 10)
}

@service_metadata(
    name="TkinterThemeManager",
    version="1.0.0",
    description="Centralized configuration for UI colors and fonts.",
    tags=["ui", "config", "theme"],
    capabilities=["ui:style"]
)
class TkinterThemeManagerMS:
    """
    The Stylist: Holds the color palette and font settings.
    All UI components query this service to decide how to draw themselves.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.theme = DEFAULT_THEME.copy()
        
        # Allow override from config
        if "overrides" in self.config:
            self.theme.update(self.config["overrides"])

    @service_endpoint(
        inputs={},
        outputs={"theme": "Dict"},
        description="Returns the current active theme dictionary.",
        tags=["ui", "read"]
    )
    def get_theme(self) -> Dict[str, Any]:
        return self.theme

    @service_endpoint(
        inputs={"key": "str", "value": "Any"},
        outputs={},
        description="Updates a specific theme attribute (e.g., changing accent color).",
        tags=["ui", "write"],
        side_effects=["ui:refresh"]
    )
    def update_key(self, key: str, value: Any):
        self.theme[key] = value

if __name__ == "__main__":
    svc = TkinterThemeManagerMS()
    print("Theme Ready:", svc.get_theme()['accent'])
--------------------------------------------------------------------------------
FILE: src\microservices\_TreeMapperMS.py
--------------------------------------------------------------------------------
import os
import datetime
import logging
from pathlib import Path
from typing import Any, Dict, List, Set, Optional

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    '.venv', 'env', 'venv', 'dist', 'build', '.DS_Store'
}
logger = logging.getLogger("TreeMapper")

# ==============================================================================
# MICROSERVICE CLASS
# ==============================================================================

@service_metadata(
    name="TreeMapper",
    version="1.0.0",
    description="Generates ASCII-art style directory maps of the file system.",
    tags=["filesystem", "map", "visualization"],
    capabilities=["filesystem:read"]
)
class TreeMapperMS:
    """
    The Cartographer: Generates ASCII-art style directory maps.
    Useful for creating context snapshots for LLMs.
    """
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

    @service_endpoint(
        inputs={"root_path": "str", "additional_exclusions": "Set[str]", "use_default_exclusions": "bool"},
        outputs={"tree_map": "str"},
        description="Generates an ASCII tree map of the directory.",
        tags=["filesystem", "visualization"],
        side_effects=["filesystem:read"]
    )
    def generate_tree(self, 
                      root_path: str, 
                      additional_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> str:
        
        start_path = Path(root_path).resolve()
        if not start_path.exists(): 
            return f"Error: Path '{root_path}' does not exist."

        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_EXCLUDES)
        if additional_exclusions:
            exclusions.update(additional_exclusions)

        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lines = [
            f"Project Map: {start_path.name}",
            f"Generated: {timestamp}",
            "-" * 40,
            f" {start_path.name}/"
        ]

        logger.info(f"Mapping directory: {start_path}")
        self._walk(start_path, "", lines, exclusions)
        return "\n".join(lines)

    def _walk(self, directory: Path, prefix: str, lines: List[str], exclusions: Set[str]):
        try:
            # Sort: Directories first, then files (alphabetical)
            children = sorted(
                [p for p in directory.iterdir() if p.name not in exclusions],
                key=lambda x: (not x.is_dir(), x.name.lower())
            )
        except PermissionError:
            lines.append(f"{prefix}  [Permission Denied]")
            return

        count = len(children)
        for index, path in enumerate(children):
            is_last = (index == count - 1)
            connector = " " if is_last else " "
            
            if path.is_dir():
                lines.append(f"{prefix}{connector} {path.name}/")
                extension = "    " if is_last else "   "
                self._walk(path, prefix + extension, lines, exclusions)
            else:
                lines.append(f"{prefix}{connector} {path.name}")


# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

    svc = TreeMapperMS()
    print("Service ready:", svc)
    
    # Map the current directory
    print("\n--- Map of Current Dir ---")
    tree = svc.generate_tree(".", additional_exclusions={"__pycache__"})
    print(tree)