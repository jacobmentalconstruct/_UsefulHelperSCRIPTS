Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_NeoCORTEX


--------------------------------------------------------------------------------
FILE: LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: requirements.txt
--------------------------------------------------------------------------------
# --- Critical UI/Graphing ---
pygame-ce>=2.3.0
Pillow>=10.0.0

# --- Networking/AI ---
requests>=2.30.0

# --- Database Extensions ---
# (Ensure your Python environment supports installing this, 
# otherwise you may need to manually place the DLL/SO)
sqlite-vec>=0.1.0

# --- Document Processing ---
pypdf>=3.0.0
beautifulsoup4>=4.12.0
--------------------------------------------------------------------------------
FILE: setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: src\app.py
--------------------------------------------------------------------------------
import sys
import argparse
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext, filedialog
import os
import sqlite3
import threading
import queue
import json
import time

# --- SYSTEM PATH SETUP ---
# Ensure src/ is importable as a top-level root so _micro_services can be imported directly.
_SRC_DIR = os.path.dirname(os.path.abspath(__file__))
if _SRC_DIR not in sys.path:
    sys.path.insert(0, _SRC_DIR)

# --- MICROSERVICE IMPORTS ---
from _micro_services._LibrarianServiceMS.librarian_service import LibrarianMS
from _micro_services._ScannerMS.scanner import ScannerMS
from _micro_services._IngestEngineMS.ingest_engine import IngestEngine
from _micro_services._GraphEngineMS.graph_view import GraphView
from _micro_services._ThoughtStreamMS.thought_stream import ThoughtStream
from _micro_services._SearchEngineMS.search_engine import SearchEngineMS
from _micro_services._ExporterMS.exporter import ExporterMS

# --- UI CONSTANTS ---
BG_COLOR = "#1e1e2f"
SIDEBAR_COLOR = "#171725"
ACCENT_COLOR = "#007ACC"
DANGER_COLOR = "#D32F2F"
SUCCESS_COLOR = "#388E3C"
TEXT_COLOR = "#e0e0e0"
EDITOR_BG = "#252526"
MODAL_BG = "#252526"

# ==============================================================================
#  CORE GUI CLASSES
# ==============================================================================

class Sidebar(tk.Frame):
    def __init__(self, parent, app, librarian: LibrarianMS):
        super().__init__(parent, bg=SIDEBAR_COLOR, width=250)
        self.app = app
        self.librarian = librarian
    
        self.pack_propagate(False)

        tk.Label(self, text="CORTEX DB", bg=SIDEBAR_COLOR, fg=ACCENT_COLOR, font=("Consolas", 14, "bold"), pady=20).pack(fill="x")
        tk.Label(self, text="ACTIVE KNOWLEDGE BASES", bg=SIDEBAR_COLOR, fg="#666", font=("Arial", 8, "bold"), anchor="w", padx=10).pack(fill="x")
        
        self.db_listbox = tk.Listbox(self, bg=SIDEBAR_COLOR, fg=TEXT_COLOR, bd=0, highlightthickness=0, selectbackground=ACCENT_COLOR)
        self.db_listbox.pack(fill="both", expand=True, padx=5, pady=5)
        self.db_listbox.bind("<<ListboxSelect>>", self.on_db_select)

        # Bottom Controls
        btn_frame = tk.Frame(self, bg=SIDEBAR_COLOR, pady=10)
        btn_frame.pack(fill="x", side="bottom")
        tk.Button(btn_frame, text="REFRESH LIST", bg="#2d2d44", fg="white", relief="flat", command=self.refresh_list).pack(fill="x", padx=10)

        self.refresh_list()

    def refresh_list(self):
        self.db_listbox.delete(0, tk.END)
        dbs = self.librarian.list_kbs()
        for db in dbs:
            self.db_listbox.insert(tk.END, db)

    def on_db_select(self, event):
        selection = self.db_listbox.curselection()
        if selection:
            db_name = self.db_listbox.get(selection[0])
            self.app.set_active_db(db_name)

class SettingsModal(tk.Toplevel):
    def __init__(self, parent, current_embed, current_helper, current_architect, ingestor_factory, callback):
        super().__init__(parent)
        self.title("Neural Configuration")
        self.geometry("450x350")
        self.configure(bg=MODAL_BG)
        self.ingestor_factory = ingestor_factory
        self.callback = callback
        
        x = parent.winfo_x() + (parent.winfo_width() // 2) - 225
        y = parent.winfo_y() + (parent.winfo_height() // 2) - 175
        self.geometry(f"+{x}+{y}")

        self.embed_var = tk.StringVar(value=current_embed)
        self.helper_var = tk.StringVar(value=current_helper)
        self.architect_var = tk.StringVar(value=current_architect)

        p = 20
        tk.Label(self, text="Main Architect (Reasoning/Chat):", bg=MODAL_BG, fg="white", font=("Arial", 9, "bold")).pack(anchor="w", padx=p, pady=(p, 5))
        self.architect_combo = ttk.Combobox(self, textvariable=self.architect_var, width=40)
        self.architect_combo.pack(padx=p, fill="x")
        
        tk.Label(self, text="Helper Agent (Fast Tags/Summary):", bg=MODAL_BG, fg="#A020F0", font=("Arial", 9, "bold")).pack(anchor="w", padx=p, pady=(15, 5))
        self.helper_combo = ttk.Combobox(self, textvariable=self.helper_var, width=40)
        self.helper_combo.pack(padx=p, fill="x")

        tk.Label(self, text="Embedder (Vectors):", bg=MODAL_BG, fg="#007ACC", font=("Arial", 9, "bold")).pack(anchor="w", padx=p, pady=(15, 5))
        self.embed_combo = ttk.Combobox(self, textvariable=self.embed_var, width=40)
        self.embed_combo.pack(padx=p, fill="x")

        btn_frame = tk.Frame(self, bg=MODAL_BG, pady=20)
        btn_frame.pack(fill="x", side="bottom")

        tk.Button(btn_frame, text="Apply Settings", bg=ACCENT_COLOR, fg="white", relief="flat", padx=15, pady=5, command=self.save_and_close).pack(side="right", padx=p)
        tk.Button(btn_frame, text="‚Üª Refresh Models", bg="#444", fg="white", relief="flat", padx=10, pady=5, command=self.refresh_models).pack(side="left", padx=p)

        # Defer model loading to allow window to render first
        self.after(100, lambda: self.refresh_models(silent=True))

    def refresh_models(self, silent=False):
        try:
            temp_engine = self.ingestor_factory("temp_scan.db")
            available_models = temp_engine.get_available_models()
            if available_models:
                self.embed_combo['values'] = tuple(available_models)
                self.helper_combo['values'] = tuple(available_models)
                self.architect_combo['values'] = tuple(available_models)
                if not silent: messagebox.showinfo("Ollama", f"Found {len(available_models)} models.")
            else:
                if not silent: messagebox.showwarning("Ollama", "No models found.")
        except Exception as e:
            if not silent: messagebox.showerror("Error", str(e))

    def save_and_close(self):
        self.callback(self.embed_var.get(), self.helper_var.get(), self.architect_var.get())
        self.destroy()

class IngestView(tk.Frame):
    def __init__(self, parent, app, scanner: ScannerMS, ingestor_factory):
        super().__init__(parent, bg=BG_COLOR)
        self.app = app
        self.scanner = scanner
        self.ingestor_factory = ingestor_factory
        self.current_tree = None
        self.current_engine = None # Reference to active engine for cancelling
        
        self.selected_embed = "nomic-embed-text"
        self.selected_helper = "qwen2.5:0.5b"
        self.selected_architect = "qwen2.5:7b"
        
        self.paned = ttk.PanedWindow(self, orient="horizontal")
        self.paned.pack(fill="both", expand=True)

        left_frame = tk.Frame(self.paned, bg=BG_COLOR)
        self.paned.add(left_frame, weight=1)

        # --- CONTROL PANEL ---
        ctrl_frame = tk.Frame(left_frame, bg=BG_COLOR, pady=10, padx=10)
        ctrl_frame.pack(fill="x")

        # Row 1: Source Picker
        row1 = tk.Frame(ctrl_frame, bg=BG_COLOR)
        row1.pack(fill="x", pady=2)
        tk.Label(row1, text="SOURCE:", bg=BG_COLOR, fg="gray", width=10, anchor="w").pack(side="left")
        self.path_entry = tk.Entry(row1, bg="#2d2d44", fg="white", insertbackground="white")
        self.path_entry.pack(side="left", fill="x", expand=True, padx=5)
        self.path_entry.insert(0, os.getcwd()) 
        tk.Button(row1, text="üìÇ Folder", bg="#444", fg="white", relief="flat", command=self.browse_folder).pack(side="left", padx=1)
        tk.Button(row1, text="üìÑ File", bg="#444", fg="white", relief="flat", command=self.browse_file).pack(side="left", padx=1)
        
        # Row 2: Scan Options & Target DB
        row2 = tk.Frame(ctrl_frame, bg=BG_COLOR)
        row2.pack(fill="x", pady=5)
        
        tk.Label(row2, text="WEB DEPTH:", bg=BG_COLOR, fg="gray", width=10, anchor="w").pack(side="left")
        self.depth_spin = tk.Spinbox(row2, from_=0, to=5, width=3, bg="#2d2d44", fg="white", buttonbackground="#444")
        self.depth_spin.pack(side="left", padx=(5, 15))

        tk.Button(row2, text="üîç SCAN TARGET", bg=ACCENT_COLOR, fg="white", relief="flat", command=self.run_scan).pack(side="left", padx=5)
        
        # Target DB Selector
        tk.Label(row2, text="TARGET DB:", bg=BG_COLOR, fg="gray", padx=10).pack(side="left")

        # Default to NONE (disables ingestion until a real DB is selected)
        self.target_db_var = tk.StringVar(value="NONE")
        self.target_db_combo = ttk.Combobox(row2, textvariable=self.target_db_var, width=90, state="readonly")
        self.target_db_combo.pack(side="left", fill="x", padx=5)
        self.target_db_combo.bind("<<ComboboxSelected>>", self.on_db_combo_select)

        # Artifact Type Selector (explicit cartridge contract)
        tk.Label(row2, text="ARTIFACT:", bg=BG_COLOR, fg="gray", padx=10).pack(side="left")
        self.artifact_type_var = tk.StringVar(value="UNKNOWN")
        self.artifact_type_combo = ttk.Combobox(row2, textvariable=self.artifact_type_var, width=14, state="readonly")
        self.artifact_type_combo['values'] = ("UNKNOWN", "CODEBASE", "DOCUMENTS", "WEBSITE", "MIXED")
        self.artifact_type_combo.pack(side="left", padx=5)
        self.artifact_type_combo.bind("<<ComboboxSelected>>", lambda e: self._sync_db_dependent_controls())

        # New DB Button (inline creation mode)
        tk.Button(row2, text="‚ûï NEW", bg="#444", fg="white", relief="flat", command=self.create_new_db_dialog).pack(side="left", padx=(5, 2))

        # Inline New-DB name entry (disabled until NEW is clicked)
        self._new_db_placeholder = "type new db name and press Enter"
        self.new_db_var = tk.StringVar(value=self._new_db_placeholder)
        self.new_db_entry = tk.Entry(row2, textvariable=self.new_db_var, bg="#2d2d44", fg="#777", insertbackground="white", state="disabled", width=28)
        self.new_db_entry.pack(side="left", padx=(2, 0))
        self.new_db_entry.bind("<FocusIn>", self._on_new_db_focus_in)
        self.new_db_entry.bind("<FocusOut>", self._on_new_db_focus_out)
        self.new_db_entry.bind("<Return>", self._on_new_db_enter)

        self.refresh_db_combo()
        self._sync_db_dependent_controls()

        # Tree View for Scan Results
        self.tree = ttk.Treeview(left_frame, selectmode="extended")
        self.tree.pack(fill="both", expand=True, padx=10, pady=5)
        
        # --- ACTION BAR ---
        action_frame = tk.Frame(left_frame, bg="#101018", pady=10, padx=10)
        action_frame.pack(fill="x", side="bottom")
        
        # Config Cog
        tk.Button(action_frame, text="‚öô", bg="#101018", fg="#666", font=("Arial", 16), bd=0, command=self.open_settings).pack(side="left", padx=(0, 10))
        self.lbl_config = tk.Label(action_frame, text="[Default Config]", bg="#101018", fg="gray", font=("Consolas", 9))
        self.lbl_config.pack(side="left")
        self._update_config_label()
        
        # Traffic Light Controls
        btn_box = tk.Frame(action_frame, bg="#101018")
        btn_box.pack(side="right")

        self.reconstruct_btn = tk.Button(btn_box, text="‚ôª RECONSTRUCT", bg="#444", fg="white", relief="flat", command=self.reconstruct_files)
        self.reconstruct_btn.pack(side="right", padx=5)

        self.cancel_btn = tk.Button(btn_box, text="üõë CANCEL", bg=DANGER_COLOR, fg="white", relief="flat", state="disabled", command=self.cancel_ingestion)
        self.cancel_btn.pack(side="right", padx=5)

        self.ingest_btn = tk.Button(btn_box, text="‚ñ∂ START INGESTION", bg="gray", fg="white", relief="flat", state="disabled", command=self.start_ingestion)
        self.ingest_btn.pack(side="right", padx=5)
        
        self.lbl_status = tk.Label(action_frame, text="Ready", bg="#101018", fg="gray")
        self.lbl_status.pack(side="right", padx=10)

        self.stream = ThoughtStream(self.paned)
        self.paned.add(self.stream, weight=0) 

    def refresh_db_combo(self):
        # Always include a NONE sentinel so the user can't accidentally ingest into "no selection"
        dbs = ["NONE"] + self.app.librarian.list_kbs()
        self.target_db_combo['values'] = dbs

        # Prefer active_db if set, else keep current selection, else default to NONE
        if self.app.active_db:
            self.target_db_combo.set(self.app.active_db)
            self.target_db_var.set(self.app.active_db)
        else:
            cur = (self.target_db_var.get() or "").strip()
            if cur not in dbs:
                self.target_db_combo.set("NONE")
                self.target_db_var.set("NONE")

        self._sync_db_dependent_controls()

    def on_db_combo_select(self, event):
        selected = (self.target_db_var.get() or "").strip()
        if not selected or selected == "NONE":
            self.app.active_db = None
            self._sync_db_dependent_controls()
            return

        self.app.set_active_db(selected)
        self._sync_db_dependent_controls()

    def _sync_db_dependent_controls(self):
        """Enable/disable controls that require a real active DB."""
        selected = (self.target_db_var.get() or "").strip()
        has_db = bool(selected) and selected != "NONE"

        artifact = "UNKNOWN"
        if hasattr(self, "artifact_type_var"):
            artifact = (self.artifact_type_var.get() or "UNKNOWN").strip().upper()
        has_artifact = bool(artifact) and artifact != "UNKNOWN"

        # If scan is ready, ingestion button should still be blocked unless a DB + Artifact are selected
        if hasattr(self, "ingest_btn"):
            if has_db and has_artifact and self.current_tree:
                self.ingest_btn.config(state="normal", bg=ACCENT_COLOR)
            else:
                self.ingest_btn.config(state="disabled", bg="gray")

        if hasattr(self, "reconstruct_btn"):
            self.reconstruct_btn.config(state=("normal" if has_db else "disabled"))

    def create_new_db_dialog(self):
        """Enter 'new DB creation' mode: enable the inline entry and focus it."""
        self.new_db_entry.config(state="normal")
        self.new_db_entry.focus_set()
        # Select placeholder text for quick overwrite
        self.new_db_entry.selection_range(0, tk.END)

    def _on_new_db_focus_in(self, event=None):
        val = self.new_db_var.get()
        if val == self._new_db_placeholder:
            self.new_db_var.set("")
            self.new_db_entry.config(fg="white")

    def _on_new_db_focus_out(self, event=None):
        val = (self.new_db_var.get() or "").strip()
        if not val:
            self.new_db_var.set(self._new_db_placeholder)
            self.new_db_entry.config(fg="#777")
            self.new_db_entry.config(state="disabled")

    def _on_new_db_enter(self, event=None):
        raw = (self.new_db_var.get() or "").strip()
        if not raw or raw == self._new_db_placeholder:
            return

        # Ensure extension
        if not raw.lower().endswith('.db'):
            raw += '.db'

        try:
            result = self.app.librarian.create_kb(raw)
            actual_name = result.get('name', raw)

            self.app.sidebar.refresh_list()
            self.refresh_db_combo()
            self.app.set_active_db(actual_name)
            self.target_db_combo.set(actual_name)
            self.target_db_var.set(actual_name)

            # Exit creation mode
            self.new_db_var.set(self._new_db_placeholder)
            self.new_db_entry.config(fg="#777")
            self.new_db_entry.config(state="disabled")
            self._sync_db_dependent_controls()
        except Exception as e:
            messagebox.showerror("Error", str(e))

    def browse_folder(self):
        path = filedialog.askdirectory()
        if path:
            self.path_entry.delete(0, tk.END)
            self.path_entry.insert(0, path)

    def browse_file(self):
        path = filedialog.askopenfilename()
        if path:
            self.path_entry.delete(0, tk.END)
            self.path_entry.insert(0, path)

    def open_settings(self):
        SettingsModal(self, self.selected_embed, self.selected_helper, self.selected_architect, self.ingestor_factory, self.on_settings_saved)

    def on_settings_saved(self, new_embed, new_helper, new_architect):
        self.selected_embed = new_embed
        self.selected_helper = new_helper
        self.selected_architect = new_architect
        self._update_config_label()

    def _update_config_label(self):
        e_name = self.selected_embed.split(':')[0]
        h_name = self.selected_helper.split(':')[0]
        a_name = self.selected_architect.split(':')[0]
        self.lbl_config.config(text=f"[{a_name} | {h_name} | {e_name}]")

    def run_scan(self):
        path = self.path_entry.get().strip()
        try:
            depth = int(self.depth_spin.get())
        except ValueError:
            depth = 0

        if not path: return

        self.lbl_status.config(text="Scanning...")
        self.config(cursor="watch")
        self.update_idletasks()
        
        try:
            # Run scan
            data = self.scanner.scan_directory(path, web_depth=depth)
        finally:
            self.config(cursor="")
        
        if not data:
            messagebox.showerror("Error", "Invalid path or URL")
            self.lbl_status.config(text="Scan Failed")
            return
            
        self.current_tree = data
        self._populate_tree("", data)
        self._sync_db_dependent_controls()
        self.lbl_status.config(text="Scan Complete. Select a DB and Ingest.")

    def _populate_tree(self, parent_id, node):
        if parent_id == "": self.tree.delete(*self.tree.get_children())
        display_text = f"{node['text']}"
        if node.get('type') == 'binary': display_text += " [BIN]"
        if node.get('type') == 'web': display_text += " [WEB]"
        
        oid = self.tree.insert(parent_id, "end", text=display_text, open=True)
        for child in node.get('children', []): self._populate_tree(oid, child)

    def start_ingestion(self):
        target_db = (self.target_db_var.get() or "").strip()
        if not target_db or target_db == "NONE":
            messagebox.showwarning("Warning", "Please select a Target DB (not NONE).")
            return

        artifact = (self.artifact_type_var.get() if hasattr(self, "artifact_type_var") else "UNKNOWN")
        artifact = (artifact or "UNKNOWN").strip().upper()
        if artifact == "UNKNOWN":
            messagebox.showwarning("Warning", "Please select an Artifact Type (not UNKNOWN).")
            return

        # Handle New DB Creation Logic
        if not target_db.lower().endswith('.db'):
            target_db += '.db'

        existing_dbs = self.app.librarian.list_kbs()
        if target_db not in existing_dbs:
            # Auto-create (IMPORTANT: use canonical returned name)
            try:
                result = self.app.librarian.create_kb(target_db)
                target_db = result.get('name', target_db)
                self.app.sidebar.refresh_list()
                self.refresh_db_combo()
            except Exception as e:
                messagebox.showerror("Error", f"Could not create DB: {e}")
                return

        # Set Active (canonical)
        self.app.set_active_db(target_db)
        self.target_db_var.set(target_db)
        self.target_db_combo.set(target_db)
        self._sync_db_dependent_controls()

        files = self.scanner.flatten_tree(self.current_tree)
        if not files: return

        # UI State Lock
        self.ingest_btn.config(state="disabled", text="RUNNING...")
        self.cancel_btn.config(state="normal")
        self.reconstruct_btn.config(state="disabled")

        embed_model = self.selected_embed
        summary_model = self.selected_helper 
        db_path = os.path.join(self.app.librarian.storage_dir, target_db)

        # Stamp cartridge contract fields into manifest BEFORE ingestion begins
        try:
            source = (self.path_entry.get() or "").strip()
            try:
                depth = int(self.depth_spin.get())
            except Exception:
                depth = 0

            artifact_profile = {
                "artifact_type": artifact,
                "source": source,
                "web_depth": depth,
                "ui": "_NeoCORTEX",
                "stamped_at": time.time()
            }
            source_prov = {
                "source": source,
                "web_depth": depth,
                "stamped_at": time.time()
            }

            conn = sqlite3.connect(db_path)
            cur = conn.cursor()
            cur.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")
            cur.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("artifact_type", artifact))
            cur.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("artifact_profile", json.dumps(artifact_profile)))
            cur.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("source_provenance", json.dumps(source_prov)))
            conn.commit()
            conn.close()
        except Exception:
            # Never block ingest if manifest stamping fails
            pass
        
        # Instantiate Engine and keep ref for cancelling
        self.current_engine = self.ingestor_factory(db_path)
        self.msg_queue = queue.Queue()
        
        def worker():
            for status in self.current_engine.process_files(files, embed_model, summary_model):
                self.msg_queue.put(status)
            self.msg_queue.put(None)

        threading.Thread(target=worker, daemon=True).start()
        self.check_queue()

    def cancel_ingestion(self):
        if self.current_engine:
            self.current_engine.abort()
            self.lbl_status.config(text="Stopping...")
            self.cancel_btn.config(state="disabled")

    def reconstruct_files(self):
        if not self.app.active_db:
            messagebox.showwarning("Warning", "Select a Knowledge Base first.")
            return

        target_dir = filedialog.askdirectory(title="Select Reconstruction Destination")
        if not target_dir: return

        try:
            db_path = os.path.join(self.app.librarian.storage_dir, self.app.active_db)
            count, errors = self.app.exporter.export_knowledge_base(db_path, target_dir)
            
            msg = f"Reconstruction Complete.\nFiles Restored: {count}"
            if errors: msg += f"\nErrors: {len(errors)} (Check logs)"
            messagebox.showinfo("Result", msg)
        except Exception as e:
            messagebox.showerror("Failed", str(e))

    def check_queue(self):
        try:
            while True:
                status = self.msg_queue.get_nowait()
                if status is None:
                    # DONE
                    self.ingest_btn.config(state="normal", text="START INGESTION")
                    self.cancel_btn.config(state="disabled")
                    self.reconstruct_btn.config(state="normal")
                    self.lbl_status.config(text="Ingestion Cycle Ended.")
                    self.current_engine = None
                    if self.app.active_db: 
                        self.app.editor_view.refresh_file_list()
                        self.app.db_view.refresh_data()
                    return
                
                self.lbl_status.config(text=f"{status.progress_percent:.1f}% - {status.log_message}")
                if status.thought_frame:
                    tf = status.thought_frame
                    self.stream.add_thought_bubble(tf['file'], tf['chunk_index'], tf['content'], tf['vector_preview'], tf['concept_color'])
        except queue.Empty:
            pass
        self.after(100, self.check_queue)

class DatabaseView(tk.Frame):
    def __init__(self, parent, app):
        super().__init__(parent, bg=BG_COLOR)
        self.app = app
        
        toolbar = tk.Frame(self, bg=SIDEBAR_COLOR, pady=5, padx=5)
        toolbar.pack(fill="x")
        
        tk.Label(toolbar, text="TABLE:", bg=SIDEBAR_COLOR, fg="gray", font=("Arial", 8, "bold")).pack(side="left")
        self.table_var = tk.StringVar(value="files")
        self.table_combo = ttk.Combobox(toolbar, textvariable=self.table_var, width=15, state="readonly")
        self.table_combo['values'] = ('files', 'chunks', 'diff_log', 'graph_nodes', 'graph_edges', 'manifest')
        self.table_combo.pack(side="left", padx=5)
        self.table_combo.bind("<<ComboboxSelected>>", self.refresh_data)

        tk.Label(toolbar, text="FILTER (SQL LIKE):", bg=SIDEBAR_COLOR, fg="gray", font=("Arial", 8, "bold")).pack(side="left", padx=(15, 5))
        self.search_entry = tk.Entry(toolbar, bg="#2d2d44", fg="white", insertbackground="white")
        self.search_entry.pack(side="left", fill="x", expand=True)
        self.search_entry.bind("<Return>", self.refresh_data)
        
        tk.Button(toolbar, text="üíæ SAVE SELECTED", bg=ACCENT_COLOR, fg="white", relief="flat", command=self.export_selected).pack(side="right", padx=5)
        
        self.tree_frame = tk.Frame(self, bg=BG_COLOR)
        self.tree_frame.pack(fill="both", expand=True, padx=5, pady=5)
        
        self.tree = ttk.Treeview(self.tree_frame, show="headings", selectmode="browse")
        vsb = ttk.Scrollbar(self.tree_frame, orient="vertical", command=self.tree.yview)
        hsb = ttk.Scrollbar(self.tree_frame, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        
        self.tree.grid(column=0, row=0, sticky='nsew')
        vsb.grid(column=1, row=0, sticky='ns')
        hsb.grid(column=0, row=1, sticky='ew')
        
        self.tree.bind("<Double-1>", self.on_double_click)
        self.tree_frame.grid_columnconfigure(0, weight=1)
        self.tree_frame.grid_rowconfigure(0, weight=1)

    def refresh_data(self, event=None):
        if not self.app.active_db: return
        table = self.table_var.get()
        query_filter = self.search_entry.get().strip()
        self.tree.delete(*self.tree.get_children())
        
        db_path = os.path.join(self.app.librarian.storage_dir, self.app.active_db)
        if not os.path.exists(db_path): return

        try:
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            sql = f"SELECT * FROM {table}"
            params = []
            
            if query_filter:
                cols = self._get_columns(cursor, table)
                text_cols = [c for c in cols if 'id' in c or 'path' in c or 'content' in c or 'label' in c]
                if text_cols:
                    conditions = " OR ".join([f"{col} LIKE ?" for col in text_cols])
                    sql += f" WHERE {conditions}"
                    params = [f"%{query_filter}%" for _ in text_cols]

            sql += " LIMIT 100"
            cursor.execute(sql, params)
            rows = cursor.fetchall()
            
            if rows:
                col_names = rows[0].keys()
                self.tree['columns'] = col_names
                for col in col_names:
                    self.tree.heading(col, text=col)
                    self.tree.column(col, width=100)
                
                for row in rows:
                    values = [str(val)[:50] + "..." if len(str(val)) > 50 else val for val in row]
                    self.tree.insert("", "end", values=values, tags=(str(row[0]),)) # Tag with ID if possible
            conn.close()
        except Exception as e:
            print(f"Inspector Error: {e}")

    def _get_columns(self, cursor, table):
        cursor.execute(f"PRAGMA table_info({table})")
        return [row[1] for row in cursor.fetchall()]

    def on_double_click(self, event):
        item = self.tree.identify_row(event.y)
        if not item: return
        values = self.tree.item(item, 'values')
        if not values: return
        
        # Simple Viewer
        top = tk.Toplevel(self)
        top.geometry("600x400")
        txt = scrolledtext.ScrolledText(top, bg=EDITOR_BG, fg=TEXT_COLOR)
        txt.pack(fill="both", expand=True)
        txt.insert("1.0", f"{values}")

    def export_selected(self):
        # Implementation similar to previous, kept brief for this file
        pass

class EditorView(tk.Frame):
    def __init__(self, parent, app):
        super().__init__(parent, bg=BG_COLOR)
        self.app = app
        self.current_file = None
        
        self.paned = ttk.PanedWindow(self, orient="horizontal")
        self.paned.pack(fill="both", expand=True)

        left_frame = tk.Frame(self.paned, bg=SIDEBAR_COLOR)
        self.paned.add(left_frame, weight=1)
        tk.Label(left_frame, text="EXPLORER", bg=SIDEBAR_COLOR, fg="#888", font=("Arial", 8, "bold"), anchor="w", padx=5).pack(fill="x", pady=(5,0))

        self.file_tree = ttk.Treeview(left_frame, selectmode="browse", show="tree")
        self.file_tree.pack(fill="both", expand=True, padx=5, pady=5)
        self.file_tree.bind("<<TreeviewSelect>>", self.on_file_select)
        
        center_frame = tk.Frame(self.paned, bg=EDITOR_BG)
        self.paned.add(center_frame, weight=4)
        
        toolbar = tk.Frame(center_frame, bg=EDITOR_BG)
        toolbar.pack(fill="x", pady=5, padx=10)
        self.lbl_current_file = tk.Label(toolbar, text="No file selected", bg=EDITOR_BG, fg=ACCENT_COLOR, font=("Consolas", 10, "bold"))
        self.lbl_current_file.pack(side="left")
        self.btn_save = tk.Button(toolbar, text="SAVE CHANGES", bg="#2d2d44", fg="white", relief="flat", state="disabled", command=self.save_changes)
        self.btn_save.pack(side="right")

        self.editor = scrolledtext.ScrolledText(center_frame, bg=EDITOR_BG, fg=TEXT_COLOR, font=("Consolas", 11), insertbackground="white", undo=True)
        self.editor.pack(fill="both", expand=True, padx=10, pady=(0, 10))

        right_frame = tk.Frame(self.paned, bg=SIDEBAR_COLOR)
        self.paned.add(right_frame, weight=2)
        
        tk.Label(right_frame, text="NEURAL SEARCH", bg=SIDEBAR_COLOR, fg="#888", font=("Arial", 8, "bold"), anchor="w", padx=5).pack(fill="x", pady=(5,0))
        
        search_box = tk.Frame(right_frame, bg=SIDEBAR_COLOR, pady=5, padx=5)
        search_box.pack(fill="x")
        
        self.search_var = tk.StringVar()
        self.entry_search = tk.Entry(search_box, textvariable=self.search_var, bg="#2d2d44", fg="white", insertbackground="white")
        self.entry_search.pack(side="left", fill="x", expand=True)
        self.entry_search.bind("<Return>", self.perform_search)
        
        tk.Button(search_box, text="GO", bg=ACCENT_COLOR, fg="white", relief="flat", width=3, command=self.perform_search).pack(side="right", padx=(5,0))
        
        self.results_tree = ttk.Treeview(right_frame, selectmode="browse", show="tree")
        self.results_tree.pack(fill="both", expand=True, padx=5, pady=5)
        self.results_tree.bind("<<TreeviewSelect>>", self.on_result_select)

    def refresh_file_list(self):
        for item in self.file_tree.get_children(): self.file_tree.delete(item)
        if not self.app.active_db: return
        try:
            files = self.app.librarian.list_files_in_kb(self.app.active_db)
            for path in files:
                self.file_tree.insert("", "end", iid=path, text=path)
        except Exception as e:
            print(f"Error listing files: {e}")

    def on_file_select(self, event):
        selection = self.file_tree.selection()
        if not selection: return
        file_path = selection[0]
        content = self.app.librarian.get_file_content(self.app.active_db, file_path)
        if content is None: return
        self.current_file = file_path
        self.lbl_current_file.config(text=f"EDITING: {file_path}")
        self.editor.delete(1.0, tk.END)
        self.editor.insert(tk.END, content)
        self.btn_save.config(state="normal", bg=ACCENT_COLOR)

    def save_changes(self):
        if not self.app.active_db or not self.current_file: return
        new_content = self.editor.get(1.0, tk.END).strip()
        try:
            result = self.app.librarian.update_file(self.app.active_db, self.current_file, new_content, author="user")
            messagebox.showinfo("Success", f"File saved.\nDiff Size: {result.get('diff_size')} bytes")
        except Exception as e:
            messagebox.showerror("Save Error", str(e))

    def perform_search(self, event=None):
        query = self.search_var.get().strip()
        if not query or not self.app.active_db: return
        self.results_tree.delete(*self.results_tree.get_children())
        db_path = os.path.join(self.app.librarian.storage_dir, self.app.active_db)
        results = self.app.search_engine.search(db_path, query, limit=15)
        for i, res in enumerate(results):
            score_pct = int(res['score'] * 100) if res['score'] <= 1 else int(res['score'])
            display = f"[{score_pct}] {os.path.basename(res['path'])}"
            self.results_tree.insert("", "end", text=display, values=(res['path'], res['snippet']))

    def on_result_select(self, event):
        selection = self.results_tree.selection()
        if not selection: return
        item = self.results_tree.item(selection[0])
        values = item['values']
        if not values: return
        file_path = values[0]
        content = self.app.librarian.get_file_content(self.app.active_db, file_path)
        if not content: return
        self.current_file = file_path
        self.lbl_current_file.config(text=f"EDITING: {file_path}")
        self.editor.delete(1.0, tk.END)
        self.editor.insert(tk.END, content)
        self.btn_save.config(state="normal", bg=ACCENT_COLOR)
        snippet_start = values[1].replace("...", "").strip()[:20] 
        start_idx = self.editor.search(snippet_start, "1.0", stopindex=tk.END)
        if start_idx:
            self.editor.see(start_idx)
            self.editor.tag_add("highlight", start_idx, f"{start_idx} lineend")
            self.editor.tag_config("highlight", background="#333344", foreground="#00FF00")

class NeoCortexApp(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("_NeoCORTEX v1.1 (Universal)")
        self.geometry("1200x800")
        self.configure(bg=BG_COLOR)

        # 1. Initialize Services
        self.librarian = LibrarianMS("./cortex_dbs")
        self.scanner = ScannerMS()
        self.search_engine = SearchEngineMS()
        self.exporter = ExporterMS()
        self.ingest_factory = lambda db_path: IngestEngine(db_path)
        
        self.active_db = None

        # 2. Setup Layout
        self._setup_ui()

    def _setup_ui(self):
        self.sidebar = Sidebar(self, self, self.librarian)
        self.sidebar.pack(side="left", fill="y")

        self.main_area = tk.Frame(self, bg=BG_COLOR)
        self.main_area.pack(side="right", fill="both", expand=True)
        
        style = ttk.Style()
        style.theme_use('clam')
        style.configure("TNotebook", background=BG_COLOR, borderwidth=0)
        style.configure("TNotebook.Tab", background="#2d2d44", foreground="white", padding=[10, 5])
        style.map("TNotebook.Tab", background=[("selected", ACCENT_COLOR)])

        self.notebook = ttk.Notebook(self.main_area)
        self.notebook.pack(fill="both", expand=True)

        self.ingest_view = IngestView(self.notebook, self, self.scanner, self.ingest_factory)
        self.notebook.add(self.ingest_view, text="  INGEST & WEAVE  ")

        self.editor_view = EditorView(self.notebook, self)
        self.notebook.add(self.editor_view, text="  KNOWLEDGE EDITOR  ")

        self.db_view = DatabaseView(self.notebook, self)
        self.notebook.add(self.db_view, text="  DATABASE INSPECTOR  ")

        self.graph_view = GraphView(self.notebook)
        self.notebook.add(self.graph_view, text="  NEURAL GRAPH  ")

    def set_active_db(self, db_name):
        self.active_db = db_name
        self.title(f"_NeoCORTEX v1.1 - Connected to [{db_name}]")
        self.editor_view.refresh_file_list()
        self.db_view.refresh_data()
        self.ingest_view.refresh_db_combo() # Sync Ingest View
        
        db_path = os.path.join(self.librarian.storage_dir, db_name)
        self.after(100, lambda: self.graph_view.load_from_db(db_path))

# ==============================================================================
#  HYBRID ENTRY POINT
# ==============================================================================

# --- CORE LOGIC (Importable) ---
def core_logic():
    # Placeholder for headless functionality or library access
    pass

# --- GUI MODE (Default / Showcase) ---
def run_gui():
    app = NeoCortexApp()
    app.mainloop()

# --- CLI MODE (Utility) ---
def run_cli():
    parser = argparse.ArgumentParser(description="_NeoCORTEX Neural Factory CLI")
    parser.add_argument("--ingest", help="Path to a folder or file to ingest headlessly", type=str)
    parser.add_argument("--db", help="Target database name for headless ingest", type=str)
    args = parser.parse_args()

    if args.ingest:
        print(f"[CLI] Headless ingestion initiated for: {args.ingest}")
        print("[CLI] NOTE: Headless mode is a work-in-progress stub.")
        print("[CLI] Please launch without arguments to use the GUI Factory.")
    else:
        parser.print_help()

def main():
    if len(sys.argv) > 1:
        run_cli()
    else:
        run_gui()

if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: src\_micro_services\_ArchitectMS\gnn_brain.py
--------------------------------------------------------------------------------
import sqlite3
import torch
import json
import struct
from torch_geometric.data import Data
from torch_geometric.nn import SAGEConv
import torch.nn.functional as F

class CodeGraphBrain:
    def __init__(self, db_path):
        self.db_path = db_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def load_graph_from_neocortex(self):
        """
        Bridges _NeoCORTEX SQLite tables to a PyTorch Geometric Graph.
        """
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # 1. Load Nodes & Features (The Embeddings you already have!)
        # We average chunk embeddings to get a single 'File Embedding'
        print("Loading embeddings...")
        sql = """
            SELECT f.path, c.embedding 
            FROM files f 
            JOIN chunks c ON f.id = c.file_id
        """
        cursor.execute(sql)
        
        node_map = {} # path -> index
        x_list = []   # features
        
        raw_data = cursor.fetchall()
        
        # Aggregate embeddings per file (Simple mean)
        temp_features = {}
        for path, emb_blob in raw_data:
            if not emb_blob: continue
            # Deserialize the BLOB from sqlite-vec format or JSON
            try:
                # Assuming JSON based on ingest_engine.py line 125
                vec = json.loads(emb_blob) 
                if path not in temp_features: temp_features[path] = []
                temp_features[path].append(vec)
            except: pass

        for i, (path, vecs) in enumerate(temp_features.items()):
            node_map[path] = i
            # Average the chunk vectors to get a File Vector
            avg_vec = torch.tensor(vecs).mean(dim=0)
            x_list.append(avg_vec)

        # 2. Load Edges (Your SynapseWeaver data)
        print("Loading edges...")
        cursor.execute("SELECT source, target FROM graph_edges")
        edge_list = []
        for src, tgt in cursor.fetchall():
            if src in node_map and tgt in node_map:
                edge_list.append([node_map[src], node_map[tgt]])
                # Add reverse edge for undirected graph
                edge_list.append([node_map[tgt], node_map[src]])

        conn.close()

        # 3. Construct PyG Data
        x = torch.stack(x_list) # Node Features [Num_Nodes, Hidden_Dim]
        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
        
        data = Data(x=x, edge_index=edge_index)
        return data.to(self.device), node_map

    def train_architect(self, data):
        """Train a tiny GNN to understand code structure"""
        model = GraphSAGE(data.num_features, 64, data.num_features).to(self.device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        
        print("Training Architect Brain...")
        model.train()
        for epoch in range(100): # Fast training loop
            optimizer.zero_grad()
            z = model(data.x, data.edge_index)
            
            # Self-Supervised: Try to reconstruct the graph edges
            # (Predict existing links)
            out = (z[data.edge_index[0]] * z[data.edge_index[1]]).sum(dim=-1)
            loss = F.binary_cross_entropy_with_logits(out, torch.ones_like(out))
            
            loss.backward()
            optimizer.step()
            
            if epoch % 20 == 0:
                print(f"Epoch {epoch}: Loss {loss.item():.4f}")
        
        return model

# --- The Model ---
class GraphSAGE(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)
    
    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        return x # Returns new, "context-aware" embeddings

if __name__ == "__main__":
    # Test run
    brain = CodeGraphBrain("./cortex_dbs/_NeoCORTEX_FirstIngestion.db")
    graph, mappings = brain.load_graph_from_neocortex()
    model = brain.train_architect(graph)
    print("Brain Trained!")
--------------------------------------------------------------------------------
FILE: src\_micro_services\_ExporterMS\exporter.py
--------------------------------------------------------------------------------
import os
import sqlite3
import logging
from pathlib import Path
from typing import List, Tuple, Optional

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("Exporter")
# ==============================================================================

class ExporterMS:
    """
    The Reconstructor: Reads the latest file snapshots from a Cortex Knowledge Base
    and rebuilds the directory structure on the local file system.
    
    UPGRADE: Uses 'vfs_path' for precise reconstruction of mixed sources (Web/File).
    """

    def export_knowledge_base(self, db_path: str, output_dir: str) -> Tuple[int, List[str]]:
        """
        Exports all files from the KB to the target directory.
        :return: (count_of_files_exported, list_of_errors)
        """
        db = Path(db_path).resolve()
        out_root = Path(output_dir).resolve()
        
        if not db.exists():
            raise FileNotFoundError(f"Database not found: {db}")

        # Create output directory if it doesn't exist
        out_root.mkdir(parents=True, exist_ok=True)

        log.info(f"Starting Export: {db.name} -> {out_root}")
        
        exported_count = 0
        errors = []

        try:
            conn = sqlite3.connect(db)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # 1. Fetch all files
            # We use vfs_path (Virtual File System) as the truth for where it goes.
            cursor.execute("SELECT vfs_path, content, origin_type FROM files")
            rows = cursor.fetchall()

            for row in rows:
                vfs_path = row['vfs_path']
                content = row['content']
                origin = row['origin_type']

                if not vfs_path: continue

                # 2. Construct Destination Path
                # Security Check: Prevent path traversal (e.g. ../../etc/passwd)
                safe_rel = self._sanitize_path(vfs_path)
                
                dest_path = out_root / safe_rel

                try:
                    # 3. Write File
                    # Ensure parent dirs exist
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    # Write content (UTF-8)
                    with open(dest_path, 'w', encoding='utf-8', newline='') as f:
                        if content:
                            f.write(content)
                    
                    exported_count += 1
                    
                except Exception as e:
                    err_msg = f"Failed to write {vfs_path}: {e}"
                    log.error(err_msg)
                    errors.append(err_msg)

            conn.close()
            log.info(f"Export Complete. {exported_count} files written. {len(errors)} errors.")
            return exported_count, errors

        except Exception as e:
            log.critical(f"Critical Export Failure: {e}")
            raise e

    def _sanitize_path(self, path_str: str) -> Path:
        """
        Ensures the path is relative and does not escape the root.
        """
        # Strip leading slashes/dots/drive letters
        clean = path_str.lstrip("/\\.").replace("..", "").replace(":", "")
        return Path(clean)

if __name__ == "__main__":
    pass
--------------------------------------------------------------------------------
FILE: src\_micro_services\_GraphEngineMS\graph_engine.py
--------------------------------------------------------------------------------
import pygame
import math
import random

# Initialize font module globally once
pygame.font.init()

class GraphRenderer:
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        self.width = width
        self.height = height
        self.bg_color = bg_color
        
        self.surface = pygame.Surface((width, height))
        
        # Camera
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction
        self.dragged_node_idx = None
        self.hovered_node_idx = None
        
        # Physics State
        self.settled = False

    def resize(self, width, height):
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    def set_data(self, nodes, links):
        self.nodes = nodes
        self.links = links
        self.settled = False # Wake up physics on new data
        
        # 1. Build an ID map so we can find parents
        node_map = {node['id']: node for node in self.nodes}

        for n in self.nodes:
            # GNN Injection: Use pre-calculated layout if available
            if 'gnn_x' in n and 'gnn_y' in n:
                n['x'] = n['gnn_x'] * self.width
                n['y'] = n['gnn_y'] * self.height

            elif 'x' not in n:
                # SMART SPAWN: If I am a satellite, spawn near my planet
                parent_id = n.get('meta', {}).get('parent')
                if parent_id and parent_id in node_map and 'x' in node_map[parent_id]:
                    p = node_map[parent_id]
                    angle = random.random() * 6.28
                    dist = 30
                    n['x'] = p['x'] + math.cos(angle) * dist
                    n['y'] = p['y'] + math.sin(angle) * dist
                else:
                    # Random spawn for Files
                    n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                    n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Semantic Coloring
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # Blue
                n['_radius'] = 6
            elif n.get('type') == 'web':
                n['_color'] = (204, 0, 122) # Purple/Pink
                n['_radius'] = 7
            elif n.get('type') == 'chunk':
                n['_color'] = (100, 200, 100) # Satellite Green
                n['_radius'] = 3
            else:
                n['_color'] = (160, 32, 240) # Default
                n['_radius'] = 6

    # --- INPUT HANDLING ---
    
    def screen_to_world(self, sx, sy):
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def get_node_at(self, sx, sy):
        wx, wy = self.screen_to_world(sx, sy)
        for n in self.nodes:
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                return n
        return None

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                self.dragged_node_idx = i
                self.settled = False # Wake up physics
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
            self.settled = False
        else:
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            return prev_hover != self.hovered_node_idx

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    # --- PHYSICS (Damped) ---

    def step_physics(self):
        if not self.nodes or self.settled: return

        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.85 # Increased damping to settle faster
        
        cx, cy = self.width / 2, self.height / 2
        total_kinetic_energy = 0

        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue

            # LOD: Freeze satellites if zoomed out
            if self.zoom < 1.2 and a.get('type') == 'chunk':
                a['vx'] = 0
                a['vy'] = 0
                continue
            
            fx, fy = 0, 0
            
            # 1. Gravity (Center pull)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # 2. Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Performance opt: Ignore far away nodes
                if dist_sq > 25000: continue 

                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 3. Attraction (Links)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 4. Apply & Measure Energy
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']
            total_kinetic_energy += (abs(n['vx']) + abs(n['vy']))

        # 5. Sleep Threshold
        if total_kinetic_energy < 0.5:
            self.settled = True

    # --- RENDERING ---

    def get_image_bytes(self):
        self.surface.fill(self.bg_color)
        
        cx, cy = self.width / 2, self.height / 2
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # Links
        for u, v in self.links:
            if self.zoom < 1.2:
                if self.nodes[u].get('type') == 'chunk' or self.nodes[v].get('type') == 'chunk':
                    continue

            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # Nodes
        for i, n in enumerate(self.nodes):
            # LOD: Hide chunks if zoomed out
            if self.zoom < 1.2 and n.get('type') == 'chunk':
                continue

            sx, sy = to_screen(n['x'], n['y'])
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20: continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')


--------------------------------------------------------------------------------
FILE: src\_micro_services\_GraphEngineMS\graph_view.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import json
import os
from .graph_engine import GraphRenderer

class GraphView(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.pack(fill="both", expand=True)
        
        # UI Container
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None 
        
        # Input State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        self.is_panning = False

        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<Double-Button-1>', self.on_double_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Start the Heartbeat
        self.animate()

    def load_from_db(self, db_path):
        """
        Loads graph data from SQLite.
        Does NOT block the UI. The physics engine will settle the nodes frame-by-frame.
        """
        if not os.path.exists(db_path): return
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # [cite_start]Fetch Nodes [cite: 198]
            db_nodes = cursor.execute("SELECT id, type, label, data_json FROM graph_nodes").fetchall()
            
            # [cite_start]Fetch Edges [cite: 198]
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
            
            conn.close()
        except Exception as e:
            print(f"Graph Load Error: {e}")
            return

        # Format for Engine
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label, raw_json = row
            meta = {}
            try:
                if raw_json: meta = json.loads(raw_json)
            except: pass
            
            id_to_index[node_id] = idx
            formatted_nodes.append({'id': node_id, 'type': n_type, 'label': label, 'meta': meta})

        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                formatted_links.append((id_to_index[src], id_to_index[tgt]))

        # Inject Data - The Physics Engine handles the "Explosion" logic internally
        self.engine.set_data(formatted_nodes, formatted_links)

    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_double_click(self, event):
        # Zoom in on the node we clicked
        hit_node = self.engine.get_node_at(event.x, event.y)
        if hit_node:
            # Center camera on node and zoom in
            self.engine.cam_x = hit_node['x']
            self.engine.cam_y = hit_node['y']
            self.engine.zoom = 2.0
            self.engine.settled = False

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        if hit:
            self.is_dragging_node = True
        else:
            self.is_panning = True

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False
        self.is_panning = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        elif self.is_panning:
            # Camera Pan
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)
        self.engine.settled = False # Wake up physics on zoom

    def on_windows_scroll(self, event):
        if event.delta > 0: self.on_zoom(1.1)
        else: self.on_zoom(0.9)

    def animate(self):
        """
        The Heartbeat Loop. 
        Runs at ~30 FPS. Handles Physics + Rendering.
        """
        # 1. Step Physics (Micro-calculations)
        self.engine.step_physics()
        
        # 2. Render to Buffer
        raw_data = self.engine.get_image_bytes()
        
        # 3. Blit to Screen
        if raw_data:
            img = Image.frombytes('RGB', (self.engine.width, self.engine.height), raw_data)
            self.photo = ImageTk.PhotoImage(img)
            self.canvas_lbl.configure(image=self.photo)
        
        # 4. Loop
        self.after(30, self.animate)
--------------------------------------------------------------------------------
FILE: src\_micro_services\_IngestEngineMS\ingest_engine - Copy.py
--------------------------------------------------------------------------------
import os
import time
import re
import sqlite3
import requests
import json
from typing import List, Generator, Dict, Any, Optional
from dataclasses import dataclass
from .semantic_chunker import SemanticChunker

# Optional Libraries for Enhanced Ingestion
try:
    import pypdf
except ImportError:
    pypdf = None

try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass
class IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

class SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    """
    def __init__(self):
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        # Only parse code files for dependencies
        if not file_path.endswith(('.py', '.js', '.ts', '.tsx', '.jsx')):
            return []

        lines = content.split('\n')
        for line in lines:
            match = None
            if file_path.endswith('.py'):
                match = self.py_pattern.match(line)
            else:
                match = self.js_pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        return dependencies

class IngestEngine:
    """
    The Heavy Lifter: Reads Files (Code, PDF, MD) & Websites.
    """
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.stop_signal = False
        self.weaver = SynapseWeaver()
        self.chunker = SemanticChunker()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    def process_files(self, file_paths: List[str], embed_model: str = "none", summary_model: str = "none") -> Generator[IngestStatus, None, None]:
        total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # --- STEP 0: STAMP MANIFEST ---
        # Note: manifest table is also created during DB creation, but keep this for safety.
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")

        # Record the models used for this ingest in a single structured blob.
        ingest_models_obj = {"embed_model": embed_model, "summary_model": summary_model}
        cursor.execute(
            "INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)",
            ("ingest_models", json.dumps(ingest_models_obj))
        )

        # Standard run metadata
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("last_ingest_time", str(time.time())))
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("file_count", str(total)))

        # Initialize provenance if not present (do not overwrite if DB creator/UI already set it)
        cursor.execute("INSERT OR IGNORE INTO manifest (key, value) VALUES (?, ?)", ("source_provenance", "{}"))

        # Cartridge interpretation hints (do not overwrite if UI already set these)
        cursor.execute("INSERT OR IGNORE INTO manifest (key, value) VALUES (?, ?)", ("artifact_type", "unknown"))

        artifact_profile_obj = {
            "embed_model": embed_model,
            "summary_model": summary_model,
            "vfs_strategy": "relpath_from_scan_root_v1",
            "supports_graph_weaving": True,
            "notes": "artifact_type/profile may be overridden by UI in future"
        }
        cursor.execute("INSERT OR IGNORE INTO manifest (key, value) VALUES (?, ?)", ("artifact_profile", json.dumps(artifact_profile_obj)))

        conn.commit()

        node_registry = {}
        file_contents = {}

        # Compute a stable scan_root for filesystem inputs so VFS paths can be portable.
        fs_paths = [p for p in file_paths if not (p.startswith("http://") or p.startswith("https://"))]
        scan_root = ""
        if fs_paths:
            try:
                scan_root = os.path.commonpath(fs_paths)
            except Exception:
                scan_root = ""

        # Persist scan_root (do not overwrite if UI already set it)
        try:
            cursor.execute("INSERT OR IGNORE INTO manifest (key, value) VALUES (?, ?)", ("scan_root", scan_root))
            cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("vfs_strategy", "relpath_from_scan_root_v1"))
            conn.commit()
        except Exception:
            pass

        # --- PHASE 1: INGESTION ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)
            if not filename and file_path.startswith("http"):
                filename = file_path.replace("https://", "").replace("http://", "").replace("/", "_")[:50]

            content = ""
            origin_type = 'filesystem'

            # 1. READ CONTENT (Universal Reader Logic)
            try:
                if file_path.startswith("http://") or file_path.startswith("https://"):
                    # --- WEB ---
                    origin_type = 'web'
                    yield IngestStatus(filename, (idx/total)*100, idx, total, f"Fetching URL: {file_path}...")
                    
                    resp = requests.get(file_path, timeout=10)
                    resp.raise_for_status()
                    
                    if BeautifulSoup:
                        soup = BeautifulSoup(resp.content, 'html.parser')
                        # Remove script/style
                        for script in soup(["script", "style"]): script.extract()
                        content = soup.get_text()
                    else:
                        # Fallback regex strip
                        content = re.sub('<[^<]+?>', '', resp.text)
                    
                    # Clean up whitespace
                    lines = (line.strip() for line in content.splitlines())
                    content = '\n'.join(chunk for chunk in lines if chunk)

                elif file_path.lower().endswith(".pdf"):
                    # --- PDF ---
                    if not pypdf:
                        yield IngestStatus(filename, (idx/total)*100, idx, total, "Skipping PDF (pypdf not installed)")
                        continue
                    
                    yield IngestStatus(filename, (idx/total)*100, idx, total, "Extracting PDF text...")
                    with open(file_path, 'rb') as f:
                        reader = pypdf.PdfReader(f)
                        text_pages = []
                        for page in reader.pages:
                            text_pages.append(page.extract_text())
                        content = "\n\n".join(text_pages)

                else:
                    # --- TEXT / CODE / MD ---
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()

            except Exception as e:
                yield IngestStatus(filename, (idx/total)*100, idx, total, f"Read Error: {e}")
                continue

            # Derive portable VFS path + stable node_id now that origin_type is known
            if origin_type == 'web':
                vfs_path = filename
                file_key = file_path  # URLs are stable enough as identity
                node_id = file_path
                node_label = filename
            else:
                if scan_root:
                    rel = os.path.relpath(file_path, scan_root)
                    rel = rel.replace('\\', '/')
                    vfs_path = rel
                else:
                    vfs_path = filename

                file_key = vfs_path
                node_id = vfs_path
                node_label = filename

            # Cache content for weaving keyed by stable node_id
            file_contents[node_id] = content

            # 2. Summarize
            summary_text = ""
            if summary_model != "none":
                yield IngestStatus(filename, (idx/total)*100, idx, total, f"Summarizing with {summary_model}...")
                summary_text = self._generate_summary(summary_model, content[:3000])

            # 3. Track File
            try:
                meta_obj = {"summary": summary_text, "tags": []}
                meta_json = json.dumps(meta_obj)

                cursor.execute("""
                    INSERT OR REPLACE INTO files 
                    (path, content, last_updated, origin_type, origin_path, vfs_path, metadata) 
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (file_key, content, time.time(), origin_type, file_path, vfs_path, meta_json))
                
                file_id = cursor.lastrowid
            except sqlite3.Error as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"DB Error: {e}")
                continue

            # 4. Graph Node
            node_type = 'web' if origin_type == 'web' else 'file'
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (node_id, node_type, node_label, json.dumps({"origin_path": file_path, "vfs_path": vfs_path})))

            node_registry[node_id] = node_id

            # 5. Chunking & Embedding
            # Note: SemanticChunker handles generic text via _chunk_generic
            chunks = self.chunker.chunk_file(content, filename)

            for i, chunk_obj in enumerate(chunks):
                chunk_text = chunk_obj.content
                if self.stop_signal: break
                
                embedding = None
                if embed_model != "none":
                    embedding = self._get_embedding(embed_model, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob, chunk_obj.name, chunk_obj.type, chunk_obj.start_line, chunk_obj.end_line))

                # --- SATELLITE NODE ---
                chunk_node_id = f"{node_id}::{chunk_obj.name}"
                cursor.execute("""
                    INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                    VALUES (?, ?, ?, ?)
                """, (chunk_node_id, 'chunk', chunk_obj.name, json.dumps({"parent": node_id})))

                cursor.execute("""
                    INSERT OR IGNORE INTO graph_edges (source, target, weight)
                    VALUES (?, ?, 2.0)
                """, (chunk_node_id, node_id))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": f"{filename} [{chunk_obj.name}]",
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#E02080" if origin_type == 'web' else "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            conn.commit()

        # --- PHASE 2: WEAVING ---
        # (Only relevant for code/text files, websites typically don't have python 'imports')
        if any(f.endswith('.py') or f.endswith('.js') for f in file_paths):
            yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
            
            for source_id, content in file_contents.items():
                if self.stop_signal: break
                deps = self.weaver.extract_dependencies(content, source_id)
                for dep in deps:
                    target_id = None
                    for potential_match in node_registry.keys():
                        base = os.path.basename(potential_match)
                        base_no_ext = os.path.splitext(base)[0]
                        if base == dep or base_no_ext == dep or potential_match.startswith(dep + '.') or potential_match == dep:
                            target_id = potential_match
                            break
                    if target_id and target_id != source_id:
                        try:
                            cursor.execute("INSERT OR IGNORE INTO graph_edges (source, target, weight) VALUES (?, ?, 1.0)", (source_id, target_id))
                        except: pass

        # --- PHASE 3: WRITE CARTRIDGE BOOT FILES (Self-Describing DB for downstream RAG) ---
        try:
            now_ts = time.time()

            # Build a lightweight inventory for INDEX.json
            ext_counts = {}
            for fp in file_paths:
                ext = os.path.splitext(fp)[1].lower() or "(none)"
                ext_counts[ext] = ext_counts.get(ext, 0) + 1

            index_obj = {
                "schema": "neocortex.cartridge.index.v1",
                "generated_at": now_ts,
                "file_count": total,
                "extensions": ext_counts,
                "scan_root": scan_root,
                "vfs_strategy": "relpath_from_scan_root_v1",
                "ingest": {
                    "embed_model": embed_model,
                    "summary_model": summary_model
                },
                "graph": {
                    "dependency_edges_created": None
                },
                "boot": {
                    "readme_vfs": "__cartridge__/README.md",
                    "index_vfs": "__cartridge__/INDEX.json"
                }
            }

            readme_text = (
                "# Neural Cartridge (NeoCORTEX)\n\n"
                "This SQLite database is a *self-describing knowledge cartridge* produced by _NeoCORTEX.\n"
                "It contains ingested source material (files/pages), semantic chunks, vector embeddings, and optional graph wiring.\n\n"
                "## Boot Protocol\n"
                "- Read the manifest table first: `SELECT key, value FROM manifest`\n"
                "- Then read this file and `__cartridge__/INDEX.json` from the `files` table via `vfs_path`.\n\n"
                "## Path Contract\n"
                "- `origin_path` is provenance (absolute filesystem path or URL)\n"
                "- `vfs_path` is the cartridge-internal portable path\n"
                f"- scan_root: {scan_root}\n"
                "- vfs_strategy: relpath_from_scan_root_v1\n\n"
                "## Tables (High Level)\n"
                "- files: full source content + provenance\n"
                "- chunks: semantic segments (may include embeddings)\n"
                "- graph_nodes / graph_edges: optional structural wiring\n\n"
                "## Ingest Metadata\n"
                f"- embed_model: {embed_model}\n"
                f"- summary_model: {summary_model}\n"
                f"- file_count: {total}\n"
            )

            boot_meta_readme = json.dumps({"summary": "Cartridge boot README", "tags": ["__cartridge__", "boot", "readme"]})
            boot_meta_index = json.dumps({"summary": "Cartridge inventory index", "tags": ["__cartridge__", "boot", "index"]})

            cursor.execute(
                """
                INSERT OR REPLACE INTO files
                (path, content, last_updated, origin_type, origin_path, vfs_path, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    "__cartridge__/README.md",
                    readme_text,
                    now_ts,
                    "system",
                    "__cartridge__",
                    "__cartridge__/README.md",
                    boot_meta_readme
                )
            )

            cursor.execute(
                """
                INSERT OR REPLACE INTO files
                (path, content, last_updated, origin_type, origin_path, vfs_path, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    "__cartridge__/INDEX.json",
                    json.dumps(index_obj, indent=2),
                    now_ts,
                    "system",
                    "__cartridge__",
                    "__cartridge__/INDEX.json",
                    boot_meta_index
                )
            )

            cursor.execute(
                "INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)",
                ("boot_files", json.dumps(["__cartridge__/README.md", "__cartridge__/INDEX.json"]))
            )

            conn.commit()
        except Exception:
            # Never fail the ingest just because boot files couldn't be written
            pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. {total} items processed. Boot files written to __cartridge__/."
        )

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(f"{OLLAMA_API_URL}/embeddings", json={"model": model, "prompt": text}, timeout=30)
            if res.status_code == 200: return res.json().get("embedding")
        except: return None
        return None

    def _generate_summary(self, model: str, text: str) -> str:
        try:
            prompt = f"Summarize this text in one concise sentence:\n\n{text}"
            res = requests.post(f"{OLLAMA_API_URL}/generate", json={"model": model, "prompt": prompt, "stream": False}, timeout=10)
            if res.status_code == 200: return res.json().get("response", "").strip()
        except: return ""

--------------------------------------------------------------------------------
FILE: src\_micro_services\_IngestEngineMS\ingest_engine.py
--------------------------------------------------------------------------------
import os
import time
import re
import sqlite3
import requests
import json
from typing import List, Generator, Dict, Any, Optional
from dataclasses import dataclass
from .semantic_chunker import SemanticChunker

# Optional Libraries for Enhanced Ingestion
try:
    import pypdf
except ImportError:
    pypdf = None

try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass
class IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

class SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    """
    def __init__(self):
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        # Only parse code files for dependencies
        if not file_path.endswith(('.py', '.js', '.ts', '.tsx', '.jsx')):
            return []

        lines = content.split('\n')
        for line in lines:
            match = None
            if file_path.endswith('.py'):
                match = self.py_pattern.match(line)
            else:
                match = self.js_pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        return dependencies

class IngestEngine:
    """
    The Heavy Lifter: Reads Files (Code, PDF, MD) & Websites.
    """
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.stop_signal = False
        self.weaver = SynapseWeaver()
        self.chunker = SemanticChunker()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    def process_files(self, file_paths: List[str], embed_model: str = "none", summary_model: str = "none") -> Generator[IngestStatus, None, None]:
        total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # --- STEP 0: STAMP MANIFEST ---
        # Note: manifest table is also created during DB creation, but keep this for safety.
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")

        # Record the models used for this ingest in a single structured blob.
        ingest_models_obj = {"embed_model": embed_model, "summary_model": summary_model}
        cursor.execute(
            "INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)",
            ("ingest_models", json.dumps(ingest_models_obj))
        )

        # Standard run metadata
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("last_ingest_time", str(time.time())))
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("file_count", str(total)))

        # Initialize provenance if not present (do not overwrite if DB creator/UI already set it)
        cursor.execute("INSERT OR IGNORE INTO manifest (key, value) VALUES (?, ?)", ("source_provenance", "{}"))

        # Cartridge interpretation hints (do not overwrite if UI already set these)
        cursor.execute("INSERT OR IGNORE INTO manifest (key, value) VALUES (?, ?)", ("artifact_type", "unknown"))

        artifact_profile_obj = {
            "embed_model": embed_model,
            "summary_model": summary_model,
            "vfs_strategy": "relpath_from_scan_root_v1",
            "supports_graph_weaving": True,
            "notes": "artifact_type/profile may be overridden by UI in future"
        }
        cursor.execute("INSERT OR IGNORE INTO manifest (key, value) VALUES (?, ?)", ("artifact_profile", json.dumps(artifact_profile_obj)))

        conn.commit()

        node_registry = {}
        file_contents = {}

        # Compute a stable scan_root for filesystem inputs so VFS paths can be portable.
        fs_paths = [p for p in file_paths if not (p.startswith("http://") or p.startswith("https://"))]
        scan_root = ""
        if fs_paths:
            try:
                scan_root = os.path.commonpath(fs_paths)
            except Exception:
                scan_root = ""

        # Persist scan_root (do not overwrite if UI already set it)
        skeleton_tree = {}
        try:
            cursor.execute("INSERT OR IGNORE INTO manifest (key, value) VALUES (?, ?)", ("scan_root", scan_root))
            cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("vfs_strategy", "relpath_from_scan_root_v1"))
            
            # --- FEATURE: SKELETON KEY (Directory Map) ---
            # Pre-calculate the folder structure so agents can navigate without O(N) scans
            skeleton_tree = self._build_skeleton_tree(file_paths, scan_root)
            cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", 
                          ("structural_skeleton", json.dumps(skeleton_tree)))

            conn.commit()
        except Exception:
            pass

        # --- PHASE 1: INGESTION ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)
            if not filename and file_path.startswith("http"):
                filename = file_path.replace("https://", "").replace("http://", "").replace("/", "_")[:50]

            content = ""
            origin_type = 'filesystem'

            # 1. READ CONTENT (Universal Reader Logic)
            try:
                if file_path.startswith("http://") or file_path.startswith("https://"):
                    # --- WEB ---
                    origin_type = 'web'
                    yield IngestStatus(filename, (idx/total)*100, idx, total, f"Fetching URL: {file_path}...")
                    
                    resp = requests.get(file_path, timeout=10)
                    resp.raise_for_status()
                    
                    if BeautifulSoup:
                        soup = BeautifulSoup(resp.content, 'html.parser')
                        # Remove script/style
                        for script in soup(["script", "style"]): script.extract()
                        content = soup.get_text()
                    else:
                        # Fallback regex strip
                        content = re.sub('<[^<]+?>', '', resp.text)
                    
                    # Clean up whitespace
                    lines = (line.strip() for line in content.splitlines())
                    content = '\n'.join(chunk for chunk in lines if chunk)

                elif file_path.lower().endswith(".pdf"):
                    # --- PDF ---
                    if not pypdf:
                        yield IngestStatus(filename, (idx/total)*100, idx, total, "Skipping PDF (pypdf not installed)")
                        continue
                    
                    yield IngestStatus(filename, (idx/total)*100, idx, total, "Extracting PDF text...")
                    with open(file_path, 'rb') as f:
                        reader = pypdf.PdfReader(f)
                        text_pages = []
                        for page in reader.pages:
                            text_pages.append(page.extract_text())
                        content = "\n\n".join(text_pages)

                else:
                    # --- TEXT / CODE / MD ---
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()

            except Exception as e:
                yield IngestStatus(filename, (idx/total)*100, idx, total, f"Read Error: {e}")
                continue

            # Derive portable VFS path + stable node_id now that origin_type is known
            if origin_type == 'web':
                vfs_path = filename
                file_key = file_path  # URLs are stable enough as identity
                node_id = file_path
                node_label = filename
            else:
                if scan_root:
                    rel = os.path.relpath(file_path, scan_root)
                    rel = rel.replace('\\', '/')
                    vfs_path = rel
                else:
                    vfs_path = filename

                file_key = vfs_path
                node_id = vfs_path
                node_label = filename

            # Cache content for weaving keyed by stable node_id
            file_contents[node_id] = content

            # 2. Summarize
            summary_text = ""
            if summary_model != "none":
                yield IngestStatus(filename, (idx/total)*100, idx, total, f"Summarizing with {summary_model}...")
                summary_text = self._generate_summary(summary_model, content[:3000])

            # 3. Track File
            try:
                meta_obj = {"summary": summary_text, "tags": []}
                meta_json = json.dumps(meta_obj)

                cursor.execute("""
                    INSERT OR REPLACE INTO files 
                    (path, content, last_updated, origin_type, origin_path, vfs_path, metadata) 
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (file_key, content, time.time(), origin_type, file_path, vfs_path, meta_json))
                
                file_id = cursor.lastrowid
            except sqlite3.Error as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"DB Error: {e}")
                continue

            # 4. Graph Node
            node_type = 'web' if origin_type == 'web' else 'file'
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (node_id, node_type, node_label, json.dumps({"origin_path": file_path, "vfs_path": vfs_path})))

            node_registry[node_id] = node_id

            # 5. Chunking & Embedding
            # Note: SemanticChunker handles generic text via _chunk_generic
            chunks = self.chunker.chunk_file(content, filename)

            for i, chunk_obj in enumerate(chunks):
                chunk_text = chunk_obj.content
                if self.stop_signal: break
                
                embedding = None
                if embed_model != "none":
                    embedding = self._get_embedding(embed_model, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob, chunk_obj.name, chunk_obj.type, chunk_obj.start_line, chunk_obj.end_line))

                # --- SATELLITE NODE ---
                chunk_node_id = f"{node_id}::{chunk_obj.name}"
                cursor.execute("""
                    INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                    VALUES (?, ?, ?, ?)
                """, (chunk_node_id, 'chunk', chunk_obj.name, json.dumps({"parent": node_id})))

                cursor.execute("""
                    INSERT OR IGNORE INTO graph_edges (source, target, weight)
                    VALUES (?, ?, 2.0)
                """, (chunk_node_id, node_id))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": f"{filename} [{chunk_obj.name}]",
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#E02080" if origin_type == 'web' else "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            conn.commit()

        # --- PHASE 2: WEAVING ---
        # (Only relevant for code/text files, websites typically don't have python 'imports')
        if any(f.endswith('.py') or f.endswith('.js') for f in file_paths):
            yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
            
            for source_id, content in file_contents.items():
                if self.stop_signal: break
                deps = self.weaver.extract_dependencies(content, source_id)
                for dep in deps:
                    target_id = None
                    for potential_match in node_registry.keys():
                        base = os.path.basename(potential_match)
                        base_no_ext = os.path.splitext(base)[0]
                        if base == dep or base_no_ext == dep or potential_match.startswith(dep + '.') or potential_match == dep:
                            target_id = potential_match
                            break
                    if target_id and target_id != source_id:
                        try:
                            cursor.execute("INSERT OR IGNORE INTO graph_edges (source, target, weight) VALUES (?, ?, 1.0)", (source_id, target_id))
                        except: pass

        # --- PHASE 3: WRITE CARTRIDGE BOOT FILES (Self-Describing DB for downstream RAG) ---
        try:
            now_ts = time.time()

            # Build a lightweight inventory for INDEX.json
            ext_counts = {}
            for fp in file_paths:
                ext = os.path.splitext(fp)[1].lower() or "(none)"
                ext_counts[ext] = ext_counts.get(ext, 0) + 1

            index_obj = {
                "schema": "neocortex.cartridge.index.v1",
                "generated_at": now_ts,
                "file_count": total,
                "structural_skeleton": skeleton_tree,
                "extensions": ext_counts,
                "scan_root": scan_root,
                "vfs_strategy": "relpath_from_scan_root_v1",
                "ingest": {
                    "embed_model": embed_model,
                    "summary_model": summary_model
                },
                "graph": {
                    "dependency_edges_created": None
                },
                "boot": {
                    "readme_vfs": "__cartridge__/README.md",
                    "index_vfs": "__cartridge__/INDEX.json"
                }
            }

            readme_text = (
                "# Neural Cartridge (NeoCORTEX)\n\n"
                "This SQLite database is a *self-describing knowledge cartridge* produced by _NeoCORTEX.\n"
                "It contains ingested source material (files/pages), semantic chunks, vector embeddings, and optional graph wiring.\n\n"
                "## Boot Protocol\n"
                "- Read the manifest table first: `SELECT key, value FROM manifest`\n"
                "- Then read this file and `__cartridge__/INDEX.json` from the `files` table via `vfs_path`.\n\n"
                "## Path Contract\n"
                "- `origin_path` is provenance (absolute filesystem path or URL)\n"
                "- `vfs_path` is the cartridge-internal portable path\n"
                f"- scan_root: {scan_root}\n"
                "- vfs_strategy: relpath_from_scan_root_v1\n\n"
                "## Tables (High Level)\n"
                "- files: full source content + provenance\n"
                "- chunks: semantic segments (may include embeddings)\n"
                "- graph_nodes / graph_edges: optional structural wiring\n\n"
                "## Ingest Metadata\n"
                f"- embed_model: {embed_model}\n"
                f"- summary_model: {summary_model}\n"
                f"- file_count: {total}\n"
            )

            boot_meta_readme = json.dumps({"summary": "Cartridge boot README", "tags": ["__cartridge__", "boot", "readme"]})
            boot_meta_index = json.dumps({"summary": "Cartridge inventory index", "tags": ["__cartridge__", "boot", "index"]})

            cursor.execute(
                """
                INSERT OR REPLACE INTO files
                (path, content, last_updated, origin_type, origin_path, vfs_path, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    "__cartridge__/README.md",
                    readme_text,
                    now_ts,
                    "system",
                    "__cartridge__",
                    "__cartridge__/README.md",
                    boot_meta_readme
                )
            )

            cursor.execute(
                """
                INSERT OR REPLACE INTO files
                (path, content, last_updated, origin_type, origin_path, vfs_path, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (
                    "__cartridge__/INDEX.json",
                    json.dumps(index_obj, indent=2),
                    now_ts,
                    "system",
                    "__cartridge__",
                    "__cartridge__/INDEX.json",
                    boot_meta_index
                )
            )

            cursor.execute(
                "INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)",
                ("boot_files", json.dumps(["__cartridge__/README.md", "__cartridge__/INDEX.json"]))
            )

            conn.commit()
        except Exception:
            # Never fail the ingest just because boot files couldn't be written
            pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. {total} items processed. Boot files written to __cartridge__/."
        )

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(f"{OLLAMA_API_URL}/embeddings", json={"model": model, "prompt": text}, timeout=30)
            if res.status_code == 200: return res.json().get("embedding")
        except: return None
        return None

    def _generate_summary(self, model: str, text: str) -> str:
        try:
            prompt = f"Summarize this text in one concise sentence:\n\n{text}"
            res = requests.post(f"{OLLAMA_API_URL}/generate", json={"model": model, "prompt": prompt, "stream": False}, timeout=10)
            if res.status_code == 200: return res.json().get("response", "").strip()
        except: return ""

    def _build_skeleton_tree(self, file_paths: List[str], scan_root: str) -> Dict[str, Any]:
        """
        Constructs a lightweight directory tree (The Skeleton Key).
        Allows agents to navigate the structure without reading all files.
        """
        tree = {"_files": []}
        for fp in file_paths:
            # Mirror VFS logic from Phase 1
            if fp.startswith("http"):
                filename = fp.replace("https://", "").replace("http://", "").replace("/", "_")[:50]
                # Web pages go to root _files list for now
                tree["_files"].append(filename)
                continue

            if scan_root:
                try:
                    rel = os.path.relpath(fp, scan_root).replace('\\', '/')
                    parts = rel.split('/')
                except:
                    parts = [os.path.basename(fp)]
            else:
                parts = [os.path.basename(fp)]

            current = tree
            for i, part in enumerate(parts):
                is_file = (i == len(parts) - 1)
                if is_file:
                    if "_files" not in current: current["_files"] = []
                    current["_files"].append(part)
                else:
                    if part not in current: current[part] = {}
                    current = current[part]
        return tree


--------------------------------------------------------------------------------
FILE: src\_micro_services\_IngestEngineMS\semantic_chunker.py
--------------------------------------------------------------------------------
import ast
import re
from dataclasses import dataclass
from typing import List, Dict, Optional

@dataclass
class CodeChunk:
    name: str          # e.g., "class AuthMS" or "def login"
    type: str          # "class", "function", "text"
    content: str       # The raw source code
    start_line: int
    end_line: int
    docstring: str = ""

class SemanticChunker:
    """
    Intelligent Code Splitter.
    Parses source code into logical units (Classes, Functions) rather than random windows.
    """
    
    def chunk_file(self, content: str, filename: str) -> List[CodeChunk]:
        if filename.endswith(".py"):
            return self._chunk_python(content)
        return self._chunk_generic(content)

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                end = node.end_lineno if hasattr(node, 'end_lineno') else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", type="function", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"class {node.name}", type="class", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))

            if not chunks:
                return self._chunk_generic(source)
                
        except SyntaxError:
            return self._chunk_generic(source)
            
        return chunks

    def _chunk_generic(self, text: str, window_size: int = 1500) -> List[CodeChunk]:
        chunks = []
        lines = text.splitlines(keepends=True)
        current_chunk = []
        current_size = 0
        chunk_idx = 1
        start_line = 1
        
        for i, line in enumerate(lines):
            current_chunk.append(line)
            current_size += len(line)
            
            if current_size >= window_size:
                chunks.append(CodeChunk(
                    name=f"Chunk {chunk_idx}", type="text_block",
                    content="".join(current_chunk), start_line=start_line, end_line=i + 1
                ))
                current_chunk = []
                current_size = 0
                chunk_idx += 1
                start_line = i + 2
                
        if current_chunk:
            chunks.append(CodeChunk(
                name=f"Chunk {chunk_idx}", type="text_block",
                content="".join(current_chunk), start_line=start_line, end_line=len(lines)
            ))
            
        return chunks
--------------------------------------------------------------------------------
FILE: src\_micro_services\_LibrarianServiceMS\librarian_service.py
--------------------------------------------------------------------------------
import os
import sqlite3
import time
import uuid
import difflib
import logging
import json
from pathlib import Path
from typing import List, Dict, Optional, Any

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("Librarian")

class LibrarianMS:
    """
    The Factory Foreman: Manages the creation and 'sealing' of Cortex Cartridges (.db files).
    Ensures that every exported DB contains a 'Manifest' so external Agents know how to read it.
    """
    
    def __init__(self, storage_dir: str = "./cortex_dbs"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)

    # --- KB Management ---

    def list_kbs(self) -> List[str]:
        if not self.storage_dir.exists():
            return []
        files = list(self.storage_dir.glob("*.db"))
        files.sort(key=os.path.getmtime, reverse=True)
        return [f.name for f in files]

    def create_kb(self, name: str) -> Dict[str, str]:
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            raise FileExistsError(f"Knowledge Base '{safe_name}' already exists.")

        try:
            self._init_schema(db_path)
            return {"status": "success", "path": str(db_path), "name": safe_name}
        except Exception as e:
            if db_path.exists(): os.remove(db_path)
            raise e

    def delete_kb(self, name: str):
        db_path = self.storage_dir / name
        if db_path.exists():
            os.remove(db_path)
            # Clean up potential WAL/SHM files
            for ext in ['-wal', '-shm']:
                aux = self.storage_dir / (name + ext)
                if aux.exists(): os.remove(aux)

    def _init_schema(self, db_path: Path):
        """Initializes the Standard Cortex Schema."""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("PRAGMA journal_mode=WAL")
        
        # 1. Manifest (The "Label" for the Agent)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS manifest (
                key TEXT PRIMARY KEY, 
                value TEXT
            )
        """)
        
        # 2. Graph (The Neural Structure)
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)")

        # 3. Files (The Artifacts)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                path TEXT UNIQUE NOT NULL,      -- Internal ID
                content TEXT,                   -- Full text snapshot
                origin_type TEXT DEFAULT 'filesystem', 
                origin_path TEXT,               
                vfs_path TEXT,                  -- Relative export path
                metadata TEXT DEFAULT '{}',     -- Tags/Authors
                last_updated TIMESTAMP,
                status TEXT DEFAULT 'indexed'
            )
        """)

        # 4. Chunks (The Vectors)
        # Note: We store embedding as a BLOB (JSON bytes) for maximum portability.
        cursor.execute("""
        CREATE TABLE IF NOT EXISTS chunks (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_id INTEGER,
            chunk_index INTEGER,
            content TEXT,
            embedding BLOB,     -- JSON UTF-8 Bytes
            name TEXT,          
            type TEXT,          
            start_line INTEGER,
            end_line INTEGER,
            FOREIGN KEY(file_id) REFERENCES files(id)
        )
        """)

        # 5. Diff Log (History)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS diff_log (
                id TEXT PRIMARY KEY,
                file_path TEXT NOT NULL,
                timestamp TIMESTAMP,
                change_type TEXT,
                diff_blob TEXT,
                author TEXT
            )
        """)
        
        # Set Creation Timestamp + Standard Cartridge Manifest Defaults
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("created_at", str(time.time())))
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("schema_version", "1.0"))

        # Core identity + interpretation hints (so external Agents know what this DB *is*)
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("cartridge_id", str(uuid.uuid4())))
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("artifact_type", "unknown"))
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("artifact_profile", "{}"))
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("root_vfs", "/"))

        # Boot-strapping cards for RAG consumers
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("entrypoints", "[]"))
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("ingest_models", "{}"))
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", ("source_provenance", "{}"))
        
        conn.commit()
        conn.close()

    def update_manifest(self, db_name: str, key: str, value: str):
        """Updates the manifest. Used by IngestEngine to record Model Names."""
        db_path = self.storage_dir / db_name
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", (key, str(value)))
        conn.commit()
        conn.close()

    # --- File Versioning ---

    def update_file(self, db_name: str, file_path: str, new_content: str, 
                   author: str = "user", origin_type: str = "filesystem", 
                   origin_path: str = None, metadata: dict = None) -> Dict[str, Any]:
        """Updates a file and logs the diff."""
        db_path = self.storage_dir / db_name
        if not db_path.exists(): raise FileNotFoundError("KB not found")

        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        now = time.time()
        meta_json = json.dumps(metadata) if metadata else "{}"

        try:
            cursor.execute("SELECT id, content FROM files WHERE path = ?", (file_path,))
            row = cursor.fetchone()

            if not row:
                # CREATE
                cursor.execute(
                    """INSERT INTO files 
                       (path, content, last_updated, origin_type, origin_path, vfs_path, metadata) 
                       VALUES (?, ?, ?, ?, ?, ?, ?)""",
                    (file_path, new_content, now, origin_type, origin_path or file_path, file_path, meta_json)
                )
                self._log_diff(cursor, file_path, "CREATE", "[New File]", author, now)
                conn.commit()
                return {"status": "created", "path": file_path}

            # EDIT
            old_content = row['content'] or ""
            diff_text = self._compute_diff(file_path, old_content, new_content)
            
            if not diff_text:
                return {"status": "unchanged", "path": file_path}

            self._log_diff(cursor, file_path, "EDIT", diff_text, author, now)
            cursor.execute(
                "UPDATE files SET content = ?, last_updated = ?, metadata = ? WHERE path = ?",
                (new_content, now, meta_json, file_path)
            )
            conn.commit()
            return {"status": "updated", "path": file_path, "diff_size": len(diff_text)}

        finally:
            conn.close()

    def get_file_content(self, db_name: str, file_path: str) -> Optional[str]:
        db_path = self.storage_dir / db_name
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT content FROM files WHERE path = ?", (file_path,))
        row = cursor.fetchone()
        conn.close()
        return row[0] if row else None

    def list_files_in_kb(self, db_name: str) -> List[str]:
        db_path = self.storage_dir / db_name
        if not db_path.exists(): return []
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        rows = cursor.execute("SELECT path FROM files ORDER BY path ASC").fetchall()
        conn.close()
        return [r[0] for r in rows]

    # --- Helpers ---

    def _compute_diff(self, path: str, old: str, new: str) -> str:
        old_lines = old.splitlines(keepends=True)
        new_lines = new.splitlines(keepends=True)
        diff_gen = difflib.unified_diff(
            old_lines, new_lines, 
            fromfile=f"a/{path}", tofile=f"b/{path}",
            lineterm=''
        )
        return "".join(diff_gen)

    def _log_diff(self, cursor, path, change_type, diff_text, author, timestamp):
        diff_id = str(uuid.uuid4())
        cursor.execute(
            "INSERT INTO diff_log (id, file_path, timestamp, change_type, diff_blob, author) VALUES (?, ?, ?, ?, ?, ?)",
            (diff_id, path, timestamp, change_type, diff_text, author)
        )

    def _sanitize_name(self, name: str) -> str:
        # Enforce .db extension strictly to ensure visibility in list_kbs
        if name.endswith('.db'):
            name = name[:-3]
        
        # Sanitize the base name (strip dots/special chars)
        clean_base = "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).strip()
        clean_base = clean_base.replace(' ', '_')
        
        return f"{clean_base}.db"



--------------------------------------------------------------------------------
FILE: src\_micro_services\_ScannerMS\scanner.py
--------------------------------------------------------------------------------
import os
import time
import requests
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Any, Optional

# Try imports for Web/PDF support
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

class ScannerMS:
    """
    The Scanner: Walks file systems OR crawls websites (Depth-Aware).
    """
    
    def __init__(self):
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage', 'site-packages'
        }
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', 
            '.zip', '.tar', '.gz', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }
        self.visited_urls = set()

    def is_binary(self, file_path: str) -> bool:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS: return True
        return False

    def scan_directory(self, root_path: str, web_depth: int = 0) -> Optional[Dict[str, Any]]:
        """
        Main Entry Point.
        :param root_path: File path or URL.
        :param web_depth: How many links deep to crawl (0 = single page).
        """
        # 1. Web Crawl Mode
        if root_path.startswith("http://") or root_path.startswith("https://"):
            self.visited_urls.clear()
            return self._crawl_web_recursive(root_path, depth=web_depth, origin_domain=urlparse(root_path).netloc)

        # 2. Local File System Mode
        target = os.path.abspath(root_path)
        if not os.path.exists(target): return None
        
        if not os.path.isdir(target): 
            return self._create_node(target, is_dir=False)
            
        return self._scan_fs_recursive(target)

    # --- Web Logic ---
    def _crawl_web_recursive(self, url: str, depth: int, origin_domain: str) -> Dict[str, Any]:
        """
        Recursively fetches links.
        """
        node = {
            'text': url, 
            'path': url, 
            'type': 'web', 
            'children': [], 
            'checked': True
        }
        
        if depth < 0 or url in self.visited_urls: return node
        self.visited_urls.add(url)

        if depth > 0 and BeautifulSoup:
            try:
                # Polite Delay
                time.sleep(0.1)
                resp = requests.get(url, timeout=5)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.content, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        parsed = urlparse(full_url)
                        
                        # Filter: Only same domain, valid schemes
                        if parsed.netloc == origin_domain and parsed.scheme in ['http', 'https']:
                            if full_url not in self.visited_urls:
                                child_node = self._crawl_web_recursive(full_url, depth - 1, origin_domain)
                                node['children'].append(child_node)
            except Exception as e:
                node['error'] = str(e)
                
        return node

    # --- File System Logic ---
    def _scan_fs_recursive(self, current_path: str) -> Dict[str, Any]:
        node = self._create_node(current_path, is_dir=True)
        node['children'] = []
        try:
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                for entry in entries:
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS: continue
                    if entry.name.startswith('.'): continue

                    if entry.is_dir():
                        child = self._scan_fs_recursive(entry.path)
                        if child: node['children'].append(child)
                    else:
                        node['children'].append(self._create_node(entry.path, is_dir=False))
        except PermissionError:
            node['error'] = "Access Denied"
        return node

    def _create_node(self, path: str, is_dir: bool) -> Dict[str, Any]:
        name = os.path.basename(path)
        node = {'text': name, 'path': path, 'type': 'folder' if is_dir else 'file', 'checked': False}
        if not is_dir and self.is_binary(path): node['type'] = 'binary'
        return node

    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        files = []
        if tree_node['type'] in ['file', 'web']:
            files.append(tree_node['path'])
        elif 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files
--------------------------------------------------------------------------------
FILE: src\_micro_services\_SearchEngineMS\search_engine.py
--------------------------------------------------------------------------------
import sqlite3
import json
import struct
import requests
import os
import math
from typing import List, Dict, Any, Optional

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    Includes a pure-Python fallback for vector math if sqlite-vec is missing.
    """

    def __init__(self, embed_model: str = "nomic-embed-text"):
        self.embed_model = embed_model
        self.vec_extension_loaded = False

    def _load_extension(self, conn: sqlite3.Connection):
        """Attempts to load sqlite-vec if available."""
        try:
            conn.enable_load_extension(True)
            import sqlite_vec
            sqlite_vec.load(conn)
            self.vec_extension_loaded = True
        except (ImportError, AttributeError, sqlite3.OperationalError):
            # Silently fail and fallback to Python math
            self.vec_extension_loaded = False

    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict]:
        if not os.path.exists(db_path):
            return []

        conn = sqlite3.connect(db_path)
        self._load_extension(conn)
        cursor = conn.cursor()

        # 1. Vectorize Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails (e.g. Ollama offline)
            conn.close()
            return self._keyword_search_fallback(db_path, query, limit)

        results = []
        
        try:
            if self.vec_extension_loaded:
                # FAST PATH: C++ Extension
                results = self._search_sqlite_vec(cursor, query_vec, limit)
            else:
                # SLOW PATH: Python Math (Robustness)
                results = self._search_pure_python(cursor, query_vec, limit)
        except Exception as e:
            print(f"[SearchEngine] Error: {e}")
            results = self._keyword_search_fallback(db_path, query, limit)

        conn.close()
        return results

    def _search_sqlite_vec(self, cursor, query_vec: List[float], limit: int) -> List[Dict]:
        """Used when sqlite-vec is successfully loaded."""
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)
        sql = """
            SELECT f.path, c.content, vec_distance_cosine(c.embedding, ?) as distance
            FROM chunks c
            JOIN files f ON c.file_id = f.id
            WHERE c.embedding IS NOT NULL
            ORDER BY distance ASC
            LIMIT ?
        """
        rows = cursor.execute(sql, (vec_bytes, limit)).fetchall()
        return self._format_results(rows, score_is_distance=True)

    def _search_pure_python(self, cursor, query_vec: List[float], limit: int) -> List[Dict]:
        """
        Fallback: Fetches all embeddings and computes Cosine Similarity in Python.
        Slower, but guarantees functionality without DLL dependencies.
        """
        sql = "SELECT c.id, f.path, c.content, c.embedding FROM chunks c JOIN files f ON c.file_id = f.id"
        cursor.execute(sql)
        
        candidates = []
        for row in cursor.fetchall():
            chunk_id, path, content, blob = row
            if not blob: continue
            
            try:
                # Deserialize: Try JSON first (as per your IngestEngine), then struct
                try:
                    vec = json.loads(blob)
                except:
                    # Fallback for binary blob if changed later
                    vec = struct.unpack(f'{len(query_vec)}f', blob)
                
                score = self._cosine_similarity(query_vec, vec)
                candidates.append((score, path, content))
            except Exception:
                continue

        # Sort by Score DESC (High similarity is better)
        candidates.sort(key=lambda x: x[0], reverse=True)
        top_n = candidates[:limit]
        
        return [{
            "path": r[1],
            "score": r[0],
            "snippet": r[2][:200].replace('\n', ' ') + "...",
            "full_content": r[2]
        } for r in top_n]

    def _keyword_search_fallback(self, db_path: str, query: str, limit: int) -> List[Dict]:
        """Used if Ollama is dead or vectors fail completely."""
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        sql = "SELECT path, content FROM files WHERE content LIKE ? LIMIT ?"
        rows = cursor.execute(sql, (f'%{query}%', limit)).fetchall()
        conn.close()
        return [{
            "path": r[0],
            "score": 0.0,
            "snippet": r[1][:200].replace('\n', ' ') + "...",
            "full_content": r[1]
        } for r in rows]

    def _cosine_similarity(self, vec_a: List[float], vec_b: List[float]) -> float:
        dot = sum(a * b for a, b in zip(vec_a, vec_b))
        norm_a = math.sqrt(sum(a * a for a in vec_a))
        norm_b = math.sqrt(sum(b * b for b in vec_b))
        return dot / (norm_a * norm_b) if norm_a and norm_b else 0.0

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.embed_model, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            pass
        return None

    def _format_results(self, rows, score_is_distance=False):
        results = []
        for r in rows:
            score = r[2]
            # Convert Cosine Distance (0..2) to Similarity (0..1)
            if score_is_distance:
                score = 1 - score 
            
            results.append({
                "path": r[0],
                "score": score,
                "snippet": r[1][:200].replace('\n', ' ') + "...",
                "full_content": r[1]
            })
        return results

if __name__ == "__main__":
    print("SearchEngineMS Ready.")
--------------------------------------------------------------------------------
FILE: src\_micro_services\_ThoughtStreamMS\thought_stream.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import datetime

class ThoughtStream(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340)
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", fg="#007ACC", bg="#1a1a25", font=("Consolas", 8)).pack(anchor="w", padx=5, pady=2)
        
        snippet = content[:400] + "..." if len(content) > 400 else content
        tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", font=("Consolas", 8), justify="left", wraplength=300).pack(fill="x", padx=5, pady=2)
        
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        if not vector: return
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        bar_w = w / len(vector)
        for i, val in enumerate(vector):
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")