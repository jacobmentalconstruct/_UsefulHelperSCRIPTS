Dump: C:\Users\jacob\Documents\_UsefulHelperAPPS\_LangBOT


--------------------------------------------------------------------------------
FILE: fix.py
--------------------------------------------------------------------------------
import os
import re

def fix_langbot_imports():
    base_dir = os.path.join(os.getcwd(), 'src')
    
    # 1. Patterns to fix
    # Fix BaseService location [cite: 13, 32]
    base_service_pattern = re.compile(r'from (?:src\.microservices\.)?microservice_std_lib import (.*?)BaseService')
    # Standardize all microservice internal imports [cite: 16, 74]
    ms_internal_pattern = re.compile(r'from (_.*?|base_service|document_utils|microservice_std_lib) import')

    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # Fix A: Move BaseService import to the correct file 
                if 'BaseService' in content:
                    content = re.sub(
                        r'from (src\.microservices\.)?microservice_std_lib import (.*?),?\s*BaseService,?\s*(.*)',
                        r'from \1microservice_std_lib import \2 \3\nfrom \1base_service import BaseService',
                        content
                    )
                    # Clean up double commas if they occurred
                    content = content.replace(', ,', ',').replace('import ,', 'import')

                # Fix B: Ensure absolute package imports for local modules 
                content = ms_internal_pattern.sub(r'from src.microservices.\1 import', content)

                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"[FIXED] {file}")

    # 2. Ensure __init__.py files exist for package recognition 
    for folder in [base_dir, os.path.join(base_dir, 'microservices')]:
        init_file = os.path.join(folder, '__init__.py')
        if not os.path.exists(init_file):
            with open(init_file, 'w') as f:
                f.write("# Package marker")
            print(f"[CREATED] {init_file}")

if __name__ == "__main__":
    print("Starting LangBOT import refactor...")
    fix_langbot_imports()
    print("Refactor complete. Try running 'python -m src.app' now.")
--------------------------------------------------------------------------------
FILE: LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: requirements.txt
--------------------------------------------------------------------------------
langgraph
langchain
requests
chromadb
pydantic
httpx
readability-lxml
numpy
faiss-cpu
--------------------------------------------------------------------------------
FILE: setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: src\app.py
--------------------------------------------------------------------------------
from src.microservices._TkinterAppShellMS import TkinterAppShellMS
from src.backend import LangBotBackend
from src.ui import LangBotUI

def main():
    # Initialize Backend logic first
    backend = LangBotBackend()
    
    # Initialize the Mother Ship (UI Shell) [cite: 121]
    shell = TkinterAppShellMS({'title': 'LangBOT - Modular Agent (Local)'})
    
    # Dock the UI into the Shell
    view = LangBotUI(shell, backend)
    
    # Ignition
    shell.launch() # [cite: 124]

if __name__ == "__main__":
    main()
--------------------------------------------------------------------------------
FILE: src\backend.py
--------------------------------------------------------------------------------
from typing import TypedDict
from langgraph.graph import StateGraph, END
from src.microservices._NeuralServiceMS import NeuralServiceMS
from src.microservices._VectorFactoryMS import VectorFactoryMS
from src.microservices._CognitiveMemoryMS import CognitiveMemoryMS

class AgentState(TypedDict):
    question: str
    context: str
    history: str
    answer: str

class LangBotBackend:
    def __init__(self):
        # 32GB RAM allows these to stay initialized, but we limit execution concurrency
        self.neural = NeuralServiceMS()
        self.memory = CognitiveMemoryMS()
        self.factory = VectorFactoryMS()
        
        # Initialize ChromaDB via your Factory [cite: 138, 140]
        self.ltm = self.factory.create('chroma', {'path': './chroma_db', 'collection': 'langbot_ltm'})
        self.workflow = self._build_graph()

    def _build_graph(self):
        workflow = StateGraph(AgentState)
        workflow.add_node("retrieve", self._retrieve_node)
        workflow.add_node("generate", self._generate_node)
        
        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "generate")
        workflow.add_edge("generate", END)
        return workflow.compile()

    def _retrieve_node(self, state: AgentState):
        # NeuralService handles the local embedding call [cite: 105]
        emb = self.neural.get_embedding(state["question"])
        results = self.ltm.search(emb, k=3) if emb else []
        context = "\n".join([r.get('content', '') for r in results])
        return {"context": context, "history": self.memory.get_context()}

    def _generate_node(self, state: AgentState):
        prompt = f"Context: {state['context']}\nHistory: {state['history']}\nUser: {state['question']}"
        # Use 'smart' tier (Qwen 7B) for generation [cite: 106]
        response = self.neural.request_inference(prompt, tier='smart')
        self.memory.add_entry("assistant", response) # [cite: 28]
        return {"answer": response}
--------------------------------------------------------------------------------
FILE: src\ui.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk

class LangBotUI:
    def __init__(self, shell, backend):
        self.shell = shell
        self.backend = backend
        self.main_frame = shell.get_main_container() # [cite: 124]
        self._build_ui()

    def _build_ui(self):
        # Model Picker [cite: 101]
        models = self.backend.neural.get_available_models()
        self.model_var = tk.StringVar(value="qwen2.5-coder:7b")
        ttk.Label(self.main_frame, text="Select Ollama Model:").pack(pady=5)
        self.picker = ttk.Combobox(self.main_frame, textvariable=self.model_var, values=models)
        self.picker.pack(pady=5)

        # Output
        self.display = tk.Text(self.main_frame, height=25, bg="#13131f", fg="#ccc", font=("Consolas", 10))
        self.display.pack(fill="both", expand=True, padx=10, pady=10)

        # Input
        self.input_field = tk.Entry(self.main_frame, bg="#1a1a25", fg="white", insertbackground="white")
        self.input_field.pack(fill="x", padx=10, pady=5)
        self.input_field.bind("<Return>", self._handle_send)

    def _handle_send(self, event):
        query = self.input_field.get()
        self.display.insert(tk.END, f"\nYOU: {query}\n")
        self.input_field.delete(0, tk.END)
        
        # Update backend model preference before invoking
        self.backend.neural.update_models(fast="qwen2.5-coder:1.5b", smart=self.model_var.get(), embed="mxbai-embed-large")
        
        res = self.backend.workflow.invoke({"question": query})
        self.display.insert(tk.END, f"BOT: {res['answer']}\n")
--------------------------------------------------------------------------------
FILE: src\__init__.py
--------------------------------------------------------------------------------
# Package marker
--------------------------------------------------------------------------------
FILE: src\microservices\base_service.py
--------------------------------------------------------------------------------
import logging
from typing import Dict, Any

class BaseService:
    """
    Standard parent class for all microservices. 
    Provides consistent logging and identity management.
    """
    def __init__(self, name: str):
        self._service_info = {
            "name": name, 
            "id": name.lower().replace(" ", "_")
        }
        
        # Setup standard logging
        logging.basicConfig(
            level=logging.INFO, 
            format='%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(name)

    def log_info(self, message: str):
        self.logger.info(message)

    def log_error(self, message: str):
        self.logger.error(message)

    def log_warning(self, message: str):
        self.logger.warning(message)

--------------------------------------------------------------------------------
FILE: src\microservices\document_utils.py
--------------------------------------------------------------------------------
from src.microservices._ContentExtractorMS import ContentExtractorMS

# Singleton instance to reuse the extractor logic
_extractor = ContentExtractorMS()

def extract_text_from_pdf(blob: bytes) -> str:
    """Proxy to ContentExtractorMS PDF logic."""
    return _extractor._extract_pdf(blob)

def extract_text_from_html(html_text: str) -> str:
    """Proxy to ContentExtractorMS HTML logic."""
    return _extractor._extract_html(html_text)

--------------------------------------------------------------------------------
FILE: src\microservices\microservice_std_lib.py
--------------------------------------------------------------------------------
"""
LIBRARY: Microservice Standard Lib
VERSION: 2.1.0
ROLE: Provides decorators for tagging Python classes as AI-discoverable services.

Change (2.1.0):
- Split dependencies into:
    internal_dependencies: local modules / microservices to vendor with the app
    external_dependencies: pip-installable packages (requirements.txt)
- Keep legacy "dependencies" as an alias for external_dependencies for backward compatibility.
- Accept unknown keyword args in @service_metadata(...) to prevent older/newer services from crashing
  (e.g. when a runner passes additional fields).
"""

import functools
import inspect
from typing import Dict, List, Any, Optional, Type

# ==============================================================================
# DECORATORS (The "Writer" Tools)
# ==============================================================================

def service_metadata(
    name: str,
    version: str,
    description: str,
    tags: List[str],
    capabilities: Optional[List[str]] = None,

    # Legacy field (kept for backward compatibility):
    # Historically this mixed stdlib + pip deps. Going forward, treat this as *external* deps.
    dependencies: Optional[List[str]] = None,

    # New fields (preferred):
    internal_dependencies: Optional[List[str]] = None,
    external_dependencies: Optional[List[str]] = None,

    # Side effects / operational hints
    side_effects: Optional[List[str]] = None,

    # Forward-compat: ignore unknown keyword args instead of crashing older/newer services
    **_ignored_kwargs: Any,
):
    """
    Class Decorator.
    Labels a Microservice class with high-level metadata for the Catalog.

    Dependency semantics:
      - internal_dependencies: local modules and/or other microservice modules that must be shipped with an app
      - external_dependencies: third-party pip packages (requirements.txt)
      - dependencies (legacy): treated as external_dependencies when external_dependencies is not provided
    """
    # Prefer explicit new key, otherwise fall back to legacy dependencies
    ext = external_dependencies if external_dependencies is not None else (dependencies or [])
    intl = internal_dependencies or []

    def decorator(cls):
        cls._is_microservice = True
        cls._service_info = {
            "name": name,
            "version": version,
            "description": description,
            "tags": tags,
            "capabilities": capabilities or [],

            # New keys
            "internal_dependencies": intl,
            "external_dependencies": ext,

            # Legacy alias (keep existing tooling working)
            "dependencies": ext,

            "side_effects": side_effects or []
        }
        return cls
    return decorator


def service_endpoint(
    inputs: Dict[str, str],
    outputs: Dict[str, str],
    description: str,
    tags: Optional[List[str]] = None,
    side_effects: Optional[List[str]] = None,
    mode: str = "sync",
):
    """
    Method Decorator.
    Defines the 'Socket' that the AI Architect can plug into.

    :param inputs: Dict of {arg_name: type_string} (e.g. {"query": "str"})
    :param outputs: Dict of {return_name: type_string}
    :param description: What the endpoint does
    :param tags: List of categories (e.g. ["read", "write"])
    :param side_effects: List of side effects (e.g. ["filesystem:write", "db:write"])
    :param mode: "sync" or "async" (informational unless your runtime uses it)
    """

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)

        # Attach metadata to the function object itself
        wrapper._is_endpoint = True
        wrapper._endpoint_info = {
            "name": func.__name__,
            "inputs": inputs,
            "outputs": outputs,
            "description": description,
            "tags": tags or [],
            "side_effects": side_effects or [],
            "mode": mode
        }
        return wrapper
    return decorator


# ==============================================================================
# INTROSPECTION (The "Reader" Tools)
# ==============================================================================

def extract_service_schema(service_cls: Type) -> Dict[str, Any]:
    """
    Scans a decorated Service Class and returns a JSON-serializable schema
    of its metadata and all its exposed endpoints.

    This is what the AI Agent uses to 'read' the manual.
    """
    if not getattr(service_cls, "_is_microservice", False):
        raise ValueError(f"Class {service_cls.__name__} is not decorated with @service_metadata")

    schema = {
        "meta": getattr(service_cls, "_service_info", {}),
        "endpoints": []
    }

    # Inspect all methods of the class
    for _, method in inspect.getmembers(service_cls, predicate=inspect.isfunction):
        endpoint_info = getattr(method, "_endpoint_info", None)
        if endpoint_info:
            schema["endpoints"].append(endpoint_info)

    return schema

--------------------------------------------------------------------------------
FILE: src\microservices\_CognitiveMemoryMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CognitiveMemoryMS
ENTRY_POINT: _CognitiveMemoryMS.py
INTERNAL_DEPENDENCIES: base_service, microservice_std_lib
EXTERNAL_DEPENDENCIES: pydantic
"""
import importlib.util
import sys
REQUIRED = ['pydantic']
MISSING = []
for lib in REQUIRED:
    if importlib.util.find_spec(lib) is None:
        MISSING.append(lib)
if MISSING:
    print(f"MISSING DEPENDENCIES: {' '.join(MISSING)}")
    print('Please run: pip install pydantic')
import datetime
import json
import logging
import uuid
import os
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional
from pydantic import BaseModel, Field

from src.microservices.microservice_std_lib import service_metadata, service_endpoint
from src.microservices.base_service import BaseService

DEFAULT_MEMORY_FILE = Path('working_memory.jsonl')

FLUSH_THRESHOLD = 5
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger('CognitiveMem')

class MemoryEntry(BaseModel):
    """Atomic unit of memory."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)
    role: str
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

@service_metadata(name='CognitiveMemory', version='1.0.0', description='Manages Short-Term (Working) Memory and orchestrates flushing to Long-Term Memory.', tags=['memory', 'history', 'context'], capabilities=['filesystem:read', 'filesystem:write'], side_effects=['filesystem:write'], internal_dependencies=['base_service', 'microservice_std_lib'], external_dependencies=['pydantic'])
class CognitiveMemoryMS(BaseService):
    """
    The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
    flushing to Long-Term Memory (Vector Store).
    """

    def __init__(self, config: Optional[Dict[str, Any]]=None):
        super().__init__('CognitiveMemory')
        self.config = config or {}
        self.file_path = Path(self.config.get('persistence_path', DEFAULT_MEMORY_FILE))
        self.summarizer = self.config.get('summarizer_func')
        self.ingestor = self.config.get('long_term_ingest_func')
        self.working_memory: List[MemoryEntry] = []
        self._load_working_memory()

    @service_endpoint(inputs={'role': 'str', 'content': 'str', 'metadata': 'Dict'}, outputs={'entry': 'MemoryEntry'}, description='Adds an item to working memory and persists it.', tags=['memory', 'write'], side_effects=['filesystem:write'])
    def add_entry(self, role: str, content: str, metadata: Dict=None) -> MemoryEntry:
        """Adds an item to working memory and persists it."""
        entry = MemoryEntry(role=role, content=content, metadata=metadata or {})
        self.working_memory.append(entry)
        self._append_to_file(entry)
        log.info(f'Added memory: [{role}] {content[:30]}...')
        return entry

    @service_endpoint(inputs={'limit': 'int'}, outputs={'context': 'str'}, description='Returns the most recent conversation history formatted for an LLM.', tags=['memory', 'read', 'llm'], side_effects=['filesystem:read'])
    def get_context(self, limit: int=10) -> str:
        """
        Returns the most recent conversation history formatted for an LLM.
        """
        recent = self.working_memory[-limit:]
        return '\n'.join([f'{e.role.upper()}: {e.content}' for e in recent])

    def get_full_history(self) -> List[Dict]:
        """Returns the raw list of memory objects."""
        return [e.dict() for e in self.working_memory]

    @service_endpoint(inputs={}, outputs={}, description='Signals that a turn is complete; checks if memory flush is needed.', tags=['memory', 'maintenance'], side_effects=['filesystem:write'])
    def commit_turn(self):
        """
        Signal that a "Turn" (User + AI response) is complete.
        Checks if memory is full and triggers a flush if needed.
        """
        if len(self.working_memory) >= FLUSH_THRESHOLD:
            self._flush_to_long_term()

    def _flush_to_long_term(self):
        """
        Compresses working memory into a summary and moves it to Long-Term storage.
        """
        if not self.summarizer or not self.ingestor:
            log.warning('Flush triggered but Summarizer/Ingestor not configured. Skipping.')
            return
        log.info('ðŸŒ€ Flushing Working Memory to Long-Term Storage...')
        full_text = '\n'.join([f'{e.role}: {e.content}' for e in self.working_memory])
        try:
            summary = self.summarizer(full_text)
            log.info(f'Summary generated: {summary[:50]}...')
        except Exception as e:
            log.error(f'Summarization failed: {e}')
            return
        try:
            meta = {'source': 'cognitive_memory_flush', 'date': datetime.datetime.utcnow().isoformat(), 'original_entry_count': len(self.working_memory)}
            self.ingestor(summary, meta)
            log.info('âœ… Saved to Long-Term Memory.')
        except Exception as e:
            log.error(f'Ingestion failed: {e}')
            return
        self.working_memory.clear()
        self._rotate_log_file()

    def _load_working_memory(self):
        """Rehydrates memory from the JSONL file."""
        if not self.file_path.exists():
            return
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.working_memory.append(MemoryEntry.parse_raw(line))
            log.info(f'Loaded {len(self.working_memory)} items from {self.file_path}')
        except Exception as e:
            log.error(f'Corrupt memory file: {e}')

    def _append_to_file(self, entry: MemoryEntry):
        """Appends a single entry to the JSONL log."""
        with open(self.file_path, 'a', encoding='utf-8') as f:
            f.write(entry.json() + '\n')

    def _rotate_log_file(self):
        """Renames the current log to an archive timestamp."""
        if self.file_path.exists():
            timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            archive_name = self.file_path.with_name(f'memory_archive_{timestamp}.jsonl')
            self.file_path.rename(archive_name)
            log.info(f'Rotated memory log to {archive_name}')
if __name__ == '__main__':

    def mock_summarizer(text):
        return f'SUMMARY OF {len(text)} CHARS: The user and AI discussed AI architecture.'

    def mock_ingest(text, metadata):
        print(f"\n[VectorDB] Indexing: '{text}'\n[VectorDB] Meta: {metadata}")
    print('--- Initializing Cognitive Memory ---')
    mem = CognitiveMemoryMS({'summarizer_func': mock_summarizer, 'long_term_ingest_func': mock_ingest})
    print(f'Service ready: {mem}')
    print('\n--- Simulating Conversation ---')
    mem.add_entry('user', 'Hello, who are you?')
    mem.add_entry('assistant', 'I am a Cognitive Agent.')
    mem.add_entry('user', 'What is your memory capacity?')
    mem.add_entry('assistant', 'I have a tiered memory system.')
    mem.add_entry('user', 'That sounds complex.')
    print(f'\nCurrent Context:\n{mem.get_context()}')
    print('\n--- Triggering Memory Flush ---')
    mem.commit_turn()
    print(f'\nWorking Memory after flush: {len(mem.working_memory)} items')
    if Path('working_memory.jsonl').exists():
        os.remove('working_memory.jsonl')
    for p in Path('.').glob('memory_archive_*.jsonl'):
        os.remove(p)

--------------------------------------------------------------------------------
FILE: src\microservices\_IngestEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IngestEngineMS
ENTRY_POINT: _IngestEngineMS.py
INTERNAL_DEPENDENCIES: base_service, microservice_std_lib
EXTERNAL_DEPENDENCIES: requests
"""
import importlib.util
import sys
REQUIRED = ['requests']
MISSING = []
for lib in REQUIRED:
    if importlib.util.find_spec(lib) is None:
        MISSING.append(lib)
if MISSING:
    print(f"MISSING DEPENDENCIES: {' '.join(MISSING)}")
    print('Please run: pip install requests')
import json
import os
import re
import sqlite3
import time
from dataclasses import dataclass
from typing import Any, Dict, Generator, List, Optional
import requests
from src.microservices.microservice_std_lib import service_metadata, service_endpoint
from src.microservices.base_service import BaseService

DEFAULT_MEMORY_FILE = Path('working_memory.jsonl')

@dataclass
class IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

class SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    Used to generate the 'DEPENDS_ON' edges in the Knowledge Graph.
    """

    def __init__(self):
        self.py_pattern = re.compile('^\\s*(?:from|import)\\s+([\\w\\.]+)')
        self.js_pattern = re.compile('(?:import\\s+.*?from\\s+[\\\'"]|require\\([\\\'"])([\\.\\/\\w\\-_]+)[\\\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        ext = os.path.splitext(file_path)[1].lower()
        lines = content.split('\n')
        for line in lines:
            match = None
            if ext == '.py':
                match = self.py_pattern.match(line)
            elif ext in ['.js', '.ts', '.tsx', '.jsx']:
                match = self.js_pattern.search(line)
            if match:
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        return dependencies

@service_metadata(name='IngestEngine', version='1.0.0', description='Reads files, chunks text, fetches embeddings, and weaves graph edges.', tags=['ingest', 'rag', 'parsing', 'embedding'], capabilities=['filesystem:read', 'network:outbound', 'db:sqlite'], side_effects=['db:write', 'network:outbound'], internal_dependencies=['base_service', 'microservice_std_lib'], external_dependencies=['requests'])
class IngestEngineMS(BaseService):
    """
    The Heavy Lifter: Reads files, chunks text, fetches embeddings,
    populates the Graph Nodes, and weaves Graph Edges.
    """

    def __init__(self, config: Optional[Dict[str, Any]]=None):
        super().__init__('IngestEngine')
        self.config = config or {}
        self.db_path = self.config.get('db_path', 'knowledge.db')
        self.stop_signal = False
        self.weaver = SynapseWeaver()
        self._init_db()

    def _init_db(self):
        """Ensures the target database has the required schema."""
        conn = sqlite3.connect(self.db_path)
        conn.execute('CREATE TABLE IF NOT EXISTS files (id INTEGER PRIMARY KEY, path TEXT, last_updated REAL)')
        conn.execute('CREATE TABLE IF NOT EXISTS chunks (id INTEGER PRIMARY KEY, file_id INT, chunk_index INT, content TEXT, embedding BLOB)')
        conn.execute('CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)')
        conn.execute('CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)')
        conn.close()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f'{OLLAMA_API_URL}/tags', timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f'{OLLAMA_API_URL}/tags')
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    @service_endpoint(inputs={'file_paths': 'List[str]', 'model_name': 'str'}, outputs={'status': 'IngestStatus'}, description='Processes a list of files, ingesting them into the knowledge graph.', tags=['ingest', 'processing'], mode='generator', side_effects=['db:write', 'network:outbound'])
    def process_files(self, file_paths: List[str], model_name: str='none') -> Generator[IngestStatus, None, None]:
        total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('PRAGMA synchronous = OFF')
        cursor.execute('PRAGMA journal_mode = MEMORY')
        node_registry = {}
        file_contents = {}
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, 'Ingestion Aborted.')
                break
            filename = os.path.basename(file_path)
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                file_contents[filename] = content
            except Exception as e:
                yield IngestStatus(file_path, idx / total * 100, idx, total, f'Error: {e}')
                continue
            try:
                cursor.execute('INSERT OR REPLACE INTO files (path, last_updated) VALUES (?, ?)', (file_path, time.time()))
                file_id = cursor.lastrowid
            except sqlite3.Error:
                continue
            cursor.execute('\n                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)\n                VALUES (?, ?, ?, ?)\n            ', (filename, 'file', filename, json.dumps({'path': file_path})))
            node_registry[filename] = filename
            chunks = self._chunk_text(content)
            for i, chunk_text in enumerate(chunks):
                if self.stop_signal:
                    break
                embedding = None
                if model_name != 'none':
                    embedding = self._get_embedding(model_name, chunk_text)
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                cursor.execute('\n                    INSERT INTO chunks (file_id, chunk_index, content, embedding)\n                    VALUES (?, ?, ?, ?)\n                ', (file_id, i, chunk_text, emb_blob))
                thought_frame = {'id': f'{file_id}_{i}', 'file': filename, 'chunk_index': i, 'content': chunk_text, 'vector_preview': embedding[:20] if embedding else [], 'concept_color': '#007ACC'}
                yield IngestStatus(current_file=filename, progress_percent=(idx + i / len(chunks)) / total * 100, processed_files=idx, total_files=total, log_message=f'Processing {filename}...', thought_frame=thought_frame)
            conn.commit()
        yield IngestStatus('Graph', 100, total, total, 'Weaving Knowledge Graph...')
        edge_count = 0
        for filename, content in file_contents.items():
            if self.stop_signal:
                break
            deps = self.weaver.extract_dependencies(content, filename)
            for dep in deps:
                target_id = None
                for potential_match in node_registry.keys():
                    if potential_match.startswith(dep + '.') or potential_match == dep:
                        target_id = potential_match
                        break
                if target_id and target_id != filename:
                    try:
                        cursor.execute('\n                            INSERT OR IGNORE INTO graph_edges (source, target, weight)\n                            VALUES (?, ?, 1.0)\n                        ', (filename, target_id))
                        edge_count += 1
                    except:
                        pass
        conn.commit()
        conn.close()
        yield IngestStatus(current_file='Complete', progress_percent=100, processed_files=total, total_files=total, log_message=f'Ingestion Complete. Created {edge_count} dependency edges.')

    def _chunk_text(self, text: str, chunk_size: int=1000, overlap: int=100) -> List[str]:
        if len(text) < chunk_size:
            return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(f'{OLLAMA_API_URL}/embeddings', json={'model': model, 'prompt': text}, timeout=30)
            if res.status_code == 200:
                return res.json().get('embedding')
        except:
            return None
if __name__ == '__main__':
    TEST_DB = 'test_ingest_v2.db'
    engine = IngestEngineMS({'db_path': TEST_DB})
    print(f'Service Ready: {engine}')
    target_file = '__IngestEngineMS.py'
    if not os.path.exists(target_file):
        with open(target_file, 'w') as f:
            f.write("import os\nimport json\nprint('Hello World')")
    print(f'Running Ingest on {target_file}...')
    files = [target_file]
    for status in engine.process_files(files, 'none'):
        print(f'[{status.progress_percent:.0f}%] {status.log_message}')
    conn = sqlite3.connect(TEST_DB)
    edges = conn.execute('SELECT * FROM graph_edges').fetchall()
    nodes = conn.execute('SELECT * FROM graph_nodes').fetchall()
    print(f'\nResult: {len(nodes)} Nodes, {len(edges)} Edges.')
    conn.close()
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)
    if os.path.exists(target_file) and 'Hello World' in open(target_file).read():
        os.remove(target_file)

--------------------------------------------------------------------------------
FILE: src\microservices\_IntakeServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IntakeServiceMS
ENTRY_POINT: _IntakeServiceMS.py
INTERNAL_DEPENDENCIES: _CartridgeServiceMS, _ScannerMS, base_service, document_utils, microservice_std_lib
EXTERNAL_DEPENDENCIES: bs4, requests
"""
import os
import mimetypes
import requests
import fnmatch
import json
from pathlib import Path
from typing import Dict, Set, List, Any
from src.microservices.base_service import BaseService
from src.microservices._CartridgeServiceMS import CartridgeServiceMS
from src.microservices._ScannerMS import ScannerMS
import document_utils
from src.microservices.microservice_std_lib import service_metadata, service_endpoint
from src.microservices.base_service import BaseService

DEFAULT_MEMORY_FILE = Path('working_memory.jsonl')
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

@service_metadata(name='IntakeServiceMS', version='1.2.0', description='The Vacuum: Handles two-phase ingestion by scanning sources and processing selected paths into the cartridge.', tags=['ingestion', 'scanner', 'vfs'], capabilities=['filesystem:read', 'web:crawl'], side_effects=['filesystem:read', 'cartridge:write'], internal_dependencies=['_CartridgeServiceMS', '_ScannerMS', 'base_service', 'document_utils', 'microservice_std_lib'], external_dependencies=['bs4', 'requests'])
class IntakeServiceMS(BaseService):
    """
    The Vacuum. 
    Now supports two-phase ingestion:
    1. Scan -> Build Tree (with .gitignore respect)
    2. Ingest -> Process selected paths
    """
    DEFAULT_IGNORE_DIRS = {'.git', '__pycache__', 'node_modules', 'venv', '.venv', 'env', '.env', '.idea', '.vscode', 'dist', 'build', 'target', 'bin', 'obj', '__cartridge__'}
    DEFAULT_IGNORE_EXTS = {'.pyc', '.pyd', '.exe', '.dll', '.so', '.db', '.sqlite', '.sqlite3', '.bin', '.iso', '.img', '.zip', '.tar', '.gz', '.7z', '.jpg', '.png'}

    def __init__(self, cartridge: CartridgeService):
        super().__init__('IntakeServiceMS')
        self.start_time = time.time()

    @service_endpoint(inputs={}, outputs={'status': 'str', 'uptime': 'float', 'cartridge_connected': 'bool'}, description='Standardized health check to verify service status and cartridge connectivity.', tags=['diagnostic', 'health'])
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the IntakeServiceMS."""
        return {'status': 'online', 'uptime': time.time() - self.start_time, 'cartridge_connected': self.cartridge is not None}
        self.cartridge = cartridge
        self.ignore_patterns: Set[str] = set()

    def ingest_source(self, source_path: str) -> Dict[str, int]:
        """Headless/CLI Entry point: Scans and Ingests in one go."""
        self.cartridge.initialize_manifest()
        self.cartridge.set_manifest('source_root', source_path)
        is_web = source_path.startswith('http')
        self.cartridge.set_manifest('source_type', 'web_root' if is_web else 'filesystem_dir')
        scanner = ScoutMS()
        tree_node = scanner.scan_directory(source_path, web_depth=1 if is_web else 0)
        if not tree_node:
            return {'error': 'Source not found'}
        files_to_ingest = scanner.flatten_tree(tree_node)
        self.cartridge.set_manifest('ingest_config', {'auto_flattened': True, 'count': len(files_to_ingest)})
        return self.ingest_selected(files_to_ingest, source_path)

    @service_endpoint(inputs={'root_path': 'str', 'web_depth': 'int'}, outputs={'tree': 'dict'}, description='Scans a local directory or URL to build a hierarchical tree structure of available files.', tags=['scan', 'discovery'])
    def scan_path(self, root_path: str, web_depth: int=0) -> Dict[str, Any]:
        """
        Unified Scanner Interface.
        Delegates to ScoutMS for both Web and Local FS to ensure consistent node structure.
        """
        scanner = ScannerMS()
        tree_root = scanner.scan_directory(root_path, web_depth=web_depth)
        if not tree_root:
            return None
        if not root_path.startswith('http'):
            saved_config = self._load_persistence(os.path.abspath(root_path))
            self._apply_persistence(tree_root, saved_config)
        return tree_root

    def _apply_persistence(self, node: Dict, saved_config: Dict):
        """Recursively applies checked state from saved config."""
        if 'rel_path' in node and node['rel_path'] in saved_config:
            node['checked'] = saved_config[node['rel_path']]
        elif 'children' in node:
            pass
        if 'children' in node:
            for child in node['children']:
                self._apply_persistence(child, saved_config)

    def _scan_recursive(self, current_path: str, root_path: str, saved_config: Dict) -> Dict:
        name = os.path.basename(current_path)
        is_dir = os.path.isdir(current_path)
        rel_path = os.path.relpath(current_path, root_path).replace('\\', '/')
        node = {'name': name, 'path': current_path, 'rel_path': rel_path, 'type': 'dir' if is_dir else 'file', 'children': [], 'checked': True}
        if saved_config and rel_path in saved_config:
            node['checked'] = saved_config[rel_path]
        elif self._is_ignored(name) or (not is_dir and self._is_binary_ext(name)):
            node['checked'] = False
        if is_dir:
            try:
                with os.scandir(current_path) as it:
                    entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                    for entry in entries:
                        child = self._scan_recursive(entry.path, root_path, saved_config)
                        node['children'].append(child)
            except PermissionError:
                pass
        return node

    @service_endpoint(inputs={'file_list': 'list', 'root_path': 'str'}, outputs={'stats': 'dict'}, description='Processes a specific list of files into the cartridge storage, handling text extraction and VFS indexing.', tags=['ingest', 'write'], side_effects=['cartridge:write'])
    def ingest_selected(self, file_list: List[str], root_path: str) -> Dict[str, int]:
        """Ingests only the specific files passed in the list."""
        stats = {'added': 0, 'skipped': 0, 'errors': 0}
        for file_path in file_list:
            try:
                try:
                    vfs_path = os.path.relpath(file_path, root_path).replace('\\', '/')
                except ValueError:
                    vfs_path = os.path.basename(file_path)
                self._read_and_store(Path(file_path), vfs_path, 'filesystem', stats)
            except Exception as e:
                self.log_error(f'Error ingesting {file_path}: {e}')
                stats['errors'] += 1
        self._rebuild_directory_index()
        return stats

    def _rebuild_directory_index(self):
        """
        Scans 'files' table and populates 'directories' table.
        This creates the navigable VFS structure.
        """
        self.log_info('Rebuilding VFS Directory Index...')
        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute('SELECT vfs_path FROM files').fetchall()
            seen_dirs = set()
            for r in rows:
                path = r[0]
                current = os.path.dirname(path).replace('\\', '/')
                while current and current != '.' and (current not in seen_dirs):
                    self.cartridge.ensure_directory(current)
                    seen_dirs.add(current)
                    current = os.path.dirname(current).replace('\\', '/')
        except Exception as e:
            self.log_error(f'Directory Index Error: {e}')
        finally:
            conn.close()

    def _load_persistence(self, root_path: str) -> Dict[str, bool]:
        """Loads config from DB Manifest (Portable) or fallback to local."""
        try:
            conn = self.cartridge._get_conn()
            row = conn.execute("SELECT value FROM manifest WHERE key='ingest_config'").fetchone()
            conn.close()
            if row:
                return json.loads(row[0])
        except:
            pass
        cfg_path = os.path.join(root_path, '.ragforge.json')
        if os.path.exists(cfg_path):
            try:
                with open(cfg_path, 'r') as f:
                    return json.load(f)
            except:
                pass
        return {}

    def save_persistence(self, root_path: str, checked_map: Dict[str, bool]):
        """Saves user selections into the Cartridge Manifest (Portable)."""
        try:
            conn = self.cartridge._get_conn()
            conn.execute('INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)', ('ingest_config', json.dumps(checked_map)))
            conn.commit()
            conn.close()
        except Exception as e:
            self.log_error(f'Failed to save persistence to DB: {e}')
        cfg_path = os.path.join(root_path, '.ragforge.json')
        try:
            with open(cfg_path, 'w') as f:
                json.dump(checked_map, f, indent=2)
        except:
            pass

    def _load_gitignore(self, root_path: str):
        gitignore_path = os.path.join(root_path, '.gitignore')
        self.ignore_patterns = self.DEFAULT_IGNORE_DIRS.copy()
        if os.path.exists(gitignore_path):
            try:
                with open(gitignore_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and (not line.startswith('#')):
                            if line.endswith('/'):
                                line = line[:-1]
                            self.ignore_patterns.add(line)
            except:
                pass

    def _is_ignored(self, name: str) -> bool:
        if name in self.ignore_patterns:
            return True
        for pattern in self.ignore_patterns:
            if fnmatch.fnmatch(name, pattern):
                return True
        return False

    def _is_binary_ext(self, name: str) -> bool:
        _, ext = os.path.splitext(name)
        return ext.lower() in self.DEFAULT_IGNORE_EXTS

    def _read_and_store(self, real_path: Path, vfs_path: str, origin_type: str, stats: Dict):
        mime_type, _ = mimetypes.guess_type(real_path)
        if not mime_type:
            mime_type = 'application/octet-stream'
        content = None
        blob = None
        try:
            with open(real_path, 'rb') as f:
                blob = f.read()
        except Exception as e:
            self.log_error(f'Read error {real_path}: {e}')
            stats['errors'] += 1
            return
        lower_path = str(real_path).lower()
        if lower_path.endswith('.pdf'):
            content = document_utils.extract_text_from_pdf(blob)
            if not content:
                mime_type = 'application/pdf'
        elif lower_path.endswith('.html') or lower_path.endswith('.htm'):
            try:
                raw_text = blob.decode('utf-8', errors='ignore')
                content = document_utils.extract_text_from_html(raw_text)
            except:
                pass
        else:
            try:
                content = blob.decode('utf-8')
            except UnicodeDecodeError:
                content = None
        success = self.cartridge.store_file(vfs_path, str(real_path), content=content, blob=blob, mime_type=mime_type, origin_type=origin_type)
        if success:
            stats['added'] += 1
        else:
            stats['errors'] += 1
        if __name__ == '__main__':
            from src.microservices._CartridgeServiceMS import CartridgeService
            mock_cartridge = CartridgeService(':memory:')
            svc = IntakeServiceMS(mock_cartridge)
            print('Service ready:', svc._service_info['name'])

--------------------------------------------------------------------------------
FILE: src\microservices\_NeuralServiceMS.py
--------------------------------------------------------------------------------
import requests
import json
import concurrent.futures
import logging
from typing import Optional, Dict, Any, List
from src.microservices.microservice_std_lib import service_metadata, service_endpoint
from src.microservices.base_service import BaseService

OLLAMA_API_URL = 'http://localhost:11434/api'
logger = logging.getLogger('NeuralService')

@service_metadata(name='NeuralService', version='1.0.0', description='The Brain Interface: Orchestrates local AI operations via Ollama.', tags=['ai', 'neural', 'inference', 'ollama'], capabilities=['text-generation', 'embeddings', 'parallel-processing'], internal_dependencies=['microservice_std_lib'], external_dependencies=['requests'])
class NeuralServiceMS:
    """
    The Brain Interface: Orchestrates local AI operations via Ollama for inference and embeddings.
    """

    def __init__(self, config: Optional[Dict[str, Any]]=None):
        self.config = config or {}
        self.max_workers = self.config.get('max_workers', 2)
        self.models = {'fast': 'qwen2.5-coder:1.5b-cpu', 'smart': 'qwen2.5:3b-cpu', 'embed': 'mxbai-embed-large:latest-cpu'}
        if 'models' in self.config:
            self.models.update(self.config['models'])

    @service_endpoint(inputs={'fast_model': 'str', 'smart_model': 'str', 'embed_model': 'str'}, outputs={'status': 'str'}, description='Updates the active model configurations on the fly.', tags=['config', 'write'], side_effects=['config:update'])
    def update_models(self, fast_model: str, smart_model: str, embed_model: str) -> Dict[str, str]:
        """Called by the UI Settings Modal to change models on the fly."""
        self.models['fast'] = fast_model
        self.models['smart'] = smart_model
        self.models['embed'] = embed_model
        logger.info(f'Models Updated: Fast={fast_model}, Smart={smart_model}')
        return {'status': 'success', 'config': str(self.models)}

    @service_endpoint(inputs={}, outputs={'models': 'List[str]'}, description='Fetches a list of available models from the local Ollama instance.', tags=['ai', 'read'], side_effects=['network:read'])
    def get_available_models(self) -> List[str]:
        """Fetches list from Ollama for the UI dropdown."""
        try:
            res = requests.get(f'{OLLAMA_API_URL}/tags', timeout=2)
            if res.status_code == 200:
                return [m['name'] for m in res.json().get('models', [])]
        except Exception as e:
            logger.error(f'Failed to fetch models: {e}')
            return []
        return []

    @service_endpoint(inputs={}, outputs={'is_alive': 'bool'}, description='Pings Ollama to verify connectivity.', tags=['health', 'read'], side_effects=['network:read'])
    def check_connection(self) -> bool:
        """Pings Ollama to see if it's alive."""
        try:
            requests.get(f'{OLLAMA_API_URL}/tags', timeout=2)
            return True
        except requests.RequestException:
            logger.error("Ollama connection failed. Is 'ollama serve' running?")
            return False

    @service_endpoint(inputs={'text': 'str'}, outputs={'embedding': 'list'}, description='Generates a vector embedding for the provided text.', tags=['nlp', 'vector', 'ai'], side_effects=['network:read'])
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Generates a vector using the configured embedding model."""
        try:
            res = requests.post(f'{OLLAMA_API_URL}/embeddings', json={'model': self.models['embed'], 'prompt': text}, timeout=30)
            if res.status_code == 200:
                return res.json().get('embedding')
        except Exception as e:
            logger.error(f'Embedding failed: {e}')
        return None

    @service_endpoint(inputs={'prompt': 'str', 'tier': 'str', 'format_json': 'bool'}, outputs={'response': 'str'}, description='Requests synchronous text generation from a local LLM.', tags=['llm', 'inference'], side_effects=['network:read'])
    def request_inference(self, prompt: str, tier: str='fast', format_json: bool=False) -> str:
        """
        Synchronous inference request.
        tier: 'fast', 'smart', or other keys in self.models
        """
        model = self.models.get(tier, self.models['fast'])
        payload = {'model': model, 'prompt': prompt, 'stream': False}
        if format_json:
            payload['format'] = 'json'
        try:
            res = requests.post(f'{OLLAMA_API_URL}/generate', json=payload, timeout=60)
            if res.status_code == 200:
                return res.json().get('response', '').strip()
        except Exception as e:
            logger.error(f'Inference ({tier}) failed: {e}')
        return ''

    def process_parallel(self, items: List[Any], worker_func) -> List[Any]:
        """
        Helper to run a function across many items using a ThreadPool.
        Useful for batch ingestion.
        Note: Not exposed as an endpoint as it takes a function as an argument.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {executor.submit(worker_func, item): item for item in items}
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    logger.error(f'Worker task failed: {e}')
        return results
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    svc = NeuralServiceMS()
    print('Service ready:', svc)
    if svc.check_connection():
        print('Ollama Connection: OK')
        print(f'Models available: {svc.get_available_models()}')
        print('Testing Inference (Fast Tier)...')
        response = svc.request_inference('Why is the sky blue? Answer in 1 sentence.')
        print(f'Response: {response}')
    else:
        print('Ollama Connection: FAILED (Is Ollama running?)')

--------------------------------------------------------------------------------
FILE: src\microservices\_ThoughtStreamMS.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import datetime
from typing import Any, Dict, Optional, List
from src.microservices.microservice_std_lib import service_metadata, service_endpoint
from src.microservices.base_service import BaseService

DEFAULT_MEMORY_FILE = Path('working_memory.jsonl')

@service_metadata(name='ThoughtStream', version='1.0.0', description='A UI widget for displaying a stream of AI thoughts/logs.', tags=['ui', 'stream', 'logs', 'widget'], capabilities=['ui:gui'], internal_dependencies=['microservice_std_lib'], external_dependencies=[])
class ThoughtStreamMS(ttk.Frame):
    """
    The Neural Inspector: A UI widget for displaying a stream of AI thoughts/logs
    visualized as 'bubbles' with sparklines.
    """

    def __init__(self, config: Optional[Dict[str, Any]]=None):
        self.config = config or {}
        parent = self.config.get('parent')
        super().__init__(parent)
        self.header = ttk.Label(self, text='NEURAL INSPECTOR', font=('Consolas', 10, 'bold'))
        self.header.pack(fill='x', padx=5, pady=5)
        self.canvas = tk.Canvas(self, bg='#13131f', highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient='vertical', command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg='#13131f')
        self.scrollable_frame.bind('<Configure>', lambda e: self.canvas.configure(scrollregion=self.canvas.bbox('all')))
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor='nw', width=340)
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        self.canvas.pack(side='left', fill='both', expand=True)
        self.scrollbar.pack(side='right', fill='y')

    @service_endpoint(inputs={'filename': 'str', 'chunk_id': 'int', 'content': 'str', 'vector_preview': 'List[float]', 'color': 'str'}, outputs={}, description='Adds a new thought bubble to the visual stream.', tags=['ui', 'update'], side_effects=['ui:update'])
    def add_thought_bubble(self, filename: str, chunk_id: int, content: str, vector_preview: List[float], color: str):
        """
        Mimics the 'InspectorFrame' from your React code.
        """
        bubble = tk.Frame(self.scrollable_frame, bg='#1a1a25', highlightbackground='#444', highlightthickness=1)
        bubble.pack(fill='x', padx=5, pady=5)
        ts = datetime.datetime.now().strftime('%H:%M:%S')
        header_lbl = tk.Label(bubble, text=f'{filename} #{chunk_id} [{ts}]', fg='#007ACC', bg='#1a1a25', font=('Consolas', 8))
        header_lbl.pack(anchor='w', padx=5, pady=2)
        snippet = content[:400] + '...' if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg='#ccc', bg='#10101a', font=('Consolas', 8), justify='left', wraplength=300)
        content_lbl.pack(fill='x', padx=5, pady=2)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector: List[float], color: str):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg='#1a1a25', highlightthickness=0)
        cv.pack(padx=5, pady=2)
        if not vector:
            return
        bar_w = w / len(vector) if len(vector) > 0 else 0
        for i, val in enumerate(vector):
            mag = abs(val)
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline='')
if __name__ == '__main__':
    import random
    root = tk.Tk()
    root.title('Thought Stream Test')
    root.geometry('400x600')
    stream = ThoughtStreamMS({'parent': root})
    print('Service ready:', stream)
    stream.pack(fill='both', expand=True)
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble('ExplorerView.tsx', 1, "import React from 'react'...", fake_vector, '#FF00FF')
    fake_vector_2 = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble('Backend.py', 42, 'def process_data(self): pass', fake_vector_2, '#00FF00')
    root.mainloop()

--------------------------------------------------------------------------------
FILE: src\microservices\_TkinterAppShellMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterAppShellMS
ENTRY_POINT: _TkinterAppShellMS.py
INTERNAL_DEPENDENCIES: _TkinterThemeManagerMS, microservice_std_lib
EXTERNAL_DEPENDENCIES: None
"""
import tkinter as tk
from tkinter import ttk
import logging
from typing import Dict, Any, Optional
from src.microservices.microservice_std_lib import service_metadata, service_endpoint
try:
    from src.microservices._TkinterThemeManagerMS import TkinterThemeManagerMS
except ImportError:
    TkinterThemeManagerMS = None
logger = logging.getLogger('AppShell')

@service_metadata(name='TkinterAppShell', version='2.0.0', description='The Application Container. Manages the root window, main loop, and global layout.', tags=['ui', 'core', 'lifecycle'], capabilities=['ui:root', 'ui:gui'], internal_dependencies=['_TkinterThemeManagerMS', 'microservice_std_lib'], external_dependencies=[])
class TkinterAppShellMS:
    """
    The Mother Ship.
    Owns the Tkinter Root. All other UI microservices dock into this.
    """

    def __init__(self, config: Optional[Dict[str, Any]]=None):
        self.config = config or {}
        self.root = tk.Tk()
        self.root.withdraw()
        self.theme_svc = self.config.get('theme_manager')
        if not self.theme_svc and TkinterThemeManagerMS:
            self.theme_svc = TkinterThemeManagerMS()
        self.colors = self.theme_svc.get_theme() if self.theme_svc else {}
        self._configure_root()

    def _configure_root(self):
        self.root.title(self.config.get('title', 'Microservice OS'))
        self.root.geometry(self.config.get('geometry', '1200x800'))
        bg = self.colors.get('background', '#1e1e1e')
        self.root.configure(bg=bg)
        style = ttk.Style()
        style.theme_use('clam')
        style.configure('TFrame', background=bg)
        style.configure('TLabel', background=bg, foreground=self.colors.get('foreground', '#ccc'))
        style.configure('TButton', background=self.colors.get('panel_bg', '#333'), foreground='white')
        self.main_container = tk.Frame(self.root, bg=bg)
        self.main_container.pack(fill='both', expand=True, padx=5, pady=5)

    @service_endpoint(inputs={}, outputs={}, description='Starts the GUI Main Loop.', tags=['lifecycle', 'start'], mode='sync', side_effects=['ui:block'])
    def launch(self):
        """Ignition sequence start."""
        self.root.deiconify()
        logger.info('AppShell Launched.')
        self.root.mainloop()

    @service_endpoint(inputs={}, outputs={'container': 'tk.Frame'}, description='Returns the main content area for other services to dock into.', tags=['ui', 'layout'])
    def get_main_container(self):
        """Other services call this to know where to .pack() themselves."""
        return self.main_container

    @service_endpoint(inputs={}, outputs={}, description='Gracefully shuts down the application.', tags=['lifecycle', 'stop'], side_effects=['ui:close'])
    def shutdown(self):
        self.root.quit()
if __name__ == '__main__':
    shell = TkinterAppShellMS({'title': 'Test Shell'})
    shell.launch()

--------------------------------------------------------------------------------
FILE: src\microservices\_VectorFactoryMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import os
import uuid
import logging
import shutil
from typing import List, Dict, Any, Optional, Protocol, Union
from pathlib import Path
REQUIRED = ['chromadb', 'faiss-cpu', 'numpy']
MISSING = []
for lib in REQUIRED:
    clean_lib = lib.split('>=')[0].replace('-', '_')
    if clean_lib == 'faiss_cpu':
        clean_lib = 'faiss'
    if importlib.util.find_spec(clean_lib) is None:
        MISSING.append(lib)
if MISSING:
    print('\n' + '!' * 60)
    print(f'MISSING DEPENDENCIES for _VectorFactoryMS:')
    print(f"Run:  pip install {' '.join(MISSING)}")
    print('!' * 60 + '\n')
from src.microservices.microservice_std_lib import service_metadata, service_endpoint
from src.microservices.base_service import BaseService

DEFAULT_MEMORY_FILE = Path('working_memory.jsonl')
logger = logging.getLogger('VectorFactory')

class VectorStore(Protocol):
    """The contract that all vector backends must fulfill."""

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]) -> None:
        ...

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        ...

    def count(self) -> int:
        ...

    def clear(self) -> None:
        ...

class FaissVectorStore:
    """Local, RAM-heavy, fast vector store using FAISS."""

    def __init__(self, index_path: str, dimension: int):
        import numpy as np
        import faiss
        self.np = np
        self.faiss = faiss
        self.index_path = index_path
        self.dim = dimension
        self.metadata_store = []
        if os.path.exists(index_path):
            try:
                self.index = faiss.read_index(index_path)
                meta_path = index_path + '.meta.json'
                if os.path.exists(meta_path):
                    import json
                    with open(meta_path, 'r') as f:
                        self.metadata_store = json.load(f)
            except Exception as e:
                logger.error(f'Failed to load FAISS index: {e}')
                self.index = faiss.IndexFlatL2(dimension)
        else:
            self.index = faiss.IndexFlatL2(dimension)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings:
            return
        vecs = self.np.array(embeddings).astype('float32')
        self.index.add(vecs)
        self.metadata_store.extend(metadatas)
        self._save()

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        if self.index.ntotal == 0:
            return []
        q_vec = self.np.array([query_vector]).astype('float32')
        distances, indices = self.index.search(q_vec, k)
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and idx < len(self.metadata_store):
                entry = self.metadata_store[idx].copy()
                entry['score'] = float(dist)
                results.append(entry)
        return results

    def count(self) -> int:
        return self.index.ntotal

    def clear(self):
        self.index.reset()
        self.metadata_store = []
        self._save()

    def _save(self):
        self.faiss.write_index(self.index, self.index_path)
        import json
        with open(self.index_path + '.meta.json', 'w') as f:
            json.dump(self.metadata_store, f)

class ChromaVectorStore:
    """Persistent, feature-rich vector store using ChromaDB."""

    def __init__(self, persist_dir: str, collection_name: str):
        import chromadb
        logging.getLogger('chromadb').setLevel(logging.ERROR)
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(collection_name)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings:
            return
        ids = [str(uuid.uuid4()) for _ in embeddings]
        clean_metas = [{k: str(v) if isinstance(v, (list, dict)) else v for k, v in m.items()} for m in metadatas]
        docs = [m.get('content', '') for m in metadatas]
        self.collection.add(ids=ids, embeddings=embeddings, metadatas=clean_metas, documents=docs)

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        results = self.collection.query(query_embeddings=[query_vector], n_results=k)
        output = []
        if not results['ids']:
            return []
        for i in range(len(results['ids'][0])):
            meta = results['metadatas'][0][i]
            if meta:
                entry = meta.copy()
                entry['score'] = results['distances'][0][i] if results['distances'] else 0.0
                entry['id'] = results['ids'][0][i]
                output.append(entry)
        return output

    def count(self) -> int:
        return self.collection.count()

    def clear(self):
        name = self.collection.name
        self.client.delete_collection(name)
        self.collection = self.client.get_or_create_collection(name)

@service_metadata(name='VectorFactory', version='1.0.0', description='Factory for creating VectorStore instances (FAISS, Chroma).', tags=['vector', 'factory', 'db'], capabilities=['filesystem:read', 'filesystem:write'], internal_dependencies=['microservice_std_lib'], external_dependencies=['chromadb', 'faiss', 'numpy'])
class VectorFactoryMS:
    """
    The Switchboard: Returns the appropriate VectorStore implementation
    based on configuration.
    """

    def __init__(self, config: Optional[Dict[str, Any]]=None):
        self.config = config or {}

    @service_endpoint(inputs={'backend': 'str', 'config': 'Dict'}, outputs={'store': 'VectorStore'}, description='Creates and returns a configured VectorStore instance.', tags=['vector', 'create'], side_effects=[])
    def create(self, backend: str, config: Dict[str, Any]) -> VectorStore:
        """
        :param backend: 'faiss' or 'chroma'
        :param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)
        """
        logger.info(f'Initializing Vector Store: {backend.upper()}')
        if backend == 'faiss':
            path = config.get('path', 'vector_index.bin')
            dim = config.get('dim', 384)
            return FaissVectorStore(path, dim)
        elif backend == 'chroma':
            path = config.get('path', './chroma_db')
            name = config.get('collection', 'default_collection')
            return ChromaVectorStore(path, name)
        else:
            raise ValueError(f'Unknown backend: {backend}')
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    print('--- Testing VectorFactoryMS ---')
    mock_vec = [0.1, 0.2, 0.3, 0.4]
    mock_meta = {'text': 'Hello World', 'source': 'test'}
    factory = VectorFactoryMS()
    print('Service ready:', factory)
    print('\n[Testing FAISS]')
    try:
        faiss_store = factory.create('faiss', {'path': 'test_faiss.index', 'dim': 4})
        faiss_store.add([mock_vec], [mock_meta])
        print(f'Count: {faiss_store.count()}')
        res = faiss_store.search(mock_vec, 1)
        if res:
            print(f"Search Result: {res[0]['text']}")
        if os.path.exists('test_faiss.index'):
            os.remove('test_faiss.index')
        if os.path.exists('test_faiss.index.meta.json'):
            os.remove('test_faiss.index.meta.json')
    except ImportError:
        print('Skipping FAISS test (library not installed)')
    except Exception as e:
        print(f'FAISS Test Failed: {e}')
    print('\n[Testing Chroma]')
    try:
        chroma_store = factory.create('chroma', {'path': './test_chroma_db', 'collection': 'test_col'})
        chroma_store.add([mock_vec], [mock_meta])
        print(f'Count: {chroma_store.count()}')
        res = chroma_store.search(mock_vec, 1)
        if res:
            print(f"Search Result: {res[0]['text']}")
        if os.path.exists('./test_chroma_db'):
            shutil.rmtree('./test_chroma_db')
    except ImportError:
        print('Skipping Chroma test (library not installed)')
    except Exception as e:
        print(f'Chroma Test Failed: {e}')

--------------------------------------------------------------------------------
FILE: src\microservices\_WebScraperMS.py
--------------------------------------------------------------------------------
import importlib.util
import sys
import httpx
import logging
import asyncio
import re
from typing import Optional, Dict, Any
REQUIRED = ['httpx', 'readability-lxml']
MISSING = []
for lib in REQUIRED:
    clean_lib = lib.split('>=')[0].replace('-', '_')
    if clean_lib == 'readability_lxml':
        clean_lib = 'readability'
    if importlib.util.find_spec(clean_lib) is None:
        MISSING.append(lib)
if MISSING:
    print('\n' + '!' * 60)
    print(f'MISSING DEPENDENCIES for _WebScraperMS:')
    print(f"Run:  pip install {' '.join(MISSING)}")
    print('!' * 60 + '\n')
try:
    from readability import Document
except ImportError:
    Document = None
from src.microservices.microservice_std_lib import service_metadata, service_endpoint
from src.microservices.base_service import BaseService

DEFAULT_MEMORY_FILE = Path('working_memory.jsonl')
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
TIMEOUT_SECONDS = 15.0
logger = logging.getLogger('WebScraper')

@service_metadata(name='WebScraper', version='1.0.0', description='Fetches URLs and extracts main content using Readability (stripping ads/nav).', tags=['scraper', 'web', 'readability'], capabilities=['network:outbound', 'compute'], internal_dependencies=['microservice_std_lib'], external_dependencies=['httpx', 'readability'])
class WebScraperMS:
    """
    The Reader: Fetches URLs and extracts the main content using Readability.
    Strips ads, navbars, and boilerplate to return clean text for LLMs.
    """

    def __init__(self, config: Optional[Dict[str, Any]]=None):
        self.config = config or {}
        self.headers = {'User-Agent': USER_AGENT}

    @service_endpoint(inputs={'url': 'str'}, outputs={'data': 'Dict[str, Any]'}, description='Fetches and cleans a URL.', tags=['scraper', 'read'], side_effects=['network:outbound'])
    def scrape(self, url: str) -> Dict[str, Any]:
        """
        Synchronous wrapper for fetching and cleaning a URL.
        Returns: {
            "url": str,
            "title": str,
            "content": str (The main body text),
            "html": str (The raw HTML of the main content area)
        }
        """
        return asyncio.run(self._scrape_async(url))

    async def _scrape_async(self, url: str) -> Dict[str, Any]:
        if Document is None:
            raise ImportError('readability-lxml is missing.')
        logger.info(f'Fetching: {url}')
        async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=TIMEOUT_SECONDS) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                logger.error(f'HTTP Error {e.response.status_code}: {e}')
                raise
            except httpx.RequestError as e:
                logger.error(f'Request failed: {e}')
                raise
        try:
            doc = Document(response.text)
            title = doc.title()
            clean_html = doc.summary()
            clean_text = self._strip_tags(clean_html)
            logger.info(f"Successfully scraped '{title}' ({len(clean_text)} chars)")
            return {'url': url, 'title': title, 'content': clean_text, 'html': clean_html}
        except Exception as e:
            logger.error(f'Parsing failed: {e}')
            raise

    def _strip_tags(self, html: str) -> str:
        """
        Removes HTML tags to leave only the readable text.
        """
        html = re.sub('<(script|style).*?>.*?</\\1>', '', html, flags=re.DOTALL)
        text = re.sub('<[^>]+>', ' ', html)
        text = re.sub('\\s+', ' ', text).strip()
        return text
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
    scraper = WebScraperMS()
    print('Service ready:', scraper)
    target_url = 'https://peps.python.org/pep-0008/'
    print(f'--- Scraping {target_url} ---')
    try:
        data = scraper.scrape(target_url)
        print(f"\nTitle: {data['title']}")
        print(f"Content Preview:\n{data['content'][:500]}...")
        print(f"\nTotal Length: {len(data['content'])} characters")
    except Exception as e:
        print(f'Scrape failed: {e}')

--------------------------------------------------------------------------------
FILE: src\microservices\__init__.py
--------------------------------------------------------------------------------
# Package marker