Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_RagFORGE


--------------------------------------------------------------------------------
FILE: .ragforge.json
--------------------------------------------------------------------------------
{
  ".": true,
  "_logs": false,
  "_logs/_project_mapper_config.json": false,
  "_logs/_RagFORGE_backup.tar.gz": false,
  "_logs/_RagFORGE_filedump.txt": false,
  "_logs/_RagFORGE_project_folder_tree.txt": false,
  "cartridges": false,
  "cartridges/_RagFORGE_CARTRIDGE.db": false,
  "src": true,
  "src/microservices": true,
  "src/microservices/__init__.py": true,
  "src/microservices/base_service.py": true,
  "src/microservices/cartridge_service.py": true,
  "src/microservices/document_utils.py": true,
  "src/microservices/graph_engine.py": true,
  "src/microservices/graph_view.py": true,
  "src/microservices/intake_service.py": true,
  "src/microservices/neural_service.py": true,
  "src/microservices/panels.py": true,
  "src/microservices/refinery_service.py": true,
  "src/microservices/scanner.py": true,
  "src/microservices/semantic_chunker.py": true,
  "src/microservices/telemetry_service.py": true,
  "src/microservices/thought_stream.py": true,
  "src/__init__.py": false,
  "src/app.py": true,
  "tests": false,
  "tests/dummy_source": false,
  "tests/dummy_source/concept.txt": false,
  "tests/test_cartridge.db": false,
  "tests/verify_forge.py": false,
  "LICENSE.md": true,
  "README.md": true,
  "requirements.txt": true,
  "setup_env.bat": true
}
--------------------------------------------------------------------------------
FILE: LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: README.md
--------------------------------------------------------------------------------
# **\_RagFORGE: The Neural Cartridge Factory**

**"Manufacture portable, self-contained RAG databases for your AI agents."**

\_RagFORGE is a "Knowledge Foundry." It ingests raw data (Source Code, PDFs, Websites) and refines it into a **Unified Neural Cartridge (.db)**. These cartridges are portable SQLite databases that contain the raw archive, the semantic vector index, and the relational knowledge graph‚Äîeverything an AI needs to understand a topic, in a single file.

## **üöÄ Features**

* **Universal Ingestion:**  
  * **Filesystem:** Recursively scans folders, respecting .gitignore.  
  * **Documents:** Automatically extracts text from **PDFs** and **HTML**.  
  * **Web Crawler:** Spiders websites to a specified depth and converts them into a navigable Virtual File System (VFS).  
* **The Refinery (Background Daemon):**  
  * **Smart Chunking:** Uses AST parsing for Python (splitting by Class/Function) and semantic windows for prose/text.  
  * **Parallel Embedding:** High-speed vector generation using local LLMs (via Ollama).  
  * **Graph Weaving:** Automatically links files based on imports and definitions.  
* **The Cartridge (UNCF v1.0):**  
  * Portable .db file (SQLite).  
  * Contains **Source** (Text/Blob), **Vectors** (sqlite-vec), and **Graph** (Nodes/Edges).  
  * Self-describing Manifest.  
* **Visual Verification:**  
  * Force-Directed Graph Visualization.  
  * Real-time **Neural Test** to verify vector search relevance immediately.

## **üõ†Ô∏è Installation**

1. **Prerequisites:**  
   * Python 3.10+  
   * [Ollama](https://ollama.ai/) running locally (ollama serve).  
   * Models pulled: ollama pull mxbai-embed-large (or your preferred embedder).  
2. **Setup:**  
   git clone \[https://github.com/yourusername/\_RagFORGE.git\](https://github.com/yourusername/\_RagFORGE.git)  
   cd \_RagFORGE  
   setup\_env.bat

3. **Run:**  
   \# Launch GUI  
   python \-m src.app

   \# Headless Mode (CLI)  
   python \-m src.app \--input "./my\_project" \--output "project\_brain.db"

## **üíæ The Cartridge Contract (UNCF v1.0)**

Every cartridge produced by \_RagFORGE adheres to the **Unified Neural Cartridge Format**. This ensures any consuming agent (like \_LocalMIND) can instantly mount and query the brain.

\[ YOUR CARTRIDGE (.db) \]  
‚îÇ  
‚îú‚îÄ‚îÄ 1\. The Archive (Verbatim Storage)   
‚îÇ   ‚îî‚îÄ‚îÄ Table: 'files'  
‚îÇ       ‚îú‚îÄ‚îÄ vfs\_path: "src/main.py"      (Hierarchy)  
‚îÇ       ‚îú‚îÄ‚îÄ content:  "import os..."     (Raw Text for LLM reading)  
‚îÇ       ‚îî‚îÄ‚îÄ blob:     \[Binary Data\]      (Original PDF/Image backup)  
‚îÇ  
‚îú‚îÄ‚îÄ 2\. The Index (Semantic Search)  
‚îÇ   ‚îú‚îÄ‚îÄ Table: 'chunks'                  (Text Segments)  
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ content: "def scan\_path..."  
‚îÇ   ‚îî‚îÄ‚îÄ Table: 'vec\_items'               (Mathematical Index)  
‚îÇ       ‚îî‚îÄ‚îÄ embedding: \[0.12, \-0.98...\]  (Fast Nearest-Neighbor Search)  
‚îÇ  
‚îî‚îÄ‚îÄ 3\. The Map (Knowledge Graph)  
    ‚îú‚îÄ‚îÄ Table: 'graph\_nodes'             (File & Function Nodes)  
    ‚îî‚îÄ‚îÄ Table: 'graph\_edges'             (Imports & Definitions)

### **1\. The Manifest (Boot Sector)**

Table: manifest  
Key-value store describing the cartridge's provenance and configuration.

* cartridge\_id: UUID4 unique identifier.  
* schema\_version: uncf\_v1.0.  
* created\_at\_utc: Timestamp of manufacture.  
* source\_root: Original path or URL of the source material.  
* ingest\_config: JSON record of which files were explicitly selected by the user.

### **2\. The Archive (Verbatim Storage)**

Table: files  
The "Physical" layer. Contains the raw data.

* vfs\_path: Portable, relative path (e.g., src/main.py or web/example.com/docs/intro.html).  
* content: UTF-8 Extracted Text (Input for the LLM).  
* blob\_data: Binary backup (Original PDF bytes, Images, etc.).  
* status: RAW (Needs processing), REFINED (Ready), or SKIPPED.

### **3\. The Index (Semantic Search)**

Tables: chunks, vec\_items  
The "Mathematical" layer. Enables similarity search.

* **chunks:** Text segments derived from the files.  
  * *Python:* Split by Class (class X) and Function (def y).  
  * *Docs:* Split by semantic window (e.g., 800 chars).  
* **vec\_items:** Virtual table (via sqlite-vec) storing the 1024-dimension embeddings.  
  * Queryable via KNN: WHERE embedding MATCH ? ORDER BY distance.

### **4\. The Map (Knowledge Graph)**

Tables: graph\_nodes, graph\_edges  
The "Relational" layer. Describes structure.

* **Nodes:** Represents Files (file), Web Pages (web), and Code Symbols (chunk).  
* **Edges:** Represents relationships like imports, defined\_in, or links\_to.

## **üñ•Ô∏è Usage Guide**

### **The Workflow**

1. **Select Source:** Choose a local folder or paste a URL.  
2. **Scan:** \_RagFORGE builds a file tree. Adjust "Web Depth" for crawlers.  
3. **Ingest:** Select the files you want. Click **INGEST**.  
4. **Refine:** The background daemon will wake up, chunk the data, and embed it. Watch the "System Log."  
5. **Verify:** Go to **Neural Topology**, type a query (e.g., "Authentication System"), and click **Neural Test**. Relevant nodes will light up.

### **Keyboard Controls (Graph View)**

* **Scroll:** Zoom In/Out.  
* **Left Click:** Pan Camera.  
* **Left Click \+ Drag Node:** Move Node (Physics active).  
* **Double Click Node:** Focus & Zoom.

## **üß© Architecture**

\[ \_RagFORGE App \]                 \[ External Services \]  
       ‚îÇ                                   ‚îÇ  
       ‚îú‚îÄ‚îÄ Intake Service  \<‚îÄ‚îÄ(Scan)‚îÄ‚îÄ‚îÄ\> Filesystem / Web  
       ‚îÇ        ‚îÇ                          ‚îÇ  
       ‚îú‚îÄ‚îÄ Refinery Service \<‚îÄ‚îÄ(Embed)‚îÄ‚îÄ\> Ollama (Localhost)  
       ‚îÇ        ‚îÇ  
       ‚îî‚îÄ‚îÄ Cartridge Service ‚îÄ‚îÄ\> \[ .db File (UNCF v1.0) \]

## **üìÑ License**

MIT License. Copyright (c) 2025 Jacob Lambert.
--------------------------------------------------------------------------------
FILE: requirements.txt
--------------------------------------------------------------------------------
# --- Critical UI/Graphing ---
pygame-ce>=2.3.0
Pillow>=10.0.0

# --- Networking/AI ---
requests>=2.30.0

# --- Database Extensions ---
# (Ensure your Python environment supports installing this, 
# otherwise you may need to manually place the DLL/SO)
sqlite-vec>=0.1.0

# --- Document Processing ---
pypdf>=3.0.0
beautifulsoup4>=4.12.0
--------------------------------------------------------------------------------
FILE: setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: src\.ragforge.json
--------------------------------------------------------------------------------
{
  ".": true,
  "microservices": true,
  "microservices/__init__.py": false,
  "microservices/base_service.py": true,
  "microservices/cartridge_service.py": true,
  "microservices/document_utils.py": true,
  "microservices/graph_engine.py": true,
  "microservices/graph_view.py": true,
  "microservices/intake_service.py": true,
  "microservices/neural_service.py": true,
  "microservices/panels.py": true,
  "microservices/refinery_service.py": true,
  "microservices/scanner.py": true,
  "microservices/semantic_chunker.py": true,
  "microservices/telemetry_service.py": true,
  "microservices/thought_stream.py": true,
  "__init__.py": false,
  "app.py": true
}
--------------------------------------------------------------------------------
FILE: src\app.py
--------------------------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
== _RagFORGE: Neural Cartridge Factory ==
"""

# 1. IMPORTS
import sys
import os

# --- PATH PATCH START ---
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)
# --- PATH PATCH END ---

import argparse
import threading
import tkinter as tk
from tkinter import ttk, messagebox

# Microservices
from microservices.base_service import BaseService
from microservices.cartridge_service import CartridgeService
from microservices.neural_service import NeuralService
from microservices.intake_service import IntakeService
from microservices.refinery_service import RefineryService
from microservices.telemetry_service import TelemetryService

# UI Components
from microservices.panels import (
    Sidebar,
    IngestToolbar,
    FileTreePanel,
    EditorPanel,
    SystemLog,
)
from microservices.graph_view import GraphView
from microservices.thought_stream import ThoughtStream

# 2. CONSTANTS
APP_TITLE = "_RagFORGE v1.0"
STORAGE_DIR = "./cartridges"
BG_COLOR = "#1e1e2f"


# 3. CORE FUNCTIONALITY (Headless Logic)

def headless_forge(source_path: str, db_name: str, verbose: bool = False):
    """CLI Entry point for automated cartridge creation."""
    if not db_name.endswith(".db"):
        db_name += ".db"

    db_path = os.path.join(STORAGE_DIR, db_name)

    print(f"[FORGE] Target Cartridge: {db_path}")
    print(f"[FORGE] Source Material: {source_path}")

    cartridge = CartridgeService(db_path)
    neural = NeuralService()
    intake = IntakeService(cartridge)
    refinery = RefineryService(cartridge, neural)

    print(">>> Phase 1: Intake (Vacuuming files...)")
    stats = intake.ingest_source(source_path)
    print(f"    Intake Result: {stats}")
    
    # Verify Manifest
    cid = cartridge.get_manifest("cartridge_id")
    print(f"    [Manifest] Cartridge ID: {cid}")

    print(">>> Phase 2: Refinery (Chunking & Weaving...)")
    while True:
        processed = refinery.process_pending(batch_size=10)
        if processed == 0:
            break
        if verbose:
            print(f"    Refined batch of {processed}...")

    print(f"[SUCCESS] Cartridge forged at {db_path}")
    return db_path


# 4. GUI LOGIC (The Workstation)

class RagForgeApp(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title(APP_TITLE)
        self.geometry("1200x800")
        self.configure(bg=BG_COLOR)

        self.neural = NeuralService()
        self.active_cartridge = None
        self.active_db_path = None
        self.refining = False

        self._apply_cyberpunk_theme()
        self._setup_ui()

    def _apply_cyberpunk_theme(self):
        """Injects the Dark/Cyberpunk visual style globally."""
        style = ttk.Style(self)

        try:
            style.theme_use("clam")
        except Exception:
            pass

        dark_bg = "#1e1e2f"
        darker_bg = "#151515"
        text_fg = "#e0e0e0"
        accent = "#007ACC"
        border = "#333344"

        # Treeviews
        style.configure(
            "Treeview",
            background=darker_bg,
            fieldbackground=darker_bg,
            foreground=text_fg,
            borderwidth=0,
            rowheight=26,
            font=("Segoe UI", 10),
        )
        style.map(
            "Treeview",
            background=[("selected", accent)],
            foreground=[("selected", "white")],
        )

        style.configure(
            "Treeview.Heading",
            background=dark_bg,
            foreground="#888",
            relief="flat",
            font=("Segoe UI", 9, "bold"),
        )
        style.map(
            "Treeview.Heading",
            background=[("active", "#2d2d44")],
        )

        # Scrollbars
        style.configure(
            "Vertical.TScrollbar",
            gripcount=0,
            background="#2d2d44",
            darkcolor=dark_bg,
            lightcolor=dark_bg,
            troughcolor=dark_bg,
            bordercolor=dark_bg,
            arrowcolor="#888",
        )
        style.map(
            "Vertical.TScrollbar",
            background=[("active", "#444"), ("disabled", dark_bg)],
        )

        # Tabs
        style.configure(
            "TNotebook",
            background=dark_bg,
            borderwidth=0,
        )
        style.configure(
            "TNotebook.Tab",
            background="#252526",
            foreground="#888",
            padding=[15, 8],
            font=("Segoe UI", 10),
        )
        style.map(
            "TNotebook.Tab",
            background=[("selected", accent)],
            foreground=[("selected", "white")],
        )

        # Panes
        style.configure("TPanedwindow", background=dark_bg)
        style.configure("Sash", background=border, handlecv_bg=border)

    def _setup_ui(self):
        self.sidebar = Sidebar(self, STORAGE_DIR, self.load_cartridge)
        self.sidebar.pack(side="left", fill="y")

        self.main_area = tk.Frame(self, bg=BG_COLOR)
        self.main_area.pack(side="right", fill="both", expand=True)

        self.notebook = ttk.Notebook(self.main_area)
        self.notebook.pack(fill="both", expand=True)

        self.tab_forge = tk.Frame(self.notebook, bg=BG_COLOR)
        self.notebook.add(self.tab_forge, text="  DATA INGESTION  ")

        self.ingest_toolbar = IngestToolbar(
            self.tab_forge,
            on_scan=self.run_scan_request,
            on_ingest=self.run_ingest_request,
        )
        self.ingest_toolbar.pack(fill="x", side="top")

        forge_panes = ttk.PanedWindow(self.tab_forge, orient="horizontal")
        forge_panes.pack(fill="both", expand=True, padx=5, pady=5)

        self.file_tree = FileTreePanel(forge_panes, None)
        forge_panes.add(self.file_tree, weight=1)

        right_col = ttk.PanedWindow(forge_panes, orient="vertical")
        forge_panes.add(right_col, weight=3)

        self.stream = ThoughtStream(right_col)
        right_col.add(self.stream, weight=3)

        self.sys_log = SystemLog(right_col)
        right_col.add(self.sys_log, weight=1)

        # Initialize the Nervous System (Telemetry)
        self.telemetry = TelemetryService(self, self.sys_log)
        self.telemetry.start()

        self.editor_panel = EditorPanel(self.notebook)
        self.notebook.add(self.editor_panel, text="  KNOWLEDGE INSPECTOR  ")

        self.graph_view = GraphView(self.notebook)
        self.notebook.add(self.graph_view, text="  NEURAL TOPOLOGY  ")

        status_frame = tk.Frame(
            self.main_area,
            bg="#101018",
            height=25,
            highlightbackground="#333",
            highlightthickness=1,
        )
        status_frame.pack(fill="x", side="bottom")
        status_frame.pack_propagate(False)

        self.status_var = tk.StringVar(value="Ready.")
        tk.Label(
            status_frame,
            textvariable=self.status_var,
            bg="#101018",
            fg="#888",
            font=("Arial", 9),
        ).pack(side="left", padx=10)

    # Remaining methods unchanged ‚Ä¶


    def load_cartridge(self, path):
        """Called when user clicks a DB in sidebar."""
        self.active_db_path = path
        self.status_var.set(f"Loaded: {os.path.basename(path)}")
        self.title(f"{APP_TITLE} - [{os.path.basename(path)}]")
        
        # 1. Init Backend for this specific cartridge
        self.active_cartridge = CartridgeService(path)
        
        # 2. Wire up panels
        new_intake = IntakeService(self.active_cartridge)
        self.file_tree.intake = new_intake
        self.editor_panel.load_db(path)
        self.sys_log.log(f"Cartridge loaded: {os.path.basename(path)}")
        
        # 3. Load Graph (Non-blocking)
        self.graph_view.bind_services(self.active_cartridge, self.neural)
        self.graph_view.load_from_db(path)
        
        # 4. Start Background Poller (The Refinery Daemon)
        if not self.refining:
            self.refining = True
            self.after(1000, self._refinery_loop)

    def run_scan_request(self, path, web_depth=0, binary_policy="Extract Text"):
        if not self.active_cartridge:
            messagebox.showwarning("No Cartridge", "Select a cartridge first.")
            return
        
        # Update Manifest with policy preference
        self.active_cartridge.set_manifest("binary_policy", binary_policy)
        self.active_cartridge.set_manifest("web_depth", web_depth)

        self.status_var.set(f"Scanning {path} (Depth: {web_depth})...")
        self.file_tree.load_tree(path, web_depth=web_depth)
        self.sys_log.log(f"Scanned source: {path}")
        self.status_var.set("Scan complete. Select files to ingest.")

    def run_ingest_request(self):
        if not self.active_cartridge:
            return
        
        files = self.file_tree.get_selected_files()
        root = self.file_tree.root_path
        
        if not files:
            messagebox.showwarning("No Files", "No files selected for ingestion.")
            return

        def worker():
            count = len(files)
            msg = f"Ingesting {count} items..."
            self.status_var.set(msg)
            self.sys_log.log(msg)
            try:
                # Access intake via file_tree which now holds the reference
                stats = self.file_tree.intake.ingest_selected(files, root)
                done_msg = f"Ingest Result: {stats}"
                self.status_var.set(done_msg)
                self.sys_log.log(done_msg)
            except Exception as e:
                err = f"Ingest Failed: {e}"
                self.sys_log.log(err)
            
            # Refresh UI elements on main thread
            def _refresh():
                self.editor_panel.refresh_list()
                self.graph_view.load_from_db(self.active_db_path)
                
            self.after(0, _refresh)

        threading.Thread(target=worker, daemon=True).start()

    def _refinery_loop(self):
        """
        The Heartbeat. Checks for RAW files and processes them in small batches.
        Keeps the UI responsive while chewing through data.
        """
        if self.active_cartridge:
            # We create a transient refinery instance to process the batch
            # Ideally, this should be persistent, but for now this works.
            refinery = RefineryService(self.active_cartridge, self.neural)
            
            try:
                # Process a small batch
                processed = refinery.process_pending(batch_size=1)
                
                if processed > 0:
                    self.status_var.set("Refining Knowledge... (Embedding & Weaving)")
                    # Update visuals occasionally
                    if processed % 5 == 0:
                        self.graph_view.load_from_db(self.active_db_path)
                        self.editor_panel.refresh_list()
                else:
                    current_status = self.status_var.get()
                    if "Refining" in current_status:
                        self.status_var.set("Refinery Idle. Cartridge up to date.")
                        self.graph_view.load_from_db(self.active_db_path)
                        self.editor_panel.refresh_list()
            except Exception as e:
                print(f"Refinery Loop Error: {e}")

        # Loop
        self.after(2000, self._refinery_loop)


# 5. CLI ENTRY POINT

def main():
    parser = argparse.ArgumentParser(description="_RagFORGE: Neural Cartridge Factory")
    
    # CLI Args for Headless Mode
    parser.add_argument("--input", "-i", type=str, help="Input source path (folder or URL)")
    parser.add_argument("--output", "-o", type=str, help="Output .db filename (e.g. 'my_brain.db')")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logs")
    
    args = parser.parse_args()

    if args.input and args.output:
        # HEADLESS MODE
        try:
            headless_forge(args.input, args.output, args.verbose)
        except Exception as e:
            print(f"[FATAL] {e}")
            sys.exit(1)
    else:
        # GUI MODE
        app = RagForgeApp()
        app.mainloop()

if __name__ == "__main__":
    main()







--------------------------------------------------------------------------------
FILE: src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: src\microservices\.ragforge.json
--------------------------------------------------------------------------------
{
  ".": false,
  "__init__.py": false,
  "base_service.py": false,
  "cartridge_service.py": false,
  "document_utils.py": false,
  "graph_engine.py": false,
  "graph_view.py": false,
  "intake_service.py": false,
  "neural_service.py": false,
  "panels.py": false,
  "refinery_service.py": false,
  "scanner.py": false,
  "semantic_chunker.py": false,
  "telemetry_service.py": false,
  "thought_stream.py": false
}
--------------------------------------------------------------------------------
FILE: src\microservices\base_service.py
--------------------------------------------------------------------------------
import logging
import sys

class BaseService:
    """
    Standard base class for all _NeoCORTEX microservices.
    Provides unified logging and error handling.
    """
    def __init__(self, service_name: str):
        self.service_name = service_name
        self.log = logging.getLogger(service_name)
        
        # Configure logging if not already set up
        if not self.log.handlers:
            handler = logging.StreamHandler(sys.stdout)
            formatter = logging.Formatter('%(asctime)s [%(name)s] %(levelname)s: %(message)s', datefmt='%H:%M:%S')
            handler.setFormatter(formatter)
            self.log.addHandler(handler)
            self.log.setLevel(logging.INFO)

    def log_info(self, msg: str):
        self.log.info(msg)

    def log_error(self, msg: str):
        self.log.error(msg)

--------------------------------------------------------------------------------
FILE: src\microservices\cartridge_service.py
--------------------------------------------------------------------------------
import sqlite3
import json
import time
import os
import uuid
import datetime
import struct
from pathlib import Path

# Try to import sqlite-vec (pip install sqlite-vec)
try:
    import sqlite_vec
except ImportError:
    sqlite_vec = None
from typing import Dict, Any, Optional, List
from .base_service import BaseService

class CartridgeService(BaseService):
    """
    The Source of Truth.
    Manages the Unified Neural Cartridge Format (UNCF v1.0).
    """
    
    SCHEMA_VERSION = "uncf_v1.0"

    def __init__(self, db_path: str):
        super().__init__("CartridgeService")
        self.db_path = Path(db_path)
        self._init_db()

    def _get_conn(self):
        # Set generous timeout (60s) for multi-threaded Ingest/Refinery contention
        conn = sqlite3.connect(self.db_path, timeout=60.0)
        if sqlite_vec:
            try:
                conn.enable_load_extension(True)
                sqlite_vec.load(conn)
                conn.enable_load_extension(False)
            except Exception as e:
                self.log_error(f"Failed to load sqlite-vec: {e}")
        return conn

    def _init_db(self):
        """Initializes the standard Schema."""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = self._get_conn()
        cursor = conn.cursor()
        
        # Enable WAL Mode: Allows concurrent Readers (Refinery) & Writers (Ingest)
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL")
        
        # 1. Manifest (The Boot Sector)
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")
        
        # 1.5 Directories (The VFS Index)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS directories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT UNIQUE NOT NULL,
                parent_path TEXT,
                metadata TEXT DEFAULT '{}'
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_dir_parent ON directories(parent_path)")

        # 2. Files (The Content Store)
        # Supports Text AND Binary (blob_data)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT NOT NULL,       -- Portable path (e.g. "src/main.py")
                origin_path TEXT,             -- Provenance (e.g. "C:/Users/...")
                origin_type TEXT,             -- 'filesystem', 'web', 'github'
                content TEXT,                 -- Text content (UTF-8)
                blob_data BLOB,               -- Binary content (Images, PDFs)
                mime_type TEXT,
                status TEXT DEFAULT 'RAW',    -- RAW, REFINED, ERROR, SKIPPED
                metadata TEXT DEFAULT '{}',   -- JSON tags, summaries
                last_updated TIMESTAMP
            )
        """)
        # Index for fast lookups by VFS path
        cursor.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_vfs ON files(vfs_path)")

        # 3. Chunks (The Vector Store)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_id INTEGER,
                chunk_index INTEGER,
                content TEXT,
                embedding BLOB,
                name TEXT,
                type TEXT,
                start_line INTEGER,
                end_line INTEGER,
                FOREIGN KEY(file_id) REFERENCES files(id)
            )
        """)

        # 3.5 Vector Index (sqlite-vec)
        # Defaulting to 1024 dimensions (mxbai-embed-large). 
        # If you use a different model, this needs to match.
        if sqlite_vec:
            try:
                cursor.execute("CREATE VIRTUAL TABLE IF NOT EXISTS vec_items USING vec0(embedding float[1024])")
            except Exception as e:
                self.log_error(f"Vector Table Init Error: {e}")

        # 4. Graph Topology (The Neural Wiring)
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, relation TEXT, weight REAL)")

        # 5. Validation Logs
        cursor.execute("CREATE TABLE IF NOT EXISTS logs (timestamp REAL, level TEXT, message TEXT, context TEXT)")
        
        conn.commit()
        conn.close()
        
        # Initialize standard keys if new
        self.initialize_manifest()

    def initialize_manifest(self):
        """Populates the boot sector with strict RagFORGE Cartridge Schema (UNCF) v1.1."""
        if not self.get_manifest("cartridge_id"):
            now = datetime.datetime.utcnow().isoformat()

            # 1. Identity & Versioning
            self.set_manifest("schema_name", "ragforge_cartridge")
            self.set_manifest("schema_version", "1.1.0")
            self.set_manifest("cartridge_id", str(uuid.uuid4()))
            self.set_manifest("created_at_utc", now)
            self.set_manifest("created_by_app", "RagFORGE")

            # 2. Provenance / Sources
            # Agents can read this to understand where the content came from and what policies were used.
            self.set_manifest("sources", [])
            self.set_manifest("source_policies", {
                "binary_policy": "Extract Text",
                "web_depth": 0
            })

            # 3. Specs (Defaults - updated by RefineryService._stamp_specs)
            self.set_manifest("embedding_spec", {
                "provider": "unknown",
                "model": "pending_init",
                "dim": 0,
                "dtype": "unknown",
                "distance": "unknown"
            })
            self.set_manifest("chunking_spec", {
                "strategy": "semantic_hybrid",
                "python_ast": True,
                "generic_window": 1500
            })

            # 4. VFS + Content Stats (populated/updated over time)
            self.set_manifest("vfs", {
                "root_label": "",
                "directories": {"count": 0},
                "files": {
                    "count": 0,
                    "by_origin_type": {},
                    "by_mime": {}
                },
                "index_built": False
            })
            self.set_manifest("content_stats", {
                "chunks": {"count": 0},
                "vector_index": {
                    "enabled": True,
                    "table": "vec_items",
                    "dims": 0
                },
                "graph": {
                    "nodes": 0,
                    "edges": 0
                }
            })

            # 5. Capabilities Contract (what an agent can assume exists / how to navigate)
            self.set_manifest("capabilities", {
                "tables": {
                    "manifest": True,
                    "directories": True,
                    "files": True,
                    "chunks": True,
                    "vec_items": True,
                    "graph_nodes": True,
                    "graph_edges": True,
                    "logs": True
                },
                "navigation": {
                    "vfs_path": "files.vfs_path",
                    "directory_index": "directories.vfs_path",
                    "list_files_query": "SELECT vfs_path, mime_type, origin_type, status FROM files ORDER BY vfs_path",
                    "list_directories_query": "SELECT vfs_path, parent_path FROM directories ORDER BY vfs_path"
                },
                "retrieval": {
                    "raw_file_content_query": "SELECT content, blob_data, mime_type FROM files WHERE vfs_path=?",
                    "chunks_by_file_query": "SELECT chunk_index, name, type, start_line, end_line, content FROM chunks WHERE file_id=? ORDER BY chunk_index",
                    "vector_search": "sqlite-vec on vec_items if available"
                }
            })

            # 6. Status & Health
            self.set_manifest("cartridge_health", "FRESH")
            self.set_manifest("ingest_complete", False)
            self.set_manifest("refine_complete", False)
            self.set_manifest("last_ingest_at_utc", "")
            self.set_manifest("last_refine_at_utc", "")
            self.set_manifest("last_error", "")
            self.set_manifest("locks", {
                "write_lock_expected": False,
                "notes": "If DB locks occur, consider batching writes and shorter-lived connections."
            })

    def set_manifest(self, key: str, value: Any):
        """Upsert metadata key."""
        conn = self._get_conn()
        val_str = json.dumps(value) if isinstance(value, (dict, list)) else str(value)
        conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", (key, val_str))
        conn.commit()
        conn.close()

    def get_manifest(self, key: str) -> Optional[str]:
        """Retrieve metadata key."""
        conn = self._get_conn()
        row = conn.execute("SELECT value FROM manifest WHERE key=?", (key,)).fetchone()
        conn.close()
        return row[0] if row else None

    def validate_cartridge(self) -> Dict[str, Any]:
        """Quality Control: Checks if the cartridge is Agent-Safe."""
        report = {"valid": True, "health": "OK", "errors": []}
        
        # 1. Check Required Keys
        # These are the minimum contract keys an agent needs to understand what it loaded.
        required = [
            "schema_name",
            "schema_version",
            "cartridge_id",
            "created_at_utc",
            "created_by_app",
            "embedding_spec",
            "chunking_spec",
            "capabilities"
        ]
        for key in required:
            if not self.get_manifest(key):
                report["valid"] = False
                report["errors"].append(f"Missing Manifest Key: {key}")
        
        # 2. Check Vector Index Presence
        conn = self._get_conn()
        try:
            # Check if vec_items table exists (sqlite-vec)
            conn.execute("SELECT count(*) FROM vec_items").fetchone()
        except Exception:
             report["errors"].append("Vector Index (vec_items) missing or not loaded.")
             # Not fatal for 'valid' but impacts capability
             report["health"] = "WARN_NO_VECTORS"
        finally:
            conn.close()
            
        return report

    def store_file(self, vfs_path: str, origin_path: str, content: str = None, blob: bytes = None, mime_type: str = "text/plain", origin_type: str = "filesystem"):
        """
        The Universal Input Method. 
        Stores raw data. If file exists, updates it and resets status to 'RAW' for re-refining.
        """
        conn = self._get_conn()
        try:
            conn.execute("""
                INSERT OR REPLACE INTO files 
                (vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, 'RAW', ?)
            """, (vfs_path, origin_path, origin_type, content, blob, mime_type, time.time()))
            conn.commit()
            return True
        except Exception as e:
            self.log_error(f"DB Store Error ({vfs_path}): {e}")
            return False
        finally:
            conn.close()

    def get_pending_files(self, limit: int = 10) -> List[Dict]:
        """Fetches files waiting for the Refinery."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        rows = conn.execute("SELECT * FROM files WHERE status = 'RAW' LIMIT ?", (limit,)).fetchall()
        conn.close()
        return [dict(row) for row in rows]

    def update_status(self, file_id: int, status: str, metadata: dict = None):
        conn = self._get_conn()
        if metadata:
            conn.execute("UPDATE files SET status = ?, metadata = ? WHERE id = ?", 
                         (status, json.dumps(metadata), file_id))
        else:
            conn.execute("UPDATE files SET status = ? WHERE id = ?", (status, file_id))
        conn.commit()
        conn.close()

    def ensure_directory(self, vfs_path: str):
        """Idempotent insert for VFS directories."""
        if not vfs_path: return
        parent = os.path.dirname(vfs_path).replace("\\", "/")
        if parent == vfs_path: parent = "" # Root case
        
        conn = self._get_conn()
        try:
            conn.execute("INSERT OR IGNORE INTO directories (vfs_path, parent_path) VALUES (?, ?)", (vfs_path, parent))
            conn.commit()
        except: pass
        finally:
            conn.close()

    # --- Agent-Friendly Helpers (No raw SQL required) ---
    def _coerce_bool(self, v: Any) -> bool:
        """Best-effort conversion for manifest values stored as strings."""
        if v is None:
            return False
        if isinstance(v, bool):
            return v
        s = str(v).strip().lower()
        return s in ("1", "true", "yes", "y", "on")

    def get_status_flags(self) -> Dict[str, Any]:
        """Returns key manifest status flags in a single call."""
        ingest_complete = self._coerce_bool(self.get_manifest("ingest_complete"))
        refine_complete = self._coerce_bool(self.get_manifest("refine_complete"))
        health = self.get_manifest("cartridge_health") or "UNKNOWN"
        return {
            "ingest_complete": ingest_complete,
            "refine_complete": refine_complete,
            "cartridge_health": health,
            "schema_name": self.get_manifest("schema_name") or "",
            "schema_version": self.get_manifest("schema_version") or "",
            "cartridge_id": self.get_manifest("cartridge_id") or ""
        }

    def list_files(self, prefix: str = "", status: Optional[str] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Enumerate files in the cartridge (optionally filtered by VFS prefix and/or status)."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            sql = "SELECT id, vfs_path, origin_path, origin_type, mime_type, status, last_updated, metadata FROM files"
            clauses = []
            params = []

            if prefix:
                # Prefix match on portable path
                clauses.append("vfs_path LIKE ?")
                params.append(prefix.rstrip("/") + "/%")

            if status:
                clauses.append("status = ?")
                params.append(status)

            if clauses:
                sql += " WHERE " + " AND ".join(clauses)

            sql += " ORDER BY vfs_path"

            if limit is not None:
                sql += " LIMIT ?"
                params.append(int(limit))

            rows = conn.execute(sql, tuple(params)).fetchall()
            out = []
            for r in rows:
                d = dict(r)
                # metadata is stored as JSON string
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    def get_file_record(self, vfs_path: str) -> Optional[Dict[str, Any]]:
        """Fetch a single file record by VFS path."""
        if not vfs_path:
            return None
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            row = conn.execute(
                "SELECT id, vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, metadata, last_updated FROM files WHERE vfs_path = ?",
                (vfs_path,)
            ).fetchone()
            if not row:
                return None
            d = dict(row)
            try:
                d["metadata"] = json.loads(d.get("metadata") or "{}")
            except Exception:
                d["metadata"] = {}
            return d
        finally:
            conn.close()

    def list_directories(self, prefix: str = "") -> List[Dict[str, Any]]:
        """Enumerate directories in the cartridge VFS."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            if prefix:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories WHERE vfs_path LIKE ? ORDER BY vfs_path",
                    (prefix.rstrip("/") + "/%",)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories ORDER BY vfs_path"
                ).fetchall()

            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    def get_directory_tree(self, root: str = "") -> Dict[str, Any]:
        """Builds a nested directory tree starting at `root` ("" for full tree)."""
        dirs = self.list_directories(prefix=root) if root else self.list_directories()
        files = self.list_files(prefix=root) if root else self.list_files()

        # Tree nodes are dicts: {"_dirs": {name: node}, "_files": [file_records...]}
        def new_node():
            return {"_dirs": {}, "_files": []}

        tree = new_node()

        # Insert directories
        for d in dirs:
            path = (d.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            cur = tree
            for p in parts:
                cur = cur["_dirs"].setdefault(p, new_node())

        # Insert files
        for f in files:
            path = (f.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            fname = parts[-1]
            cur = tree
            for p in parts[:-1]:
                cur = cur["_dirs"].setdefault(p, new_node())
            # Store a light file record for tree browsing
            cur["_files"].append({
                "name": fname,
                "vfs_path": f.get("vfs_path"),
                "mime_type": f.get("mime_type"),
                "origin_type": f.get("origin_type"),
                "status": f.get("status")
            })

        return tree

    def get_status_summary(self) -> Dict[str, Any]:
        """Counts files by status and provides a quick cartridge overview."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            rows = conn.execute("SELECT status, COUNT(*) as n FROM files GROUP BY status").fetchall()
            by_status = {r["status"]: r["n"] for r in rows}

            dcnt = conn.execute("SELECT COUNT(*) FROM directories").fetchone()[0]
            fcnt = conn.execute("SELECT COUNT(*) FROM files").fetchone()[0]
            ccnt = conn.execute("SELECT COUNT(*) FROM chunks").fetchone()[0]
            ncnt = conn.execute("SELECT COUNT(*) FROM graph_nodes").fetchone()[0]
            ecnt = conn.execute("SELECT COUNT(*) FROM graph_edges").fetchone()[0]

            return {
                "directories": int(dcnt),
                "files": int(fcnt),
                "chunks": int(ccnt),
                "graph_nodes": int(ncnt),
                "graph_edges": int(ecnt),
                "files_by_status": by_status,
                "flags": self.get_status_flags()
            }
        finally:
            conn.close()

    # --- Graph Helpers ---
    def add_node(self, node_id: str, node_type: str, label: str, data: dict = None):
        conn = self._get_conn()
        conn.execute("INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json) VALUES (?, ?, ?, ?)",
                     (node_id, node_type, label, json.dumps(data or {})))
        conn.commit()
        conn.close()

    def add_edge(self, source: str, target: str, relation: str = "related", weight: float = 1.0):
        conn = self._get_conn()
        conn.execute("INSERT OR IGNORE INTO graph_edges (source, target, relation, weight) VALUES (?, ?, ?, ?)",
                     (source, target, relation, weight))
        conn.commit()
        conn.close()

    # --- Vector Search ---
    def search_embeddings(self, query_vector: List[float], limit: int = 5) -> List[Dict]:
        """Performs semantic search using sqlite-vec."""
        if not sqlite_vec or not query_vector:
            return []

        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        results = []
        
        try:
            # Pack vector to binary if needed, but sqlite-vec usually handles raw lists in parameterized queries
            # dependent on the binding. We'll pass binary for safety if using standard bindings,
            # but typically raw list works with the extension's adapters. 
            # For now, we assume the extension handles the list->vector conversion.
            
            rows = conn.execute("""
                SELECT
                    rowid,
                    distance
                FROM vec_items
                WHERE embedding MATCH ?
                ORDER BY distance
                LIMIT ?
            """, (json.dumps(query_vector), limit)).fetchall()
            
            # Resolve back to chunks with VFS context
            for r in rows:
                chunk_id = r['rowid']
                # Join with files to get vfs_path
                query = """
                    SELECT c.*, f.vfs_path 
                    FROM chunks c 
                    JOIN files f ON c.file_id = f.id 
                    WHERE c.id=?
                """
                chunk = conn.execute(query, (chunk_id,)).fetchone()
                
                if chunk:
                    res = dict(chunk)
                    res['score'] = r['distance']
                    results.append(res)
                    
        except Exception as e:
            self.log_error(f"Vector Search Error: {e}")
        finally:
            conn.close()
            
        return results





--------------------------------------------------------------------------------
FILE: src\microservices\document_utils.py
--------------------------------------------------------------------------------
import io
import re
from typing import Tuple, Optional

# Third-party imports (from requirements.txt)
try:
    from pypdf import PdfReader
except ImportError:
    PdfReader = None

try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

def extract_text_from_pdf(file_bytes: bytes) -> str:
    """Extracts text from a PDF blob using pypdf."""
    if not PdfReader:
        return ""
    
    text_content = []
    try:
        # Wrap bytes in a stream for PdfReader
        stream = io.BytesIO(file_bytes)
        reader = PdfReader(stream)
        
        for page in reader.pages:
            extracted = page.extract_text()
            if extracted:
                text_content.append(extracted)
        
        return "\n".join(text_content)
    except Exception as e:
        print(f"[DocumentUtils] PDF Extraction Error: {e}")
        return ""

def extract_text_from_html(html_content: str) -> str:
    """Cleans HTML to raw text using BeautifulSoup."""
    if not BeautifulSoup:
        return strip_tags_regex(html_content)
    
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Kill all script and style elements
        for script in soup(["script", "style", "meta", "noscript"]):
            script.decompose()
            
        text = soup.get_text()
        
        # Collapse whitespace
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = '\n'.join(chunk for chunk in chunks if chunk)
        
        return text
    except Exception as e:
        print(f"[DocumentUtils] HTML Parsing Error: {e}")
        return strip_tags_regex(html_content)

def strip_tags_regex(html: str) -> str:
    """Fallback if BS4 is missing."""
    clean = re.compile('<.*?>')
    return re.sub(clean, '', html)

--------------------------------------------------------------------------------
FILE: src\microservices\graph_engine.py
--------------------------------------------------------------------------------
import pygame
import math
import random

# Initialize font module globally once
pygame.font.init()

class GraphRenderer:
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        self.width = width
        self.height = height
        self.bg_color = bg_color
        
        self.surface = pygame.Surface((width, height))
        
        # Camera
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction
        self.dragged_node_idx = None
        self.hovered_node_idx = None
        
        # Physics State
        self.settled = False

    def resize(self, width, height):
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    def set_data(self, nodes, links):
        self.nodes = nodes
        self.links = links
        self.settled = False # Wake up physics on new data
        
        # 1. Build an ID map so we can find parents
        node_map = {node['id']: node for node in self.nodes}

        for n in self.nodes:
            # GNN Injection: Use pre-calculated layout if available
            if 'gnn_x' in n and 'gnn_y' in n:
                n['x'] = n['gnn_x'] * self.width
                n['y'] = n['gnn_y'] * self.height

            elif 'x' not in n:
                # SMART SPAWN: If I am a satellite, spawn near my planet
                parent_id = n.get('meta', {}).get('parent')
                if parent_id and parent_id in node_map and 'x' in node_map[parent_id]:
                    p = node_map[parent_id]
                    angle = random.random() * 6.28
                    dist = 30
                    n['x'] = p['x'] + math.cos(angle) * dist
                    n['y'] = p['y'] + math.sin(angle) * dist
                else:
                    # Random spawn for Files
                    n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                    n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Semantic Coloring
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # Blue
                n['_radius'] = 6
            elif n.get('type') == 'web':
                n['_color'] = (204, 0, 122) # Purple/Pink
                n['_radius'] = 7
            elif n.get('type') == 'chunk':
                n['_color'] = (100, 200, 100) # Satellite Green
                n['_radius'] = 3
            else:
                n['_color'] = (160, 32, 240) # Default
                n['_radius'] = 6

    # --- INPUT HANDLING ---
    
    def screen_to_world(self, sx, sy):
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def get_node_at(self, sx, sy):
        wx, wy = self.screen_to_world(sx, sy)
        for n in self.nodes:
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                return n
        return None

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                self.dragged_node_idx = i
                self.settled = False # Wake up physics
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
            self.settled = False
        else:
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            return prev_hover != self.hovered_node_idx

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    def highlight_nodes(self, node_ids):
        """Highlights specific nodes by ID."""
        for n in self.nodes:
            # 1. Reset to defaults
            if n.get('type') == 'file': 
                n['_color'] = (0, 122, 204)
                n['_radius'] = 6
            elif n.get('type') == 'web': 
                n['_color'] = (204, 0, 122)
                n['_radius'] = 7
            elif n.get('type') == 'chunk': 
                n['_color'] = (100, 200, 100)
                n['_radius'] = 3
            else: 
                n['_color'] = (160, 32, 240)
                n['_radius'] = 6
                
            # 2. Apply Highlight
            if n['id'] in node_ids:
                n['_color'] = (255, 255, 0) # Bright Yellow
                n['_radius'] = 12
                
        self.settled = False # Wake up physics

    # --- PHYSICS (Damped) ---

    def step_physics(self):
        if not self.nodes or self.settled: return

        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.85 # Increased damping to settle faster
        
        cx, cy = self.width / 2, self.height / 2
        total_kinetic_energy = 0

        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue

            # LOD: Freeze satellites if zoomed out
            if self.zoom < 1.2 and a.get('type') == 'chunk':
                a['vx'] = 0
                a['vy'] = 0
                continue
            
            fx, fy = 0, 0
            
            # 1. Gravity (Center pull)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # 2. Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Performance opt: Ignore far away nodes
                if dist_sq > 25000: continue 

                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 3. Attraction (Links)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 4. Apply & Measure Energy
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']
            total_kinetic_energy += (abs(n['vx']) + abs(n['vy']))

        # 5. Sleep Threshold
        if total_kinetic_energy < 0.5:
            self.settled = True

    # --- RENDERING ---

    def get_image_bytes(self):
        self.surface.fill(self.bg_color)
        
        cx, cy = self.width / 2, self.height / 2
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # Links
        for u, v in self.links:
            if self.zoom < 1.2:
                if self.nodes[u].get('type') == 'chunk' or self.nodes[v].get('type') == 'chunk':
                    continue

            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # Nodes
        for i, n in enumerate(self.nodes):
            # LOD: Hide chunks if zoomed out
            if self.zoom < 1.2 and n.get('type') == 'chunk':
                continue

            sx, sy = to_screen(n['x'], n['y'])
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20: continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')



--------------------------------------------------------------------------------
FILE: src\microservices\graph_view.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import json
import os
from .graph_engine import GraphRenderer

class GraphView(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.pack(fill="both", expand=True)
        
        # Search Overlay
        self.controls = tk.Frame(self, bg="#101018")
        self.controls.pack(fill="x", side="top", padx=5, pady=5)
        
        self.entry_search = tk.Entry(self.controls, bg="#252526", fg="white", insertbackground="white", font=("Consolas", 10))
        self.entry_search.pack(side="left", fill="x", expand=True, padx=(0, 5))
        self.entry_search.bind("<Return>", self.run_search)
        
        btn = tk.Button(self.controls, text="NEURAL TEST", command=self.run_search, bg="#007ACC", fg="white", relief="flat")
        btn.pack(side="right")

        # UI Container
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Services
        self.cartridge = None
        self.neural = None
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None 
        
        # Input State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        self.is_panning = False

        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<Double-Button-1>', self.on_double_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Start the Heartbeat
        self.animate()

    def bind_services(self, cartridge, neural):
        self.cartridge = cartridge
        self.neural = neural

    def run_search(self, event=None):
        if not self.cartridge or not self.neural:
            return
            
        query = self.entry_search.get().strip()
        if not query: return
        
        # 1. Embed
        vec = self.neural.get_embedding(query)
        if not vec: return
        
        # 2. Search
        results = self.cartridge.search_embeddings(vec, limit=5)
        
        # 3. Resolve IDs for Graph
        # Graph Node ID format: "{vfs_path}::{chunk_name}"
        ids = set()
        for r in results:
            if 'vfs_path' in r and 'name' in r:
                ids.add(f"{r['vfs_path']}::{r['name']}")
                
        # 4. Highlight
        self.engine.highlight_nodes(ids)

    def load_from_db(self, db_path):
        """
        Loads graph data from SQLite.
        Does NOT block the UI. The physics engine will settle the nodes frame-by-frame.
        """
        if not os.path.exists(db_path): return
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # [cite_start]Fetch Nodes [cite: 198]
            db_nodes = cursor.execute("SELECT id, type, label, data_json FROM graph_nodes").fetchall()
            
            # [cite_start]Fetch Edges [cite: 198]
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
            
            conn.close()
        except Exception as e:
            print(f"Graph Load Error: {e}")
            return

        # Format for Engine
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label, raw_json = row
            meta = {}
            try:
                if raw_json: meta = json.loads(raw_json)
            except: pass
            
            id_to_index[node_id] = idx
            formatted_nodes.append({'id': node_id, 'type': n_type, 'label': label, 'meta': meta})

        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                formatted_links.append((id_to_index[src], id_to_index[tgt]))

        # Inject Data - The Physics Engine handles the "Explosion" logic internally
        self.engine.set_data(formatted_nodes, formatted_links)

    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_double_click(self, event):
        # Zoom in on the node we clicked
        hit_node = self.engine.get_node_at(event.x, event.y)
        if hit_node:
            # Center camera on node and zoom in
            self.engine.cam_x = hit_node['x']
            self.engine.cam_y = hit_node['y']
            self.engine.zoom = 2.0
            self.engine.settled = False

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        if hit:
            self.is_dragging_node = True
        else:
            self.is_panning = True

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False
        self.is_panning = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        elif self.is_panning:
            # Camera Pan
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)
        self.engine.settled = False # Wake up physics on zoom

    def on_windows_scroll(self, event):
        if event.delta > 0: self.on_zoom(1.1)
        else: self.on_zoom(0.9)

    def animate(self):
        """
        The Heartbeat Loop. 
        Runs at ~30 FPS. Handles Physics + Rendering.
        """
        # 1. Step Physics (Micro-calculations)
        self.engine.step_physics()
        
        # 2. Render to Buffer
        raw_data = self.engine.get_image_bytes()
        
        # 3. Blit to Screen
        if raw_data:
            img = Image.frombytes('RGB', (self.engine.width, self.engine.height), raw_data)
            self.photo = ImageTk.PhotoImage(img)
            self.canvas_lbl.configure(image=self.photo)
        
        # 4. Loop
        self.after(30, self.animate)

--------------------------------------------------------------------------------
FILE: src\microservices\intake_service.py
--------------------------------------------------------------------------------
import os
import mimetypes
import requests
import fnmatch
import json
from pathlib import Path
from typing import Dict, Set, List, Any
from .base_service import BaseService
from .cartridge_service import CartridgeService
from .scanner import ScannerMS
from . import document_utils

# Optional import for Web
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

class IntakeService(BaseService):
    """
    The Vacuum. 
    Now supports two-phase ingestion:
    1. Scan -> Build Tree (with .gitignore respect)
    2. Ingest -> Process selected paths
    """

    DEFAULT_IGNORE_DIRS = {
        '.git', '__pycache__', 'node_modules', 'venv', '.venv', 'env', '.env', 
        '.idea', '.vscode', 'dist', 'build', 'target', 'bin', 'obj', 
        '__cartridge__'
    }
    
    DEFAULT_IGNORE_EXTS = {
        '.pyc', '.pyd', '.exe', '.dll', '.so', '.db', '.sqlite', '.sqlite3', 
        '.bin', '.iso', '.img', '.zip', '.tar', '.gz', '.7z', '.jpg', '.png'
    }

    def __init__(self, cartridge: CartridgeService):
        super().__init__("IntakeService")
        self.cartridge = cartridge
        self.ignore_patterns: Set[str] = set()

    def ingest_source(self, source_path: str) -> Dict[str, int]:
        """Headless/CLI Entry point: Scans and Ingests in one go."""
        self.cartridge.initialize_manifest()
        
        # Update Manifest source info
        self.cartridge.set_manifest("source_root", source_path)
        
        is_web = source_path.startswith("http")
        self.cartridge.set_manifest("source_type", "web_root" if is_web else "filesystem_dir")

        scanner = ScannerMS()
        tree_node = scanner.scan_directory(source_path, web_depth=1 if is_web else 0)
        
        if not tree_node:
             return {"error": "Source not found"}

        # Flatten tree to list of paths
        files_to_ingest = scanner.flatten_tree(tree_node)
        self.cartridge.set_manifest("ingest_config", {"auto_flattened": True, "count": len(files_to_ingest)})
        
        return self.ingest_selected(files_to_ingest, source_path)

    # --- PHASE 1: SCANNING ---

    def scan_path(self, root_path: str, web_depth: int = 0) -> Dict[str, Any]:
        """
        Unified Scanner Interface.
        Delegates to ScannerMS for both Web and Local FS to ensure consistent node structure.
        """
        scanner = ScannerMS()

        # 1. Delegate to Scanner
        tree_root = scanner.scan_directory(root_path, web_depth=web_depth)
        if not tree_root: return None

        # 2. Apply Persistence / Checked State
        # (We only do this for FS usually, but we can try for web if we had it)
        if not root_path.startswith("http"):
            saved_config = self._load_persistence(os.path.abspath(root_path))
            self._apply_persistence(tree_root, saved_config)
    
        return tree_root

    def _apply_persistence(self, node: Dict, saved_config: Dict):
        """Recursively applies checked state from saved config."""
        if 'rel_path' in node and node['rel_path'] in saved_config:
            node['checked'] = saved_config[node['rel_path']]
        elif 'children' in node:
            # Default check all if no config? Or check logic from before?
            pass
    
        if 'children' in node:
            for child in node['children']:
                self._apply_persistence(child, saved_config)

    def _scan_recursive(self, current_path: str, root_path: str, saved_config: Dict) -> Dict:
        name = os.path.basename(current_path)
        is_dir = os.path.isdir(current_path)
        rel_path = os.path.relpath(current_path, root_path).replace("\\", "/")
        
        node = {
            'name': name,
            'path': current_path,
            'rel_path': rel_path,
            'type': 'dir' if is_dir else 'file',
            'children': [],
            'checked': True
        }

        # Determine Check State
        if saved_config and rel_path in saved_config:
            # Respect user persistence
            node['checked'] = saved_config[rel_path]
        elif self._is_ignored(name) or (not is_dir and self._is_binary_ext(name)):
            # Default to unchecked if ignored
            node['checked'] = False

        if is_dir:
            try:
                with os.scandir(current_path) as it:
                    entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                    for entry in entries:
                        child = self._scan_recursive(entry.path, root_path, saved_config)
                        node['children'].append(child)
            except PermissionError:
                pass
        
        return node

    # --- PHASE 2: INGESTION ---

    def ingest_selected(self, file_list: List[str], root_path: str) -> Dict[str, int]:
        """Ingests only the specific files passed in the list."""
        stats = {"added": 0, "skipped": 0, "errors": 0}
        
        for file_path in file_list:
            try:
                # Calculate VFS Path
                try:
                    vfs_path = os.path.relpath(file_path, root_path).replace("\\", "/")
                except ValueError:
                    vfs_path = os.path.basename(file_path)

                self._read_and_store(Path(file_path), vfs_path, "filesystem", stats)
            except Exception as e:
                self.log_error(f"Error ingesting {file_path}: {e}")
                stats["errors"] += 1
        
        # --- POST-INGESTION: Update Manifest ---
        self._rebuild_directory_index()
        
        return stats

    def _rebuild_directory_index(self):
        """
        Scans 'files' table and populates 'directories' table.
        This creates the navigable VFS structure.
        """
        self.log_info("Rebuilding VFS Directory Index...")
        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            seen_dirs = set()
            
            for r in rows:
                path = r[0]
                # Walk up the path to register all parents
                current = os.path.dirname(path).replace("\\", "/")
                while current and current != "." and current not in seen_dirs:
                    self.cartridge.ensure_directory(current)
                    seen_dirs.add(current)
                    current = os.path.dirname(current).replace("\\", "/")
            
        except Exception as e:
            self.log_error(f"Directory Index Error: {e}")
        finally:
            conn.close()

    # --- HELPERS ---

    def _load_persistence(self, root_path: str) -> Dict[str, bool]:
        """Loads config from DB Manifest (Portable) or fallback to local."""
        # 1. Try DB Manifest
        try:
            conn = self.cartridge._get_conn()
            row = conn.execute("SELECT value FROM manifest WHERE key='ingest_config'").fetchone()
            conn.close()
            if row:
                return json.loads(row[0])
        except: pass
        
        # 2. Fallback to local (Legacy)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        if os.path.exists(cfg_path):
            try:
                with open(cfg_path, 'r') as f: return json.load(f)
            except: pass
        return {}

    def save_persistence(self, root_path: str, checked_map: Dict[str, bool]):
        """Saves user selections into the Cartridge Manifest (Portable)."""
        # 1. Save to DB
        try:
            conn = self.cartridge._get_conn()
            conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", 
                         ("ingest_config", json.dumps(checked_map)))
            conn.commit()
            conn.close()
        except Exception as e:
            self.log_error(f"Failed to save persistence to DB: {e}")

        # 2. Save local backup (Optional, keeps scan state if DB is deleted)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        try:
            with open(cfg_path, 'w') as f: json.dump(checked_map, f, indent=2)
        except: pass

    def _load_gitignore(self, root_path: str):
        gitignore_path = os.path.join(root_path, '.gitignore')
        self.ignore_patterns = self.DEFAULT_IGNORE_DIRS.copy()
        if os.path.exists(gitignore_path):
            try:
                with open(gitignore_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            if line.endswith('/'): line = line[:-1]
                            self.ignore_patterns.add(line)
            except: pass

    def _is_ignored(self, name: str) -> bool:
        if name in self.ignore_patterns: return True
        for pattern in self.ignore_patterns:
            if fnmatch.fnmatch(name, pattern): return True
        return False

    def _is_binary_ext(self, name: str) -> bool:
        _, ext = os.path.splitext(name)
        return ext.lower() in self.DEFAULT_IGNORE_EXTS

    def _read_and_store(self, real_path: Path, vfs_path: str, origin_type: str, stats: Dict):
        mime_type, _ = mimetypes.guess_type(real_path)
        if not mime_type: mime_type = "application/octet-stream"
        
        content = None
        blob = None
        
        # 1. Try Binary Read First (Covers PDF/Images/Safe Read)
        try:
            with open(real_path, 'rb') as f:
                blob = f.read()
        except Exception as e:
            self.log_error(f"Read error {real_path}: {e}")
            stats["errors"] += 1
            return

        # 2. Text Extraction / Decoding Strategy
        lower_path = str(real_path).lower()
        
        if lower_path.endswith(".pdf"):
            # PDF: Extract text, keep blob
            content = document_utils.extract_text_from_pdf(blob)
            if not content: mime_type = "application/pdf" # Fallback if extraction fails
            
        elif lower_path.endswith(".html") or lower_path.endswith(".htm"):
            # HTML: Decode and Clean
            try:
                raw_text = blob.decode('utf-8', errors='ignore')
                content = document_utils.extract_text_from_html(raw_text)
            except: pass
            
        else:
            # Default: Try UTF-8 Decode
            try:
                content = blob.decode('utf-8')
            except UnicodeDecodeError:
                content = None # Leave as binary blob

        # 3. Store in Cartridge
        # If content is set, it will be chunked/indexed. If only blob, it's stored but skipped by refinery.
        success = self.cartridge.store_file(
            vfs_path, 
            str(real_path), 
            content=content, 
            blob=blob, 
            mime_type=mime_type, 
            origin_type=origin_type
        )
        
        if success: stats["added"] += 1
        else: stats["errors"] += 1
--------------------------------------------------------------------------------
FILE: src\microservices\neural_service.py
--------------------------------------------------------------------------------
import requests
import json
import concurrent.futures
from typing import Optional, Dict, Any, List
from .base_service import BaseService

# Configuration constants
OLLAMA_API_URL = "http://localhost:11434/api"

class NeuralService(BaseService):
    def __init__(self, max_workers: int = 4):
        super().__init__("NeuralService")
        self.max_workers = max_workers
        # Default configs
        self.config = {
            "fast": "qwen2.5-coder:1.5b-cpu",
            "smart": "qwen2.5:3b-cpu",
            "embed": "mxbai-embed-large:latest-cpu"
        }

    def update_models(self, fast_model: str, smart_model: str, embed_model: str):
        """Called by the UI Settings Modal to change models on the fly."""
        self.config["fast"] = fast_model
        self.config["smart"] = smart_model
        self.config["embed"] = embed_model
        self.log_info(f"Models Updated: Fast={fast_model}, Smart={smart_model}")

    def get_available_models(self) -> List[str]:
        """Fetches list from Ollama for the UI dropdown."""
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            if res.status_code == 200:
                return [m['name'] for m in res.json().get('models', [])]
        except:
            return []
        return []

    def check_connection(self) -> bool:
        """Pings Ollama to see if it's alive."""
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except requests.RequestException:
            self.log_error("Ollama connection failed. Is 'ollama serve' running?")
            return False

    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Generates a vector using the CPU embedder."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.config["embed"], "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            self.log_error(f"Embedding failed: {e}")
        return None

    def request_inference(self, prompt: str, tier: str = "fast", format_json: bool = False) -> str:
        """
        Synchronous inference request.
        tier: 'fast' (1.5b-cpu), 'smart' (3b-cpu), or 'architect' (7b-gpu)
        """
        model = self.config.get(tier, self.config["fast"])
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        if format_json:
            payload["format"] = "json"

        try:
            res = requests.post(f"{OLLAMA_API_URL}/generate", json=payload, timeout=60)
            if res.status_code == 200:
                return res.json().get("response", "").strip()
        except Exception as e:
            self.log_error(f"Inference ({tier}) failed: {e}")
        return ""

    def process_parallel(self, items: List[Any], worker_func) -> List[Any]:
        """
        Helper to run a function across many items using the ThreadPool.
        Useful for batch ingestion.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # worker_func should take a single item and return a result
            futures = {executor.submit(worker_func, item): item for item in items}
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    self.log_error(f"Worker task failed: {e}")
        return results


--------------------------------------------------------------------------------
FILE: src\microservices\panels.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, filedialog, messagebox, scrolledtext, simpledialog
import os
import time
import sqlite3
import threading
import time

# --- UI CONSTANTS ---
BG_COLOR = "#1e1e2f"
SIDEBAR_COLOR = "#171725"
ACCENT_COLOR = "#007ACC"
TEXT_COLOR = "#e0e0e0"
SUCCESS_COLOR = "#388E3C"

class SystemLog(tk.Frame):
    """A read-only scrolling log for system events."""
    def __init__(self, parent):
        super().__init__(parent, bg=BG_COLOR)
        tk.Label(self, text="SYSTEM LOG", bg=BG_COLOR, fg="#666", font=("Consolas", 9, "bold")).pack(anchor="w", padx=5, pady=(5,0))
        self.text = scrolledtext.ScrolledText(self, bg="#151515", fg="#00FF00", font=("Consolas", 9), height=8, bd=0)
        self.text.pack(fill="both", expand=True, padx=5, pady=5)
        self.text.config(state="disabled")

    def log(self, msg):
        self.text.config(state="normal")
        ts = time.strftime("%H:%M:%S")
        self.text.insert("end", f"[{ts}] {msg}\n")
        self.text.see("end")
        self.text.config(state="disabled")

class IngestToolbar(tk.Frame):
    """Top bar for selecting source and triggering actions."""
    def __init__(self, parent, on_scan, on_ingest):
        super().__init__(parent, bg=BG_COLOR, pady=5)
        self.on_scan = on_scan
        self.on_ingest = on_ingest
        self.path_var = tk.StringVar()

        # Label
        tk.Label(self, text="SOURCE:", bg=BG_COLOR, fg="#888", font=("Arial", 9, "bold")).pack(side="left", padx=(10, 5))
        
        # Entry
        self.entry = tk.Entry(self, textvariable=self.path_var, bg="#252526", fg="white", 
                              insertbackground="white", relief="flat", font=("Consolas", 10))
        self.entry.pack(side="left", fill="x", expand=True, padx=5, ipady=4)

        # Options
        tk.Label(self, text="Depth:", bg=BG_COLOR, fg="#666", font=("Arial", 8)).pack(side="left")
        self.spin_depth = tk.Spinbox(self, from_=0, to=5, width=3, bg="#333", fg="white", relief="flat")
        self.spin_depth.pack(side="left", padx=2)
        
        self.combo_policy = ttk.Combobox(self, values=["Extract Text", "Store Blob", "Skip Binary"], width=12, state="readonly")
        self.combo_policy.set("Extract Text")
        self.combo_policy.pack(side="left", padx=5)

        # Buttons
        btn_cfg = {"bg": "#444", "fg": "white", "relief": "flat", "padx": 10, "font": ("Arial", 9)}
        
        tk.Button(self, text="üìÑ File", command=self._browse_file, **btn_cfg).pack(side="left", padx=2)
        tk.Button(self, text="üìÇ Folder", command=self._browse_folder, **btn_cfg).pack(side="left", padx=2)
        
        # Separator
        tk.Frame(self, width=1, bg="#555").pack(side="left", fill="y", padx=10, pady=5)

        tk.Button(self, text="üîç SCAN", command=self._trigger_scan, **btn_cfg).pack(side="left", padx=2)
        
        # Ingest (Green)
        ing_cfg = btn_cfg.copy()
        ing_cfg["bg"] = SUCCESS_COLOR
        ing_cfg["font"] = ("Arial", 9, "bold")
        tk.Button(self, text="‚ñ∂ INGEST", command=self._trigger_ingest, **ing_cfg).pack(side="left", padx=(10, 10))

    def _browse_folder(self):
        d = filedialog.askdirectory()
        if d:
            self.path_var.set(d)
            self._trigger_scan()

    def _browse_file(self):
        f = filedialog.askopenfilename()
        if f:
            self.path_var.set(f)
            self._trigger_scan()

    def _trigger_scan(self):
        path = self.path_var.get().strip()
        try:
            depth = int(self.spin_depth.get())
        except: depth = 0
        
        policy = self.combo_policy.get()
        if path: self.on_scan(path, web_depth=depth, binary_policy=policy)

    def _trigger_ingest(self):
        self.on_ingest()


class FileTreePanel(tk.Frame):
    """Hierarchy Explorer with Checkboxes."""
    def __init__(self, parent, intake_service):
        super().__init__(parent, bg=BG_COLOR)
        self.intake = intake_service
        self.root_path = None
        self.node_map = {}
        
        # --- UI ASSETS: Programmatic Icons ---
        # Create 16x16 checkboxes using empty PhotoImages and coloring pixels (or simple rectangles)
        # 1. Unchecked (Gray outline)
        self.img_off = tk.PhotoImage(width=16, height=16)
        self.img_off.put(("#666",), to=(2, 2, 14, 14))   # Border
        self.img_off.put(("#1e1e2f",), to=(4, 4, 12, 12)) # Center (BG Color)
        
        # 2. Checked (Green Fill)
        self.img_on = tk.PhotoImage(width=16, height=16)
        self.img_on.put(("#388E3C",), to=(2, 2, 14, 14)) # Green Box
        self.img_on.put(("#ffffff",), to=(5, 7, 7, 12))  # Checkmark (simple L shape)
        self.img_on.put(("#ffffff",), to=(7, 10, 11, 7)) 

        # Header
        tk.Label(self, text="HIERARCHY EXPLORER", bg=BG_COLOR, fg="#666", 
                 font=("Consolas", 9, "bold")).pack(anchor="w", padx=5, pady=(5,0))

        # Tree (Inherits 'Treeview' style from app.py)
        self.tree = ttk.Treeview(self, show="tree", selectmode="none")
        self.tree.pack(fill="both", expand=True, padx=5, pady=5)
        self.tree.bind("<Button-1>", self._on_tree_click)
        
        sb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        sb.place(relx=1, rely=0, relheight=1, anchor="ne")
        self.tree.configure(yscrollcommand=sb.set)

    def load_tree(self, path, web_depth=0):
        self.root_path = path
        self.tree.delete(*self.tree.get_children())
        self.node_map = {}
        # Pass web_depth to scanner
        tree_data = self.intake.scan_path(path, web_depth=web_depth)
        if tree_data:
            self._insert_node("", tree_data)

    def _insert_node(self, parent_id, node):
        # Use the image property for the checkbox, keep text clean for the name
        img = self.img_on if node['checked'] else self.img_off
        
        item_id = self.tree.insert(parent_id, "end", text=f" {node['name']}", image=img, open=(parent_id==""))
        self.node_map[item_id] = node
        
        # Safely handle children
        children = node.get('children', [])
        for child in children:
            self._insert_node(item_id, child)

    def _on_tree_click(self, event):
        item_id = self.tree.identify_row(event.y)
        if not item_id: return
        
        # Identify what part of the item was clicked
        element = self.tree.identify_element(event.x, event.y)
        
        # Case 1: Clicked the Checkbox (Image) -> Toggle Selection
        if element == "image":
            self._toggle_check(item_id)
            return "break"
            
        # Case 2: Clicked the Row/Text -> Toggle Expand (if folder)
        else:
            # If it has children, toggle the open state
            if self.tree.get_children(item_id) or self.node_map[item_id]['type'] in ['dir', 'folder']:
                current_state = self.tree.item(item_id, "open")
                self.tree.item(item_id, open=not current_state)
                return "break"
            # If file, do nothing (or select row standard behavior)
            return

    def _toggle_check(self, item_id):
        node = self.node_map[item_id]
        new_state = not node['checked']
        self._set_node_state(item_id, new_state)
        self._save_config()

    def _set_node_state(self, item_id, state):
        node = self.node_map[item_id]
        node['checked'] = state
        
        # Update image only
        img = self.img_on if state else self.img_off
        self.tree.item(item_id, image=img)
        
        for child_id in self.tree.get_children(item_id):
            self._set_node_state(child_id, state)

    def _save_config(self):
        if not self.root_path or os.path.isfile(self.root_path): return
        flat_config = {n['rel_path']: n['checked'] for n in self.node_map.values()}
        self.intake.save_persistence(self.root_path, flat_config)

    def get_selected_files(self):
        selected = []
        for node in self.node_map.values():
            if node['checked'] and (node['type'] == 'file' or node['type'] == 'web'):
                selected.append(node['path'])
        return selected

class Sidebar(tk.Frame):
    """
    Manages the list of available Cartridges (.db files).
    """
    def __init__(self, parent, storage_dir, on_select_callback):
        super().__init__(parent, bg=SIDEBAR_COLOR, width=220)
        self.storage_dir = storage_dir
        self.on_select = on_select_callback
        self.pack_propagate(False)

        # Header
        tk.Label(self, text="_RagFORGE", bg=SIDEBAR_COLOR, fg=ACCENT_COLOR, 
                 font=("Consolas", 14, "bold"), pady=15).pack(fill="x")
        
        # List
        tk.Label(self, text="CARTRIDGES", bg=SIDEBAR_COLOR, fg="#666", 
                 font=("Arial", 8, "bold"), anchor="w", padx=10).pack(fill="x")
        
        self.listbox = tk.Listbox(self, bg=SIDEBAR_COLOR, fg=TEXT_COLOR, bd=0, 
                                  highlightthickness=0, selectbackground=ACCENT_COLOR)
        self.listbox.pack(fill="both", expand=True, padx=5, pady=5)
        self.listbox.bind("<<ListboxSelect>>", self._on_click)

        # Footer Actions
        btn_frame = tk.Frame(self, bg=SIDEBAR_COLOR, pady=10)
        btn_frame.pack(fill="x", side="bottom")
        
        row1 = tk.Frame(btn_frame, bg=SIDEBAR_COLOR)
        row1.pack(fill="x", pady=2)
        tk.Button(row1, text="üìÇ ROOT", bg="#2d2d44", fg="#aaa", relief="flat", font=("Arial", 8), 
                  command=self._change_root).pack(side="left", padx=10, fill="x", expand=True)
        
        row2 = tk.Frame(btn_frame, bg=SIDEBAR_COLOR)
        row2.pack(fill="x", pady=2)
        tk.Button(row2, text="REFRESH", bg="#2d2d44", fg="white", relief="flat", 
                  command=self.refresh).pack(side="left", padx=10, fill="x", expand=True)
        tk.Button(row2, text="+ NEW", bg="#2d2d44", fg="white", relief="flat", 
                  command=self._create_new).pack(side="right", padx=10, fill="x", expand=True)

        self.refresh()

    def _change_root(self):
        d = filedialog.askdirectory(title="Select Cartridge Library")
        if d:
            self.storage_dir = d
            self.refresh()

    def refresh(self):
        self.listbox.delete(0, tk.END)
        if not os.path.exists(self.storage_dir):
            os.makedirs(self.storage_dir)
        
        dbs = [f for f in os.listdir(self.storage_dir) if f.endswith(".db")]
        for db in dbs:
            self.listbox.insert(tk.END, db)

    def _on_click(self, event):
        sel = self.listbox.curselection()
        if sel:
            db_name = self.listbox.get(sel[0])
            self.on_select(os.path.join(self.storage_dir, db_name))

    def _create_new(self):
        name = simpledialog.askstring("New Cartridge", "Enter name (e.g. 'project_alpha'):")
        if name:
            if not name.endswith(".db"): name += ".db"
            path = os.path.join(self.storage_dir, name)
            # Just touching the file is enough, CartridgeService will init schema on load
            sqlite3.connect(path).close()
            self.refresh()

class EditorPanel(tk.Frame):
    """
    Views the 'files' table content.
    """
    def __init__(self, parent):
        super().__init__(parent, bg=BG_COLOR)
        self.active_db = None
        
        paned = ttk.PanedWindow(self, orient="horizontal")
        paned.pack(fill="both", expand=True)
        
        # Left: File Tree
        self.tree = ttk.Treeview(paned, show="tree", selectmode="browse")
        self.tree.heading("#0", text="VFS Path", anchor="w")
        self.tree.bind("<<TreeviewSelect>>", self._on_select)
        paned.add(self.tree, weight=1)
        
        # Right: Content
        self.editor = scrolledtext.ScrolledText(paned, bg="#252526", fg=TEXT_COLOR, 
                                                font=("Consolas", 10), insertbackground="white")
        paned.add(self.editor, weight=3)

    def load_db(self, db_path):
        self.active_db = db_path
        self.refresh_list()

    def refresh_list(self):
        self.tree.delete(*self.tree.get_children())
        self.editor.delete("1.0", tk.END)
        
        if not self.active_db or not os.path.exists(self.active_db): return
        
        try:
            conn = sqlite3.connect(self.active_db)
            rows = conn.execute("SELECT id, vfs_path, status FROM files ORDER BY vfs_path").fetchall()
            conn.close()
            
            for rid, path, status in rows:
                display = f"[{status}] {path}"
                self.tree.insert("", "end", iid=rid, text=display, values=(path,))
        except Exception as e:
            print(f"Tree load error: {e}")

    def _on_select(self, event):
        sel = self.tree.selection()
        if not sel: return
        file_id = sel[0]
        
        try:
            conn = sqlite3.connect(self.active_db)
            row = conn.execute("SELECT content FROM files WHERE id=?", (file_id,)).fetchone()
            conn.close()
            
            self.editor.delete("1.0", tk.END)
            if row and row[0]:
                self.editor.insert("1.0", row[0])
            else:
                self.editor.insert("1.0", "(Binary content or empty)")
        except: pass







--------------------------------------------------------------------------------
FILE: src\microservices\refinery_service.py
--------------------------------------------------------------------------------
import json
import re
import os
import ast
import concurrent.futures
from typing import Dict, List, Any, Optional, Tuple
from .base_service import BaseService
from .cartridge_service import CartridgeService
from .neural_service import NeuralService
from .semantic_chunker import SemanticChunker

class RefineryService(BaseService):
    """
    The Night Shift.
    Polls the DB for 'RAW' files and processes them into Chunks and Graph Nodes.

    Graph Enrichment:
    - Code: function/class nodes, resolved import edges when possible.
    - Docs: section/chapter nodes for long-form text (md/txt/rst).
    """

    def __init__(self, cartridge: CartridgeService, neural: NeuralService):
        super().__init__("RefineryService")
        self.cartridge = cartridge
        self.neural = neural
        self.chunker = SemanticChunker()

        # --- Spec Enforcement ---
        # Update the cartridge manifest to reflect the ACTUAL tools we are using.
        self._stamp_specs()

        # Import parsing / resolution
        # Regex remains as a fallback (JS + non-parseable cases)
        self.import_pattern = re.compile(r"""(?:from|import)\s+([\w\.]+)|require\(['"]([\w\.\-/]+)['"]\)""")

        # Lightweight module/path index cache for resolving imports to VFS files
        self._module_index: Dict[str, str] = {}
        self._path_index: Dict[str, str] = {}
        self._index_built: bool = False

        # Simple section/chapter detection
        self._md_heading = re.compile(r"^(#{1,6})\s+(.+?)\s*$")
        self._chapter_heading = re.compile(r"^\s*(chapter|CHAPTER)\s+([0-9]+|[IVXLC]+)\b\s*[:\-]?\s*(.*)$")

    def _stamp_specs(self):
        """Writes the active Neural/Chunker configuration to the Manifest."""
        try:
            # 1. Embedding Spec
            # We assume 1024 dim for mxbai-large, but ideally we'd probe it.
            embed_model = self.neural.config["embed"]
            spec = {
                "provider": "ollama",
                "model": embed_model,
                "dim": 1024,  # Hardcoded for now based on mxbai-embed-large
                "dtype": "float32",
                "distance": "cosine"
            }
            self.cartridge.set_manifest("embedding_spec", spec)

            # 2. Chunking Spec
            # (We could expose chunker config params here if they were dynamic)

        except Exception as e:
            self.log_error(f"Failed to stamp specs: {e}")

    def _build_import_index(self):
        """Builds caches for resolving imports to VFS targets.

        - _path_index: exact VFS path -> VFS path
        - _module_index: python module name (a.b.c) -> VFS path (a/b/c.py, a/b/c/__init__.py)
        """
        if self._index_built:
            return

        path_index: Dict[str, str] = {}
        module_index: Dict[str, str] = {}

        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            for (vp,) in rows:
                if not vp:
                    continue
                vfs_path = str(vp).replace("\\", "/")
                path_index[vfs_path] = vfs_path

                # Python module mapping
                if vfs_path.endswith(".py"):
                    mod = vfs_path[:-3].strip("/")
                    mod = mod.replace("/", ".")
                    if mod:
                        module_index[mod] = vfs_path

                    # If it's a package __init__.py, map the package name too
                    if vfs_path.endswith("/__init__.py"):
                        pkg = vfs_path[:-len("/__init__.py")].strip("/").replace("/", ".")
                        if pkg:
                            module_index[pkg] = vfs_path

        finally:
            conn.close()

        self._path_index = path_index
        self._module_index = module_index
        self._index_built = True

    def process_pending(self, batch_size: int = 5) -> int:
        """Main loop. Returns number of files processed."""
        pending = self.cartridge.get_pending_files(limit=batch_size)
        if not pending:
            return 0

        self.log_info(f"Refining batch of {len(pending)} files...")

        for file_row in pending:
            self._refine_file(file_row)

        return len(pending)

    def _refine_file(self, row: Dict):
        file_id = row['id']
        vfs_path = row['vfs_path']
        content = row['content']

        # Skip binary files for now (unless we add OCR later)
        if not content:
            self.cartridge.update_status(file_id, "SKIPPED_BINARY")
            return

        try:
            # 1. Semantic Chunking
            chunks = self.chunker.chunk_file(content, vfs_path)

            # 2. Vectorization & Storage
            chunk_texts = [c.content for c in chunks]

            # Buffer graph writes while DB transaction is open (prevents nested-writer locks)
            pending_nodes: List[Tuple[str, str, str, Dict[str, Any]]] = []
            pending_edges: List[Tuple[str, str, str, float]] = []

            # Use ThreadPool to embed in parallel (preserve order with map)
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.neural.max_workers) as executor:
                vectors = list(executor.map(self.neural.get_embedding, chunk_texts))

            conn = self.cartridge._get_conn()
            try:
                cursor = conn.cursor()

                for i, chunk in enumerate(chunks):
                    vector = vectors[i]
                    vec_blob = json.dumps(vector).encode('utf-8') if vector else None

                    # Store Chunk
                    cursor.execute(
                        """
                        INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (file_id, i, chunk.content, vec_blob, chunk.name, chunk.type, chunk.start_line, chunk.end_line)
                    )

                    chunk_row_id = cursor.lastrowid

                    # Insert into Vector Index (if vector exists)
                    if vector:
                        try:
                            cursor.execute(
                                "INSERT INTO vec_items(rowid, embedding) VALUES (?, ?)",
                                (chunk_row_id, json.dumps(vector))
                            )
                        except Exception as ve:
                            self.log_error(f"Vector Index Insert Failed: {ve}")

                    # Graph Node for Chunks (Functions/Classes)
                    # IMPORTANT: Do NOT call CartridgeService.add_node/add_edge while this conn is open.
                    if chunk.type in ['class', 'function']:
                        node_id = f"{vfs_path}::{chunk.name}"
                        pending_nodes.append(
                            (
                                node_id,
                                'chunk',
                                chunk.name,
                                {
                                    'parent': vfs_path,
                                    'file_id': file_id,
                                    'chunk_row_id': chunk_row_id,
                                    'chunk_type': chunk.type,
                                    'start_line': chunk.start_line,
                                    'end_line': chunk.end_line
                                }
                            )
                        )
                        pending_edges.append((node_id, vfs_path, "defined_in", 1.0))

                conn.commit()

            finally:
                conn.close()

            # 3. File Level Graph Node (after close)
            pending_nodes.append((vfs_path, 'file', vfs_path.split('/')[-1], {'path': vfs_path, 'file_id': file_id}))

            # 4. Section/Chapter Weaving (docs)
            self._weave_sections(vfs_path, content)

            # 5. Import Weaving (resolved when possible)
            self._weave_imports(vfs_path, content)

            # 6. Flush buffered graph writes
            for nid, ntype, label, data in pending_nodes:
                self.cartridge.add_node(nid, ntype, label, data)
            for src, tgt, rel, w in pending_edges:
                self.cartridge.add_edge(src, tgt, rel, w)

            # 7. Mark file refined
            self.cartridge.update_status(file_id, "REFINED")

        except Exception as e:
            self.log_error(f"Refining failed for {vfs_path}: {e}")
            self.cartridge.update_status(file_id, "ERROR", {"error": str(e)})

    def _extract_imports_python(self, source_path: str, content: str) -> List[Tuple[str, int, int]]:
        """Returns list of (module_or_path, level, lineno).

        - level: 0 for absolute imports, >=1 for relative import-from statements.
        """
        out: List[Tuple[str, int, int]] = []
        try:
            tree = ast.parse(content)
        except Exception:
            return out

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias and alias.name:
                        out.append((alias.name, 0, getattr(node, 'lineno', 0)))
            elif isinstance(node, ast.ImportFrom):
                level = int(getattr(node, 'level', 0) or 0)
                mod = getattr(node, 'module', None) or ""
                if mod:
                    out.append((mod, level, getattr(node, 'lineno', 0)))
                else:
                    # from . import x
                    for alias in node.names:
                        if alias and alias.name:
                            out.append((alias.name, level, getattr(node, 'lineno', 0)))

        return out

    def _resolve_python_import(self, source_path: str, module: str, level: int) -> List[str]:
        """Resolve a python import to possible VFS target paths."""
        self._build_import_index()

        # Absolute: try direct module mapping
        if level <= 0:
            if module in self._module_index:
                return [self._module_index[module]]
            return []

        # Relative: resolve from the source directory
        src_dir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        base_parts = src_dir.split("/") if src_dir else []

        # level=1 means "from .", so pop 0; level=2 means "from .." pop 1, etc.
        pops = max(level - 1, 0)
        if pops > 0 and pops <= len(base_parts):
            base_parts = base_parts[:-pops]

        rel_base = "/".join([p for p in base_parts if p])
        mod_path = module.replace(".", "/").strip("/")

        candidates: List[str] = []
        if rel_base:
            if mod_path:
                candidates.append(f"{rel_base}/{mod_path}.py")
                candidates.append(f"{rel_base}/{mod_path}/__init__.py")
            else:
                candidates.append(f"{rel_base}/__init__.py")
        else:
            if mod_path:
                candidates.append(f"{mod_path}.py")
                candidates.append(f"{mod_path}/__init__.py")

        return [c for c in candidates if c in self._path_index]

    def _resolve_js_like_import(self, source_path: str, imp: str) -> List[str]:
        """Resolve require('./x') / import ... from './x' to VFS candidates."""
        self._build_import_index()

        sdir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        raw = imp.strip().replace("\\", "/")

        # Only try to resolve relative-ish paths
        if not (raw.startswith(".") or raw.startswith("/")):
            return []

        # Normalize
        if raw.startswith("/"):
            rel = raw.lstrip("/")
        else:
            rel = os.path.normpath(os.path.join(sdir, raw)).replace("\\", "/").lstrip("./")

        ext_candidates = [rel]
        # Common extensions
        if not os.path.splitext(rel)[1]:
            ext_candidates.extend([rel + ".js", rel + ".ts", rel + ".json"]) 
            ext_candidates.extend([rel + "/index.js", rel + "/index.ts"]) 

        return [c for c in ext_candidates if c in self._path_index]

    def _weave_imports(self, source_path: str, content: str):
        """Scans content for imports and links them in the graph.

        Creates edges:
        - imports_file: source_path -> target_vfs_path (resolved)
        - imports_unresolved: source_path -> module_string (fallback)
        """
        targets_resolved: List[str] = []

        # Python: use AST when possible
        if source_path.endswith(".py"):
            for mod, level, lineno in self._extract_imports_python(source_path, content):
                resolved = self._resolve_python_import(source_path, mod, level)
                if resolved:
                    for tgt in resolved:
                        self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                        targets_resolved.append(tgt)
                else:
                    self.cartridge.add_edge(source_path, mod, "imports_unresolved", 0.25)

            return

        # JS / generic: regex fallback
        for line in content.splitlines():
            match = self.import_pattern.search(line)
            if not match:
                continue

            imp = match.group(1) or match.group(2)
            if not imp:
                continue

            resolved = self._resolve_js_like_import(source_path, imp)
            if resolved:
                for tgt in resolved:
                    self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                    targets_resolved.append(tgt)
            else:
                self.cartridge.add_edge(source_path, imp, "imports_unresolved", 0.25)

    def _weave_sections(self, vfs_path: str, content: str):
        """Creates section/chapter nodes for long-form text and links them to the file node."""
        ext = os.path.splitext(vfs_path)[1].lower()
        if ext not in (".md", ".markdown", ".txt", ".rst"):
            return

        lines = content.splitlines()
        for idx, line in enumerate(lines):
            lineno = idx + 1

            m = self._md_heading.match(line)
            if m:
                hashes = m.group(1)
                title = (m.group(2) or "").strip()
                level = len(hashes)
                if title:
                    node_id = f"{vfs_path}::section::{lineno}:{title}"
                    self.cartridge.add_node(node_id, "section", title, {
                        "parent": vfs_path,
                        "level": level,
                        "line": lineno
                    })
                    self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)
                continue

            c = self._chapter_heading.match(line)
            if c:
                chap_num = (c.group(2) or "").strip()
                chap_title = (c.group(3) or "").strip()
                title = f"Chapter {chap_num}" + (f": {chap_title}" if chap_title else "")
                node_id = f"{vfs_path}::chapter::{lineno}:{chap_num}"
                self.cartridge.add_node(node_id, "section", title, {
                    "parent": vfs_path,
                    "level": 1,
                    "line": lineno
                })
                self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)





--------------------------------------------------------------------------------
FILE: src\microservices\scanner.py
--------------------------------------------------------------------------------
import os
import time
import requests
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Any, Optional

# Try imports for Web/PDF support
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

class ScannerMS:
    """
    The Scanner: Walks file systems OR crawls websites (Depth-Aware).
    """
    
    def __init__(self):
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage', 'site-packages'
        }
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', 
            '.zip', '.tar', '.gz', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }
        self.visited_urls = set()

    def is_binary(self, file_path: str) -> bool:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS: return True
        return False

    def scan_directory(self, root_path: str, web_depth: int = 0) -> Optional[Dict[str, Any]]:
        """
        Main Entry Point.
        :param root_path: File path or URL.
        :param web_depth: How many links deep to crawl (0 = single page).
        """
        # 1. Web Crawl Mode
        if root_path.startswith("http://") or root_path.startswith("https://"):
            self.visited_urls.clear()
            return self._crawl_web_recursive(root_path, depth=web_depth, origin_domain=urlparse(root_path).netloc)

        # 2. Local File System Mode
        target = os.path.abspath(root_path)
        if not os.path.exists(target): return None
        
        if not os.path.isdir(target): 
            return self._create_node(target, is_dir=False)
            
        return self._scan_fs_recursive(target)

    # --- Web Logic ---
    def _crawl_web_recursive(self, url: str, depth: int, origin_domain: str) -> Dict[str, Any]:
        """
        Recursively fetches links.
        """
        # Generate a nice VFS path: web/domain/path
        parsed = urlparse(url)
        clean_path = parsed.path.strip("/")
        if not clean_path: clean_path = "index.html"
        rel_path = f"web/{parsed.netloc}/{clean_path}"

        node = {
            'name': url,
            'path': url,
            'rel_path': rel_path,
            'type': 'web',
            'children': [],
            'checked': True
        }
        
        if depth < 0 or url in self.visited_urls: return node
        self.visited_urls.add(url)

        if depth > 0 and BeautifulSoup:
            try:
                # Polite Delay
                time.sleep(0.1)
                resp = requests.get(url, timeout=5)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.content, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        parsed = urlparse(full_url)
                        
                        # Filter: Only same domain, valid schemes
                        if parsed.netloc == origin_domain and parsed.scheme in ['http', 'https']:
                            if full_url not in self.visited_urls:
                                child_node = self._crawl_web_recursive(full_url, depth - 1, origin_domain)
                                node['children'].append(child_node)
            except Exception as e:
                node['error'] = str(e)
                
        return node

    # --- File System Logic ---
    def _scan_fs_recursive(self, current_path: str, root_path: str = None) -> Dict[str, Any]:
        if root_path is None: root_path = current_path
        
        node = self._create_node(current_path, is_dir=True, root_path=root_path)
        node['children'] = []
        try:
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                for entry in entries:
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS: continue
                    if entry.name.startswith('.'): continue

                    if entry.is_dir():
                        child = self._scan_fs_recursive(entry.path, root_path=root_path)
                        if child: node['children'].append(child)
                    else:
                        node['children'].append(self._create_node(entry.path, is_dir=False, root_path=root_path))
        except PermissionError:
            node['error'] = "Access Denied"
        return node

    def _create_node(self, path: str, is_dir: bool, root_path: str = None) -> Dict[str, Any]:
        name = os.path.basename(path)
        # Calculate relative path for VFS
        rel_path = name
        if root_path:
            try:
                rel_path = os.path.relpath(path, root_path).replace("\\", "/")
            except ValueError:
                pass

        node = {
            'name': name, 
            'path': path, 
            'rel_path': rel_path,
            'type': 'folder' if is_dir else 'file', 
            'children': [],
            'checked': False
        }
        return node

    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        files = []
        if tree_node['type'] in ['file', 'web']:
            files.append(tree_node['path'])
        elif 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files


--------------------------------------------------------------------------------
FILE: src\microservices\semantic_chunker.py
--------------------------------------------------------------------------------
import ast
from dataclasses import dataclass
from typing import List

@dataclass
class CodeChunk:
    name: str          # e.g., "class AuthMS"
    type: str          # "class", "function", "text"
    content: str       # The raw source
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

class SemanticChunker:
    """
    Intelligent Code Splitter.
    Parses source code into logical units (Classes, Functions) 
    rather than arbitrary text windows.
    """
    
    def chunk_file(self, content: str, filename: str) -> List[CodeChunk]:
        # 1. Python Code
        if filename.endswith(".py"):
            return self._chunk_python(content)
            
        # 2. Text / Prose Documents (Smaller semantic windows)
        lower = filename.lower()
        if lower.endswith(('.md', '.txt', '.pdf', '.html', '.htm', '.rst')):
            return self._chunk_generic(content, window_size=800)
            
        # 3. Fallback (Generic Code/Binary)
        return self._chunk_generic(content, window_size=1500)

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                end = node.end_lineno if hasattr(node, 'end_lineno') else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", type="function", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"class {node.name}", type="class", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))

            # Fallback: If no classes/functions found (e.g., script file), treat as generic
            if not chunks:
                return self._chunk_generic(source)
                
        except SyntaxError:
            return self._chunk_generic(source)
            
        return chunks

    def _chunk_generic(self, text: str, window_size: int = 1500) -> List[CodeChunk]:
        """Sliding window for non-code files."""
        chunks = []
        # normalize newlines to avoid massive single-line blobs
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        lines = text.splitlines(keepends=True)
        
        current_chunk = []
        current_size = 0
        chunk_idx = 1
        start_line = 1
        
        for i, line in enumerate(lines):
            current_chunk.append(line)
            current_size += len(line)
            
            if current_size >= window_size:
                chunks.append(CodeChunk(
                    name=f"Chunk {chunk_idx}", type="text_block",
                    content="".join(current_chunk), start_line=start_line, end_line=i + 1
                ))
                current_chunk = []
                current_size = 0
                chunk_idx += 1
                start_line = i + 2
                
        if current_chunk:
            chunks.append(CodeChunk(
                name=f"Chunk {chunk_idx}", type="text_block",
                content="".join(current_chunk), start_line=start_line, end_line=len(lines)
            ))
            
        return chunks


--------------------------------------------------------------------------------
FILE: src\microservices\telemetry_service.py
--------------------------------------------------------------------------------
import logging
import queue
from .base_service import BaseService

class TelemetryService(BaseService):
    """
    The Nervous System.
    Watches the thread-safe LogQueue and updates the GUI Panels.
    """
    def __init__(self, root, panels):
        super().__init__("TelemetryService")
        self.root = root
        self.panels = panels
        self.log_queue = queue.Queue()
        
        # We set up the global logging hook HERE, inside the service
        self._setup_logging_hook()

    def _setup_logging_hook(self):
        """Redirects Python's standard logging to our Queue."""
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        
        # Create our custom handler that feeds the queue
        q_handler = QueueHandler(self.log_queue)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
        q_handler.setFormatter(formatter)
        logger.addHandler(q_handler)

    def start(self):
        """Begins the GUI update loop."""
        self.log_info("Telemetry Service starting...")
        self._poll_queue()

    def _poll_queue(self):
        """The heartbeat that drains the queue into the GUI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                msg = f"[{record.levelname}] {record.message}" # Removed \n because .log() adds it
                
                # Update the GUI
                self.panels.log(msg)
                
        except queue.Empty:
            pass
        finally:
            # Check again in 100ms
            self.root.after(100, self._poll_queue)

# Helper Class for the Queue
class QueueHandler(logging.Handler):
    def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.format(record)
        self.log_queue.put(record)


--------------------------------------------------------------------------------
FILE: src\microservices\thought_stream.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import datetime

class ThoughtStream(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340)
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", fg="#007ACC", bg="#1a1a25", font=("Consolas", 8)).pack(anchor="w", padx=5, pady=2)
        
        snippet = content[:400] + "..." if len(content) > 400 else content
        tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", font=("Consolas", 8), justify="left", wraplength=300).pack(fill="x", padx=5, pady=2)
        
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        if not vector: return
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        bar_w = w / len(vector)
        for i, val in enumerate(vector):
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")
--------------------------------------------------------------------------------
FILE: src\microservices\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: tests\verify_forge.py
--------------------------------------------------------------------------------
import sys
import os
import time
import json

# Fix path so we can import src
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'src')))

from microservices.cartridge_service import CartridgeService
from microservices.neural_service import NeuralService
from microservices.intake_service import IntakeService
from microservices.refinery_service import RefineryService

TEST_DB = "tests/test_cartridge.db"
TEST_SOURCE_DIR = "tests/dummy_source"

def setup_dummy_data():
    if not os.path.exists(TEST_SOURCE_DIR):
        os.makedirs(TEST_SOURCE_DIR)
    
    # Create a text file with specific distinct concepts
    with open(f"{TEST_SOURCE_DIR}/concept.txt", "w") as f:
        f.write("""
        Project: Project Necromancy.
        Objective: To resurrect dead code using AI patches.
        Lead Engineer: Jacob Lambert.
        Tools: _RagFORGE, _TokenizingPATCHER.
        Status: ACTIVE.
        """)
    print("[TEST] Dummy data created.")

def run_test():
    # 1. Cleanup old test
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)

    setup_dummy_data()

    print(f"[TEST] Forging Cartridge: {TEST_DB}")
    
    # 2. Initialize Services
    cartridge = CartridgeService(TEST_DB)
    neural = NeuralService()
    intake = IntakeService(cartridge)
    refinery = RefineryService(cartridge, neural)

    # 3. Intake Phase
    print(">>> Phase 1: Intake")
    # Using the new ingest_source method we patched
    stats = intake.ingest_source(TEST_SOURCE_DIR)
    print(f"    Stats: {stats}")

    # 4. Refinery Phase
    print(">>> Phase 2: Refinery")
    processed = refinery.process_pending(batch_size=10)
    print(f"    Refined {processed} files.")
    
    # Allow DB to settle
    time.sleep(1)

    # 5. Verification: Manifest
    print("\n[VERIFY] Checking Manifest...")
    cid = cartridge.get_manifest("cartridge_id")
    schema = cartridge.get_manifest("schema_version")
    print(f"    Cartridge ID: {cid}")
    print(f"    Schema Ver:   {schema}")
    
    if not cid:
        print("    [FAIL] Manifest missing!")
        return

    # 6. Verification: Vector Search
    print("\n[VERIFY] Testing Vector Search...")
    # We search for a concept that shouldn't match by keyword alone but works semantically
    query = "Who is the lead engineer?" 
    print(f"    Query: '{query}'")
    
    q_vec = neural.get_embedding(query)
    results = cartridge.search_embeddings(q_vec, limit=1)
    
    if results:
        top = results[0]
        print(f"    [SUCCESS] Match found!")
        print(f"    Score: {top['score']}")
        print(f"    Content: {top['content'].strip()}")
    else:
        print("    [FAIL] No vector results found. Is sqlite-vec working?")

if __name__ == "__main__":
    try:
        run_test()
    except Exception as e:
        print(f"[FATAL] Test failed: {e}")

--------------------------------------------------------------------------------
FILE: tests\dummy_source\concept.txt
--------------------------------------------------------------------------------

        Project: Project Necromancy.
        Objective: To resurrect dead code using AI patches.
        Lead Engineer: Jacob Lambert.
        Tools: _RagFORGE, _TokenizingPATCHER.
        Status: ACTIVE.
        