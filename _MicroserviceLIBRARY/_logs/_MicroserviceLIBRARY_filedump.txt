Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_MicroserviceLIBRARY


--------------------------------------------------------------------------------
FILE: base_service.py
--------------------------------------------------------------------------------
import logging
from typing import Dict, Any

class BaseService:
    """
    Standard parent class for all microservices. 
    Provides consistent logging and identity management.
    """
    def __init__(self, name: str):
        self._service_info = {
            "name": name, 
            "id": name.lower().replace(" ", "_")
        }
        
        # Setup standard logging
        logging.basicConfig(
            level=logging.INFO, 
            format='%(asctime)s [%(levelname)s] %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(name)

    def log_info(self, message: str):
        self.logger.info(message)

    def log_error(self, message: str):
        self.logger.error(message)

    def log_warning(self, message: str):
        self.logger.warning(message)

--------------------------------------------------------------------------------
FILE: boiler_plate.py
--------------------------------------------------------------------------------
from microservice_std_lib import service_metadata, service_endpoint
from typing import Dict, Any, Optional

@service_metadata(
    name="YourServiceName",
    version="1.0.0",
    description="Briefly describe the 'Purpose' here.",
    tags=["category", "utility"],
    capabilities=["filesystem:read"] # Optional: what it actually touches
)
class YourServiceMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        # Initialize your core logic/engines here

    @service_endpoint(
        inputs={"param1": "str", "param2": "int"},
        outputs={"result": "str"},
        description="Detailed description of what this specific method does.",
        tags=["action"],
        side_effects=["filesystem:write"] # Be explicit for the AI safety
    )
    def perform_action(self, param1: str, param2: int = 10) -> Dict[str, Any]:
        # Your translated logic goes here
        return {"result": f"Processed {param1}"}

if __name__ == "__main__":
    # Standard independent test block for the catalogue
    svc = YourServiceMS()
    print("Service ready:", svc)
    # Add a print test of your logic here
--------------------------------------------------------------------------------
FILE: document_utils.py
--------------------------------------------------------------------------------
from __ContentExtractorMS import ContentExtractorMS

# Singleton instance to reuse the extractor logic
_extractor = ContentExtractorMS()

def extract_text_from_pdf(blob: bytes) -> str:
    """Proxy to ContentExtractorMS PDF logic."""
    return _extractor._extract_pdf(blob)

def extract_text_from_html(html_text: str) -> str:
    """Proxy to ContentExtractorMS HTML logic."""
    return _extractor._extract_html(html_text)

--------------------------------------------------------------------------------
FILE: fixme.py
--------------------------------------------------------------------------------
import os
import re

def sync_class_names():
    print("--- ðŸ”„ Starting Class Name Sync ---")
    
    # Get all python files in the current folder
    files = [f for f in os.listdir('.') if f.endswith('.py')]
    
    updates_count = 0

    for filename in files:
        # We only strictly enforce this on your "MS" files (starting with __)
        # to avoid messing up system files like base_service.py
        if not filename.startswith("__"):
            continue

        # 1. Determine the Target Class Name
        # Remove extension
        name_no_ext = filename[:-3] 
        # Remove leading underscores (e.g., "__CartridgeServiceMS" -> "CartridgeServiceMS")
        target_class_name = name_no_ext.lstrip("_")

        # 2. Read the file
        with open(filename, 'r', encoding='utf-8') as f:
            content = f.read()

        # 3. Find the current class definition
        # Regex explanation:
        # ^class\s+  -> Starts with 'class' followed by whitespace
        # (\w+)      -> Capture the class name (Group 1)
        # (.*):      -> Capture inheritance/rest of line until colon (Group 2)
        match = re.search(r'class\s+(\w+)(.*):', content)

        if match:
            current_class_name = match.group(1)
            inheritance_part = match.group(2)

            # 4. Check if mismatch
            if current_class_name != target_class_name:
                print(f"ðŸ”§ Updating {filename}...")
                print(f"   - Old: class {current_class_name}")
                print(f"   - New: class {target_class_name}")

                # 5. Replace the definition line
                # We use regex sub to replace only the definition line
                new_def_line = f"class {target_class_name}{inheritance_part}:"
                content = content.replace(match.group(0), new_def_line)

                # 6. OPTIONAL: Attempt to replace self-references in the file
                # If the file instantiates itself (e.g. app = OldName()), update that too.
                # Use word boundaries (\b) to avoid replacing partial words.
                content = re.sub(rf'\b{current_class_name}\b', target_class_name, content)

                # 7. Write back
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                updates_count += 1
            else:
                print(f"âœ… {filename} is already correct ({target_class_name}).")
        else:
            print(f"âš ï¸ Skipped {filename}: Could not find a standard class definition.")

    print(f"--- âœ¨ Sync Complete. Updated {updates_count} files. ---")

if __name__ == "__main__":
    sync_class_names()
--------------------------------------------------------------------------------
FILE: microservice_std_lib.py
--------------------------------------------------------------------------------
"""
LIBRARY: Microservice Standard Lib
VERSION: 2.0.0
ROLE: Provides decorators for tagging Python classes as AI-discoverable services.
"""

import functools
import inspect
from typing import Dict, List, Any, Optional, Type

# ==============================================================================
# DECORATORS (The "Writer" Tools)
# ==============================================================================

def service_metadata(name: str, version: str, description: str, tags: List[str], capabilities: List[str] = None):
    """
    Class Decorator.
    Labels a Microservice class with high-level metadata for the Catalog.
    """
    def decorator(cls):
        cls._is_microservice = True
        cls._service_info = {
            "name": name,
            "version": version,
            "description": description,
            "tags": tags,
            "capabilities": capabilities or []
        }
        return cls
    return decorator

def service_endpoint(inputs: Dict[str, str], outputs: Dict[str, str], description: str, tags: List[str] = None, side_effects: List[str] = None, mode: str = "sync"):
    """
    Method Decorator.
    Defines the 'Socket' that the AI Architect can plug into.
    
    :param inputs: Dict of {arg_name: type_string} (e.g. {"query": "str"})
    :param outputs: Dict of {return_name: type_string} (e.g. {"results": "List[Dict]"})
    :param description: What this specific function does.
    :param tags: Keywords for searching (e.g. ["search", "read-only"])
    :param side_effects: List of impact types (e.g. ["network:outbound", "disk:write"])
    :param mode: 'sync', 'async', or 'ui_event'
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        
        # Attach metadata to the function object itself
        wrapper._endpoint_info = {
            "name": func.__name__,
            "inputs": inputs,
            "outputs": outputs,
            "description": description,
            "tags": tags or [],
            "side_effects": side_effects or [],
            "mode": mode
        }
        return wrapper
    return decorator

# ==============================================================================
# INTROSPECTION (The "Reader" Tools)
# ==============================================================================

def extract_service_schema(service_cls: Type) -> Dict[str, Any]:
    """
    Scans a decorated Service Class and returns a JSON-serializable schema 
    of its metadata and all its exposed endpoints.
    
    This is what the AI Agent uses to 'read' the manual.
    """
    if not getattr(service_cls, "_is_microservice", False):
        raise ValueError(f"Class {service_cls.__name__} is not decorated with @service_metadata")

    schema = {
        "meta": getattr(service_cls, "_service_info", {}),
        "endpoints": []
    }

    # Inspect all methods of the class
    for name, method in inspect.getmembers(service_cls, predicate=inspect.isfunction):
        # Unwrap decorators if necessary to find our tags
        # (Though usually the wrapper has the tag attached)
        endpoint_info = getattr(method, "_endpoint_info", None)
        
        if endpoint_info:
            schema["endpoints"].append(endpoint_info)

    return schema
--------------------------------------------------------------------------------
FILE: TESTME.py
--------------------------------------------------------------------------------
import sys
import os

print("--- ðŸ”Œ SYSTEM BOOT CHECK ---")

try:
    print("1. Loading Base Service...", end=" ")
    from base_service import BaseService
    print("âœ… OK")

    print("2. Loading Cartridge Service...", end=" ")
    from __CartridgeServiceMS import CartridgeServiceMS
    print("âœ… OK")

    print("3. Loading Scanner Service...", end=" ")
    from __ScannerMS import ScannerMS
    print("âœ… OK")

    print("4. Loading Intake Service (The one you just fixed)...", end=" ")
    from __IntakeServiceMS import IntakeServiceMS
    print("âœ… OK")

    print("\nðŸŽ‰ SUCCESS: All microservices linked and loaded correctly!")

except ImportError as e:
    print(f"\nâŒ FAIL: Import Error detected.\n   {e}")
except Exception as e:
    print(f"\nâŒ FAIL: Runtime Error detected.\n   {e}")
--------------------------------------------------------------------------------
FILE: __ArchiveBotMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ArchiveBotMS
ENTRY_POINT: __ArchiveBotMS.py
DEPENDENCIES: None
"""

"""
SERVICE: ArchiveBot
VERSION: 1.1.0
ROLE: Create timestamped compressed .tar.gz backups of directory trees.
INPUTS:
- source_path: (str) The root folder to backup.
- output_dir: (str) Where to save the resulting file.
OUTPUTS:
- archive_path: (str) Absolute path to the created file.
- file_count: (int) Total files compressed.
DEPENDENCIES: None (Standard Library)
NOTES:
Includes default ignore lists for common dev artifacts (node_modules, venv, etc).
"""

import datetime
import fnmatch
import logging
import os
import tarfile
from pathlib import Path
from typing import Any, Dict, Optional, Set, Tuple

from microservice_std_lib import service_metadata, service_endpoint

# === [ CONFIGURATION ] ========================================================
SERVICE_TITLE = "ArchiveBot"
SERVICE_VERSION = "1.1.0"
LOG_LEVEL = logging.INFO

# Default exclusions (Dev artifacts, caches, system files)
DEFAULT_IGNORE_DIRS: Set[str] = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", "dist",
    "build", "coverage", "target", "out", "bin", "obj"
}

DEFAULT_IGNORE_FILES: Set[str] = {
    ".DS_Store", "Thumbs.db", "*.pyc", "*.pyo", "*.log", "*.tmp"
}

logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(SERVICE_TITLE)

# === [ SERVICE DEFINITION ] ===================================================
@service_metadata(
    name=SERVICE_TITLE,
    version=SERVICE_VERSION,
    description="Creates timestamped .tar.gz backups of directory trees.",
    tags=["utility", "backup", "filesystem"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ArchiveBotMS:
    
    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}

    # === [ CORE ENDPOINTS ] ===================================================
    @service_endpoint(
        inputs={
            "source_path": "str",
            "output_dir": "str",
            "extra_exclusions": "List[str] | None",
            "use_default_exclusions": "bool"
        },
        outputs={
            "archive_path": "str",
            "file_count": "int"
        },
        description="Compresses a directory into a .tar.gz archive.",
        tags=["action", "backup"],
        side_effects=["filesystem:write"]
    )
    def create_backup(
        self,
        source_path: str,
        output_dir: str,
        extra_exclusions: Optional[Set[str]] = None,
        use_default_exclusions: bool = True,
    ) -> Dict[str, Any]:
        
        src = Path(source_path).resolve()
        out = Path(output_dir).resolve()

        if not src.exists():
            logger.error(f"Source not found: {src}")
            raise FileNotFoundError(f"Source path does not exist: {src}")

        out.mkdir(parents=True, exist_ok=True)

        # Build exclusion set
        exclude_patterns: Set[str] = set()
        if use_default_exclusions:
            exclude_patterns.update(DEFAULT_IGNORE_DIRS)
            exclude_patterns.update(DEFAULT_IGNORE_FILES)
        if extra_exclusions:
            exclude_patterns.update(extra_exclusions)

        # Generate filename: backup_FOLDERNAME_YYYY-MM-DD_HH-MM-SS.tar.gz
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        archive_name = f"backup_{src.name}_{timestamp}.tar.gz"
        archive_path = out / archive_name

        file_count = 0

        try:
            with tarfile.open(archive_path, "w:gz") as tar:
                for root, dirs, files in os.walk(src):
                    # Filter directories in-place to prevent walking into them
                    dirs[:] = [d for d in dirs if not self._is_excluded(d, exclude_patterns)]

                    for file_name in files:
                        if self._is_excluded(file_name, exclude_patterns):
                            continue

                        full_path = Path(root) / file_name
                        # Don't zip the file if we are writing it inside the source folder
                        if full_path == archive_path: continue

                        rel_path = full_path.relative_to(src)
                        tar.add(full_path, arcname=rel_path)
                        file_count += 1

            logger.info(f"Archive created: {archive_path} ({file_count} files)")
            return {
                "archive_path": str(archive_path),
                "file_count": file_count
            }

        except Exception as exc:
            logger.exception(f"Backup failed: {exc}")
            if archive_path.exists():
                try:
                    archive_path.unlink()
                except Exception: pass
            raise exc

    # === [ HELPERS ] ==========================================================
    def _is_excluded(self, name: str, patterns: Set[str]) -> bool:
        for pattern in patterns:
            if name == pattern or fnmatch.fnmatch(name, pattern):
                return True
        return False

# === [ SELF-TEST / RUNNER ] ===================================================
if __name__ == "__main__":
    # Create a dummy file to backup for testing
    import tempfile
    
    bot = ArchiveBotMS()
    
    with tempfile.TemporaryDirectory() as tmp_source:
        with tempfile.TemporaryDirectory() as tmp_out:
            # Create a test file
            p = Path(tmp_source) / "test_file.txt"
            p.write_text("Hello Archive")
            
            print(f"Backing up {tmp_source} to {tmp_out}...")
            result = bot.create_backup(tmp_source, tmp_out)
            print(f"Result: {result}")
--------------------------------------------------------------------------------
FILE: __AuthMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _AuthMS
ENTRY_POINT: __AuthMS.py
DEPENDENCIES: None
"""

import base64
import hashlib
import json
import logging
import time
from typing import Any, Dict, Optional

from microservice_std_lib import service_metadata, service_endpoint

"""
SERVICE: Auth
ROLE: Manage user authentication and signed session tokens.
INPUTS:
  - username: Login identifier.
  - password: Secret credential.
  - token: Serialized session token string.
OUTPUTS:
  - token: Signed session token (str) or None on failure.
  - is_valid: Boolean indicating whether a token is valid and not expired.
NOTES:
  This is a simplified in-memory auth system intended for local tools and
  pipelines, not production-grade security.
"""

logger = logging.getLogger(__name__)

# ==============================================================================
# CONFIGURATION
# ==============================================================================

DEFAULT_SECRET_KEY = "super_secret_cortex_key"
DEFAULT_SALT = "cortex_salt"


# ==============================================================================


@service_metadata(
    name="Auth",
    version="1.0.0",
    description="Manages user authentication and session tokens.",
    tags=["auth", "security", "crypto"],
    capabilities=["crypto"],
)
class AuthMS:
    """
    ROLE: Simple authentication microservice providing username/password login
          and signed session tokens.

    INPUTS:
      - config: Optional configuration dict. Recognized keys:
          - 'secret_key': Secret used to sign tokens.

    OUTPUTS:
      - Exposes `login` and `validate_session` endpoints for use in pipelines.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}
        self.secret_key: str = self.config.get("secret_key", DEFAULT_SECRET_KEY)

        # In a real scenario, this might load from a secure config file or DB.
        # For now, we keep a minimal in-memory user database.
        self.users_db: Dict[str, str] = {
            "admin": self._hash_password("admin123"),
        }

    # --------------------------------------------------------------------- #
    # Public endpoints
    # --------------------------------------------------------------------- #

    @service_endpoint(
        inputs={"username": "str", "password": "str"},
        outputs={"token": "Optional[str]"},
        description="Attempts to log in and returns a signed session token.",
        tags=["auth", "security", "session"],
    )
    def login(self, username: str, password: str) -> Optional[str]:
        """
        Attempt to log in with the provided username and password.

        :param username: Login identifier.
        :param password: Plain-text password.
        :returns: Signed session token if successful, None otherwise.
        """
        if username not in self.users_db:
            return None

        stored_hash = self.users_db[username]
        if self._verify_password(password, stored_hash):
            return self._create_token(username)

        return None

    @service_endpoint(
        inputs={"token": "str"},
        outputs={"is_valid": "bool"},
        description="Checks whether a token is valid and not expired.",
        tags=["auth", "security"],
    )
    def validate_session(self, token: str) -> bool:
        """
        Check if a serialized token is valid and not expired.

        :param token: Session token string.
        :returns: True if token is valid and not expired, False otherwise.
        """
        payload = self._decode_token(token)
        return payload is not None

    # --------------------------------------------------------------------- #
    # Internal helpers
    # --------------------------------------------------------------------- #

    def _hash_password(self, password: str) -> str:
        """
        Securely hashes a password using SHA-256 with a static salt.
        """
        return hashlib.sha256((password + DEFAULT_SALT).encode("utf-8")).hexdigest()

    def _verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verifies a provided password against the stored hash.
        """
        return self._hash_password(plain_password) == hashed_password

    def _create_token(self, user_id: str, expires_in: int = 3600) -> str:
        """
        Generates a signed session token.

        Payload includes:
          - 'sub' (subject)
          - 'exp' (expiration time)
          - 'iat' (issued-at time)
          - 'scope' (authorization scope)
        """
        now = int(time.time())
        payload = {
            "sub": user_id,
            "exp": now + expires_in,
            "iat": now,
            "scope": "admin",
        }

        json_payload = json.dumps(payload).encode("utf-8")
        token_part = base64.b64encode(json_payload).decode("utf-8")

        signature = hashlib.sha256((token_part + self.secret_key).encode("utf-8")).hexdigest()
        return f"{token_part}.{signature}"

    def _decode_token(self, token: str) -> Optional[Dict[str, Any]]:
        """
        Parses and validates the incoming token.

        Returns the payload if valid, None otherwise.
        """
        try:
            if not token or "." not in token:
                return None

            token_part, signature = token.split(".", 1)

            # Recalculate signature to verify integrity
            recalc_signature = hashlib.sha256(
                (token_part + self.secret_key).encode("utf-8")
            ).hexdigest()

            if signature != recalc_signature:
                return None  # Invalid signature

            # Decode payload
            payload_json = base64.b64decode(token_part).decode("utf-8")
            payload: Dict[str, Any] = json.loads(payload_json)

            # Check expiration
            if payload.get("exp", 0) < time.time():
                return None  # Expired

            return payload

        except Exception:
            # Intentionally swallow details here and just treat token as invalid.
            logger.exception("Failed to decode or validate auth token.")
            return None


if __name__ == "__main__":
    svc = AuthMS()
    print("Service ready:", svc)
    # Example (manual) usage:
    # token = svc.login("admin", "admin123")
    # print("Token:", token)
    # print("Valid:", svc.validate_session(token) if token else False)

--------------------------------------------------------------------------------
FILE: __CartridgeServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CartridgeServiceMS
ENTRY_POINT: __CartridgeServiceMS.py
DEPENDENCIES: None
"""

import sqlite3
import json
import time
import os
import uuid
import datetime
import struct
from pathlib import Path

# Try to import sqlite-vec (pip install sqlite-vec)
try:
    import sqlite_vec
except ImportError:
    sqlite_vec = None
from typing import Dict, Any, Optional, List
from base_service import BaseService
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="CartridgeServiceMS",
    version="1.1.0",
    description="The Source of Truth. Manages the Unified Neural Cartridge Format (UNCF v1.0).",
    tags=["storage", "database", "RAG"],
    capabilities=["sqlite", "vector-search", "graph-storage"]
)
class CartridgeServiceMS(BaseService):
    """
    The Source of Truth.
    Manages the Unified Neural Cartridge Format (UNCF v1.0).
    """
    
    SCHEMA_VERSION = "uncf_v1.0"

    def __init__(self, db_path: str):
        super().__init__("CartridgeServiceMS")
        self.db_path = Path(db_path)
        self._init_db()

    def _get_conn(self):
        # Set generous timeout (60s) for multi-threaded Ingest/Refinery contention
        conn = sqlite3.connect(self.db_path, timeout=60.0)
        if sqlite_vec:
            try:
                conn.enable_load_extension(True)
                sqlite_vec.load(conn)
                conn.enable_load_extension(False)
            except Exception as e:
                self.log_error(f"Failed to load sqlite-vec: {e}")
        return conn

    def get_vector_dim(self) -> int:
        """Retrieves the expected vector dimension from the manifest spec."""
        spec = self.get_manifest("embedding_spec") or {}
        if isinstance(spec, str):
            try: spec = json.loads(spec)
            except: spec = {}
        return int(spec.get("dim", 0))

    def _init_db(self):
        """Initializes the standard Schema."""
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = self._get_conn()
        cursor = conn.cursor()
        
        # Enable WAL Mode: Allows concurrent Readers (Refinery) & Writers (Ingest)
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA synchronous=NORMAL")
        
        # 1. Manifest (The Boot Sector)
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")
        
        # 1.5 Directories (The VFS Index)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS directories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT UNIQUE NOT NULL,
                parent_path TEXT,
                metadata TEXT DEFAULT '{}'
            )
        """)
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_dir_parent ON directories(parent_path)")

        # 2. Files (The Content Store)
        # Supports Text AND Binary (blob_data)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                vfs_path TEXT NOT NULL,       -- Portable path (e.g. "src/main.py")
                origin_path TEXT,             -- Provenance (e.g. "C:/Users/...")
                origin_type TEXT,             -- 'filesystem', 'web', 'github'
                content TEXT,                 -- Text content (UTF-8)
                blob_data BLOB,               -- Binary content (Images, PDFs)
                mime_type TEXT,
                status TEXT DEFAULT 'RAW',    -- RAW, REFINED, ERROR, SKIPPED
                metadata TEXT DEFAULT '{}',   -- JSON tags, summaries
                last_updated TIMESTAMP
            )
        """)
        # Index for fast lookups by VFS path
        cursor.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_vfs ON files(vfs_path)")

        # 3. Chunks (The Vector Store)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_id INTEGER,
                chunk_index INTEGER,
                content TEXT,
                embedding BLOB,
                name TEXT,
                type TEXT,
                start_line INTEGER,
                end_line INTEGER,
                FOREIGN KEY(file_id) REFERENCES files(id)
            )
        """)

        # 3.5 Vector Index (sqlite-vec)
        if sqlite_vec:
            dim = self.get_vector_dim()
            if dim > 0:
                try:
                    cursor.execute(f"CREATE VIRTUAL TABLE IF NOT EXISTS vec_items USING vec0(embedding float[{dim}])")
                except Exception as e:
                    self.log_error(f"Vector Table Init Error: {e}")
            else:
                self.log_info("Vector table creation deferred: No dimensions found in manifest yet.")

        # 4. Graph Topology (The Neural Wiring)
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
        cursor.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, relation TEXT, weight REAL)")

        # 5. Validation Logs
        cursor.execute("CREATE TABLE IF NOT EXISTS logs (timestamp REAL, level TEXT, message TEXT, context TEXT)")
        
        conn.commit()
        conn.close()
        
        # Initialize standard keys if new
        self.initialize_manifest()

    def initialize_manifest(self):
        """Populates the boot sector with strict RagFORGE Cartridge Schema (UNCF) v1.1."""
        if not self.get_manifest("cartridge_id"):
            now = datetime.datetime.utcnow().isoformat()

            # 1. Identity & Versioning
            self.set_manifest("schema_name", "ragforge_cartridge")
            self.set_manifest("schema_version", "1.1.0")
            self.set_manifest("cartridge_id", str(uuid.uuid4()))
            self.set_manifest("created_at_utc", now)
            self.set_manifest("created_by_app", "RagFORGE")

            # 2. Provenance / Sources
            # Agents can read this to understand where the content came from and what policies were used.
            self.set_manifest("sources", [])
            self.set_manifest("source_policies", {
                "binary_policy": "Extract Text",
                "web_depth": 0
            })

            # 3. Specs (Defaults - updated by RefineryService._stamp_specs)
            self.set_manifest("embedding_spec", {
                "provider": "unknown",
                "model": "pending_init",
                "dim": 0,
                "dtype": "unknown",
                "distance": "unknown"
            })
            self.set_manifest("chunking_spec", {
                "strategy": "semantic_hybrid",
                "python_ast": True,
                "generic_window": 1500
            })

            # 4. VFS + Content Stats (populated/updated over time)
            self.set_manifest("vfs", {
                "root_label": "",
                "directories": {"count": 0},
                "files": {
                    "count": 0,
                    "by_origin_type": {},
                    "by_mime": {}
                },
                "index_built": False
            })
            self.set_manifest("content_stats", {
                "chunks": {"count": 0},
                "vector_index": {
                    # Don't assume sqlite-vec is available until proven.
                    "enabled": False,
                    "table": "vec_items",
                    "backend": "sqlite-vec",
                    "dims": 0,
                    "status": "unknown"
                },
                "graph": {
                    "nodes": 0,
                    "edges": 0
                }
            })

            # 5. Capabilities Contract (what an agent can assume exists / how to navigate)
            self.set_manifest("capabilities", {
                "tables": {
                    "manifest": True,
                    "directories": True,
                    "files": True,
                    "chunks": True,
                    "vec_items": True,
                    "graph_nodes": True,
                    "graph_edges": True,
                    "logs": True
                },
                "navigation": {
                    "vfs_path": "files.vfs_path",
                    "directory_index": "directories.vfs_path",
                    "list_files_query": "SELECT vfs_path, mime_type, origin_type, status FROM files ORDER BY vfs_path",
                    "list_directories_query": "SELECT vfs_path, parent_path FROM directories ORDER BY vfs_path"
                },
                "retrieval": {
                    "raw_file_content_query": "SELECT content, blob_data, mime_type FROM files WHERE vfs_path=?",
                    "chunks_by_file_query": "SELECT chunk_index, name, type, start_line, end_line, content FROM chunks WHERE file_id=? ORDER BY chunk_index",
                    "vector_search": "sqlite-vec on vec_items if available"
                },
                "python_helper_api": {
                    "note": "Optional convenience layer for agents running inside Python. For non-Python consumers, use the SQL queries above.",
                    "methods": [
                        "CartridgeServiceMS.get_status_flags",
                        "CartridgeServiceMS.list_files",
                        "CartridgeServiceMS.list_directories",
                        "CartridgeServiceMS.get_file_record",
                        "CartridgeServiceMS.get_directory_tree",
                        "CartridgeServiceMS.get_status_summary",
                        "CartridgeServiceMS.add_node",
                        "CartridgeServiceMS.add_edge",
                        "CartridgeServiceMS.search_embeddings"
                    ]
                }
            })

            # 6. Status & Health
            self.set_manifest("cartridge_health", "FRESH")
            self.set_manifest("ingest_complete", False)
            self.set_manifest("refine_complete", False)
            self.set_manifest("last_ingest_at_utc", "")
            self.set_manifest("last_refine_at_utc", "")
            self.set_manifest("last_error", "")
            self.set_manifest("locks", {
                "write_lock_expected": False,
                "notes": "If DB locks occur, consider batching writes and shorter-lived connections."
            })

    def set_manifest(self, key: str, value: Any):
        """Upsert metadata key."""
        conn = self._get_conn()
        val_str = json.dumps(value) if isinstance(value, (dict, list)) else str(value)
        conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", (key, val_str))
        conn.commit()
        conn.close()

    def get_manifest(self, key: str) -> Optional[str]:
        """Retrieve metadata key."""
        conn = self._get_conn()
        row = conn.execute("SELECT value FROM manifest WHERE key=?", (key,)).fetchone()
        conn.close()
        return row[0] if row else None

    def validate_cartridge(self) -> Dict[str, Any]:
        """Quality Control: Checks if the cartridge is Agent-Safe."""
        report = {"valid": True, "health": "OK", "errors": []}
        
        # 1. Check Required Keys
        # These are the minimum contract keys an agent needs to understand what it loaded.
        required = [
            "schema_name",
            "schema_version",
            "cartridge_id",
            "created_at_utc",
            "created_by_app",
            "embedding_spec",
            "chunking_spec",
            "capabilities"
        ]
        for key in required:
            if not self.get_manifest(key):
                report["valid"] = False
                report["errors"].append(f"Missing Manifest Key: {key}")
        
        # 2. Check Vector Index Presence (and stamp reality into the manifest)
        conn = self._get_conn()
        vec_enabled = False
        vec_status = "unknown"
        try:
            # If this succeeds, the vec_items table exists and is queryable.
            conn.execute("SELECT count(*) FROM vec_items").fetchone()
            vec_enabled = True
            vec_status = "available"
        except Exception:
            vec_enabled = False
            vec_status = "unavailable"
            report["errors"].append("Vector Index (vec_items) missing or not loaded.")
            # Not fatal for 'valid' but impacts capability
            report["health"] = "WARN_NO_VECTORS"
        finally:
            conn.close()

        # Update manifest content_stats.vector_index to reflect truth
        try:
            content_stats = self.get_manifest("content_stats") or {}
            vec = content_stats.get("vector_index", {}) if isinstance(content_stats, dict) else {}

            # Use embedding_spec dim if present; otherwise keep existing dims value
            embed_spec = self.get_manifest("embedding_spec") or {}
            spec_dim = 0
            if isinstance(embed_spec, dict):
                spec_dim = int(embed_spec.get("dim", 0) or 0)

            vec["enabled"] = bool(vec_enabled)
            vec["status"] = vec_status
            if spec_dim > 0:
                vec["dims"] = spec_dim

            # Preserve backend/table fields if present
            if "table" not in vec:
                vec["table"] = "vec_items"
            if "backend" not in vec:
                vec["backend"] = "sqlite-vec"

            content_stats["vector_index"] = vec
            self.set_manifest("content_stats", content_stats)
        except Exception as e:
            # Non-fatal; validation should still return a report
            report["errors"].append(f"Failed to stamp vector_index status into manifest: {e}")
            report["health"] = "WARN_MANIFEST_STAMP_FAIL"
            
        return report

    def store_file(self, vfs_path: str, origin_path: str, content: str = None, blob: bytes = None, mime_type: str = "text/plain", origin_type: str = "filesystem"):
        """
        The Universal Input Method. 
        Stores raw data. If file exists, updates it and resets status to 'RAW' for re-refining.
        """
        conn = self._get_conn()
        try:
            conn.execute("""
                INSERT OR REPLACE INTO files 
                (vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, last_updated)
                VALUES (?, ?, ?, ?, ?, ?, 'RAW', ?)
            """, (vfs_path, origin_path, origin_type, content, blob, mime_type, time.time()))
            conn.commit()
            return True
        except Exception as e:
            self.log_error(f"DB Store Error ({vfs_path}): {e}")
            return False
        finally:
            conn.close()

    def get_pending_files(self, limit: int = 10) -> List[Dict]:
        """Fetches files waiting for the Refinery."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        rows = conn.execute("SELECT * FROM files WHERE status = 'RAW' LIMIT ?", (limit,)).fetchall()
        conn.close()
        return [dict(row) for row in rows]

    def update_status(self, file_id: int, status: str, metadata: dict = None):
        conn = self._get_conn()
        if metadata:
            conn.execute("UPDATE files SET status = ?, metadata = ? WHERE id = ?", 
                         (status, json.dumps(metadata), file_id))
        else:
            conn.execute("UPDATE files SET status = ? WHERE id = ?", (status, file_id))
        conn.commit()
        conn.close()

    def ensure_directory(self, vfs_path: str):
        """Idempotent insert for VFS directories."""
        if not vfs_path: return
        parent = os.path.dirname(vfs_path).replace("\\", "/")
        if parent == vfs_path: parent = "" # Root case
        
        conn = self._get_conn()
        try:
            conn.execute("INSERT OR IGNORE INTO directories (vfs_path, parent_path) VALUES (?, ?)", (vfs_path, parent))
            conn.commit()
        except: pass
        finally:
            conn.close()

    # --- Agent-Friendly Helpers (No raw SQL required) ---
    def _coerce_bool(self, v: Any) -> bool:
        """Best-effort conversion for manifest values stored as strings."""
        if v is None:
            return False
        if isinstance(v, bool):
            return v
        s = str(v).strip().lower()
        return s in ("1", "true", "yes", "y", "on")

    @service_endpoint(
        inputs={},
        outputs={"ingest_complete": "bool", "refine_complete": "bool", "cartridge_health": "str"},
        description="Returns key manifest status flags (ingest/refine status and health) in a single call.",
        tags=["status", "health"]
    )
    def get_status_flags(self) -> Dict[str, Any]:
        """Returns key manifest status flags in a single call."""
        ingest_complete = self._coerce_bool(self.get_manifest("ingest_complete"))
        refine_complete = self._coerce_bool(self.get_manifest("refine_complete"))
        health = self.get_manifest("cartridge_health") or "UNKNOWN"
        return {
            "ingest_complete": ingest_complete,
            "refine_complete": refine_complete,
            "cartridge_health": health,
            "schema_name": self.get_manifest("schema_name") or "",
            "schema_version": self.get_manifest("schema_version") or "",
            "cartridge_id": self.get_manifest("cartridge_id") or ""
        }

    def list_files(self, prefix: str = "", status: Optional[str] = None, limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """Enumerate files in the cartridge (optionally filtered by VFS prefix and/or status)."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            sql = "SELECT id, vfs_path, origin_path, origin_type, mime_type, status, last_updated, metadata FROM files"
            clauses = []
            params = []

            if prefix:
                # Prefix match on portable path
                clauses.append("vfs_path LIKE ?")
                params.append(prefix.rstrip("/") + "/%")

            if status:
                clauses.append("status = ?")
                params.append(status)

            if clauses:
                sql += " WHERE " + " AND ".join(clauses)

            sql += " ORDER BY vfs_path"

            if limit is not None:
                sql += " LIMIT ?"
                params.append(int(limit))

            rows = conn.execute(sql, tuple(params)).fetchall()
            out = []
            for r in rows:
                d = dict(r)
                # metadata is stored as JSON string
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    def get_file_record(self, vfs_path: str) -> Optional[Dict[str, Any]]:
        """Fetch a single file record by VFS path."""
        if not vfs_path:
            return None
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            row = conn.execute(
                "SELECT id, vfs_path, origin_path, origin_type, content, blob_data, mime_type, status, metadata, last_updated FROM files WHERE vfs_path = ?",
                (vfs_path,)
            ).fetchone()
            if not row:
                return None
            d = dict(row)
            try:
                d["metadata"] = json.loads(d.get("metadata") or "{}")
            except Exception:
                d["metadata"] = {}
            return d
        finally:
            conn.close()

    def list_directories(self, prefix: str = "") -> List[Dict[str, Any]]:
        """Enumerate directories in the cartridge VFS."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            if prefix:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories WHERE vfs_path LIKE ? ORDER BY vfs_path",
                    (prefix.rstrip("/") + "/%",)
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT id, vfs_path, parent_path, metadata FROM directories ORDER BY vfs_path"
                ).fetchall()

            out = []
            for r in rows:
                d = dict(r)
                try:
                    d["metadata"] = json.loads(d.get("metadata") or "{}")
                except Exception:
                    d["metadata"] = {}
                out.append(d)
            return out
        finally:
            conn.close()

    @service_endpoint(
        inputs={"root": "str"},
        outputs={"tree": "dict"},
        description="Builds a nested directory tree structure for UI navigation or context mapping.",
        tags=["vfs", "navigation"]
    )
    def get_directory_tree(self, root: str = "") -> Dict[str, Any]:
        """Builds a nested directory tree starting at `root` ("" for full tree)."""
        dirs = self.list_directories(prefix=root) if root else self.list_directories()
        files = self.list_files(prefix=root) if root else self.list_files()

        # Tree nodes are dicts: {"_dirs": {name: node}, "_files": [file_records...]}
        def new_node():
            return {"_dirs": {}, "_files": []}

        tree = new_node()

        # Insert directories
        for d in dirs:
            path = (d.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            cur = tree
            for p in parts:
                cur = cur["_dirs"].setdefault(p, new_node())

        # Insert files
        for f in files:
            path = (f.get("vfs_path") or "").strip("/")
            if not path:
                continue
            parts = path.split("/")
            fname = parts[-1]
            cur = tree
            for p in parts[:-1]:
                cur = cur["_dirs"].setdefault(p, new_node())
            # Store a light file record for tree browsing
            cur["_files"].append({
                "name": fname,
                "vfs_path": f.get("vfs_path"),
                "mime_type": f.get("mime_type"),
                "origin_type": f.get("origin_type"),
                "status": f.get("status")
            })

        return tree

    def get_status_summary(self) -> Dict[str, Any]:
        """Counts files by status and provides a quick cartridge overview."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        try:
            rows = conn.execute("SELECT status, COUNT(*) as n FROM files GROUP BY status").fetchall()
            by_status = {r["status"]: r["n"] for r in rows}

            dcnt = conn.execute("SELECT COUNT(*) FROM directories").fetchone()[0]
            fcnt = conn.execute("SELECT COUNT(*) FROM files").fetchone()[0]
            ccnt = conn.execute("SELECT COUNT(*) FROM chunks").fetchone()[0]
            ncnt = conn.execute("SELECT COUNT(*) FROM graph_nodes").fetchone()[0]
            ecnt = conn.execute("SELECT COUNT(*) FROM graph_edges").fetchone()[0]

            return {
                "directories": int(dcnt),
                "files": int(fcnt),
                "chunks": int(ccnt),
                "graph_nodes": int(ncnt),
                "graph_edges": int(ecnt),
                "files_by_status": by_status,
                "flags": self.get_status_flags()
            }
        finally:
            conn.close()

    # --- Graph Helpers ---
    def add_node(self, node_id: str, node_type: str, label: str, data: dict = None):
        conn = self._get_conn()
        conn.execute("INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json) VALUES (?, ?, ?, ?)",
                     (node_id, node_type, label, json.dumps(data or {})))
        conn.commit()
        conn.close()

    def add_edge(self, source: str, target: str, relation: str = "related", weight: float = 1.0):
        conn = self._get_conn()
        conn.execute("INSERT OR IGNORE INTO graph_edges (source, target, relation, weight) VALUES (?, ?, ?, ?)",
                     (source, target, relation, weight))
        conn.commit()
        conn.close()

    # --- Vector Search ---
    @service_endpoint(
        inputs={"query_vector": "list", "limit": "int"},
        outputs={"results": "list"},
        description="Performs semantic vector search using sqlite-vec against the cartridge chunks.",
        tags=["search", "vector"]
    )
    def search_embeddings(self, query_vector: List[float], limit: int = 5) -> List[Dict]:
        """Performs semantic search using sqlite-vec."""
        if not sqlite_vec or not query_vector:
            return []

        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        results = []
        
        try:
            # Pack vector to binary if needed, but sqlite-vec usually handles raw lists in parameterized queries
            # dependent on the binding. We'll pass binary for safety if using standard bindings,
            # but typically raw list works with the extension's adapters. 
            # For now, we assume the extension handles the list->vector conversion.
            
            rows = conn.execute("""
                SELECT
                    rowid,
                    distance
                FROM vec_items
                WHERE embedding MATCH ?
                ORDER BY distance
                LIMIT ?
            """, (json.dumps(query_vector), limit)).fetchall()
            
            # Resolve back to chunks with VFS context
            for r in rows:
                chunk_id = r['rowid']
                # Join with files to get vfs_path
                query = """
                    SELECT c.*, f.vfs_path 
                    FROM chunks c 
                    JOIN files f ON c.file_id = f.id 
                    WHERE c.id=?
                """
                chunk = conn.execute(query, (chunk_id,)).fetchone()
                
                if chunk:
                    res = dict(chunk)
                    res['score'] = r['distance']
                    results.append(res)
                    
        except Exception as e:
            self.log_error(f"Vector Search Error: {e}")
        finally:
            conn.close()
            
        return results










--------------------------------------------------------------------------------
FILE: __ChalkBoardMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ChalkBoardMS
ENTRY_POINT: __ChalkBoardMS.py
DEPENDENCIES: None
"""

import webview 
import json
import os

# --- EMBEDDED HTML WITH NEW THEMES ---
HTML_CONTENT = """
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>OBS Signboard</title>
    <link href="https://fonts.googleapis.com/css2?family=Neonderthaw&family=Press+Start+2P&family=Fredericka+the+Great&family=Orbitron:wght@700&family=Special+Elite&display=swap" rel="stylesheet">
    <style>
        body, html { margin: 0; padding: 0; height: 100%; overflow: hidden; display: flex; justify-content: center; align-items: center; transition: all 0.5s ease; }
        #sign-container { width: 90%; text-align: center; outline: none; cursor: text; transition: transform 0.2s; }
        
        /* --- THEME 1: NEON NIGHTS --- */
        body.neon { background-color: #050505; font-family: 'Neonderthaw', cursive; }
        body.neon #sign-container { color: #fff; font-size: 8rem; text-shadow: 0 0 7px #fff, 0 0 42px #bc13fe, 0 0 102px #bc13fe; animation: flicker 1.5s infinite alternate; }

        /* --- THEME 2: 8-BIT HACKER --- */
        body.terminal { background-color: #000; font-family: 'Press Start 2P', cursive; }
        body.terminal #sign-container { color: #00ff41; font-size: 3.5rem; text-shadow: 0 0 10px #00ff41; text-transform: uppercase; }
        body.terminal #sign-container::after { content: '_'; animation: blink 1s step-end infinite; }

        /* --- THEME 3: CHALKBOARD --- */
        body.chalk { background-color: #2b3a28; font-family: 'Fredericka the Great', cursive; background-image: radial-gradient(circle, rgba(255,255,255,0.05) 1px, transparent 1px); background-size: 20px 20px; }
        body.chalk #sign-container { color: rgba(255,255,255,0.9); font-size: 6rem; transform: rotate(-1deg); }

        /* --- NEW THEME 4: BLUEPRINT (Technical) --- */
        body.blueprint { background-color: #003366; font-family: 'Orbitron', sans-serif; background-image: linear-gradient(#004080 1px, transparent 1px), linear-gradient(90deg, #004080 1px, transparent 1px); background-size: 50px 50px; }
        body.blueprint #sign-container { color: #00d9ff; font-size: 5rem; text-transform: uppercase; border: 2px solid #00d9ff; padding: 20px; box-shadow: 0 0 15px #00d9ff; }

        /* --- NEW THEME 5: RETRO WOOD --- */
        body.retro { background-color: #3d2b1f; font-family: 'Special Elite', serif; background-image: repeating-linear-gradient(90deg, transparent, transparent 40px, rgba(0,0,0,0.1) 41px); }
        body.retro #sign-container { color: #e6b450; font-size: 5.5rem; text-shadow: 2px 2px 0px #20150d; }

        /* --- NEW THEME 6: CYBERPUNK (Yellow/Black) --- */
        body.cyber { background-color: #fcee0a; font-family: 'Orbitron', sans-serif; }
        body.cyber #sign-container { color: #000; font-size: 5rem; font-weight: 900; text-transform: uppercase; font-style: italic; background: #000; color: #fcee0a; padding: 10px 40px; clip-path: polygon(0% 0%, 100% 0%, 95% 100%, 5% 100%); }

        /* ANIMATIONS & EFFECTS */
        @keyframes flicker { 0%, 19%, 21%, 100% { opacity: 1; } 20% { opacity: 0.5; } }
        @keyframes blink { 0%, 100% { opacity: 1; } 50% { opacity: 0; } }
        .shake { animation: shake 0.5s cubic-bezier(.36,.07,.19,.97) both; }
        @keyframes shake { 10%, 90% { transform: translate3d(-1px, 0, 0); } 20%, 80% { transform: translate3d(2px, 0, 0); } 30%, 50%, 70% { transform: translate3d(-4px, 0, 0); } 40%, 60% { transform: translate3d(4px, 0, 0); } }
    </style>
</head>
<body class="neon">
    <div id="sign-container" contenteditable="true" spellcheck="false">ON AIR</div>

    <script>
        const container = document.getElementById('sign-container');

        function updateDisplay(text, theme) {
            container.innerText = text;
            document.body.className = theme;
        }

        function triggerEffect(effect) {
            if (effect === 'shake') {
                container.classList.add('shake');
                setTimeout(() => container.classList.remove('shake'), 500);
            }
        }

        // Notify Python on load
        window.addEventListener('pywebviewready', () => {
            window.pywebview.api.loaded().then(state => {
                updateDisplay(state.text, state.theme);
            });
        });

        document.addEventListener('keydown', (e) => {
            const themes = { 'F1': 'neon', 'F2': 'terminal', 'F3': 'chalk', 'F4': 'blueprint', 'F5': 'retro', 'F6': 'cyber' };
            if (themes[e.key]) {
                document.body.className = themes[e.key];
                window.pywebview.api.log_action('switch_theme_' + themes[e.key]);
            }
        });
    </script>
</body>
</html>
"""

# --- PYTHON MICROSERVICE ---
try:
    from microservice_std_lib import service_metadata, service_endpoint
except ImportError:
    def service_metadata(**kwargs): return lambda c: c
    def service_endpoint(**kwargs): return lambda f: f

@service_metadata(
    name="ChalkboardWeb",
    version="2.0.1",
    description="Integrated HTML5/CSS3 Digital Signage Engine",
    tags=["ui", "webview", "obs"],
    capabilities=["ui:gui"]
)
class ChalkBoardMS:
    def __init__(self):
        self._window = None
        self.state = {"text": "ON AIR", "theme": "neon"}

    def loaded(self):
        """Called by JS when the page is ready."""
        print("Frontend handshake complete.")
        return self.state

    def log_action(self, action_name):
        """Called by JS when user interacts."""
        print(f"Webview Event: {action_name}")

    @service_endpoint(inputs={"text": "str", "theme": "str"}, outputs={})
    def update_sign(self, text: str, theme: str = "neon"):
        """Updates the embedded HTML via JS injection."""
        self.state["text"] = text
        self.state["theme"] = theme
        if self._window:
            sanitized_text = json.dumps(text)
            self._window.evaluate_js(f"updateDisplay({sanitized_text}, '{theme}')")

    @service_endpoint(inputs={"effect": "str"}, outputs={})
    def trigger_effect(self, effect: str):
        """Triggers CSS animations like 'shake'."""
        if self._window:
            self._window.evaluate_js(f"triggerEffect('{effect}')")

def start_app():
    api = ChalkBoardMS()
    window = webview.create_window(
        'OBS Signboard v2', 
        html=HTML_CONTENT, # No external file dependency now!
        js_api=api,
        width=1000, 
        height=700,
        background_color='#000000'
    )
    api._window = window
    webview.start(debug=True)

if __name__ == "__main__":
    start_app()
--------------------------------------------------------------------------------
FILE: __ChunkingRouterMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ChunkingRouterMS
ENTRY_POINT: __ChunkingRouterMS.py
DEPENDENCIES: None
"""

import re
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint
from __PythonChunkerMS import PythonChunkerMS, CodeChunk

@service_metadata(
    name="ChunkingRouterMS",
    version="1.1.0",
    description="The Dispatcher: Routes files to specialized chunkers based on extension (AST for Python, Recursive for Prose).",
    tags=["orchestration", "chunking", "nlp"],
    capabilities=["routing", "text-processing"]
)
class ChunkingRouterMS:
    """
The Editor: A 'Recursive' text splitter.
It respects the natural structure of text (Paragraphs -> Sentences -> Words)
rather than just hacking it apart by character count.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    self.python_specialist = PythonChunkerMS()
    # Separators for the Prose Specialist logic
    self.separators = ["\n\n", "\n", "(?<=[.?!])\s+", " ", ""]

    @service_endpoint(
        inputs={"text": "str", "filename": "str", "max_size": "int", "overlap": "int"},
        outputs={"chunks": "list"},
        description="Routes text to the appropriate specialist. Returns a list of CodeChunk objects or raw strings.",
        tags=["routing", "chunking"]
    )
    def chunk_file(self, text: str, filename: str, max_size: int = 1000, overlap: int = 100) -> List[Any]:
        """
        Extension-aware router.
        """
        if filename.endswith(".py"):
            return self.python_specialist.chunk(text)
        
        # Fallback to the internal Prose Specialist (Recursive Splitter)
        raw_chunks = self._recursive_split(text, self.separators, max_size, overlap)
        
        # Standardize output for the Refinery: Wrap prose in CodeChunk objects
        return [
            CodeChunk(
                name=f"prose_chunk_{i}", 
                type="text", 
                content=c, 
                start_line=0, 
                end_line=0
            ) for i, c in enumerate(raw_chunks)
        ]

    def _recursive_split(self, text: str, separators: List[str], max_size: int, overlap: int) -> List[str]:
        final_chunks = []
        
        # 1. Base Case: If the text fits, return it
        if len(text) <= max_size:
            return [text]
        
        # 2. Edge Case: No more separators, forced hard split
        if not separators:
            return self._hard_split(text, max_size, overlap)

        # 3. Recursive Step: Try to split by the current separator
        current_sep = separators[0]
        next_separators = separators[1:]
        
        # Regex split to keep delimiters if possible (logic varies by regex complexity)
        # For simple string splits like \n\n, we just split.
        if len(current_sep) > 1 and "(" in current_sep: 
            # It's a regex lookbehind (sentence splitter), use re.split
            splits = re.split(current_sep, text)
        else:
            splits = text.split(current_sep)

        # Now we have a list of smaller pieces. We need to merge them back together
        # until they fill the 'max_size' bucket, then start a new bucket.
        current_doc = []
        current_length = 0
        
        for split in splits:
            if not split: continue
            
            # If a single split is STILL too big, recurse deeper on it
            if len(split) > max_size:
                # If we have stuff in the buffer, flush it first
                if current_doc:
                    final_chunks.append(current_sep.join(current_doc))
                    current_doc = []
                    current_length = 0
                
                # Recurse on the big chunk using the NEXT separator
                sub_chunks = self._recursive_split(split, next_separators, max_size, overlap)
                final_chunks.extend(sub_chunks)
                continue

            # Check if adding this split would overflow
            if current_length + len(split) + len(current_sep) > max_size:
                # Flush the current buffer
                doc_text = current_sep.join(current_doc)
                final_chunks.append(doc_text)
# Start new buffer with overlap logic?
                # For simplicity in recursion, we often just start fresh or carry over 
                # a small tail if we implemented a rolling window here.
                # To keep this "Pure logic" simple, we start fresh with the current split.
                current_doc = [split]
                current_length = len(split)
            else:
                # Add to buffer
                current_doc.append(split)
                current_length += len(split) + len(current_sep)

        # Flush remaining
        if current_doc:
            final_chunks.append(current_sep.join(current_doc))

        return final_chunks

    def _hard_split(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Last resort: naive character sliding window."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    chunker = SmartChunkerMS()
    print("Service ready:", chunker)
    
    # Example: A technical document with structure
    doc = """
    # Intro to AI
    Artificial Intelligence is great. It helps us code.
    
    ## How it works
    1. Ingestion: Reading data.
    2. Processing: Thinking about data.
    
    This is a very long paragraph that effectively serves as a stress test for the sentence splitter. It should hopefully not break in the middle of a thought! We want to keep sentences whole.
    """
    
    print("--- Testing Smart Chunking (Max 60 chars) ---")
    # We set max_size very small to force it to use the sentence/word splitters
    chunks = chunker.chunk(doc, max_size=60, overlap=0)
    
    for i, c in enumerate(chunks):
        print(f"[{i}] {repr(c)}")


--------------------------------------------------------------------------------
FILE: __CodeChunkerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeChunkerMS
ENTRY_POINT: __CodeChunkerMS.py
DEPENDENCIES: None
"""

import re
from typing import List, Dict, Any, Optional
from pathlib import Path
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="CodeChunker",
version="1.0.0",
description="Splits code into semantic blocks (Classes, Functions) using indentation and regex heuristics.",
tags=["parsing", "chunking", "code"],
capabilities=["filesystem:read"]
)
class CodeChunkerMS:
    """
The Surgeon (Pure Python Edition): Splits code into semantic blocks
    (Classes, Functions) using indentation and regex heuristics.
    
    Advantages: Zero dependencies. Works on any machine.
    Disadvantages: Slightly less precise than Tree-Sitter for messy code.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    # Regex to find definitions. Capture group 1 is the indentation.
    # Supports Python, JS, TS, Go signatures loosely.
        self.def_pattern = re.compile(
            r'^(\s*)(?:async\s+)?(?:class|def|function|func|var|const)\s+([a-zA-Z0-9_]+)', 
            re.MULTILINE
        )

    @service_endpoint(
    inputs={"file_path": "str", "max_chars": "int"},
    outputs={"chunks": "List[Dict]"},
    description="Reads a file and breaks it into logical blocks based on indentation.",
    tags=["parsing", "chunking"],
    side_effects=["filesystem:read"]
    )
    def chunk_file(self, file_path: str, max_chars: int = 1500) -> List[Dict[str, Any]]:
    """
    Reads a file and breaks it into logical blocks based on indentation.
    """
        path = Path(file_path)
        try:
            code = path.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return []

        return self._chunk_by_indentation(code, max_chars)

    def _chunk_by_indentation(self, code: str, max_chars: int) -> List[Dict]:
lines = code.splitlines()
        chunks = []
        
        current_chunk_lines = []
        current_start_line = 0
        current_indent = 0
        in_block = False
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # 1. Skip empty lines if we aren't in a block
            if not stripped and not in_block:
                continue

            # 2. Calculate Indentation
            indent_match = re.match(r'^(\s*)', line)
            indent_level = len(indent_match.group(1)) if indent_match else 0

            # 3. Check for Block Start (def/class at root level or low indent)
            # We allow indent < 4 spaces to catch top-level stuff or slight nesting
            match = self.def_pattern.match(line)
            is_def = match is not None and indent_level <= 4
            
            # IF we hit a new definition AND we have a chunk pending:
            if is_def and current_chunk_lines:
                # Save previous chunk
                self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                # Reset
                current_chunk_lines = []
                current_start_line = i + 1
                in_block = True
                current_indent = indent_level

            # IF we hit a line with LESS indentation than the current block start,
            # the block has ended. (Python/Yaml logic, mostly holds for C-style too if formatted)
            if in_block and stripped and indent_level <= current_indent and not is_def:
                # Special case: Closing braces '}' often have same indent as start
                if not stripped.startswith('}'):
                    self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                    current_chunk_lines = []
                    current_start_line = i + 1
                    in_block = False

            current_chunk_lines.append(line)

        # Flush remaining
        if current_chunk_lines:
            self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)

        return chunks

    def _finalize_chunk(self, chunks, lines, start_line, max_chars):
        """Recursively splits huge chunks if they exceed max_chars."""
        full_text = "\n".join(lines)
        if not full_text.strip(): return

        # If chunk is too big, split it by lines (naive fallback for massive functions)
        if len(full_text) > max_chars:
            self._split_large_block(chunks, lines, start_line, max_chars)
        else:
            chunks.append({
                "type": "block", # Generic type since we aren't parsing AST
                "text": full_text,
                "start_line": start_line,
                "end_line": start_line + len(lines)
            })

    def _split_large_block(self, chunks, lines, start_line, max_chars):
        """Force split a large block while keeping line boundaries."""
        current_sub = []
        current_len = 0
        sub_start = start_line
        
        for i, line in enumerate(lines):
            if current_len + len(line) > max_chars:
                if current_sub:
                    chunks.append({
                        "type": "fragment",
                        "text": "\n".join(current_sub),
                        "start_line": sub_start,
                        "end_line": sub_start + len(current_sub)
                    })
                current_sub = []
                current_len = 0
                sub_start = start_line + i
            
            current_sub.append(line)
            current_len += len(line)
            
        if current_sub:
            chunks.append({
                "type": "fragment",
                "text": "\n".join(current_sub),
                "start_line": sub_start,
                "end_line": sub_start + len(current_sub)
            })

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = CodeChunkerMS()
print("Service ready:", chunker)
    
# Test Python Code
    py_code = """
import os

def small_helper():
    return True

class DataProcessor:
    def __init__(self):
        self.data = []

    def process(self, raw_input):
        # This is a comment inside the function
        if raw_input:
            self.data.append(raw_input)
        return True
    """
    
    # Write temp file
    import tempfile
    with tempfile.NamedTemporaryFile(suffix=".py", mode="w+", delete=False) as tmp:
        tmp.write(py_code)
        tmp_path = tmp.name
        
    print(f"--- Chunking {tmp_path} (Pure Python) ---")
    chunks = chunker.chunk_file(tmp_path)
    
    for i, c in enumerate(chunks):
        print(f"\n[Chunk {i}] Lines {c['start_line']}-{c['end_line']}")
        print(f"{'-'*20}\n{c['text'].strip()}\n{'-'*20}")
        
    os.remove(tmp_path)

--------------------------------------------------------------------------------
FILE: __CodeGrapherMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CodeGrapherMS
ENTRY_POINT: __CodeGrapherMS.py
DEPENDENCIES: None
"""

import ast
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="CodeGrapher",
version="1.0.0",
description="Parses Python code to extract symbols (nodes) and call relationships (edges).",
tags=["parsing", "graph", "analysis"],
capabilities=["filesystem:read"]
)
class CodeGrapherMS:
    """
    The Cartographer of Logic: Parses Python code to extract high-level 
    symbols (classes, functions) and maps their 'Call' relationships.
    
    Output: A graph structure (Nodes + Edges) suitable for visualization 
    or dependency analysis.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
    self.nodes = [] # List of symbols (functions, classes)
    self.edges = [] # List of relationships (source -> target)

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"graph_data": "Dict[str, Any]"},
    description="Recursively scans a directory for .py files and builds the graph.",
    tags=["parsing", "graph"],
    side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Dict[str, Any]:
    """
    Recursively scans a directory for .py files and builds the graph.
    """
        root = Path(root_path).resolve()
        self.nodes = []
        self.edges = []
        
        if not root.exists():
            return {"error": f"Path {root} does not exist"}

        # 1. Parsing Pass (Create Nodes)
        for path in root.rglob("*.py"):
            try:
                # Skip hidden/venv folders
                if any(p.startswith('.') for p in path.parts) or "venv" in path.parts:
                    continue
                    
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    source = f.read()
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_symbols = self._parse_source(source, rel_path)
                self.nodes.extend(file_symbols)
                
            except Exception as e:
                print(f"Failed to parse {path.name}: {e}")

        # 2. Linking Pass (Create Edges)
        self._build_edges()

        return {
            "root": str(root),
            "node_count": len(self.nodes),
            "edge_count": len(self.edges),
            "nodes": self.nodes,
            "edges": self.edges
        }

    def _parse_source(self, source: str, file_path: str) -> List[Dict]:
        """
        Uses Python's AST to extract surgical symbol info.
        """
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return []

        visitor = SurgicalVisitor(file_path)
        visitor.visit(tree)
        return visitor.symbols

    def _build_edges(self):
        """
        Resolves 'calls' strings into explicit graph edges.
        """
        # Create a quick lookup map: "my_function" -> NodeID
        # Note: This is a naive lookup (name collision possible). 
        # A robust version would use "module.class.method" fully qualified names.
        name_map = {n['name']: n['id'] for n in self.nodes}

        for node in self.nodes:
            source_id = node['id']
            calls = node.get('calls', [])
            
            for target_name in calls:
                if target_name in name_map:
                    target_id = name_map[target_name]
                    
                    # Avoid self-loops for cleanliness
                    if source_id != target_id:
                        self.edges.append({
                            "source": source_id,
                            "target": target_id,
                            "type": "calls"
                        })

# --- Helper Class: The AST Walker ---

class SurgicalVisitor(ast.NodeVisitor):
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.symbols = []

    def visit_FunctionDef(self, node):
        self._handle_func(node, "function")

    def visit_AsyncFunctionDef(self, node):
        self._handle_func(node, "async_function")

    def visit_ClassDef(self, node):
        # Record the class
        class_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": class_id,
            "file": self.file_path,
            "name": node.name,
            "type": "class",
            "line": node.lineno,
            "calls": [] # Classes don't 'call' things directly usually, their methods do
        })
        # Visit children (methods)
        self.generic_visit(node)

    def _handle_func(self, node, type_name):
        # Extract outgoing calls from the function body
        calls = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.append(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.append(child.func.attr)
        
        unique_calls = list(set(calls))
        
        node_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": node_id,
            "file": self.file_path,
            "name": node.name,
            "type": type_name,
            "line": node.lineno,
            "calls": unique_calls
        })

# --- Independent Test Block ---
if __name__ == "__main__":
    import sys
    
    # Defaults to current directory
    target_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    
    print(f"Mapping Logic in: {target_dir}")
    grapher = CodeGrapherMS()
    print("Service ready:", grapher)
    graph_data = grapher.scan_directory(target_dir)
    
    print(f"\n--- Scan Complete ---")
    print(f"Nodes Found: {graph_data['node_count']}")
    print(f"Edges Built: {graph_data['edge_count']}")
    
    # Save to JSON for inspection
    out_file = "code_graph_dump.json"
    with open(out_file, "w") as f:
        json.dump(graph_data, f, indent=2)
    print(f"Graph saved to {out_file}")

--------------------------------------------------------------------------------
FILE: __CognitiveMemoryMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _CognitiveMemoryMS
ENTRY_POINT: __CognitiveMemoryMS.py
DEPENDENCIES: pip install pydantic
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pip install pydantic"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _CognitiveMemoryMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import uuid
import json
import logging
import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_MEMORY_FILE = Path("working_memory.jsonl")
FLUSH_THRESHOLD = 5  # Number of turns before summarizing to Long Term Memory
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("CognitiveMem")
# ==============================================================================

class CognitiveMemoryMS(BaseModel):
    """Atomic unit of memory."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)
    role: str # 'user', 'assistant', 'system', 'tool'
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

@service_metadata(
name="CognitiveMemory",
version="1.0.0",
description="Manages Short-Term (Working) Memory and orchestrates flushing to Long-Term Memory.",
tags=["memory", "history", "context"],
capabilities=["filesystem:read", "filesystem:write"]
)
class CognitiveMemoryMS:
    """
The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
flushing to Long-Term Memory (Vector Store).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.file_path = Path(self.config.get("persistence_path", DEFAULT_MEMORY_FILE))
self.summarizer = self.config.get("summarizer_func")
self.ingestor = self.config.get("long_term_ingest_func")
        
self.working_memory: List[CognitiveMemoryMS] = []
self._load_working_memory()

    # --- Working Memory Operations ---

    @service_endpoint(
    inputs={"role": "str", "content": "str", "metadata": "Dict"},
    outputs={"entry": "CognitiveMemoryMS"},
    description="Adds an item to working memory and persists it.",
    tags=["memory", "write"],
    side_effects=["filesystem:write"]
    )
    def add_entry(self, role: str, content: str, metadata: Dict = None) -> CognitiveMemoryMS:
    """Adds an item to working memory and persists it."""
        entry = CognitiveMemoryMS(role=role, content=content, metadata=metadata or {})
        self.working_memory.append(entry)
        self._append_to_file(entry)
        log.info(f"Added memory: [{role}] {content[:30]}...")
        return entry

    @service_endpoint(
    inputs={"limit": "int"},
    outputs={"context": "str"},
    description="Returns the most recent conversation history formatted for an LLM.",
    tags=["memory", "read", "llm"],
    side_effects=["filesystem:read"]
    )
    def get_context(self, limit: int = 10) -> str:
    """
    Returns the most recent conversation history formatted for an LLM.
    """
        recent = self.working_memory[-limit:]
        return "\n".join([f"{e.role.upper()}: {e.content}" for e in recent])

    def get_full_history(self) -> List[Dict]:
        """Returns the raw list of memory objects."""
        return [e.dict() for e in self.working_memory]

    # --- Consolidation (The "Sleep" Cycle) ---

    @service_endpoint(
    inputs={},
    outputs={},
    description="Signals that a turn is complete; checks if memory flush is needed.",
    tags=["memory", "maintenance"],
    side_effects=["filesystem:write"]
def commit_turn(self):
        """
        Signal that a "Turn" (User + AI response) is complete.
        Checks if memory is full and triggers a flush if needed.
        """
        if len(self.working_memory) >= FLUSH_THRESHOLD:
            self._flush_to_long_term()

    def _flush_to_long_term(self):
        """
        Compresses working memory into a summary and moves it to Long-Term storage.
        """
        if not self.summarizer or not self.ingestor:
            log.warning("Flush triggered but Summarizer/Ingestor not configured. Skipping.")
            return

        log.info("ðŸŒ€ Flushing Working Memory to Long-Term Storage...")
        
        # 1. Combine Text
        full_text = "\n".join([f"{e.role}: {e.content}" for e in self.working_memory])
        
        # 2. Summarize
        try:
            summary = self.summarizer(full_text)
            log.info(f"Summary generated: {summary[:50]}...")
        except Exception as e:
            log.error(f"Summarization failed: {e}")
            return

        # 3. Ingest into Vector DB
        try:
            meta = {
                "source": "cognitive_memory_flush", 
                "date": datetime.datetime.utcnow().isoformat(),
                "original_entry_count": len(self.working_memory)
            }
            self.ingestor(summary, meta)
            log.info("âœ… Saved to Long-Term Memory.")
        except Exception as e:
            log.error(f"Ingestion failed: {e}")
            return

        # 4. Clear Working Memory (but keep file history or archive it?)
        # For this pattern, we clear the 'Active' RAM, and maybe rotate the log file.
        self.working_memory.clear()
        self._rotate_log_file()

    # --- Persistence Helpers ---

    def _load_working_memory(self):
        """Rehydrates memory from the JSONL file."""
        if not self.file_path.exists():
            return
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.working_memory.append(CognitiveMemoryMS.parse_raw(line))
            log.info(f"Loaded {len(self.working_memory)} items from {self.file_path}")
        except Exception as e:
            log.error(f"Corrupt memory file: {e}")

    def _append_to_file(self, entry: CognitiveMemoryMS):
        """Appends a single entry to the JSONL log."""
        with open(self.file_path, 'a', encoding='utf-8') as f:
            f.write(entry.json() + "\n")

    def _rotate_log_file(self):
        """Renames the current log to an archive timestamp."""
        if self.file_path.exists():
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = self.file_path.with_name(f"memory_archive_{timestamp}.jsonl")
            self.file_path.rename(archive_name)
            log.info(f"Rotated memory log to {archive_name}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
# 1. Setup Mock Dependencies
def mock_summarizer(text):
    return f"SUMMARY OF {len(text)} CHARS: The user and AI discussed AI architecture."

def mock_ingest(text, metadata):
    print(f"\n[VectorDB] Indexing: '{text}'\n[VectorDB] Meta: {metadata}")

# 2. Initialize
print("--- Initializing Cognitive Memory ---")
mem = CognitiveMemoryMS({
    "summarizer_func": mock_summarizer,
    "long_term_ingest_func": mock_ingest
})
print("Service ready:", mem)

# 3. Simulate Conversation
print("\n--- Simulating Conversation ---")
mem.add_entry("user", "Hello, who are you?")
mem.add_entry("assistant", "I am a Cognitive Agent.")
mem.add_entry("user", "What is your memory capacity?")
mem.add_entry("assistant", "I have a tiered memory system.")
mem.add_entry("user", "That sounds complex.")

print(f"\nCurrent Context:\n{mem.get_context()}")

# 4. Trigger Flush (Threshold is 5)
print("\n--- Triggering Memory Flush ---")
mem.commit_turn() # Should trigger flush because count is 5

print(f"\nWorking Memory after flush: {len(mem.working_memory)} items")

# Cleanup
if Path("working_memory.jsonl").exists():
    os.remove("working_memory.jsonl")
# Clean up archives if any were made
for p in Path(".").glob("memory_archive_*.jsonl"):
    os.remove(p)

--------------------------------------------------------------------------------
FILE: __ContentExtractorMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ContentExtractorMS
ENTRY_POINT: __ContentExtractorMS.py
DEPENDENCIES: None
"""

import io
import re
import time
from typing import Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# Configuration for the Graph Mapper
@service_metadata(
    name="ContentExtractorMS",
    version="1.0.0",
    dependencies=["pypdf", "beautifulsoup4"],  # Automated for your script creator
    description="The Decoder: A specialist service for extracting clean text from complex formats like PDF and HTML.",
    tags=["utility", "extraction", "nlp"],
    capabilities=["pdf-to-text", "html-cleaning"]
)
class ContentExtractorMS:
    """
    The Decoder.
    A standalone utility microservice that separates the concern of 
    document parsing from ingestion logic.
    """
    
    def __init__(self):
        self.start_time = time.time()
        
        # Lazy load imports to prevent service crash if dependencies are missing
        self._pdf_ready = False
        try:
            from pypdf import PdfReader
            self._pdf_ready = True
        except ImportError:
            pass
            
        self._html_ready = False
        try:
            from bs4 import BeautifulSoup
            self._html_ready = True
        except ImportError:
            pass

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "pdf_support": "bool", "html_support": "bool"},
        description="Health check to verify which extraction backends are installed.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status and library availability."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "pdf_support": self._pdf_ready,
            "html_support": self._html_ready
        }

    @service_endpoint(
        inputs={"blob": "bytes", "mime_type": "str"},
        outputs={"text": "str"},
        description="Unified entry point for text extraction. Routes to the correct parser based on mime_type.",
        tags=["processing", "extraction"]
    )
    def extract_text(self, blob: bytes, mime_type: str) -> str:
        """
        Main routing logic for extraction. 
         logic is internalized here.
        """
        if "pdf" in mime_type.lower():
            return self._extract_pdf(blob)
        elif "html" in mime_type.lower():
            # Decode bytes to string for HTML parser
            try:
                html_content = blob.decode('utf-8', errors='ignore')
                return self._extract_html(html_content)
            except:
                return ""
        return ""

    def _extract_pdf(self, file_bytes: bytes) -> str:
        """Extracts text from a PDF blob using pypdf. [cite: 96-97]"""
        if not self._pdf_ready:
            return ""
        
        from pypdf import PdfReader
        text_content = []
        try:
            stream = io.BytesIO(file_bytes)
            reader = PdfReader(stream)
            for page in reader.pages:
                extracted = page.extract_text()
                if extracted:
                    text_content.append(extracted)
            return "\n".join(text_content)
        except Exception as e:
            return f"PDF Extraction Error: {e}"

    def _extract_html(self, html_content: str) -> str:
        """Cleans HTML to raw text using BeautifulSoup. [cite: 98-99]"""
        if not self._html_ready:
            return self._strip_tags_regex(html_content)
        
        from bs4 import BeautifulSoup
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            for script in soup(["script", "style", "meta", "noscript"]):
                script.decompose()
                
            text = soup.get_text()
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            return '\n'.join(chunk for chunk in chunks if chunk)
        except Exception:
            return self._strip_tags_regex(html_content)

    def _strip_tags_regex(self, html: str) -> str:
        """Fallback if BS4 is missing. [cite: 100]"""
        clean = re.compile('<.*?>')
        return re.sub(clean, '', html)

if __name__ == "__main__":
    svc = ContentExtractorMS()
    print("Service ready:", svc._service_info["name"])
    print("Health:", svc.get_health())
--------------------------------------------------------------------------------
FILE: __ContextAggregatorMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ContextAggregatorMS
ENTRY_POINT: __ContextAggregatorMS.py
DEPENDENCIES: None
"""

import os
import fnmatch
import datetime
from pathlib import Path
from typing import Set, Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
# Extensions known to be binary/non-text (Images, Archives, Executables)
DEFAULT_BINARY_EXTENSIONS = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".mp4", ".mkv", ".avi", ".mov", ".webm",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".exe", ".dll", ".so",
    ".db", ".sqlite", ".mdb", ".pyc", ".pyo", ".class", ".jar", ".wasm"
}

# Folders to ignore by default
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", ".env", 
    "dist", "build", "coverage", ".idea", ".vscode"
}
# ==============================================================================

@service_metadata(
name="ContextAggregator",
version="1.0.0",
description="Flattens a project folder into a single readable text file.",
tags=["filesystem", "context", "compilation"],
capabilities=["filesystem:read", "filesystem:write"]
)
class ContextAggregatorMS:
    """
The Context Builder: Flattens a project folder into a single readable text file.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
max_file_size_mb = self.config.get("max_file_size_mb", 1)
self.max_file_size_bytes = max_file_size_mb * 1024 * 1024

    @service_endpoint(
    inputs={"root_path": "str", "output_file": "str", "extra_exclusions": "Set[str]", "use_default_exclusions": "bool"},
    outputs={"file_count": "int"},
    description="Aggregates project files into a single text dump.",
    tags=["filesystem", "dump"],
    side_effects=["filesystem:read", "filesystem:write"]
    )
def aggregate(self, 
    root_path: str, 
    output_file: str, 
    extra_exclusions: Optional[Set[str]] = None,
    use_default_exclusions: bool = True) -> int:
        
        project_root = Path(root_path).resolve()
        out_path = Path(output_file).resolve()
        
        # Build Exclusions
        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_IGNORE_DIRS)
        if extra_exclusions:
            exclusions.update(extra_exclusions)

        # Build Binary List
        binary_exts = DEFAULT_BINARY_EXTENSIONS.copy() # Always keep binaries as base unless manually cleared
        
        file_count = 0
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.write(f"File Dump from Project: {project_root.name}\nGenerated: {timestamp}\n{'='*60}\n\n")

                for root, dirs, files in os.walk(project_root):
                    dirs[:] = [d for d in dirs if d not in exclusions]
                    
                    for filename in files:
                        if self._should_exclude(filename, exclusions): continue

                        file_path = Path(root) / filename
                        if file_path.resolve() == out_path: continue

                        if self._is_safe_to_dump(file_path, binary_exts):
                            self._write_file_content(out_f, file_path, project_root)
                            file_count += 1                            
        except IOError as e: print(f"Error writing dump: {e}")
return file_count

    def _should_exclude(self, filename: str, exclusions: Set[str]) -> bool:
        return any(fnmatch.fnmatch(filename, pattern) for pattern in exclusions)

    def _is_safe_to_dump(self, file_path: Path, binary_exts: Set[str]) -> bool:
        if "".join(file_path.suffixes).lower() in binary_exts: return False
        try:
            if file_path.stat().st_size > self.max_file_size_bytes: return False
            with open(file_path, 'rb') as f:
                if b'\0' in f.read(1024): return False
        except (IOError, OSError): return False
        return True

    def _write_file_content(self, out_f, file_path: Path, project_root: Path):
        relative_path = file_path.relative_to(project_root)
        header = f"\n{'-'*20} FILE: {relative_path} {'-'*20}\n"
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as in_f:
                out_f.write(header + in_f.read() + f"\n{'-'*60}\n")
        except Exception as e:
            out_f.write(f"\n[Error reading file: {e}]\n")

if __name__ == "__main__":
    svc = ContextAggregatorMS()
    print("Service ready:", svc)

--------------------------------------------------------------------------------
FILE: __DiffEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _DiffEngineMS
ENTRY_POINT: __DiffEngineMS.py
DEPENDENCIES: None
"""

import sqlite3
import difflib
import datetime
import uuid
import logging
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "diff_engine.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("DiffEngine")
# ==============================================================================

@service_metadata(
name="DiffEngine",
version="1.0.0",
description="Implements hybrid versioning (Head + Diff History) for file content.",
tags=["version-control", "diff", "db"],
capabilities=["db:sqlite", "filesystem:write"]
)
class DiffEngineMS:
    """
The Timekeeper: Implements a 'Hybrid' versioning architecture.
1. HEAD: Stores full current content for fast read access (UI/RAG).
2. HISTORY: Stores diff deltas using difflib for audit trails.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = Path(self.config.get("db_path", DB_PATH))
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. The Head (Fast Access Cache)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id TEXT PRIMARY KEY,
                    path TEXT UNIQUE NOT NULL,
                    content TEXT,
                    last_updated TIMESTAMP
                )
            """)
            
            # 2. The Rising Edge (Diff History)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS diff_log (
                    id TEXT PRIMARY KEY,
                    file_id TEXT NOT NULL,
                    timestamp TIMESTAMP,
                    change_type TEXT,  -- 'CREATE', 'EDIT', 'DELETE'
                    diff_blob TEXT,    -- The text output of difflib
                    author TEXT,
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)

    # --- Core Workflow ---

    @service_endpoint(
    inputs={"path": "str", "new_content": "str", "author": "str"},
    outputs={"status": "str", "file_id": "str"},
    description="Updates a file, creating a diff history entry and updating the head state.",
    tags=["version-control", "write"],
    side_effects=["db:write"]
    )
    def update_file(self, path: str, new_content: str, author: str = "agent") -> Dict[str, Any]:
    """
    The Atomic Update Operation:
    1. Checks current state.
        2. Calculates Diff.
        3. Writes Diff to History.
        4. Updates Head to New Content.
        
        Returns: Dict with status (changed/unchanged), file_id, and diff_summary.
        """
        path = str(Path(path).as_posix()) # Normalize path
        now = datetime.datetime.utcnow()
        
        with self._get_conn() as conn:
            # 1. Fetch Head
            row = conn.execute("SELECT id, content FROM files WHERE path = ?", (path,)).fetchone()
if not row:
                # --- CASE: NEW FILE ---
                file_id = str(uuid.uuid4())
                conn.execute(
                    "INSERT INTO files (id, path, content, last_updated) VALUES (?, ?, ?, ?)",
                    (file_id, path, new_content, now)
                )
                self._log_diff(conn, file_id, "CREATE", "[New File Created]", author, now)
                log.info(f"Created new file: {path}")
                return {"status": "created", "file_id": file_id}

            # --- CASE: EXISTING FILE ---
            file_id = row['id']
            old_content = row['content'] or ""

            # 2. Calculate Diff
            # difflib needs lists of lines
            old_lines = old_content.splitlines(keepends=True)
            new_lines = new_content.splitlines(keepends=True)

            # Standard unified diff
            diff_gen = difflib.unified_diff(
                old_lines, new_lines, 
                fromfile=f"a/{path}", tofile=f"b/{path}",
                lineterm=''
            )
            diff_text = "".join(diff_gen)

            if not diff_text:
                return {"status": "unchanged", "file_id": file_id}

            # 3. Write History
            self._log_diff(conn, file_id, "EDIT", diff_text, author, now)

            # 4. Update Head
            conn.execute(
                "UPDATE files SET content = ?, last_updated = ? WHERE id = ?",
                (new_content, now, file_id)
            )
            log.info(f"Updated file: {path}")
            return {"status": "updated", "file_id": file_id, "diff_size": len(diff_text)}

    def _log_diff(self, conn, file_id, change_type, diff_text, author, timestamp):
        diff_id = str(uuid.uuid4())
        conn.execute(
            "INSERT INTO diff_log (id, file_id, timestamp, change_type, diff_blob, author) VALUES (?, ?, ?, ?, ?, ?)",
            (diff_id, file_id, timestamp, change_type, diff_text, author)
        )

    # --- Retrieval ---

    @service_endpoint(
    inputs={"path": "str"},
    outputs={"content": "Optional[str]"},
    description="Fast retrieval of current content.",
    tags=["version-control", "read"],
    side_effects=["db:read"]
    )
    def get_head(self, path: str) -> Optional[str]:
    """Fast retrieval of current content."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT content FROM files WHERE path = ?", (path,)).fetchone()
            return row['content'] if row else None

    @service_endpoint(
    inputs={"path": "str"},
    outputs={"history": "List[Dict]"},
    description="Retrieves the full evolution history of a file.",
    tags=["version-control", "read"],
    side_effects=["db:read"]
    )
    def get_history(self, path: str) -> List[Dict]:
    """Retrieves the full evolution history of a file."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT id FROM files WHERE path = ?", (path,)).fetchone()
            if not row: return []
            
            rows = conn.execute(
                "SELECT timestamp, change_type, diff_blob, author FROM diff_log WHERE file_id = ? ORDER BY timestamp DESC",
                (row['id'],)
            ).fetchall()
            
            return [dict(r) for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
import os
if DB_PATH.exists(): os.remove(DB_PATH)
    
engine = DiffEngineMS()
print("Service ready:", engine)
    
    print("--- 1. Creating File ---")
    engine.update_file("notes.txt", "Todo List:\n1. Buy Milk\n")
    
    print("\n--- 2. Updating File (The Rising Edge) ---")
    # Change: Add 'Buy Eggs', Remove 'Buy Milk' (Simulating a replacement)
    new_text = "Todo List:\n1. Buy Eggs\n2. Code Python\n"
    res = engine.update_file("notes.txt", new_text, author="Jacob")
    
    print(f"Update Result: {res['status']}")
    
    print("\n--- 3. Inspecting History ---")
    history = engine.get_history("notes.txt")
    for event in history:
        print(f"\n[{event['timestamp']}] {event['change_type']} by {event['author']}")
        print(f"Diff Preview:\n{event['diff_blob'].strip()}")

    print("\n--- 4. Inspecting Head (Cache) ---")
    print(engine.get_head("notes.txt"))
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)


--------------------------------------------------------------------------------
FILE: __ExplorerWidgetMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ExplorerWidgetMS
ENTRY_POINT: __ExplorerWidgetMS.py
DEPENDENCIES: microservice-std-lib>=1.0.0
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["microservice-std-lib>=1.0.0"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _ExplorerWidgetMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import os
import queue
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional

import tkinter as tk
from tkinter import ttk

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
DEFAULT_EXCLUDED_FOLDERS = {
    "node_modules",
    ".git",
    "__pycache__",
    ".venv",
    ".mypy_cache",
    "_logs",
    "dist",
    "build",
    ".vscode",
    ".idea",
    "target",
    "out",
    "bin",
    "obj",
    "Debug",
    "Release",
    "logs",
}
# ==============================================================================


@service_metadata(
    name="ExplorerWidget",
    version="1.0.0",
    description="A standalone file system tree viewer widget.",
    tags=["ui", "filesystem", "widget"],
    capabilities=["ui:gui", "filesystem:read"],
)
class ExplorerWidgetMS(ttk.Frame):
    """
    A standalone file system tree viewer.
    """

    GLYPH_CHECKED = "[X]"
    GLYPH_UNCHECKED = "[ ]"

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config_data: Dict[str, Any] = config or {}
        parent = self.config_data.get("parent")
        super().__init__(parent)

        self.root_path: Path = Path(
            self.config_data.get("root_path", ".")
        ).resolve()
        self.use_defaults: bool = self.config_data.get(
            "use_default_exclusions", True
        )

        # GUI coordination
        self.gui_queue: queue.Queue = queue.Queue()
        self.folder_item_states: Dict[str, str] = {}
        self.state_lock = threading.RLock()

        self._setup_styles()
        self._build_ui()
        self.process_gui_queue()
        self.refresh_tree()

    # ------------------------------------------------------------------ UI SETUP

    def _setup_styles(self) -> None:
        style = ttk.Style()
        if "clam" in style.theme_names():
            style.theme_use("clam")

        style.configure(
            "Explorer.Treeview",
            background="#252526",
            foreground="lightgray",
            fieldbackground="#252526",
            borderwidth=0,
            font=("Consolas", 10),
        )
        style.map(
            "Explorer.Treeview",
            background=[("selected", "#007ACC")],
            foreground=[("selected", "white")],
        )

    def _build_ui(self) -> None:
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)

        self.tree = ttk.Treeview(
            self,
            show="tree",
            columns=("size",),
            selectmode="none",
            style="Explorer.Treeview",
        )
        self.tree.column("size", width=80, anchor="e")

        ysb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        xsb = ttk.Scrollbar(self, orient="horizontal", command=self.tree.xview)

        self.tree.configure(yscrollcommand=ysb.set, xscrollcommand=xsb.set)

        self.tree.grid(row=0, column=0, sticky="nsew")
        ysb.grid(row=0, column=1, sticky="ns")
        xsb.grid(row=1, column=0, sticky="ew")

        self.tree.bind("<ButtonRelease-1>", self._on_click)

    # ----------------------------------------------------------------- SERVICE API

    @service_endpoint(
        inputs={},
        outputs={},
        description="Rescans the directory and refreshes the tree view.",
        tags=["ui", "refresh"],
        side_effects=["filesystem:read", "ui:update"],
    )
    def refresh_tree(self) -> None:
        # Clear the tree view
        for item in self.tree.get_children():
            self.tree.delete(item)

        # Reset state
        with self.state_lock:
            self.folder_item_states.clear()
            self.folder_item_states[str(self.root_path)] = "checked"

        # Build a flat list of tree items for insertion
        root_id = str(self.root_path)
        tree_data: List[Dict[str, Any]] = [
            {
                "parent": "",
                "iid": root_id,
                "text": f" {self.root_path.name} (Root)",
                "open": True,
            }
        ]

        self._scan_recursive(self.root_path, root_id, tree_data)

        # Insert into Treeview
        for item in tree_data:
            self.tree.insert(
                item["parent"],
                "end",
                iid=item["iid"],
                text=item["text"],
                open=item.get("open", False),
            )
            self.tree.set(item["iid"], "size", "...")

        # Update glyphs
        self._refresh_visuals(root_id)

        # Kick off background size calculation (currently stubbed)
        threading.Thread(
            target=self._calc_sizes_thread,
            args=(root_id,),
            daemon=True,
        ).start()

    # ----------------------------------------------------------------- INTERNALS

    def _scan_recursive(
        self, current_path: Path, parent_id: str, data_list: List[Dict[str, Any]]
    ) -> None:
        try:
            items = sorted(
                current_path.iterdir(),
                key=lambda x: (not x.is_dir(), x.name.lower()),
            )
            for item in items:
                if not item.is_dir():
                    continue

                path_str = str(item.resolve())

                state = "checked"
                if self.use_defaults and item.name in DEFAULT_EXCLUDED_FOLDERS:
                    state = "unchecked"

                with self.state_lock:
                    self.folder_item_states[path_str] = state

                data_list.append(
                    {"parent": parent_id, "iid": path_str, "text": f" {item.name}"}
                )
                self._scan_recursive(item, path_str, data_list)
        except (PermissionError, OSError):
            # Skip directories we can't read
            pass

    def _on_click(self, event: tk.Event) -> None:
        item_id = self.tree.identify_row(event.y)
        if not item_id:
            return

        with self.state_lock:
            curr = self.folder_item_states.get(item_id, "unchecked")
            self.folder_item_states[item_id] = (
                "checked" if curr == "unchecked" else "unchecked"
            )

        self._refresh_visuals(str(self.root_path))

    def _refresh_visuals(self, start_node: str) -> None:
        def _update(node_id: str) -> None:
            if not self.tree.exists(node_id):
                return

            with self.state_lock:
                state = self.folder_item_states.get(node_id, "unchecked")

            glyph = (
                self.GLYPH_CHECKED if state == "checked" else self.GLYPH_UNCHECKED
            )

            name = Path(node_id).name
            if node_id == str(self.root_path):
                name += " (Root)"

            self.tree.item(node_id, text=f"{glyph} {name}")

            for child in self.tree.get_children(node_id):
                _update(child)

        _update(start_node)

    def _calc_sizes_thread(self, root_id: str) -> None:
        """
        Background worker for calculating folder sizes.

        Currently a stub so that the thread exits cleanly without errors.
        You can later extend this to walk the filesystem and push
        size updates via self.gui_queue.
        """
        return

    # ----------------------------------------------------------------- SERVICE API

    @service_endpoint(
        inputs={},
        outputs={"selected_paths": "List[str]"},
        description="Returns a list of currently checked folder paths.",
        tags=["ui", "read"],
        side_effects=["ui:read"],
    )
    def get_selected_paths(self) -> List[str]:
        selected: List[str] = []
        with self.state_lock:
            for path, state in self.folder_item_states.items():
                if state == "checked":
                    selected.append(path)
        return selected

    # ------------------------------------------------------------------ GUI QUEUE

    def process_gui_queue(self) -> None:
        while not self.gui_queue.empty():
            try:
                callback = self.gui_queue.get_nowait()
            except queue.Empty:
                break
            else:
                try:
                    callback()
                except Exception:
                    # In production you might want logging here.
                    pass

        # Schedule next pump
        self.after(100, self.process_gui_queue)


if __name__ == "__main__":
    # Simple harness for manual testing.
    root = tk.Tk()
    root.title("ExplorerWidgetMS Test Harness")

    widget = ExplorerWidgetMS({"parent": root, "root_path": os.getcwd()})
    widget.pack(fill="both", expand=True)

    root.mainloop()

--------------------------------------------------------------------------------
FILE: __FingerprintScannerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _FingerprintScannerMS
ENTRY_POINT: __FingerprintScannerMS.py
DEPENDENCIES: None
"""

import hashlib
import os
import logging
from pathlib import Path
from typing import Any, Dict, List, Set, Optional, Tuple
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Folders to ignore during the scan (Standard developer noise)
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", 
    "dist", "build", "coverage", "target", "out", "bin", "obj",
    "_project_library", "_sandbox", "_logs"
}

# Files to ignore
DEFAULT_IGNORE_FILES = {
    ".DS_Store", "Thumbs.db", "*.log", "*.tmp", "*.lock"
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("Fingerprint")
# ==============================================================================

@service_metadata(
name="FingerprintScanner",
version="1.0.0",
description="Scans a directory tree and generates a deterministic SHA-256 fingerprint.",
tags=["scanning", "integrity", "hashing"],
capabilities=["filesystem:read"]
)
class FingerprintScannerMS:
    """
The Detective: Scans a directory tree and generates a deterministic
'Fingerprint' (SHA-256 Merkle Root) representing its exact state.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"state": "Dict[str, Any]"},
    description="Scans the project and returns a comprehensive state object (hashes + Merkle root).",
    tags=["scanning", "read"],
    side_effects=["filesystem:read"]
    )
def scan_project(self, root_path: str) -> Dict[str, Any]:
        """
        Scans the project and returns a comprehensive state object.
            output = {
                "root": str,
                "project_fingerprint": str (The global hash),
                "file_hashes": {rel_path: sha256},
                "file_count": int
            }
        """
        root = Path(root_path).resolve()
        if not root.exists():
            raise FileNotFoundError(f"Path not found: {root}")

        file_map = {}
        
        # 1. Walk and Hash
        # Use sorted() to ensure iteration order doesn't affect the final hash
        for path in sorted(root.rglob("*")):
            if path.is_file():
                if self._should_ignore(path, root):
                    continue
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_hash = self._hash_file(path)
                
                if file_hash:
                    file_map[rel_path] = file_hash

        # 2. Calculate Merkle Root (Global Fingerprint)
        # We sort by relative path to ensure deterministic ordering
        sorted_hashes = [file_map[p] for p in sorted(file_map.keys())]
        combined_data = "".join(sorted_hashes).encode('utf-8')
        project_fingerprint = hashlib.sha256(combined_data).hexdigest()

        log.info(f"Scanned {len(file_map)} files. Fingerprint: {project_fingerprint[:8]}...")

        return {
            "root": str(root),
            "project_fingerprint": project_fingerprint,
            "file_hashes": file_map,
            "file_count": len(file_map)
        }

    def _should_ignore(self, path: Path, root: Path) -> bool:
        """Checks path against exclusion lists."""
        try:
            rel_parts = path.relative_to(root).parts
            
            # Check directories
            # If any parent directory is in the ignore list, skip
            for part in rel_parts[:-1]: 
                if part in DEFAULT_IGNORE_DIRS:
                    return True
            
            # Check filename
            import fnmatch
            name = path.name
            if name in DEFAULT_IGNORE_FILES:
                return True
            if any(fnmatch.fnmatch(name, pat) for pat in DEFAULT_IGNORE_FILES):
                return True
                
            return False
        except ValueError:
            return True

    def _hash_file(self, path: Path) -> Optional[str]:
        """Reads file bytes and returns SHA256 hash."""
        try:
            # Read binary to avoid encoding issues and to hash exact content
            content = path.read_bytes()
            return hashlib.sha256(content).hexdigest()
        except (PermissionError, OSError):
            log.warning(f"Could not read/hash: {path}")
            return None

# --- Independent Test Block ---
if __name__ == "__main__":
    import time
    
    # 1. Create a dummy project
    test_dir = Path("test_fingerprint_proj")
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
    test_dir.mkdir()
    
    (test_dir / "main.py").write_text("print('hello')")
    (test_dir / "utils.py").write_text("def add(a,b): return a+b")
    
    scanner = FingerprintScannerMS()
    print("Service ready:", scanner)
    
    # 2. Initial Scan
    print("--- Scan 1 (Initial) ---")
    state_1 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 1: {state_1['project_fingerprint']}")
    
    # 3. Modify a file
    print("\n--- Modifying 'main.py' ---")
    time.sleep(0.1) # Ensure filesystem timestamp tick (though we hash content)
    (test_dir / "main.py").write_text("print('hello world')")
    
    # 4. Scan again
    print("--- Scan 2 (After Modification) ---")
    state_2 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 2: {state_2['project_fingerprint']}")
    
    # 5. Compare
    if state_1['project_fingerprint'] != state_2['project_fingerprint']:
        print("\nâœ… SUCCESS: Fingerprint changed as expected.")
        # Find the diff
        for path, h in state_2['file_hashes'].items():
            if state_1['file_hashes'].get(path) != h:
                print(f"   Changed File: {path}")
    else:
        print("\nâŒ FAILURE: Fingerprint did not change.")

    # Cleanup
if test_dir.exists():
    import shutil
    shutil.rmtree(test_dir)

--------------------------------------------------------------------------------
FILE: __GitPilotMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _GitPilotMS
ENTRY_POINT: __GitPilotMS.py
DEPENDENCIES: None
"""

import os
import subprocess
import threading
import queue
import time
import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple, Any, Callable, Dict
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Detect if GitHub CLI is available
def which(cmd: str) -> Optional[str]:
    for p in os.environ.get("PATH", "").split(os.pathsep):
        f = Path(p) / cmd
        if os.name == 'nt':
            for ext in (".exe", ".cmd", ".bat"): 
                if (f.with_suffix(ext)).exists(): return str(f.with_suffix(ext))
        if f.exists() and os.access(f, os.X_OK): return str(f)
    return None

USE_GH = which("gh") is not None
# ==============================================================================

@dataclass GitPilotMS GitStatusEntry:
    path: str
    index: str
    workdir: str

@dataclass
GitPilotMS GitStatus:
    repo_path: str
    branch: Optional[str]
    ahead: int
    behind: int
    entries: List[GitStatusEntry]

# --- Backend: The Git Wrapper ---
GitPilotMS GitCLI:
    """
    A robust wrapper around the git command line executable.
    """
    def __init__(self, repo_path: Path):
        self.root = self._resolve_repo_root(repo_path)
def _run(self, args: List[str], *, cwd: Optional[Path] = None) -> Tuple[str, str]:
        cmd = ["git", *args]
        # Prevent console window popping up on Windows
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        proc = subprocess.run(
            cmd,
            cwd=str(cwd or self.root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
            startupinfo=startupinfo
        )
        if proc.returncode != 0:
            raise RuntimeError(proc.stderr.strip() or f"git {' '.join(args)} failed")
        return proc.stdout, proc.stderr

    @staticmethod
    def _resolve_repo_root(path: Path) -> Path:
        path = path.resolve()
        if (path / ".git").exists(): return path
        p = path
        while True:
            if (p / ".git").exists(): return p
            if p.parent == p: break
            p = p.parent
        return path

    def init(self) -> None:
        self._run(["init"])

    def status(self) -> GitStatus:
        try:
            out, _ = self._run(["rev-parse", "--abbrev-ref", "HEAD"])
            branch = out.strip()
        except Exception: branch = None
        
        ahead = behind = 0
        try:
            out, _ = self._run(["rev-list", "--left-right", "--count", "@{upstream}...HEAD"])
            left, right = out.strip().split()
            behind, ahead = int(left), int(right)
        except Exception: pass
        
        out, _ = self._run(["status", "--porcelain=v1"])
        entries = []
        for line in out.splitlines():
            if not line.strip(): continue
            xy = line[:2]
            path = line[3:]
            index, work = xy[0], xy[1]
            entries.append(GitStatusEntry(path=path, index=index, workdir=work))
        return GitStatus(str(self.root), branch, ahead, behind, entries)

    def stage(self, paths: List[str]) -> None:
        if paths: self._run(["add", "--"] + paths)

    def unstage(self, paths: List[str]) -> None:
        if paths: self._run(["reset", "HEAD", "--"] + paths)

    def diff(self, file: Optional[str] = None) -> str:
        args = ["diff"]
        if file: args += ["--", file]
        out, _ = self._run(args)
        return out

    def commit(self, message: str, author_name: str, author_email: str) -> str:
        env = os.environ.copy()
        if author_name: 
            env["GIT_AUTHOR_NAME"] = author_name
            env["GIT_COMMITTER_NAME"] = author_name
        if author_email:
            env["GIT_AUTHOR_EMAIL"] = author_email
            env["GIT_COMMITTER_EMAIL"] = author_email
            
        proc = subprocess.run(
            ["git", "commit", "-m", message], 
            cwd=str(self.root), 
            capture_output=True, 
            text=True, 
            env=env
        )
        if proc.returncode != 0: raise RuntimeError(proc.stderr.strip() or proc.stdout.strip())
        out, _ = self._run(["rev-parse", "HEAD"])
        return out.strip()

    def log(self, limit: int = 100) -> List[Tuple[str, str, str, int]]:
        fmt = "%H%x1f%s%x1f%an%x1f%at"
        try:
            out, _ = self._run(["log", f"-n{limit}", f"--pretty=format:{fmt}"])
            items = []
            for line in out.splitlines():
                commit, summary, author, at = line.split("\x1f")
                items.append((commit, summary, author, int(at)))
            return items
        except Exception: return []

    def branches(self) -> List[Tuple[str, bool]]:
        try:
            out, _ = self._run(["branch"])
            res = []
            for line in out.splitlines():
                is_head = line.strip().startswith("*")
                name = line.replace("*", "", 1).strip()
                res.append((name, is_head))
            return res
        except Exception: return []

    def checkout(self, name: str, create: bool = False) -> None:
        if create: self._run(["checkout", "-B", name])
        else: self._run(["checkout", name])

    def push(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        args = ["push", remote]
        if branch: args.append(branch)
        out, _ = self._run(args)
return out

    def pull(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        if branch: out, _ = self._run(["pull", remote, branch])
        else: out, _ = self._run(["pull", remote])
        return out

# --- Threading Helper ---
GitPilotMS Worker:
    def __init__(self, ui_callback):
        self.q = queue.Queue()
        self.ui_callback = ui_callback
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()

    def submit(self, op: str, func, *args, **kwargs):
        self.q.put((op, func, args, kwargs))

    def _loop(self):
        while True:
            op, func, args, kwargs = self.q.get()
            try:
                result = op, True, func(*args, **kwargs)
            except Exception as e:
                result = op, False, e
            finally:
                self.ui_callback(result)

# --- Frontend: The GUI Panel ---
@service_metadata(
name="GitPilot",
version="1.0.0",
description="A Tkinter GUI panel for Git operations (Stage, Commit, Push, Pull).",
tags=["ui", "git", "version-control", "widget"],
capabilities=["ui:gui", "filesystem:read", "filesystem:write", "network:outbound"]
)
GitPilotMS GitPilotMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
super().__init__(parent)

initial_path = self.config.get("initial_path")
self.repo_path = None
self.git = None
self.worker = Worker(self._on_worker_done)

self._build_ui()
if initial_path:
    self.set_repo(initial_path)

@service_endpoint(
inputs={"path": "Path"},
outputs={},
description="Sets the active repository path and refreshes status.",
tags=["git", "config"],
side_effects=["filesystem:read", "ui:update"]
)
def set_repo(self, path: Path):
    try:
        self.git = GitCLI(path)
        self.repo_path = self.git.root
        self.path_var.set(f"Repo: {self.repo_path}")
        self._refresh()
    except Exception as e:
        self.path_var.set(f"Error: {e}")

def _build_ui(self):
    self.columnconfigure(0, weight=1)
    self.rowconfigure(1, weight=1)

    # Status Bar
    bar = ttk.Frame(self)
    bar.grid(row=0, column=0, sticky="ew")
    self.path_var = tk.StringVar(value="No Repo Selected")
    self.busy_var = tk.StringVar()
    ttk.Label(bar, textvariable=self.path_var).pack(side="left", padx=5)
    ttk.Label(bar, textvariable=self.busy_var, foreground="blue").pack(side="right", padx=5)

    # Tabs
        self.nb = ttk.Notebook(self)
        self.nb.grid(row=1, column=0, sticky="nsew")
        
        self.tab_changes = self._build_changes_tab(self.nb)
        self.tab_log = self._build_log_tab(self.nb)
        
        self.nb.add(self.tab_changes, text="Changes")
        self.nb.add(self.tab_log, text="History")

    def _build_changes_tab(self, parent):
        frame = ttk.Frame(parent)
        paned = ttk.PanedWindow(frame, orient=tk.VERTICAL)
        paned.pack(fill="both", expand=True)

        # File List
        top = ttk.Frame(paned)
        top.rowconfigure(1, weight=1)
        top.columnconfigure(0, weight=1)
        
        # Toolbar
        tb = ttk.Frame(top)
        tb.grid(row=0, column=0, sticky="ew")
        ttk.Button(tb, text="Refresh", command=self._refresh).pack(side="left")
        ttk.Button(tb, text="Stage", command=self._stage).pack(side="left")
        ttk.Button(tb, text="Unstage", command=self._unstage).pack(side="left")
        ttk.Button(tb, text="Diff", command=self._show_diff).pack(side="left")
        ttk.Button(tb, text="Push", command=self._push).pack(side="left", padx=10)
        ttk.Button(tb, text="Pull", command=self._pull).pack(side="left")

        # Treeview
        self.tree = ttk.Treeview(top, columns=("path", "idx", "wd"), show="headings", selectmode="extended")
        self.tree.heading("path", text="Path")
        self.tree.heading("idx", text="Index")
        self.tree.heading("wd", text="Workdir")
        self.tree.column("path", width=400)
        self.tree.column("idx", width=50, anchor="center")
        self.tree.column("wd", width=50, anchor="center")
        self.tree.grid(row=1, column=0, sticky="nsew")
        
        paned.add(top, weight=3)

        # Commit Area
        bot = ttk.Frame(paned)
        bot.columnconfigure(1, weight=1)
        ttk.Label(bot, text="Message:").grid(row=0, column=0, sticky="nw")
        self.msg_text = tk.Text(bot, height=4)
        self.msg_text.grid(row=0, column=1, sticky="nsew")
        ttk.Button(bot, text="Commit", command=self._commit).grid(row=1, column=1, sticky="e", pady=5)
        
        paned.add(bot, weight=1)
        return frame

    def _build_log_tab(self, parent):
        frame = ttk.Frame(parent)
        self.log_tree = ttk.Treeview(frame, columns=("sha", "msg", "auth", "time"), show="headings")
        self.log_tree.heading("sha", text="SHA")
        self.log_tree.heading("msg", text="Message")
        self.log_tree.heading("auth", text="Author")
        self.log_tree.heading("time", text="Time")
        self.log_tree.column("sha", width=80)
        self.log_tree.column("msg", width=400)
        self.log_tree.pack(fill="both", expand=True)
        return frame

    # --- Actions ---

    def _submit(self, label, func, *args):
        self.busy_var.set(f"{label}...")
        self.worker.submit(label, func, *args)

    def _on_worker_done(self, result):
        self.after(0, self._handle_result, result)

    def _handle_result(self, result):
        label, ok, data = result
        self.busy_var.set("")
        if not ok:
            messagebox.showerror("Error", str(data))
            return
        
        if label == "refresh":
            status, logs = data
            self.tree.delete(*self.tree.get_children())
            for e in status.entries:
                self.tree.insert("", "end", values=(e.path, e.index, e.workdir))
            
            self.log_tree.delete(*self.log_tree.get_children())
            for sha, msg, auth, ts in logs:
                t_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(ts))
                self.log_tree.insert("", "end", values=(sha[:7], msg, auth, t_str))
        
        if label == "diff":
            top = tk.Toplevel(self)
            top.title("Diff")
            txt = tk.Text(top, font=("Consolas", 10))
            txt.pack(fill="both", expand=True)
            txt.insert("1.0", data)

        if label in ["stage", "unstage", "commit", "push", "pull"]:
            self._refresh()

    def _refresh(self):
        if not self.git: return
        self._submit("refresh", lambda: (self.git.status(), self.git.log()))

    def _get_selection(self):
        return [self.tree.item(i)['values'][0] for i in self.tree.selection()]

    def _stage(self):
        paths = self._get_selection()
        if paths: self._submit("stage", self.git.stage, paths)

    def _unstage(self):
        paths = self._get_selection()
        if paths: self._submit("unstage", self.git.unstage, paths)

    def _commit(self):
        msg = self.msg_text.get("1.0", "end").strip()
        if not msg: return
        self._submit("commit", self.git.commit, msg, "GitPilot", "pilot@local")
        self.msg_text.delete("1.0", "end")

    def _push(self):
        self._submit("push", self.git.push)

    def _pull(self):
        self._submit("pull", self.git.pull)

    def _show_diff(self):
        sel = self._get_selection()
        file = sel[0] if sel else None
        self._submit("diff", self.git.diff, file)

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Git Pilot Test")
    root.geometry("800x600")
    
    # Use current directory
    cwd = Path(os.getcwd())
    
    panel = GitPilotMS({"parent": root, "initial_path": cwd})
    print("Service ready:", panel)
    panel.pack(fill="both", expand=True)
    
    root.mainloop()

--------------------------------------------------------------------------------
FILE: __HeuristicSumMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _HeuristicSumMS
ENTRY_POINT: __HeuristicSumMS.py
DEPENDENCIES: None
"""

import re
import os
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: REGEX PATTERNS
# ==============================================================================
# Captures: def my_func, class MyClass, function myFunc, interface MyInterface
SIG_RE = re.compile(r'^\s*(def|class|function|interface|struct|impl|func)\s+([A-Za-z_][A-Za-z0-9_]*)')

# Captures: # Heading, ## Subheading
MD_HDR_RE = re.compile(r'^\s{0,3}(#{1,3})\s+(.+)')

# Captures: """ Docstring """ or ''' Docstring ''' (Start of block)
DOC_RE = re.compile(r'^\s*("{3}|\'{3})(.*)', re.DOTALL)
# ==============================================================================

@service_metadata(
name="HeuristicSum",
version="1.0.0",
description="Generates quick summaries of code/text files using regex heuristics (No AI).",
tags=["parsing", "summary", "heuristics"],
capabilities=["compute"]
)
class HeuristicSumMS:
    """
The Skimmer: Generates quick summaries of code/text files without AI.
Scans for high-value lines (headers, signatures, docstrings) and concatenates them.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"text": "str", "filename": "str", "max_chars": "int"},
outputs={"summary": "str"},
description="Generates a summary string from the provided text.",
tags=["summary", "parsing"]
)
def summarize(self, text: str, filename: str = "", max_chars: int = 480) -> str:
        """
        Generates a summary string from the provided text.
        """
        lines = text.splitlines()
        picks = []

        # 1. Scan top 20 lines for Markdown Headers
        for ln in lines[:20]:
            m = MD_HDR_RE.match(ln)
            if m:
                picks.append(f"Heading: {m.group(2).strip()}")

        # 2. Scan top 40 lines for Code Signatures (Functions/Classes)
        for ln in lines[:40]:
            m = SIG_RE.match(ln)
            if m:
                picks.append(f"{m.group(1)} {m.group(2)}")

        # 3. Check for Docstrings / Preamble
        if lines:
            # Join first 80 lines to check for multi-line docstrings
            joined = "\n".join(lines[:80])
            m = DOC_RE.match(joined)
            if m:
                # Grab the first few lines of the docstring content
                after = joined.splitlines()[1:3]
                if after:
                    clean_doc = " ".join(s.strip() for s in after).strip()
                    picks.append(f"Doc: {clean_doc}")

        # 4. Fallback: First non-empty line if nothing else found
        if not picks:
            head = " ".join(l.strip() for l in lines[:2] if l.strip())
            if head:
                picks.append(head)

        # 5. Add Filename Context
        if filename:
            picks.append(f"[{os.path.basename(filename)}]")

        # 6. Deduplicate and Format
        seen = set()
        uniq = []
        for p in picks:
            if p and p not in seen:
                uniq.append(p)
                seen.add(p)

        summary = " | ".join(uniq)
        
        # 7. Truncate
        if len(summary) > max_chars:
            summary = summary[:max_chars-3] + "..."
            
        return summary.strip() if summary else "[No summary available]"

# --- Independent Test Block ---
if __name__ == "__main__":
skimmer = HeuristicSumMS()
print("Service ready:", skimmer)
    
# Test 1: Python Code
    py_code = """
    class DataProcessor:
        '''
        Handles the transformation of raw input data into structured formats.
        '''
        def process(self, data):
            pass
    """
    print(f"Python Summary: {skimmer.summarize(py_code, 'processor.py')}")

    # Test 2: Markdown
    md_text = """
    # Project Roadmap
    ## Phase 1
    We begin with ingestion.
    """
    print(f"Markdown Summary: {skimmer.summarize(md_text, 'README.md')}")

--------------------------------------------------------------------------------
FILE: __IngestEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IngestEngineMS
ENTRY_POINT: __IngestEngineMS.py
DEPENDENCIES: requests
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["requests"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _IngestEngineMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import os
import time
import re
import sqlite3
import requests
import json
from typing import List, Generator, Dict, Any, Optional, Set
from dataclasses import dataclass
from microservice_std_lib import service_metadata, service_endpoint

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass IngestEngineMS IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

IngestEngineMS SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    Used to generate the 'DEPENDS_ON' edges in the Knowledge Graph.
    """
    def __init__(self):
        # Python: "from x import y", "import x"
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        # JS/TS: "import ... from 'x'", "require('x')"
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        ext = os.path.splitext(file_path)[1].lower()
        
        lines = content.split('\n')
        for line in lines:
            match = None
            if ext == '.py':
                match = self.py_pattern.match(line)
            elif ext in ['.js', '.ts', '.tsx', '.jsx']:
                match = self.js_pattern.search(line)
            
            if match:
                # Clean up the module name (e.g., "backend.database" -> "database")
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        
        return dependencies

@service_metadata(
name="IngestEngine",
version="1.0.0",
description="Reads files, chunks text, fetches embeddings, and weaves graph edges.",
tags=["ingest", "rag", "parsing", "embedding"],
capabilities=["filesystem:read", "network:outbound", "db:sqlite"]
)
IngestEngineMS IngestEngineMS:
    """
The Heavy Lifter: Reads files, chunks text, fetches embeddings,
populates the Graph Nodes, and weaves Graph Edges.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", "knowledge.db")
self.stop_signal = False
self.weaver = SynapseWeaver()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    @service_endpoint(
    inputs={"file_paths": "List[str]", "model_name": "str"},
    outputs={"status": "IngestStatus"},
    description="Processes a list of files, ingesting them into the knowledge graph.",
    tags=["ingest", "processing"],
    mode="generator",
    side_effects=["db:write", "network:outbound"]
    )
    def process_files(self, file_paths: List[str], model_name: str = "none") -> Generator[IngestStatus, None, None]:
    total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Optimization settings
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # Memory for graph weaving (Node Name -> Node ID)
        node_registry = {}
        file_contents = {} # Cache content for the weaving pass

        # --- PHASE 1: INGESTION (Files, Chunks, Nodes) ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)

            # 1. Read
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                file_contents[filename] = content # Cache for Phase 2
            except Exception as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"Error: {e}")
                continue

            # 2. Track File
            try:
                cursor.execute("INSERT OR REPLACE INTO files (path, last_updated) VALUES (?, ?)", 
                              (file_path, time.time()))
                file_id = cursor.lastrowid
            except sqlite3.Error:
                continue

            # 3. Create Graph Node (for Visualization)
            # We use the filename as the unique ID for the graph to make linking easier
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (filename, 'file', filename, json.dumps({"path": file_path})))
            
            node_registry[filename] = filename

            # 4. Chunking & Embedding
            chunks = self._chunk_text(content)
            
            for i, chunk_text in enumerate(chunks):
                if self.stop_signal: break
                
                embedding = None
                if model_name != "none":
                    embedding = self._get_embedding(model_name, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                    INSERT INTO chunks (file_id, chunk_index, content, embedding)
                    VALUES (?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": filename,
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            # Checkpoint per file
            conn.commit()

        # --- PHASE 2: WEAVING (Edges) ---
        yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
        
        edge_count = 0
        for filename, content in file_contents.items():
            if self.stop_signal: break
            
            # Find imports
            deps = self.weaver.extract_dependencies(content, filename)
            
            for dep in deps:
                # Naive matching: if 'database' is imported, look for 'database.py' or 'database.ts'
                # in our registry.
                target_id = None
                for potential_match in node_registry.keys():
                    if potential_match.startswith(dep + '.') or potential_match == dep:
                        target_id = potential_match
                        break
                
                if target_id and target_id != filename:
                    try:
                        cursor.execute("""
                            INSERT OR IGNORE INTO graph_edges (source, target, weight)
VALUES (?, ?, 1.0)
                        """, (filename, target_id))
                        edge_count += 1
                    except:
                        pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. Created {edge_count} dependency edges."
        )

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        if len(text) < chunk_size: return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += (chunk_size - overlap)
        return chunks

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": model, "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
# --- Independent Test Block ---
if __name__ == "__main__":
    TEST_DB = "test_ingest_v2.db"
    
    # Init DB Schema manually for test
    conn = sqlite3.connect(TEST_DB)
    conn.execute("CREATE TABLE IF NOT EXISTS files (id INTEGER PRIMARY KEY, path TEXT, last_updated REAL)")
    conn.execute("CREATE TABLE IF NOT EXISTS chunks (id INTEGER PRIMARY KEY, file_id INT, chunk_index INT, content TEXT, embedding BLOB)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)")
    conn.close()

    engine = IngestEngineMS({"db_path": TEST_DB})
    # Self-ingest to test dependency parsing
    files = ["_IngestEngineMS.py"] 
    
    print("Running Ingest V2...")
    for status in engine.process_files(files, "none"):
        print(f"[{status.progress_percent:.0f}%] {status.log_message}")
    
    # Verify Edges
    conn = sqlite3.connect(TEST_DB)
    edges = conn.execute("SELECT * FROM graph_edges").fetchall()
    nodes = conn.execute("SELECT * FROM graph_nodes").fetchall()
    print(f"\nResult: {len(nodes)} Nodes, {len(edges)} Edges.")
    conn.close()
    
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)

--------------------------------------------------------------------------------
FILE: __IntakeServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IntakeServiceMS
ENTRY_POINT: __IntakeServiceMS.py
DEPENDENCIES: None
"""

import os
import mimetypes
import requests
import fnmatch
import json
from pathlib import Path
from typing import Dict, Set, List, Any
from base_service import BaseService
from __CartridgeServiceMS import CartridgeServiceMS
from __ScannerMS import ScannerMS
import document_utils
from microservice_std_lib import service_metadata, service_endpoint

# Optional import for Web
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

@service_metadata(
    name="IntakeServiceMS",
    version="1.2.0",
    description="The Vacuum: Handles two-phase ingestion by scanning sources and processing selected paths into the cartridge.",
    tags=["ingestion", "scanner", "vfs"],
    capabilities=["filesystem:read", "web:crawl"]
)
class IntakeServiceMS(BaseService):
    """
    The Vacuum. 
    Now supports two-phase ingestion:
    1. Scan -> Build Tree (with .gitignore respect)
    2. Ingest -> Process selected paths
    """

    DEFAULT_IGNORE_DIRS = {
        '.git', '__pycache__', 'node_modules', 'venv', '.venv', 'env', '.env', 
        '.idea', '.vscode', 'dist', 'build', 'target', 'bin', 'obj', 
        '__cartridge__'
    }
    
    DEFAULT_IGNORE_EXTS = {
        '.pyc', '.pyd', '.exe', '.dll', '.so', '.db', '.sqlite', '.sqlite3', 
        '.bin', '.iso', '.img', '.zip', '.tar', '.gz', '.7z', '.jpg', '.png'
    }

    def service_metadata(self, **kwargs):
            """
            Accepts metadata dependencies to prevent crash on startup.
            """
            self.metadata = kwargs
            if hasattr(self, 'log_info'):
                self.log_info(f"Metadata registered: {list(kwargs.keys())}")

    def __init__(self, cartridge: CartridgeService):
        super().__init__("IntakeServiceMS")
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "cartridge_connected": "bool"},
        description="Standardized health check to verify service status and cartridge connectivity.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the IntakeServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "cartridge_connected": self.cartridge is not None
        }
        self.cartridge = cartridge
        self.ignore_patterns: Set[str] = set()

    def ingest_source(self, source_path: str) -> Dict[str, int]:
        """Headless/CLI Entry point: Scans and Ingests in one go."""
        self.cartridge.initialize_manifest()
        
        # Update Manifest source info
        self.cartridge.set_manifest("source_root", source_path)
        
        is_web = source_path.startswith("http")
        self.cartridge.set_manifest("source_type", "web_root" if is_web else "filesystem_dir")

        scanner = ScoutMS()
        tree_node = scanner.scan_directory(source_path, web_depth=1 if is_web else 0)
        
        if not tree_node:
             return {"error": "Source not found"}

        # Flatten tree to list of paths
        files_to_ingest = scanner.flatten_tree(tree_node)
        self.cartridge.set_manifest("ingest_config", {"auto_flattened": True, "count": len(files_to_ingest)})
        
        return self.ingest_selected(files_to_ingest, source_path)

    # --- PHASE 1: SCANNING ---

    @service_endpoint(
        inputs={"root_path": "str", "web_depth": "int"},
        outputs={"tree": "dict"},
        description="Scans a local directory or URL to build a hierarchical tree structure of available files.",
        tags=["scan", "discovery"]
    )
    def scan_path(self, root_path: str, web_depth: int = 0) -> Dict[str, Any]:
        """
        Unified Scanner Interface.
        Delegates to ScoutMS for both Web and Local FS to ensure consistent node structure.
        """
        scanner = ScannerMS()

        # 1. Delegate to Scanner
        tree_root = scanner.scan_directory(root_path, web_depth=web_depth)
        if not tree_root: return None

        # 2. Apply Persistence / Checked State
        # (We only do this for FS usually, but we can try for web if we had it)
        if not root_path.startswith("http"):
            saved_config = self._load_persistence(os.path.abspath(root_path))
            self._apply_persistence(tree_root, saved_config)
    
        return tree_root

    def _apply_persistence(self, node: Dict, saved_config: Dict):
        """Recursively applies checked state from saved config."""
        if 'rel_path' in node and node['rel_path'] in saved_config:
            node['checked'] = saved_config[node['rel_path']]
        elif 'children' in node:
            # Default check all if no config? Or check logic from before?
            pass
    
        if 'children' in node:
            for child in node['children']:
                self._apply_persistence(child, saved_config)

    def _scan_recursive(self, current_path: str, root_path: str, saved_config: Dict) -> Dict:
        name = os.path.basename(current_path)
        is_dir = os.path.isdir(current_path)
        rel_path = os.path.relpath(current_path, root_path).replace("\\", "/")
        
        node = {
            'name': name,
            'path': current_path,
            'rel_path': rel_path,
            'type': 'dir' if is_dir else 'file',
            'children': [],
            'checked': True
        }

        # Determine Check State
        if saved_config and rel_path in saved_config:
            # Respect user persistence
            node['checked'] = saved_config[rel_path]
        elif self._is_ignored(name) or (not is_dir and self._is_binary_ext(name)):
            # Default to unchecked if ignored
            node['checked'] = False

        if is_dir:
            try:
                with os.scandir(current_path) as it:
                    entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                    for entry in entries:
                        child = self._scan_recursive(entry.path, root_path, saved_config)
                        node['children'].append(child)
            except PermissionError:
                pass
        
        return node

    # --- PHASE 2: INGESTION ---

    @service_endpoint(
        inputs={"file_list": "list", "root_path": "str"},
        outputs={"stats": "dict"},
        description="Processes a specific list of files into the cartridge storage, handling text extraction and VFS indexing.",
        tags=["ingest", "write"],
        side_effects=["cartridge:write"]
    )
    def ingest_selected(self, file_list: List[str], root_path: str) -> Dict[str, int]:
        """Ingests only the specific files passed in the list."""
        stats = {"added": 0, "skipped": 0, "errors": 0}
        
        for file_path in file_list:
            try:
                # Calculate VFS Path
                try:
                    vfs_path = os.path.relpath(file_path, root_path).replace("\\", "/")
                except ValueError:
                    vfs_path = os.path.basename(file_path)

                self._read_and_store(Path(file_path), vfs_path, "filesystem", stats)
            except Exception as e:
                self.log_error(f"Error ingesting {file_path}: {e}")
                stats["errors"] += 1
        
        # --- POST-INGESTION: Update Manifest ---
        self._rebuild_directory_index()
        
        return stats

    def _rebuild_directory_index(self):
        """
        Scans 'files' table and populates 'directories' table.
        This creates the navigable VFS structure.
        """
        self.log_info("Rebuilding VFS Directory Index...")
        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            seen_dirs = set()
            
            for r in rows:
                path = r[0]
                # Walk up the path to register all parents
                current = os.path.dirname(path).replace("\\", "/")
                while current and current != "." and current not in seen_dirs:
                    self.cartridge.ensure_directory(current)
                    seen_dirs.add(current)
                    current = os.path.dirname(current).replace("\\", "/")
            
        except Exception as e:
            self.log_error(f"Directory Index Error: {e}")
        finally:
            conn.close()

    # --- HELPERS ---

    def _load_persistence(self, root_path: str) -> Dict[str, bool]:
        """Loads config from DB Manifest (Portable) or fallback to local."""
        # 1. Try DB Manifest
        try:
            conn = self.cartridge._get_conn()
            row = conn.execute("SELECT value FROM manifest WHERE key='ingest_config'").fetchone()
            conn.close()
            if row:
                return json.loads(row[0])
        except: pass
        
        # 2. Fallback to local (Legacy)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        if os.path.exists(cfg_path):
            try:
                with open(cfg_path, 'r') as f: return json.load(f)
            except: pass
        return {}

    def save_persistence(self, root_path: str, checked_map: Dict[str, bool]):
        """Saves user selections into the Cartridge Manifest (Portable)."""
        # 1. Save to DB
        try:
            conn = self.cartridge._get_conn()
            conn.execute("INSERT OR REPLACE INTO manifest (key, value) VALUES (?, ?)", 
                         ("ingest_config", json.dumps(checked_map)))
            conn.commit()
            conn.close()
        except Exception as e:
            self.log_error(f"Failed to save persistence to DB: {e}")

        # 2. Save local backup (Optional, keeps scan state if DB is deleted)
        cfg_path = os.path.join(root_path, ".ragforge.json")
        try:
            with open(cfg_path, 'w') as f: json.dump(checked_map, f, indent=2)
        except: pass

    def _load_gitignore(self, root_path: str):
        gitignore_path = os.path.join(root_path, '.gitignore')
        self.ignore_patterns = self.DEFAULT_IGNORE_DIRS.copy()
        if os.path.exists(gitignore_path):
            try:
                with open(gitignore_path, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            if line.endswith('/'): line = line[:-1]
                            self.ignore_patterns.add(line)
            except: pass

    def _is_ignored(self, name: str) -> bool:
        if name in self.ignore_patterns: return True
        for pattern in self.ignore_patterns:
            if fnmatch.fnmatch(name, pattern): return True
        return False

    def _is_binary_ext(self, name: str) -> bool:
        _, ext = os.path.splitext(name)
        return ext.lower() in self.DEFAULT_IGNORE_EXTS

    def _read_and_store(self, real_path: Path, vfs_path: str, origin_type: str, stats: Dict):
        mime_type, _ = mimetypes.guess_type(real_path)
        if not mime_type: mime_type = "application/octet-stream"
        
        content = None
        blob = None
        
        # 1. Try Binary Read First (Covers PDF/Images/Safe Read)
        try:
            with open(real_path, 'rb') as f:
                blob = f.read()
        except Exception as e:
            self.log_error(f"Read error {real_path}: {e}")
            stats["errors"] += 1
            return

        # 2. Text Extraction / Decoding Strategy
        lower_path = str(real_path).lower()
        
        if lower_path.endswith(".pdf"):
            # PDF: Extract text, keep blob
            content = document_utils.extract_text_from_pdf(blob)
            if not content: mime_type = "application/pdf" # Fallback if extraction fails
            
        elif lower_path.endswith(".html") or lower_path.endswith(".htm"):
            # HTML: Decode and Clean
            try:
                raw_text = blob.decode('utf-8', errors='ignore')
                content = document_utils.extract_text_from_html(raw_text)
            except: pass
            
        else:
            # Default: Try UTF-8 Decode
            try:
                content = blob.decode('utf-8')
            except UnicodeDecodeError:
                content = None # Leave as binary blob

        # 3. Store in Cartridge
        # If content is set, it will be chunked/indexed. If only blob, it's stored but skipped by refinery.
        success = self.cartridge.store_file(
            vfs_path, 
            str(real_path), 
            content=content, 
            blob=blob, 
            mime_type=mime_type, 
            origin_type=origin_type
        )
        
        if success: stats["added"] += 1
        else: stats["errors"] += 1

        if __name__ == "__main__":
            # Manual test setup requires a CartridgeService instance
            from __CartridgeServiceMS import CartridgeService
            mock_cartridge = CartridgeService(":memory:")
            svc = IntakeServiceMS(mock_cartridge)
            print("Service ready:", svc._service_info["name"])



--------------------------------------------------------------------------------
FILE: __IsoProcessMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _IsoProcessMS
ENTRY_POINT: __IsoProcessMS.py
DEPENDENCIES: None
"""

import multiprocessing as mp
import logging
import logging.handlers
import time
import queue
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# WORKER LOGIC (Runs in Child Process)
# ==============================================================================
def _isolated_worker(result_queue: mp.Queue, log_queue: mp.Queue, payload: Any, config: Dict[str, Any]):
    """
    Entry point for the child process.
    Configures a logging handler to send records back to the parent.
    """
    # 1. Setup Logging Bridge
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    # Clear default handlers to avoid duplicate prints in child
    for h in root.handlers[:]:
        root.removeHandler(h)
    
    # Send all logs to the parent via the queue
    qh = logging.handlers.QueueHandler(log_queue)
    root.addHandler(qh)
    
    log = logging.getLogger("IsoWorker")

    try:
        log.info(f"Worker PID {mp.current_process().pid} started.")
        
        # --- 2. Heavy Imports (Simulated) ---
        log.info("Loading heavy libraries (Torch/Transformers)...")
        # from transformers import pipeline
        time.sleep(0.2) # Simulate import time

        # --- 3. The Logic ---
        model_name = config.get("model_name", "default-model")
        log.info(f"Initializing model '{model_name}'...")
        
        # Simulate processing steps with progress reporting
        for i in range(1, 4):
            time.sleep(0.3)
            log.info(f"Processing chunk {i}/3...")
        
        processed_data = f"Processed({payload}) via {model_name}"
        
        # --- 4. Return Result ---
        log.info("Work complete. Returning result.")
        result_queue.put({"success": True, "data": processed_data})

    except Exception as e:
        log.exception("Critical failure in worker process.")
        result_queue.put({"success": False, "error": str(e)})

# ==============================================================================
# PARENT CONTROLLER (Runs in Main Process)
# ==============================================================================
@service_metadata(
name="IsoProcess",
version="1.0.0",
description="Spawns isolated processes with real-time logging feedback.",
tags=["process", "isolation", "safety"],
capabilities=["process:spawn"]
)
class IsoProcessMS:
    """
The Safety Valve: Spawns isolated processes with real-time logging feedback.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.timeout = self.config.get("timeout_seconds", 60)
        
# Setup main logger
        self.log = logging.getLogger("IsoParent")
        if not self.log.handlers:
            logging.basicConfig(
                level=logging.INFO, 
                format='%(asctime)s [%(name)s] %(message)s',
                datefmt='%H:%M:%S'
            )

    @service_endpoint(
    inputs={"payload": "Any", "config": "Dict"},
    outputs={"result": "Any"},
    description="Executes a payload in an isolated child process.",
    tags=["process", "execution"],
    side_effects=["process:spawn"]
    )
def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        config = config or {}
        
        # 1. Setup Queues
        ctx = mp.get_context("spawn")
        result_queue = ctx.Queue()
        log_queue = ctx.Queue()

        # 2. Setup Log Listener (The "Ear" of the parent)
        # This thread pulls logs from the queue and handles them in the main process
        listener = logging.handlers.QueueListener(log_queue, *logging.getLogger().handlers)
        listener.start()

        # 3. Launch Process
        process = ctx.Process(
            target=_isolated_worker,
            args=(result_queue, log_queue, payload, config)
        )
        
        self.log.info("ðŸš€ Spawning isolated process...")
        process.start()
        
        try:
            # 4. Wait for Result
            result_packet = result_queue.get(timeout=self.timeout)
            process.join()

            if result_packet["success"]:
                return result_packet["data"]
            else:
                raise RuntimeError(f"Worker Error: {result_packet['error']}")

        except queue.Empty:
            self.log.error("â³ Worker timed out! Terminating...")
            process.terminate()
            process.join()
            raise TimeoutError(f"Task exceeded {self.timeout}s limit.")
            
        finally:
            # Clean up the log listener so it doesn't hang
listener.stop()

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing IsoProcessMS with Live Logging ---")
    iso = IsoProcessMS({"timeout_seconds": 5})
    print("Service ready:", iso)
    
try:
        result = iso.execute("Sensitive Data", {"model_name": "DeepSeek-V3"})
        print(f"\n[Parent] Final Result: {result}")
    except Exception as e:
        print(f"\n[Parent] Failed: {e}")

--------------------------------------------------------------------------------
FILE: __LexicalSearchMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _LexicalSearchMS
ENTRY_POINT: __LexicalSearchMS.py
DEPENDENCIES: None
"""

import sqlite3
import json
import os
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="LexicalSearch",
version="1.0.0",
description="Lightweight BM25 keyword search using SQLite FTS5 (No AI required).",
tags=["search", "index", "sqlite"],
capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class LexicalSearchMS:
    """
The Librarian's Index: A lightweight, AI-free search engine.
    
Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
Ideal for environments where installing PyTorch/Transformers is impossible
or overkill.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
default_db = str(Path(__file__).parent / "lexical_index.db")
self.db_path = self.config.get("db_path", default_db)
self._init_db()

    def _init_db(self):
        """
        Sets up the schema. 
        Uses Triggers to automatically keep the FTS index in sync with the main table.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1. Main Content Table (Stores the actual data)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                metadata TEXT  -- JSON blob for extra info (path, author, etc)
            );
        """)
        
        # 2. Virtual FTS Table (The Search Index)
        # content='documents' means it references the table above (saves space)
        cur.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                content,
                content='documents',
                content_rowid='rowid'  -- Internal SQLite mapping
            );
        """)

        # 3. Triggers (The "Magic" - Auto-sync index on Insert/Delete/Update)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_au AFTER UPDATE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        
        conn.commit()
        conn.close()

    @service_endpoint(
    inputs={"doc_id": "str", "text": "str", "metadata": "Dict"},
    outputs={},
    description="Adds or updates a document in the FTS index.",
    tags=["search", "write"],
    side_effects=["db:write"]
    )
    def add_document(self, doc_id: str, text: str, metadata: Dict[str, Any] = None):
    """
    Adds or updates a document in the index.
    """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        meta_json = json.dumps(metadata or {})
        
        # Upsert logic (Replace if ID exists)
        cur.execute("""
            INSERT OR REPLACE INTO documents (id, content, metadata)
            VALUES (?, ?, ?)
        """, (doc_id, text, meta_json))
        
        conn.commit()
        conn.close()

    @service_endpoint(
    inputs={"query": "str", "top_k": "int"},
    outputs={"results": "List[Dict]"},
    description="Performs a BM25 ranked keyword search.",
    tags=["search", "read"],
    side_effects=["db:read"]
    )
    def search(self, query: str, top_k: int = 20) -> List[Dict]:
    """
    Performs a BM25 Ranked Search.
    """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row # Allows dict-like access
        cur = conn.cursor()
        
        try:
            # The SQL Magic: 'bm25(documents_fts)' calculates relevance score
            sql = """
                SELECT 
                    d.id, 
                    d.content, 
                    d.metadata,
                    snippet(documents_fts, 0, '<b>', '</b>', '...', 15) as preview,
                    bm25(documents_fts) as score
                FROM documents_fts 
                JOIN documents d ON d.rowid = documents_fts.rowid
                WHERE documents_fts MATCH ? 
                ORDER BY score ASC
                LIMIT ?
            """
            # FTS5 query syntax: quotes typically help with special chars
            safe_query = f'"{query}"'
            rows = cur.execute(sql, (safe_query, top_k)).fetchall()
            
            results = []
            for r in rows:
                results.append({
                    "id": r['id'],
                    "score": round(r['score'], 4),
                    "preview": r['preview'], # FTS5 auto-generates snippets!
                    "metadata": json.loads(r['metadata']),
                    "full_content": r['content']
                })
            
            return results
            
        except sqlite3.OperationalError as e:
            # Usually happens if query syntax is bad (e.g. unmatched quotes)
            print(f"Search syntax error: {e}")
            return []
        finally:
            conn.close()

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_name = "test_lexical.db"
    
    # 1. Init
    engine = LexicalSearchMS({"db_path": db_name})
    print("Service ready:", engine)
    
    # 2. Ingest Data
    print("Ingesting test data...")
    engine.add_document("doc1", "Python is a great language for data science.", {"category": "coding"})
    engine.add_document("doc2", "The snake python is a reptile found in jungles.", {"category": "biology"})
    engine.add_document("doc3", "Data science involves python, pandas, and SQL.", {"category": "coding"})
    
# 3. Search
query = "python data"
print(f"\nSearching for: '{query}'")
hits = engine.search(query)
    
for hit in hits:
    print(f"[{hit['score']:.4f}] {hit['id']} ({hit['metadata']['category']})")
    print(f"   Preview: {hit['preview']}")
        
# Cleanup
if os.path.exists(db_name):
    os.remove(db_name)

--------------------------------------------------------------------------------
FILE: __LibrarianServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _LibrarianServiceMS
ENTRY_POINT: __LibrarianServiceMS.py
DEPENDENCIES: None
"""

import os
import shutil
import sqlite3
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="LibrarianService",
version="1.0.0",
description="Manages the lifecycle (create, list, delete) of Knowledge Base files.",
tags=["kb", "management", "filesystem"],
capabilities=["filesystem:read", "filesystem:write", "db:sqlite"]
)
class LibrarianServiceMS:
    """
The Librarian: Manages the physical creation, deletion, and listing
of Knowledge Base (KB) files.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
storage_dir = self.config.get("storage_dir", "./cortex_dbs")
self.storage_dir = Path(storage_dir)
self.storage_dir.mkdir(parents=True, exist_ok=True)

    @service_endpoint(
    inputs={},
    outputs={"kbs": "List[str]"},
    description="Lists available Knowledge Base files.",
    tags=["kb", "read"],
    side_effects=["filesystem:read"]
    )
    def list_kbs(self) -> List[str]:
    """
    Scans the storage directory for .db files.
    Equivalent to api.listKBs() in Sidebar.tsx.
    """
        if not self.storage_dir.exists():
            return []
        
        # Return simple filenames sorted by modification time (newest first)
        files = list(self.storage_dir.glob("*.db"))
        files.sort(key=os.path.getmtime, reverse=True)
        return [f.name for f in files]

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"status": "Dict"},
    description="Creates a new Knowledge Base with the standard schema.",
    tags=["kb", "create"],
    side_effects=["filesystem:write", "db:write"]
    )
    def create_kb(self, name: str) -> Dict[str, str]:
    """
    Creates a new SQLite database and initializes the Cortex Schema.
    """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            raise FileExistsError(f"Knowledge Base '{safe_name}' already exists.")

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # --- THE CORTEX SCHEMA ---
            # 1. System Config: Stores version and global metadata
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS config (
                    key TEXT PRIMARY KEY,
                    value TEXT
                )
            """)
            
            # 2. Files: Tracks scanned files to avoid re-ingesting unchanged ones
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    path TEXT UNIQUE NOT NULL,
                    checksum TEXT,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'indexed'
                )
            """)
            
            # 3. Chunks: The actual atomic units of knowledge
            # Note: 'embedding' is stored as a BLOB (bytes) for raw vector data
cursor.execute("""
                CREATE TABLE IF NOT EXISTS chunks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_id INTEGER,
                    chunk_index INTEGER,
                    content TEXT,
                    embedding BLOB, 
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)
            
            # 4. Graph Nodes: For the GraphView visualization
            # Distinguishes between 'file' nodes and 'concept' nodes
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_nodes (
                    id TEXT PRIMARY KEY,
                    type TEXT,  -- 'file' or 'concept'
                    label TEXT,
                    data_json TEXT -- Flexible JSON for positions/colors
                )
            """)
            
            # 5. Graph Edges: The connections
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_edges (
                    source TEXT,
                    target TEXT,
                    weight REAL DEFAULT 1.0,
                    FOREIGN KEY(source) REFERENCES graph_nodes(id),
                    FOREIGN KEY(target) REFERENCES graph_nodes(id)
                )
            """)
            
            # Timestamp creation
            cursor.execute("INSERT INTO config (key, value) VALUES (?, ?)", 
                           ("created_at", str(time.time())))
            
            conn.commit()
            conn.close()
            return {"status": "success", "path": str(db_path), "name": safe_name}
            
        except Exception as e:
            # Cleanup on failure
            if db_path.exists():
                os.remove(db_path)
            raise e

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"success": "bool"},
    description="Deletes a Knowledge Base file.",
    tags=["kb", "delete"],
    side_effects=["filesystem:write"]
    )
    def delete_kb(self, name: str) -> bool:
    """
    Physically removes the database file.
    """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            os.remove(db_path)
            return True
        return False

    @service_endpoint(
    inputs={"source_name": "str"},
    outputs={"status": "Dict"},
    description="Creates a copy of an existing KB.",
    tags=["kb", "copy"],
    side_effects=["filesystem:write"]
    )
    def duplicate_kb(self, source_name: str) -> Dict[str, str]:
    """
    Creates a copy of an existing KB.
    """
        safe_source = self._sanitize_name(source_name)
        source_path = self.storage_dir / safe_source
        
if not source_path.exists():
            raise FileNotFoundError(f"Source KB '{safe_source}' not found.")

# Generate new name
base = safe_source.replace('.db', '')
new_name = f"{base}_copy.db"
dest_path = self.storage_dir / new_name

# Handle collision if copy already exists
counter = 1
while dest_path.exists():
    new_name = f"{base}_copy_{counter}.db"
    dest_path = self.storage_dir / new_name
    counter += 1

shutil.copy2(source_path, dest_path)
return {"status": "success", "name": new_name}

def _sanitize_name(self, name: str) -> str:
    """Ensures the filename ends in .db and has no illegal chars."""
    clean = "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).strip()
    clean = clean.replace(' ', '_')
    if not clean.endswith('.db'):
        clean += '.db'
    return clean

# --- Independent Test Block ---
if __name__ == "__main__":
print("Initializing Librarian Service...")
lib = LibrarianServiceMS({"storage_dir": "./test_brains"})
print("Service ready:", lib)
    
    # 1. Create
    print("Creating 'Project_Alpha'...")
    try:
        lib.create_kb("Project Alpha")
    except FileExistsError:
        print("Project Alpha already exists.")
        
    # 2. List
kbs = lib.list_kbs()
print(f"Available Brains: {kbs}")

# 3. Duplicate
if "Project_Alpha.db" in kbs:
    print("Duplicating Alpha...")
    lib.duplicate_kb("Project_Alpha.db")

# 4. Final List
print(f"Final Brains: {lib.list_kbs()}")

--------------------------------------------------------------------------------
FILE: __LogViewMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _LogViewMS
ENTRY_POINT: __LogViewMS.py
DEPENDENCIES: None
"""

import tkinter as tk
from tkinter import scrolledtext, filedialog
import queue
import logging
import datetime
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

class LogViewMS(logging.Handler):
    """Sends log records to a Tkinter-safe queue."""
def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.log_queue.put(record)

@service_metadata(
name="LogView",
version="1.0.0",
description="A thread-safe log viewer widget for Tkinter.",
tags=["ui", "logs", "widget"],
capabilities=["ui:gui", "filesystem:write"]
)
class LogViewMS(tk.Frame):
    """
The Console: A professional log viewer widget.
Features:
- Thread-safe (consumes from a Queue).
- Message Consolidation ("Error occurred (x5)").
- Level Filtering (Toggle INFO/DEBUG/ERROR).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    parent = self.config.get("parent")
    super().__init__(parent)
    self.log_queue = self.config.get("log_queue")

    # State for consolidation
    self.last_msg = None
    self.last_count = 0
    self.last_line_index = None

    self._build_ui()
    self._poll_queue()

def _build_ui(self):
    # Toolbar
    toolbar = tk.Frame(self, bg="#2d2d2d", height=30)
    toolbar.pack(fill="x", side="top")
# Filters
        self.filters = {
            "INFO": tk.BooleanVar(value=True),
            "DEBUG": tk.BooleanVar(value=True),
            "WARNING": tk.BooleanVar(value=True),
            "ERROR": tk.BooleanVar(value=True)
        }
        
        for level, var in self.filters.items():
            cb = tk.Checkbutton(
                toolbar, text=level, variable=var, 
                bg="#2d2d2d", fg="white", selectcolor="#444",
                activebackground="#2d2d2d", activeforeground="white"
            )
            cb.pack(side="left", padx=5)

        tk.Button(toolbar, text="Clear", command=self.clear, bg="#444", fg="white", relief="flat").pack(side="right", padx=5)
        tk.Button(toolbar, text="Save", command=self.save, bg="#444", fg="white", relief="flat").pack(side="right")

        # Text Area
        self.text = scrolledtext.ScrolledText(
            self, state="disabled", bg="#1e1e1e", fg="#d4d4d4", 
            font=("Consolas", 10), insertbackground="white"
        )
        self.text.pack(fill="both", expand=True)
        
        # Color Tags
        self.text.tag_config("INFO", foreground="#d4d4d4")
        self.text.tag_config("DEBUG", foreground="#569cd6")
        self.text.tag_config("WARNING", foreground="#ce9178")
        self.text.tag_config("ERROR", foreground="#f44747")
        self.text.tag_config("timestamp", foreground="#608b4e")

    def _poll_queue(self):
        """Pulls logs from the queue and updates UI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                self._display(record)
except queue.Empty:
            pass
        finally:
            self.after(100, self._poll_queue)

    def _display(self, record):
        level = record.levelname
        if not self.filters.get(level, tk.BooleanVar(value=True)).get():
            return

        msg = record.getMessage()
        ts = datetime.datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        
        self.text.config(state="normal")
        
        # Consolidation Logic
        if msg == self.last_msg:
            self.last_count += 1
            # Delete previous line content (keep timestamp)
            # This is complex in Tk text, simplified approach:
            # We just append (xN) if we can, otherwise standard print
            # For simplicity in this microservice, we will just append standard lines 
            # to ensure stability, or implement simple dedup:
            pass 
        else:
            self.last_msg = msg
            self.last_count = 1
        
        self.text.insert("end", f"[{ts}] ", "timestamp")
        self.text.insert("end", f"{msg}\n", level)
        self.text.see("end")
        self.text.config(state="disabled")

    @service_endpoint(
    inputs={},
    outputs={},
    description="Clears the log console.",
    tags=["ui", "logs"],
    side_effects=["ui:update"]
    )
    def clear(self):
    self.text.config(state="normal")
    self.text.delete("1.0", "end")
    self.text.config(state="disabled")

    @service_endpoint(
    inputs={},
    outputs={},
    description="Opens a dialog to save logs to a file.",
    tags=["ui", "filesystem"],
    side_effects=["filesystem:write", "ui:dialog"]
    )
    def save(self):
    path = filedialog.asksaveasfilename(defaultextension=".log", filetypes=[("Log Files", "*.log")])
        if path:
            try:
                with open(path, "w", encoding="utf-8") as f:
                    f.write(self.text.get("1.0", "end"))
            except Exception as e:
                print(f"Save failed: {e}")

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Log View Test")
    root.geometry("600x400")
    
    # 1. Setup Queue
    q = queue.Queue()
    
    # 2. Setup Logger
    logger = logging.getLogger("TestApp")
    logger.setLevel(logging.DEBUG)
    logger.addHandler(LogViewMS(q))
    
    # 3. Mount View
    log_view = LogViewMS({"parent": root, "log_queue": q})
    print("Service ready:", log_view)
    log_view.pack(fill="both", expand=True)
    
    # 4. Generate Logs
    def generate_noise():
        logger.info("System initializing...")
        logger.debug("Checking sensors...")
        logger.warning("Sensor 4 response slow.")
        logger.error("Connection failed!")
        root.after(2000, generate_noise)
        
    generate_noise()
    root.mainloop()

--------------------------------------------------------------------------------
FILE: __MonacoHostMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _MonacoHostMS
ENTRY_POINT: __MonacoHostMS.py
DEPENDENCIES: pywebview
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pywebview"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _MonacoHostMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import webview
import threading
import json
from pathlib import Path
from typing import Any, Dict, Optional, Callable
from microservice_std_lib import service_metadata, service_endpoint

# --- EMBEDDED MONACO HTML ---
MONACO_HTML = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Monaco Host</title>
    <style>
        html, body { margin: 0; padding: 0; width: 100%; height: 100%; overflow: hidden; background-color: #1e1e1e; font-family: sans-serif; }
        #container { display: flex; flex-direction: column; height: 100%; }
        #tabs { background: #252526; display: flex; overflow-x: auto; height: 35px; border-bottom: 1px solid #3e3e3e; }
        .tab { 
            padding: 8px 15px; color: #969696; background: #2d2d2d; cursor: pointer; border-right: 1px solid #1e1e1e; font-size: 12px;
            display: flex; align-items: center; white-space: nowrap;
        }
        .tab.active { background: #1e1e1e; color: #fff; border-top: 1px solid #007acc; }
        .tab:hover { background: #323233; color: #fff; }
        #editor { flex-grow: 1; }
    </style>
</head>
<body>
    <div id="container">
        <div id="tabs"></div>
        <div id="editor"></div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs/loader.js"></script>
    <script>
        require.config({ paths: { 'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs' }});
        let editor;
        let models = {}; 
        let currentPath = null;

        require(['vs/editor/editor.main'], function() {
            editor = monaco.editor.create(document.getElementById('editor'), {
                value: "# Monaco Editor Ready\\n",
                language: 'python',
                theme: 'vs-dark',
                automaticLayout: true,
                fontSize: 14
            });

            if (window.pywebview) window.pywebview.api.signal_editor_ready();

            editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.KeyS, function() {
                if (currentPath) {
                    window.pywebview.api.save_file(currentPath, editor.getValue());
                }
            });
        });

        window.pywebview = window.pywebview || {};
        window.pywebview.api = window.pywebview.api || {};

        window.pywebview.api.open_in_tab = function(filepath, content) {
            let ext = filepath.split('.').pop();
            let langMap = { 'py': 'python', 'js': 'javascript', 'html': 'html', 'json': 'json', 'css': 'css' };
            let lang = langMap[ext] || 'plaintext';

            if (!models[filepath]) {
                models[filepath] = monaco.editor.createModel(content, lang, monaco.Uri.file(filepath));
                const tab = document.createElement('div');
                tab.className = 'tab';
                tab.innerText = filepath.split(/[\\\\/]/).pop();
                tab.title = filepath;
                tab.onclick = () => switchTo(filepath);
                tab.dataset.path = filepath;
                document.getElementById('tabs').appendChild(tab);
            }
            switchTo(filepath);
        };

        window.pywebview.api.reveal_range = function(filepath, startLine, endLine) {
            if (filepath !== currentPath) switchTo(filepath);
            editor.revealLineInCenter(startLine);
            editor.setSelection({ startLineNumber: startLine, startColumn: 1, endLineNumber: endLine, endColumn: 1000 });
        };

        function switchTo(filepath) {
            if (!models[filepath]) return;
            editor.setModel(models[filepath]);
            currentPath = filepath;
            document.querySelectorAll('.tab').forEach(t => t.classList.toggle('active', t.dataset.path === filepath));
        }
    </script>
</body>
</html>
"""

class MonacoHostMS:
    def __init__(self):
        self._window = None
        self._ready_event = threading.Event()
        self.on_save_callback: Optional[Callable[[str, str], None]] = None

    def set_window(self, window):
        self._window = window

    def signal_editor_ready(self):
        self._ready_event.set()
        print("Monaco Editor is ready.")

    def save_file(self, filepath: str, content: str):
        if self.on_save_callback:
            self.on_save_callback(filepath, content)
        else:
            print(f"Saved {filepath} (No callback registered)")

    def open_file(self, filepath: str, content: str):
        self._ready_event.wait(timeout=10)
        if not self._window: return
        # Using json.dumps for both handles escaping perfectly
        js = f"window.pywebview.api.open_in_tab({json.dumps(filepath)}, {json.dumps(content)})"
        self._window.evaluate_js(js)

@service_metadata(
    name="MonacoHost",
    version="1.1.0",
    description="Hosts an embedded Monaco Editor instance.",
    tags=["ui", "editor", "webview"],
    capabilities=["ui:gui"]
)
class MonacoHostMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.api = MonacoHostMS()
        self.window = None

    @service_endpoint(mode="sync")
    def launch(self, title="Monaco Editor", width=1000, height=700, func=None):
        self.window = webview.create_window(
            title, 
            html=MONACO_HTML, # Pass the string directly here
            js_api=self.api,
            width=width, 
            height=height
        )
        self.api.set_window(self.window)
        webview.start(func, debug=True) if func else webview.start(debug=True)

if __name__ == "__main__":
    host = MonacoHostMS()
    
    def background_actions():
        host.api._ready_event.wait()
        host.api.open_file("demo.py", "print('Hello World')\\n# Try Ctrl+S")
        host.api.on_save_callback = lambda p, c: print(f"File: {p} was saved with {len(c)} chars.")

    host.launch(func=background_actions)
--------------------------------------------------------------------------------
FILE: __NetworkLayoutMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NetworkLayoutMS
ENTRY_POINT: __NetworkLayoutMS.py
DEPENDENCIES: pip install networkx
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pip install networkx"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _NetworkLayoutMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import networkx as nx
import logging
from typing import List, Dict, Any, Tuple, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("NetLayout")
# ==============================================================================

@service_metadata(
name="NetworkLayout",
version="1.0.0",
description="Calculates visual (x,y) coordinates for graph nodes using NetworkX.",
tags=["graph", "layout", "visualization"],
capabilities=["compute"]
)
class NetworkLayoutMS:
    """
The Topologist: Calculates visual coordinates for graph nodes using
server-side algorithms (NetworkX). 
Useful for generating static map snapshots or pre-calculating positions 
to offload client-side rendering.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"nodes": "List[str]", "edges": "List[Tuple]", "algorithm": "str"},
    outputs={"positions": "Dict[str, Tuple]"},
    description="Computes (x, y) coordinates for the given graph nodes and edges.",
    tags=["graph", "compute"],
    side_effects=[]
    )
    def calculate_layout(self, nodes: List[str], edges: List[Tuple[str, str]], 
    algorithm: str = "spring", **kwargs) -> Dict[str, Tuple[float, float]]:
    """
    Computes (x, y) coordinates for the given graph.
        
    :param nodes: List of node IDs.
        :param edges: List of (source, target) tuples.
        :param algorithm: 'spring' (Force-directed) or 'circular'.
        :return: Dictionary {node_id: (x, y)}
        """
        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        G.add_edges_from(edges)
        
        log.info(f"Computing layout for {len(nodes)} nodes, {len(edges)} edges...")
        
        try:
            if algorithm == "circular":
                pos = nx.circular_layout(G)
            else:
                # Spring layout (Fruchterman-Reingold) is standard for knowledge graphs
                k_val = kwargs.get('k', 0.15) # Optimal distance between nodes
                iter_val = kwargs.get('iterations', 50)
                pos = nx.spring_layout(G, k=k_val, iterations=iter_val, seed=42)
                
            # Convert numpy arrays to simple lists/tuples for JSON serialization
            return {n: (float(p[0]), float(p[1])) for n, p in pos.items()}
            
        except Exception as e:
            log.error(f"Layout calculation failed: {e}")
            return {}

# --- Independent Test Block ---
if __name__ == "__main__":
layout = NetworkLayoutMS()
print("Service ready:", layout)
    
# 1. Define a simple graph
    test_nodes = ["Main", "Utils", "Config", "DB", "Auth"]
    test_edges = [
        ("Main", "Utils"),
        ("Main", "Config"),
        ("Main", "DB"),
        ("Main", "Auth"),
        ("DB", "Config"),
        ("Auth", "DB")
    ]
    
    # 2. Compute Layout
    positions = layout.calculate_layout(test_nodes, test_edges, k=0.5)
    
    print("--- Calculated Positions ---")
    for node, (x, y) in positions.items():
        print(f"{node:<10}: ({x: .4f}, {y: .4f})")

--------------------------------------------------------------------------------
FILE: __NeuralGraphEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NeuralGraphEngineMS
ENTRY_POINT: __NeuralGraphEngineMS.py
DEPENDENCIES: None
"""

import pygame
import math
import random
import time

# Initialize font module globally once
pygame.font.init()

from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="NeuralGraphViewerMS",
    version="1.1.0",
    description="The Cartographer: A physics-driven rendering engine for visualizing complex neural relationships in a 2D force-directed graph.",
    tags=["visualization", "graph", "pygame"],
    capabilities=["force-directed-layout", "real-time-rendering"]
)
class NeuralGraphEngineMS:
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        self.width = width
        self.start_time = time.time()
        self.height = height
        self.bg_color = bg_color
        
        self.surface = pygame.Surface((width, height))
        
        # Camera
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction
        self.dragged_node_idx = None
        self.hovered_node_idx = None
        
        # Physics State
        self.settled = False

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "nodes": "int", "settled": "bool"},
        description="Standardized health check to verify the operational state of the graph renderer.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the NeuralGraphEngineMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "nodes": len(self.nodes),
            "settled": self.settled
        }

    def resize(self, width, height):
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    @service_endpoint(
        inputs={"nodes": "list", "links": "list"},
        outputs={},
        description="Injects new node and edge data into the engine and wakes up the physics simulation.",
        tags=["data", "update"]
    )
    def set_data(self, nodes, links):
        self.nodes = nodes
        self.links = links
        self.settled = False # Wake up physics on new data
        
        # 1. Build an ID map so we can find parents
        node_map = {node['id']: node for node in self.nodes}

        for n in self.nodes:
            # GNN Injection: Use pre-calculated layout if available
            if 'gnn_x' in n and 'gnn_y' in n:
                n['x'] = n['gnn_x'] * self.width
                n['y'] = n['gnn_y'] * self.height

            elif 'x' not in n:
                # SMART SPAWN: If I am a satellite, spawn near my planet
                parent_id = n.get('meta', {}).get('parent')
                if parent_id and parent_id in node_map and 'x' in node_map[parent_id]:
                    p = node_map[parent_id]
                    angle = random.random() * 6.28
                    dist = 30
                    n['x'] = p['x'] + math.cos(angle) * dist
                    n['y'] = p['y'] + math.sin(angle) * dist
                else:
                    # Random spawn for Files
                    n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                    n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Semantic Coloring
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # Blue
                n['_radius'] = 6
            elif n.get('type') == 'web':
                n['_color'] = (204, 0, 122) # Purple/Pink
                n['_radius'] = 7
            elif n.get('type') == 'chunk':
                n['_color'] = (100, 200, 100) # Satellite Green
                n['_radius'] = 3
            else:
                n['_color'] = (160, 32, 240) # Default
                n['_radius'] = 6

    # --- INPUT HANDLING ---
    
    def screen_to_world(self, sx, sy):
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def get_node_at(self, sx, sy):
        wx, wy = self.screen_to_world(sx, sy)
        for n in self.nodes:
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                return n
        return None

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2:
                self.dragged_node_idx = i
                self.settled = False # Wake up physics
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
            self.settled = False
        else:
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            return prev_hover != self.hovered_node_idx

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    def highlight_nodes(self, node_ids):
        """Highlights specific nodes by ID."""
        for n in self.nodes:
            # 1. Reset to defaults
            if n.get('type') == 'file': 
                n['_color'] = (0, 122, 204)
                n['_radius'] = 6
            elif n.get('type') == 'web': 
                n['_color'] = (204, 0, 122)
                n['_radius'] = 7
            elif n.get('type') == 'chunk': 
                n['_color'] = (100, 200, 100)
                n['_radius'] = 3
            else: 
                n['_color'] = (160, 32, 240)
                n['_radius'] = 6
                
            # 2. Apply Highlight
            if n['id'] in node_ids:
                n['_color'] = (255, 255, 0) # Bright Yellow
                n['_radius'] = 12
                
        self.settled = False # Wake up physics

    # --- PHYSICS (Damped) ---

    @service_endpoint(
        inputs={},
        outputs={"settled": "bool"},
        description="Performs one iteration of the force-directed physics calculation.",
        tags=["physics", "lifecycle"],
        mode="async"
    )
    def step_physics(self):
        if not self.nodes or self.settled: return

        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.85 # Increased damping to settle faster
        
        cx, cy = self.width / 2, self.height / 2
        total_kinetic_energy = 0

        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue

            # LOD: Freeze satellites if zoomed out
            if self.zoom < 1.2 and a.get('type') == 'chunk':
                a['vx'] = 0
                a['vy'] = 0
                continue
            
            fx, fy = 0, 0
            
            # 1. Gravity (Center pull)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # 2. Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Performance opt: Ignore far away nodes
                if dist_sq > 25000: continue 

                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 3. Attraction (Links)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 4. Apply & Measure Energy
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']
            total_kinetic_energy += (abs(n['vx']) + abs(n['vy']))

        # 5. Sleep Threshold
        if total_kinetic_energy < 0.5:
            self.settled = True

    # --- RENDERING ---

    @service_endpoint(
        inputs={},
        outputs={"raw_data": "bytes"},
        description="Renders the current frame to a byte buffer for display in UI components.",
        tags=["render", "output"]
    )
    def get_image_bytes(self):
        self.surface.fill(self.bg_color)
        
        cx, cy = self.width / 2, self.height / 2
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # Links
        for u, v in self.links:
            if self.zoom < 1.2:
                if self.nodes[u].get('type') == 'chunk' or self.nodes[v].get('type') == 'chunk':
                    continue

            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # Nodes
        for i, n in enumerate(self.nodes):
            # LOD: Hide chunks if zoomed out
            if self.zoom < 1.2 and n.get('type') == 'chunk':
                continue

            sx, sy = to_screen(n['x'], n['y'])
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20: continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')

        if __name__ == "__main__":
            # Manual test setup
            engine = NeuralGraphEngineMS(400, 300)
            print("Service ready:", engine._service_info['name'])
            test_nodes = [{'id': 'A', 'type': 'file', 'label': 'Node A'}, {'id': 'B', 'type': 'chunk', 'label': 'Node B'}]
            test_links = [(0, 1)]
            engine.set_data(test_nodes, test_links)
            engine.step_physics()
            print("Physics step completed.")





--------------------------------------------------------------------------------
FILE: __NeuralGraphViewerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NeuralGraphViewerMS
ENTRY_POINT: __NeuralGraphViewerMS.py
DEPENDENCIES: None
"""

import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import json
import os
from __NeuralGraphEngineMS import GraphRenderer
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="NeuralGraphViewerMS",
    version="1.0.0",
    description="The Lens: A Tkinter-based UI component that hosts the neural graph engine and provides search/highlighting overlays.",
    tags=["ui", "visualization", "tkinter"],
    capabilities=["graph-rendering", "search-highlighting"]
)
class NeuralGraphViewerMS(ttk.Frame):
    def __init__(self, parent):
        super().__init__(parent)
        self.pack(fill="both", expand=True)
        
        # Search Overlay
        self.controls = tk.Frame(self, bg="#101018")
        self.controls.pack(fill="x", side="top", padx=5, pady=5)
        
        self.entry_search = tk.Entry(self.controls, bg="#252526", fg="white", insertbackground="white", font=("Consolas", 10))
        self.entry_search.pack(side="left", fill="x", expand=True, padx=(0, 5))
        self.entry_search.bind("<Return>", self.run_search)
        
        btn = tk.Button(self.controls, text="NEURAL TEST", command=self.run_search, bg="#007ACC", fg="white", relief="flat")
        btn.pack(side="right")

        # UI Container
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Services
        self.cartridge = None
        self.neural = None
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None 
        
        # Input State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        self.is_panning = False

        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<Double-Button-1>', self.on_double_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Start the Heartbeat
        self.animate()

    def bind_services(self, cartridge, neural):
        self.cartridge = cartridge
        self.neural = neural

    @service_endpoint(
        inputs={"event": "any"},
        outputs={},
        description="Triggers a neural search based on the entry field and highlights resulting nodes in the viewer.",
        tags=["ui-action", "search"]
    )
    def run_search(self, event=None):
        if not self.cartridge or not self.neural:
            return
            
        query = self.entry_search.get().strip()
        if not query: return
        
        # 1. Embed
        vec = self.neural.get_embedding(query)
        if not vec: return
        
        # 2. Search
        results = self.cartridge.search_embeddings(vec, limit=5)
        
        # 3. Resolve IDs for Graph
        # Graph Node ID format: "{vfs_path}::{chunk_name}"
        ids = set()
        for r in results:
            if 'vfs_path' in r and 'name' in r:
                ids.add(f"{r['vfs_path']}::{r['name']}")
                
        # 4. Highlight
        self.engine.highlight_nodes(ids)

    @service_endpoint(
        inputs={"db_path": "str"},
        outputs={},
        description="Loads graph nodes and edges from a Cartridge database and triggers the physics engine.",
        tags=["data-load", "sqlite"],
        side_effects=["filesystem:read"]
    )
    def load_from_db(self, db_path):
        """
        Loads graph data from SQLite.
        Does NOT block the UI. The physics engine will settle the nodes frame-by-frame.
        """
        if not os.path.exists(db_path): return
        
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # [cite_start]Fetch Nodes [cite: 198]
            db_nodes = cursor.execute("SELECT id, type, label, data_json FROM graph_nodes").fetchall()
            
            # [cite_start]Fetch Edges [cite: 198]
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
            
            conn.close()
        except Exception as e:
            print(f"Graph Load Error: {e}")
            return

        # Format for Engine
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label, raw_json = row
            meta = {}
            try:
                if raw_json: meta = json.loads(raw_json)
            except: pass
            
            id_to_index[node_id] = idx
            formatted_nodes.append({'id': node_id, 'type': n_type, 'label': label, 'meta': meta})

        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                formatted_links.append((id_to_index[src], id_to_index[tgt]))

        # Inject Data - The Physics Engine handles the "Explosion" logic internally
        self.engine.set_data(formatted_nodes, formatted_links)

    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_double_click(self, event):
        # Zoom in on the node we clicked
        hit_node = self.engine.get_node_at(event.x, event.y)
        if hit_node:
            # Center camera on node and zoom in
            self.engine.cam_x = hit_node['x']
            self.engine.cam_y = hit_node['y']
            self.engine.zoom = 2.0
            self.engine.settled = False

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        if hit:
            self.is_dragging_node = True
        else:
            self.is_panning = True

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False
        self.is_panning = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        elif self.is_panning:
            # Camera Pan
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)
        self.engine.settled = False # Wake up physics on zoom

    def on_windows_scroll(self, event):
        if event.delta > 0: self.on_zoom(1.1)
        else: self.on_zoom(0.9)

    @service_endpoint(
        inputs={},
        outputs={},
        description="The primary heartbeat loop that orchestrates frame-by-frame physics steps and UI blitting.",
        tags=["lifecycle", "rendering"],
        mode="async"
    )
    def animate(self):
        """
        The Heartbeat Loop.
        Runs at ~30 FPS. Handles Physics + Rendering.
        """
        # 1. Step Physics (Micro-calculations)
        self.engine.step_physics()
        
        # 2. Render to Buffer
        raw_data = self.engine.get_image_bytes()
        
        # 3. Blit to Screen
        if raw_data:
            img = Image.frombytes('RGB', (self.engine.width, self.engine.height), raw_data)
            self.photo = ImageTk.PhotoImage(img)
            self.canvas_lbl.configure(image=self.photo)
        
        # 4. Loop
        self.after(30, self.animate)

        if __name__ == "__main__":
            root = tk.Tk()
            root.title("NeuralGraphViewerMS Test")
            view = NeuralGraphViewerMS(root)
            print("Service ready:", view._service_info['name'])
            # Note: Requires a valid DB and Pygame environment to fully render
            root.mainloop()


--------------------------------------------------------------------------------
FILE: __NeuralServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _NeuralServiceMS
ENTRY_POINT: __NeuralServiceMS.py
DEPENDENCIES: None
"""

import requests
import json
import concurrent.futures
from typing import Optional, Dict, Any, List
from base_service import BaseService
from microservice_std_lib import service_metadata, service_endpoint

# Configuration constants
OLLAMA_API_URL = "http://localhost:11434/api"

@service_metadata(
    name="NeuralServiceMS",
    version="1.0.0",
    description="The Brain Interface: Orchestrates local AI operations via Ollama for inference and embeddings.",
    tags=["ai", "neural", "inference", "ollama"],
    capabilities=["text-generation", "embeddings", "parallel-processing"]
)
class NeuralServiceMS(BaseService):
    def __init__(self, max_workers: int = 4):
        super().__init__("NeuralServiceMS")
        self.max_workers = max_workers
        # Default configs
        self.config = {
            "fast": "qwen2.5-coder:1.5b-cpu",
            "smart": "qwen2.5:3b-cpu",
            "embed": "mxbai-embed-large:latest-cpu"
        }

    def update_models(self, fast_model: str, smart_model: str, embed_model: str):
        """Called by the UI Settings Modal to change models on the fly."""
        self.config["fast"] = fast_model
        self.config["smart"] = smart_model
        self.config["embed"] = embed_model
        self.log_info(f"Models Updated: Fast={fast_model}, Smart={smart_model}")

    def get_available_models(self) -> List[str]:
        """Fetches list from Ollama for the UI dropdown."""
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            if res.status_code == 200:
                return [m['name'] for m in res.json().get('models', [])]
        except:
            return []
        return []

    def check_connection(self) -> bool:
        """Pings Ollama to see if it's alive."""
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except requests.RequestException:
            self.log_error("Ollama connection failed. Is 'ollama serve' running?")
            return False

    @service_endpoint(
        inputs={"text": "str"},
        outputs={"embedding": "list"},
        description="Generates a high-dimensional vector embedding for the provided text using the configured model.",
        tags=["nlp", "vector"]
    )
    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Generates a vector using the CPU embedder."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.config["embed"], "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            self.log_error(f"Embedding failed: {e}")
        return None

    @service_endpoint(
        inputs={"prompt": "str", "tier": "str", "format_json": "bool"},
        outputs={"response": "str"},
        description="Requests a synchronous text generation/inference from a local LLM tier.",
        tags=["llm", "inference"]
    )
    def request_inference(self, prompt: str, tier: str = "fast", format_json: bool = False) -> str:
        """
        Synchronous inference request.
        tier: 'fast' (1.5b-cpu), 'smart' (3b-cpu), or 'architect' (7b-gpu)
        """
        model = self.config.get(tier, self.config["fast"])
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        if format_json:
            payload["format"] = "json"

        try:
            res = requests.post(f"{OLLAMA_API_URL}/generate", json=payload, timeout=60)
            if res.status_code == 200:
                return res.json().get("response", "").strip()
        except Exception as e:
            self.log_error(f"Inference ({tier}) failed: {e}")
        return ""

    def process_parallel(self, items: List[Any], worker_func) -> List[Any]:
        """
        Helper to run a function across many items using the ThreadPool.
        Useful for batch ingestion.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # worker_func should take a single item and return a result
            futures = {executor.submit(worker_func, item): item for item in items}
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    self.log_error(f"Worker task failed: {e}")
                            return results

                    if __name__ == "__main__":
                        svc = NeuralServiceMS()
                        print("Service ready:", svc._service_info["name"])
                        if svc.check_connection():
                            print("Ollama Connection: OK")
                        else:
                            print("Ollama Connection: FAILED")



--------------------------------------------------------------------------------
FILE: __PromptOptimizerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _PromptOptimizerMS
ENTRY_POINT: __PromptOptimizerMS.py
DEPENDENCIES: None
"""

import json
import logging
from typing import List, Dict, Any, Callable, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: META-PROMPTS
# ==============================================================================
# The system prompt used to turn the LLM into a Prompt Engineer
REFINE_SYSTEM_PROMPT = (
    "You are a world-class prompt engineer. "
    "Given an original prompt and specific feedback, "
    "provide an improved, refined version of the prompt that incorporates the feedback. "
    "Return ONLY the refined prompt text, no preamble."
)

# The system prompt used to generate A/B test variations
VARIATION_SYSTEM_PROMPT = (
    "You are a creative AI assistant. "
    "Generate {num} innovative and diverse variations of the following prompt. "
    "Return the result as a valid JSON array of strings. "
    "Example: [\"variation 1\", \"variation 2\"]"
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptOpt")
# ==============================================================================

@service_metadata(
name="PromptOptimizer",
version="1.0.0",
description="Uses an LLM to refine prompts or generate variations.",
tags=["llm", "prompt-engineering", "optimization"],
capabilities=["network:outbound"]
)
class PromptOptimizerMS:
    """
The Tuner: Uses an LLM to refine prompts or generate variations.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.infer = self.config.get("inference_func")

    @service_endpoint(
    inputs={"draft_prompt": "str", "feedback": "str"},
    outputs={"refined_prompt": "str"},
    description="Rewrites a prompt based on feedback.",
    tags=["llm", "refine"],
    side_effects=["network:outbound"]
    )
def refine_prompt(self, draft_prompt: str, feedback: str) -> str:
    """
    Rewrites a prompt based on feedback.
    """
        full_prompt = (
            f"{REFINE_SYSTEM_PROMPT}\n\n"
            f"[Original Prompt]:\n{draft_prompt}\n\n"
            f"[Feedback]:\n{feedback}\n\n"
            f"[Refined Prompt]:"
        )
        
        log.info("Refining prompt...")
        try:
            result = self.infer(full_prompt)
            return result.strip()
        except Exception as e:
            log.error(f"Refinement failed: {e}")
            return draft_prompt # Fallback to original

@service_endpoint(
    inputs={"draft_prompt": "str", "num_variations": "int", "context_data": "Dict"},
    outputs={"variations": "List[str]"},
    description="Generates multiple versions of a prompt for testing.",
    tags=["llm", "variations"],
    side_effects=["network:outbound"]
)
def generate_variations(self, draft_prompt: str, num_variations: int = 3, context_data: Optional[Dict] = None) -> List[str]:
    """
    Generates multiple versions of a prompt for testing.
    """
        meta_prompt = VARIATION_SYSTEM_PROMPT.format(num=num_variations)
        
        prompt_content = draft_prompt
        if context_data:
            prompt_content += f"\n\n--- Context ---\n{json.dumps(context_data, indent=2)}"

        full_prompt = (
            f"{meta_prompt}\n\n"
            f"[Original Prompt]:\n{prompt_content}\n\n"
            f"[JSON Array of Variations]:"
        )

        log.info(f"Generating {num_variations} variations...")
        try:
            # We explicitly ask for JSON, but LLMs are chatty, so we might need cleaning logic here
            raw_response = self.infer(full_prompt)
            
            # Simple cleanup to find the JSON array if the LLM added text around it
            start = raw_response.find('[')
            end = raw_response.rfind(']') + 1
            if start == -1 or end == 0:
                raise ValueError("No JSON array found in response")
                
            clean_json = raw_response[start:end]
            variations = json.loads(clean_json)
            
            if isinstance(variations, list):
                return [str(v) for v in variations]
            return []
            
        except Exception as e:
            log.error(f"Variation generation failed: {e}")
            return []

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Mock Inference Engine (Simulating an LLM)
    def mock_llm(prompt: str) -> str:
        if "[Refined Prompt]" in prompt:
            return "You are a helpful assistant who speaks like a pirate. How may I help ye?"
        if "[JSON Array]" in prompt:
            return '["Variation A: Pirate Mode", "Variation B: Formal Mode", "Variation C: Concise Mode"]'
        return "Error"

    optimizer = PromptOptimizerMS({"inference_func": mock_llm})
    print("Service ready:", optimizer)

    # 2. Test Refine
    print("--- Test: Refine ---")
    draft = "Help me."
feedback = "Make it sound like a pirate."
    refined = optimizer.refine_prompt(draft, feedback)
    print(f"Original: {draft}")
    print(f"Refined:  {refined}")

    # 3. Test Variations
    print("\n--- Test: Variations ---")
    vars = optimizer.generate_variations(draft, num_variations=3)
    for i, v in enumerate(vars):
        print(f" {i+1}. {v}")

--------------------------------------------------------------------------------
FILE: __PromptVaultMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _PromptVaultMS
ENTRY_POINT: __PromptVaultMS.py
DEPENDENCIES: pydantic, jinja2
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pydantic", "jinja2"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _PromptVaultMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Callable
from pydantic import BaseModel, Field, ValidationError
from jinja2 import Environment, BaseLoader

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "prompt_vault.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptVault")
# ==============================================================================

# --- Data Models ---

class PromptVaultMS(BaseModel):
    """A specific historical version of a prompt."""
    version_num: int
    content: str
    author: str
    timestamp: datetime.datetime
    embedding: Optional[List[float]] = None

class PromptTemplate(BaseModel):
    """The master record for a prompt."""
    id: str
    slug: str
    title: str
    description: Optional[str] = ""
    tags: List[str] = []
    latest_version_num: int
    versions: List[PromptVaultMS] = []
    
    @property
    def latest(self) -> PromptVaultMS:
        """Helper to get the most recent content."""
        if not self.versions:
            raise ValueError("No versions found.")
        # versions are stored sorted by DB insertion usually, but let's be safe
        return sorted(self.versions, key=lambda v: v.version_num)[-1]

# --- Database Management ---

class PromptVaultMS:
    """
    The Vault: A persistent SQLite store for managing, versioning, 
    and rendering AI prompts.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()
        self.jinja_env = Environment(loader=BaseLoader())

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        """Bootstraps the schema."""
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    slug TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    tags_json TEXT,
                    latest_version INTEGER DEFAULT 1,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS versions (
                    id TEXT PRIMARY KEY,
                    template_id TEXT,
                    version_num INTEGER,
                    content TEXT,
                    author TEXT,
                    timestamp TIMESTAMP,
                    embedding_json TEXT,
                    FOREIGN KEY(template_id) REFERENCES templates(id)
                )
            """)
# --- CRUD Operations ---

    def create_template(self, slug: str, title: str, content: str, author: str = "system", tags: List[str] = None) -> PromptTemplate:
        """Creates a new prompt template with an initial version 1."""
        tags = tags or []
        now = datetime.datetime.utcnow()
        t_id = str(uuid.uuid4())
        v_id = str(uuid.uuid4())

        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO templates (id, slug, title, description, tags_json, latest_version, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                    (t_id, slug, title, "", json.dumps(tags), 1, now, now)
                )
                conn.execute(
                    "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                    (v_id, t_id, 1, content, author, now)
                )
            log.info(f"Created template: {slug}")
            return self.get_template(slug)
        except sqlite3.IntegrityError:
            raise ValueError(f"Template '{slug}' already exists.")

    def add_version(self, slug: str, content: str, author: str = "user") -> PromptTemplate:
        """Adds a new version to an existing template."""
        current = self.get_template(slug)
        if not current:
            raise ValueError(f"Template '{slug}' not found.")

        new_ver = current.latest_version_num + 1
        now = datetime.datetime.utcnow()
        v_id = str(uuid.uuid4())

        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                (v_id, current.id, new_ver, content, author, now)
            )
conn.execute(
                "UPDATE templates SET latest_version = ?, updated_at = ? WHERE id = ?",
                (new_ver, now, current.id)
            )
        log.info(f"Updated {slug} to v{new_ver}")
        return self.get_template(slug)

    def get_template(self, slug: str) -> Optional[PromptTemplate]:
        """Retrieves a full template with all history."""
        with self._get_conn() as conn:
            # 1. Fetch Template
            row = conn.execute("SELECT * FROM templates WHERE slug = ?", (slug,)).fetchone()
            if not row: return None

            # 2. Fetch Versions
            v_rows = conn.execute("SELECT * FROM versions WHERE template_id = ? ORDER BY version_num ASC", (row['id'],)).fetchall()

            versions = []
            for v in v_rows:
                versions.append(PromptVaultMS(
                    version_num=v['version_num'],
                    content=v['content'],
                    author=v['author'],
                    timestamp=v['timestamp']
                    # embedding logic skipped for brevity
                ))

            return PromptTemplate(
                id=row['id'],
                slug=row['slug'],
                title=row['title'],
                description=row['description'],
                tags=json.loads(row['tags_json']),
                latest_version_num=row['latest_version'],
                versions=versions
            )

    def render(self, slug: str, context: Dict[str, Any] = None) -> str:
        """Fetches the latest version and renders it with Jinja2."""
        template = self.get_template(slug)
if not template:
            raise ValueError(f"Template '{slug}' not found.")
        
        raw_text = template.latest.content
        jinja_template = self.jinja_env.from_string(raw_text)
        return jinja_template.render(**(context or {}))

    def list_slugs(self) -> List[str]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT slug FROM templates").fetchall()
            return [r[0] for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup
    if DB_PATH.exists(): os.remove(DB_PATH)
    vault = PromptVaultMS()
    
    # 2. Create
    print("--- Creating Prompt ---")
    vault.create_template(
        slug="greet_user",
        title="Greeting Protocol",
        content="Hello {{ name }}, welcome to the {{ system_name }}!",
        tags=["ui", "onboarding"]
    )
    
    # 3. Versioning
    print("--- Updating Prompt ---")
    vault.add_version("greet_user", "Greetings, {{ name }}. System {{ system_name }} is online.")
    
    # 4. Retrieval & Rendering
    print("--- Rendering ---")
    final_text = vault.render("greet_user", {"name": "Alice", "system_name": "Nexus"})
    print(f"Rendered Output: {final_text}")
    
    # 5. Inspection
    tpl = vault.get_template("greet_user")
print(f"Current Version: v{tpl.latest_version_num}")
print(f"History: {[v.content for v in tpl.versions]}")
# Cleanup
if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: __PythonChunkerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _PythonChunkerMS
ENTRY_POINT: __PythonChunkerMS.py
DEPENDENCIES: None
"""

import ast
import time
from dataclasses import dataclass
from typing import List, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

@dataclass PythonChunkerMS CodeChunk:
    name: str          # e.g., "PythonChunkerMS AuthMS"
    type: str          # "PythonChunkerMS", "function", "text"
    content: str       # The raw source
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

@service_metadata(
    name="PythonChunkerMS",
    version="1.2.0",
    description="The Python Surgeon: Specialist in Abstract Syntax Tree (AST) parsing for Python source code.",
    tags=["chunking", "python", "ast"],
    capabilities=["python-ast"]
)
PythonChunkerMS PythonChunkerMS:
    """
    Specialized Python AST Chunker.
    Focuses exclusively on identifying classes and functions to preserve code logic.
    """
    
    def __init__(self):
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "specialty": "str"},
        description="Standardized health check for the Python specialist service.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the PythonChunkerMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "specialty": "python_ast"
        }

    @service_endpoint(
        inputs={"content": "str"},
        outputs={"chunks": "list"},
        description="Primary entry point for high-fidelity Python-specific AST chunking.",
        tags=["processing", "python"]
    )
    def chunk(self, content: str) -> List[CodeChunk]:
        """Parses Python source into semantic CodeChunks."""
        return self._chunk_python(content)

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                end = node.end_lineno if hasattr(node, "end_lineno") else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", type="function", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"PythonChunkerMS {node.name}", type="PythonChunkerMS", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))

            # Fallback: Return as single chunk if no structures found (e.g., flat script)
            if not chunks:
                return [CodeChunk(
                    name="module_level", 
                    type="text", 
                    content=source, 
                    start_line=1, 
                    end_line=len(lines)
                )]
                
        except SyntaxError:
            # If the code is unparseable, return the whole block as a raw chunk
            return [CodeChunk(
                name="syntax_error_fallback", 
                type="text", 
                content=source, 
                start_line=1, 
                end_line=source.count('\n') + 1
            )]
            
        return chunks

if __name__ == "__main__":
    svc = PythonChunkerMS()
    print("Service ready:", svc._service_info["name"])
    # Test on a Python snippet
    test_code = "PythonChunkerMS Test:\n    def run(self):\n        pass"
    results = svc.chunk(test_code)
    for c in results:
        print(f"[{c.type}] {c.name} (Lines {c.start_line}-{c.end_line})")
--------------------------------------------------------------------------------
FILE: __RefineryServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _RefineryServiceMS
ENTRY_POINT: __RefineryServiceMS.py
DEPENDENCIES: None
"""

import json
import re
import os
import time
import ast
import concurrent.futures
from typing import Dict, List, Any, Optional, Tuple
from base_service import BaseService
from __CartridgeServiceMS import CartridgeService
from __NeuralServiceMS import NeuralService
from __ChunkingRouterMS import ChunkingRouterMS
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="RefineryServiceMS",
    version="1.1.0",
    description="The Night Shift: Processes 'RAW' files into semantic chunks and weaves them into a knowledge graph.",
    tags=["processing", "refinery", "graph", "RAG"],
    capabilities=["smart-chunking", "graph-weaving", "parallel-embedding"]
)
class RefineryServiceMS(BaseService):
    """
    The Night Shift.
    Polls the DB for 'RAW' files and processes them into Chunks and Graph Nodes.

    Graph Enrichment:
    - Code: function/class nodes, resolved import edges when possible.
    - Docs: section/chapter nodes for long-form text (md/txt/rst).
    """

    def __init__(self, cartridge: CartridgeService, neural: NeuralService):
        super().__init__("RefineryServiceMS")
        self.cartridge = cartridge
        self.neural = neural
        self.chunker = ChunkingRouterMS()
        self.start_time = time.time()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "cartridge_health": "str"},
        description="Standardized health check to verify the operational state of the Refinery service.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the RefineryServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "cartridge_health": self.cartridge.get_status_flags().get("cartridge_health", "UNKNOWN")
        }

        # --- Spec Enforcement ---
        # Update the cartridge manifest to reflect the ACTUAL tools we are using.
        self._stamp_specs()

        # Import parsing / resolution
        # Regex remains as a fallback (JS + non-parseable cases)
        self.import_pattern = re.compile(r"""(?:from|import)\s+([\w\.]+)|require\(['"]([\w\.\-/]+)['"]\)""")

        # Lightweight module/path index cache for resolving imports to VFS files
        self._module_index: Dict[str, str] = {}
        self._path_index: Dict[str, str] = {}
        self._index_built: bool = False

        # Simple section/chapter detection
        self._md_heading = re.compile(r"^(#{1,6})\s+(.+?)\s*$")
        self._chapter_heading = re.compile(r"^\s*(chapter|CHAPTER)\s+([0-9]+|[IVXLC]+)\b\s*[:\-]?\s*(.*)$")

    def _stamp_specs(self):
        """Writes the active Neural/Chunker configuration to the Manifest."""
        try:
            # 1. Embedding Spec
            # We assume 1024 dim for mxbai-large, but ideally we'd probe it.
            embed_model = self.neural.config["embed"]
            spec = {
                "provider": "ollama",
                "model": embed_model,
                "dim": 1024,  # Hardcoded for now based on mxbai-embed-large
                "dtype": "float32",
                "distance": "cosine"
            }
            self.cartridge.set_manifest("embedding_spec", spec)

            # 2. Chunking Spec
            # (We could expose chunker config params here if they were dynamic)

        except Exception as e:
            self.log_error(f"Failed to stamp specs: {e}")

    def _build_import_index(self):
        """Builds caches for resolving imports to VFS targets.

        - _path_index: exact VFS path -> VFS path
        - _module_index: python module name (a.b.c) -> VFS path (a/b/c.py, a/b/c/__init__.py)
        """
        if self._index_built:
            return

        path_index: Dict[str, str] = {}
        module_index: Dict[str, str] = {}

        conn = self.cartridge._get_conn()
        try:
            rows = conn.execute("SELECT vfs_path FROM files").fetchall()
            for (vp,) in rows:
                if not vp:
                    continue
                vfs_path = str(vp).replace("\\", "/")
                path_index[vfs_path] = vfs_path

                # Python module mapping
                if vfs_path.endswith(".py"):
                    mod = vfs_path[:-3].strip("/")
                    mod = mod.replace("/", ".")
                    if mod:
                        module_index[mod] = vfs_path

                    # If it's a package __init__.py, map the package name too
                    if vfs_path.endswith("/__init__.py"):
                        pkg = vfs_path[:-len("/__init__.py")].strip("/").replace("/", ".")
                        if pkg:
                            module_index[pkg] = vfs_path

        finally:
            conn.close()

        self._path_index = path_index
        self._module_index = module_index
        self._index_built = True

    @service_endpoint(
        inputs={"batch_size": "int"},
        outputs={"processed_count": "int"},
        description="Polls the database for files with 'RAW' status and processes them into chunks and graph nodes.",
        tags=["pipeline", "batch"],
        side_effects=["cartridge:write", "neural:inference"]
    )
    def process_pending(self, batch_size: int = 5) -> int:
        """Main loop. Returns number of files processed."""
        pending = self.cartridge.get_pending_files(limit=batch_size)
        if not pending:
            return 0

        self.log_info(f"Refining batch of {len(pending)} files...")

        for file_row in pending:
            self._refine_file(file_row)

        return len(pending)

    def _refine_file(self, row: Dict):
        file_id = row['id']
        vfs_path = row['vfs_path']
        content = row['content']

        # Skip binary files for now (unless we add OCR later)
        if not content:
            self.cartridge.update_status(file_id, "SKIPPED_BINARY")
            return

        try:
            # 1. Specialized Chunking via Router
            chunks = self.chunker.chunk_file(content, vfs_path)

            # 2. Vectorization & Storage
            chunk_texts = [c.content for c in chunks]

            # Buffer graph writes while DB transaction is open (prevents nested-writer locks)
            pending_nodes: List[Tuple[str, str, str, Dict[str, Any]]] = []
            pending_edges: List[Tuple[str, str, str, float]] = []

            # Use ThreadPool to embed in parallel (preserve order with map)
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.neural.max_workers) as executor:
                vectors = list(executor.map(self.neural.get_embedding, chunk_texts))

            conn = self.cartridge._get_conn()
            try:
                cursor = conn.cursor()

                for i, chunk in enumerate(chunks):
                    vector = vectors[i]
                    vec_blob = json.dumps(vector).encode('utf-8') if vector else None

                    # Store Chunk
                    cursor.execute(
                        """
                        INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """,
                        (file_id, i, chunk.content, vec_blob, chunk.name, chunk.type, chunk.start_line, chunk.end_line)
                    )

                    chunk_row_id = cursor.lastrowid

                    # Insert into Vector Index (if vector exists)
                    if vector:
                        try:
                            cursor.execute(
                                "INSERT INTO vec_items(rowid, embedding) VALUES (?, ?)",
                                (chunk_row_id, json.dumps(vector))
                            )
                        except Exception as ve:
                            self.log_error(f"Vector Index Insert Failed: {ve}")

                    # Graph Node for Chunks (Functions/Classes)
                    # IMPORTANT: Do NOT call CartridgeService.add_node/add_edge while this conn is open.
                    if chunk.type in ['class', 'function']:
                        node_id = f"{vfs_path}::{chunk.name}"
                        pending_nodes.append(
                            (
                                node_id,
                                'chunk',
                                chunk.name,
                                {
                                    'parent': vfs_path,
                                    'file_id': file_id,
                                    'chunk_row_id': chunk_row_id,
                                    'chunk_type': chunk.type,
                                    'start_line': chunk.start_line,
                                    'end_line': chunk.end_line
                                }
                            )
                        )
                        pending_edges.append((node_id, vfs_path, "defined_in", 1.0))

                conn.commit()

            finally:
                conn.close()

            # 3. File Level Graph Node (after close)
            pending_nodes.append((vfs_path, 'file', vfs_path.split('/')[-1], {'path': vfs_path, 'file_id': file_id}))

            # 4. Section/Chapter Weaving (docs)
            self._weave_sections(vfs_path, content)

            # 5. Import Weaving (resolved when possible)
            self._weave_imports(vfs_path, content)

            # 6. Flush buffered graph writes
            for nid, ntype, label, data in pending_nodes:
                self.cartridge.add_node(nid, ntype, label, data)
            for src, tgt, rel, w in pending_edges:
                self.cartridge.add_edge(src, tgt, rel, w)

            # 7. Mark file refined
            self.cartridge.update_status(file_id, "REFINED")

        except Exception as e:
            self.log_error(f"Refining failed for {vfs_path}: {e}")
            self.cartridge.update_status(file_id, "ERROR", {"error": str(e)})

    def _extract_imports_python(self, source_path: str, content: str) -> List[Tuple[str, int, int]]:
        """Returns list of (module_or_path, level, lineno).

        - level: 0 for absolute imports, >=1 for relative import-from statements.
        """
        out: List[Tuple[str, int, int]] = []
        try:
            tree = ast.parse(content)
        except Exception:
            return out

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias and alias.name:
                        out.append((alias.name, 0, getattr(node, 'lineno', 0)))
            elif isinstance(node, ast.ImportFrom):
                level = int(getattr(node, 'level', 0) or 0)
                mod = getattr(node, 'module', None) or ""
                if mod:
                    out.append((mod, level, getattr(node, 'lineno', 0)))
                else:
                    # from . import x
                    for alias in node.names:
                        if alias and alias.name:
                            out.append((alias.name, level, getattr(node, 'lineno', 0)))

        return out

    def _resolve_python_import(self, source_path: str, module: str, level: int) -> List[str]:
        """Resolve a python import to possible VFS target paths."""
        self._build_import_index()

        # Absolute: try direct module mapping
        if level <= 0:
            if module in self._module_index:
                return [self._module_index[module]]
            return []

        # Relative: resolve from the source directory
        src_dir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        base_parts = src_dir.split("/") if src_dir else []

        # level=1 means "from .", so pop 0; level=2 means "from .." pop 1, etc.
        pops = max(level - 1, 0)
        if pops > 0 and pops <= len(base_parts):
            base_parts = base_parts[:-pops]

        rel_base = "/".join([p for p in base_parts if p])
        mod_path = module.replace(".", "/").strip("/")

        candidates: List[str] = []
        if rel_base:
            if mod_path:
                candidates.append(f"{rel_base}/{mod_path}.py")
                candidates.append(f"{rel_base}/{mod_path}/__init__.py")
            else:
                candidates.append(f"{rel_base}/__init__.py")
        else:
            if mod_path:
                candidates.append(f"{mod_path}.py")
                candidates.append(f"{mod_path}/__init__.py")

        return [c for c in candidates if c in self._path_index]

    def _resolve_js_like_import(self, source_path: str, imp: str) -> List[str]:
        """Resolve require('./x') / import ... from './x' to VFS candidates."""
        self._build_import_index()

        sdir = os.path.dirname(source_path).replace("\\", "/").strip("/")
        raw = imp.strip().replace("\\", "/")

        # Only try to resolve relative-ish paths
        if not (raw.startswith(".") or raw.startswith("/")):
            return []

        # Normalize
        if raw.startswith("/"):
            rel = raw.lstrip("/")
        else:
            rel = os.path.normpath(os.path.join(sdir, raw)).replace("\\", "/").lstrip("./")

        ext_candidates = [rel]
        # Common extensions
        if not os.path.splitext(rel)[1]:
            ext_candidates.extend([rel + ".js", rel + ".ts", rel + ".json"]) 
            ext_candidates.extend([rel + "/index.js", rel + "/index.ts"]) 

        return [c for c in ext_candidates if c in self._path_index]

    def _weave_imports(self, source_path: str, content: str):
        """Scans content for imports and links them in the graph.

        Creates edges:
        - imports_file: source_path -> target_vfs_path (resolved)
        - imports_unresolved: source_path -> module_string (fallback)
        """
        targets_resolved: List[str] = []

        # Python: use AST when possible
        if source_path.endswith(".py"):
            for mod, level, lineno in self._extract_imports_python(source_path, content):
                resolved = self._resolve_python_import(source_path, mod, level)
                if resolved:
                    for tgt in resolved:
                        self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                        targets_resolved.append(tgt)
                else:
                    self.cartridge.add_edge(source_path, mod, "imports_unresolved", 0.25)

            return

        # JS / generic: regex fallback
        for line in content.splitlines():
            match = self.import_pattern.search(line)
            if not match:
                continue

            imp = match.group(1) or match.group(2)
            if not imp:
                continue

            resolved = self._resolve_js_like_import(source_path, imp)
            if resolved:
                for tgt in resolved:
                    self.cartridge.add_edge(source_path, tgt, "imports_file", 1.0)
                    targets_resolved.append(tgt)
            else:
                self.cartridge.add_edge(source_path, imp, "imports_unresolved", 0.25)

    def _weave_sections(self, vfs_path: str, content: str):
        """Creates section/chapter nodes for long-form text and links them to the file node."""
        ext = os.path.splitext(vfs_path)[1].lower()
        if ext not in (".md", ".markdown", ".txt", ".rst"):
            return

        lines = content.splitlines()
        for idx, line in enumerate(lines):
            lineno = idx + 1

            m = self._md_heading.match(line)
            if m:
                hashes = m.group(1)
                title = (m.group(2) or "").strip()
                level = len(hashes)
                if title:
                    node_id = f"{vfs_path}::section::{lineno}:{title}"
                    self.cartridge.add_node(node_id, "section", title, {
                        "parent": vfs_path,
                        "level": level,
                        "line": lineno
                    })
                    self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)
                                    continue

                    if __name__ == "__main__":
                        # Requires Cartridge and Neural services for testing
                        from __CartridgeServiceMS import CartridgeService
                        from __NeuralServiceMS import NeuralService
                        c = CartridgeService(":memory:")
                        n = NeuralService()
                        svc = RefineryServiceMS(c, n)
                        print("Service ready:", svc._service_info["name"])

            c = self._chapter_heading.match(line)
            if c:
                chap_num = (c.group(2) or "").strip()
                chap_title = (c.group(3) or "").strip()
                title = f"Chapter {chap_num}" + (f": {chap_title}" if chap_title else "")
                node_id = f"{vfs_path}::chapter::{lineno}:{chap_num}"
                self.cartridge.add_node(node_id, "section", title, {
                    "parent": vfs_path,
                    "level": 1,
                    "line": lineno
                })
                self.cartridge.add_edge(node_id, vfs_path, "in_file", 1.0)








--------------------------------------------------------------------------------
FILE: __RegexWeaverMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _RegexWeaverMS
ENTRY_POINT: __RegexWeaverMS.py
DEPENDENCIES: None
"""

import re
import logging
from typing import Any, Dict, List, Optional, Set
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: PATTERNS
# ==============================================================================
# Python: "import x", "from x import y"
PY_IMPORT = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')

# JS/TS: "import ... from 'x'", "require('x')"
JS_IMPORT = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RegexWeaver")
# ==============================================================================

@service_metadata(
name="RegexWeaver",
version="1.0.0",
description="Fault-tolerant dependency extractor using Regex.",
tags=["parsing", "dependencies", "regex"],
capabilities=["compute"]
)
class RegexWeaverMS:
    """
The Weaver: A fault-tolerant dependency extractor.
Uses Regex to find imports, making it faster and more permissive
than AST parsers (works on broken code).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"content": "str", "language": "str"},
    outputs={"dependencies": "List[str]"},
    description="Scans code content for import statements.",
    tags=["parsing", "dependencies"],
    side_effects=[]
    )
    def extract_dependencies(self, content: str, language: str) -> List[str]:
    """
    Scans code content for import statements.
    :param language: 'python' or 'javascript' (includes ts/jsx).
    """
        dependencies: Set[str] = set()
        lines = content.splitlines()
        
        pattern = PY_IMPORT if language == 'python' else JS_IMPORT
        
        for line in lines:
            # Skip comments roughly
            if line.strip().startswith(('#', '//')):
                continue
                
            if language == 'python':
                match = pattern.match(line)
            else:
                match = pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                # Clean up: "backend.database" -> "database"
                # We usually want the leaf name for simple linking
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                dependencies.add(clean_dep)
                
        return sorted(list(dependencies))

# --- Independent Test Block ---
if __name__ == "__main__":
weaver = RegexWeaverMS()
print("Service ready:", weaver)
    
    # 1. Python Test
    py_code = """
    import os
    from backend.utils import helper
    # from commented.out import ignore_me
    import pandas as pd
    """
    print(f"Python Deps: {weaver.extract_dependencies(py_code, 'python')}")
    
    # 2. JS Test
    js_code = """
    import React from 'react';
    const utils = require('./lib/utils');
    // import hidden from 'hidden';
    """

print(f"JS Deps:     {weaver.extract_dependencies(js_code, 'javascript')}")

--------------------------------------------------------------------------------
FILE: __RoleManagerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _RoleManagerMS
ENTRY_POINT: __RoleManagerMS.py
DEPENDENCIES: pydantic
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pydantic"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _RoleManagerMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("roles.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RoleManager")
# ==============================================================================

class RoleManagerMS(BaseModel):
    id: str
    name: str
    description: Optional[str] = ""
    system_prompt: str
    knowledge_bases: List[str] = []
    memory_policy: str = "scratchpad" # or 'auto_commit'
    created_at: datetime.datetime

@service_metadata(
name="RoleManager",
version="1.0.0",
description="Manages Agent Personas (Roles), including System Prompts and Memory Settings.",
tags=["roles", "personas", "db"],
capabilities=["db:sqlite"]
)
class RoleManagerMS:
    """
The Casting Director: Manages Agent Personas (Roles).
Persists configuration for System Prompts, Attached KBs, and Memory Settings.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", DB_PATH)
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS roles (
                    id TEXT PRIMARY KEY,
                    name TEXT UNIQUE NOT NULL,
                    description TEXT,
                    system_prompt TEXT NOT NULL,
                    knowledge_bases_json TEXT,
                    memory_policy TEXT,
                    created_at TIMESTAMP
                )
            """)

    @service_endpoint(
    inputs={"name": "str", "system_prompt": "str", "description": "str", "kbs": "List[str]"},
    outputs={"role": "RoleManagerMS"},
    description="Creates a new Agent Persona.",
    tags=["roles", "create"],
    side_effects=["db:write"]
    )
    def create_role(self, name: str, system_prompt: str, description: str = "", kbs: List[str] = None) -> RoleManagerMS:
    """Creates a new Agent Persona."""
        role_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        kbs_json = json.dumps(kbs or [])
        
        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO roles (id, name, description, system_prompt, knowledge_bases_json, memory_policy, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    (role_id, name, description, system_prompt, kbs_json, "scratchpad", now)
                )
            log.info(f"Created RoleManagerMS: {name}")
            return self.get_role(name)
        except sqlite3.IntegrityError:
            raise ValueError(f"RoleManagerMS '{name}' already exists.")

    @service_endpoint(
    inputs={"name_or_id": "str"},
    outputs={"role": "Optional[RoleManagerMS]"},
    description="Retrieves a role by Name or ID.",
    tags=["roles", "read"]
    )
    def get_role(self, name_or_id: str) -> Optional[RoleManagerMS]:
    """Retrieves a role by Name or ID."""
        with self._get_conn() as conn:
            # Try ID first
            row = conn.execute("SELECT * FROM roles WHERE id = ?", (name_or_id,)).fetchone()
            if not row:
                # Try Name
                row = conn.execute("SELECT * FROM roles WHERE name = ?", (name_or_id,)).fetchone()
            
            if not row: return None

            return RoleManagerMS(
                id=row['id'],
                name=row['name'],
                description=row['description'],
                system_prompt=row['system_prompt'],
                knowledge_bases=json.loads(row['knowledge_bases_json']),
                memory_policy=row['memory_policy'],
                created_at=row['created_at'] # Adapter might need datetime.fromisoformat if stored as str
            )

    @service_endpoint(
    inputs={},
    outputs={"roles": "List[Dict]"},
    description="Lists all available roles.",
    tags=["roles", "read"]
    )
    def list_roles(self) -> List[Dict]:
    with self._get_conn() as conn:
            rows = conn.execute("SELECT id, name, description FROM roles").fetchall()
            return [dict(r) for r in rows]

    @service_endpoint(
    inputs={"name": "str"},
    outputs={},
    description="Deletes a role by name.",
    tags=["roles", "delete"],
    side_effects=["db:write"]
    )
    def delete_role(self, name: str):
    with self._get_conn() as conn:
            conn.execute("DELETE FROM roles WHERE name = ?", (name,))
        log.info(f"Deleted RoleManagerMS: {name}")

# --- Independent Test Block ---
if __name__ == "__main__":
import os
if DB_PATH.exists(): os.remove(DB_PATH)
    
mgr = RoleManagerMS()
print("Service ready:", mgr)
    
    # 1. Create
    mgr.create_role(
        name="SeniorDev", 
        system_prompt="You are a senior Python developer. Prefer Clean Code principles.",
        description="Expert coding assistant",
        kbs=["python_docs", "project_repo"]
    )
    
    # 2. Retrieve
    role = mgr.get_role("SeniorDev")
    print(f"RoleManagerMS: {role.name}")
    print(f"Prompt: {role.system_prompt}")
    print(f"KBs: {role.knowledge_bases}")
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: __SandboxManagerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SandboxManagerMS
ENTRY_POINT: __SandboxManagerMS.py
DEPENDENCIES: None
"""

import shutil
import hashlib
import os
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Default folders to ignore when syncing or diffing
DEFAULT_EXCLUDES = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "_sandbox", "_project_library"
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("SandboxMgr")
# ==============================================================================

class SandboxManagerMS:
    """
    The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
    Allows for safe experimentation, diffing, and atomic promotion of changes.
    """
    def __init__(self, live_path: str, sandbox_path: str):
        self.live_root = Path(live_path).resolve()
        self.sandbox_root = Path(sandbox_path).resolve()

    def init_sandbox(self, force: bool = False):
        """
        Creates or resets the sandbox by mirroring the live project.
        """
        if self.sandbox_root.exists():
            if not force:
                raise FileExistsError(f"Sandbox already exists at {self.sandbox_root}")
            log.info("Wiping existing sandbox...")
            shutil.rmtree(self.sandbox_root)
        
        log.info(f"Cloning {self.live_root} -> {self.sandbox_root}...")
        self._mirror_tree(self.live_root, self.sandbox_root)
        log.info("Sandbox initialized.")

    def reset_sandbox(self):
        """
        Discards all sandbox changes and re-syncs from live.
        """
        self.init_sandbox(force=True)

    def get_diff(self) -> Dict[str, List[str]]:
        Compares Sandbox vs Live. Returns added, modified, and deleted files.
        """
        sandbox_files = self._scan_files(self.sandbox_root)
        live_files = self._scan_files(self.live_root)
        
        sandbox_paths = set(sandbox_files.keys())
        live_paths = set(live_files.keys())

        # 1. Added: In sandbox but not in live
        added = sorted(list(sandbox_paths - live_paths))
        
        # 2. Deleted: In live but not in sandbox
        deleted = sorted(list(live_paths - sandbox_paths))
        
        # 3. Modified: In both, but hashes differ
        common = sandbox_paths.intersection(live_paths)
        modified = []
        for rel_path in common:
            if sandbox_files[rel_path] != live_files[rel_path]:
                modified.append(rel_path)
        modified.sort()

        return {
            "added": added,
            "modified": modified,
            "deleted": deleted
        }

    def promote_changes(self) -> Tuple[int, int, int]:
        """
        Applies changes from Sandbox to Live.
        Returns (added_count, modified_count, deleted_count).
        """
        diff = self.get_diff()
        
        # 1. Additions & Modifications (Copy file -> file)
        for rel_path in diff['added'] + diff['modified']:
            src = self.sandbox_root / rel_path
            dst = self.live_root / rel_path
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
            
        # 2. Deletions (Remove file)
        for rel_path in diff['deleted']:
            target = self.live_root / rel_path
            if target.exists():
                os.remove(target)
                
        log.info(f"Promoted: {len(diff['added'])} added, {len(diff['modified'])} modified, {len(diff['deleted'])} deleted.")
        return len(diff['added']), len(diff['modified']), len(diff['deleted'])

    # --- Internal Helpers ---

    def _mirror_tree(self, src_root: Path, dst_root: Path):
        """Recursive copy that respects the exclusion list."""
        if not dst_root.exists():
            dst_root.mkdir(parents=True, exist_ok=True)

        for item in src_root.iterdir():
            if item.name in DEFAULT_EXCLUDES:
                continue
                
            dst_path = dst_root / item.name
            
            if item.is_dir():
                self._mirror_tree(item, dst_path)
            else:
                shutil.copy2(item, dst_path)

    def _scan_files(self, root: Path) -> Dict[str, str]:
        """
        Scans directory and returns {relative_path: sha256_hash}.
        """
        file_map = {}
        if not root.exists():
            return {}
            
        for path in root.rglob("*"):
            if path.is_file() and not self._is_excluded(path, root):
                rel = str(path.relative_to(root)).replace("\\", "/")
                file_map[rel] = self._get_hash(path)
        return file_map

    def _is_excluded(self, path: Path, root: Path) -> bool:
        """Checks if any part of the path is in the exclusion list."""
        try:
            rel_parts = path.relative_to(root).parts
            return any(p in DEFAULT_EXCLUDES for p in rel_parts)
        except ValueError:
            return False

    def _get_hash(self, path: Path) -> str:
        """Fast SHA-256 for file content."""
        try:
            # Skip binary files if needed, or hash them too (hashing is safe)
            return hashlib.sha256(path.read_bytes()).hexdigest()
        except Exception:
            return "read_error"

# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup test environment
    base = Path("test_env")
    live = base / "live_project"
    box = base / "sandbox"
    
    if base.exists(): shutil.rmtree(base)
    live.mkdir(parents=True)
    
    # 1. Create Mock Live Project
    (live / "main.py").write_text("print('v1')")
    (live / "utils.py").write_text("def help(): pass")
    (live / "node_modules").mkdir() # Should be ignored
    (live / "node_modules" / "junk.js").write_text("junk")
    
    print("--- Initializing Sandbox ---")
    mgr = SandboxManagerMS(str(live), str(box))
    mgr.init_sandbox()
    
    # 2. Make Changes in Sandbox
print("\n--- Modifying Sandbox ---")
(box / "main.py").write_text("print('v2')")  # Modify
(box / "new_feature.py").write_text("print('new')")  # Add
os.remove(box / "utils.py")  # Delete

# 3. Check Diff
diff = mgr.get_diff()
print(f"Diff Analysis:\n Added: {diff['added']}\n Modified: {diff['modified']}\n Deleted: {diff['deleted']}")

# 4. Promote
print("\n--- Promoting Changes ---")
mgr.promote_changes()

# Verify Live
print(f"Live 'main.py' content: {(live / 'main.py').read_text()}")
print(f"Live 'utils.py' exists? {(live / 'utils.py').exists()})

# Cleanup
if base.exists(): shutil.rmtree(base)

--------------------------------------------------------------------------------
FILE: __ScannerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ScannerMS
ENTRY_POINT: __ScannerMS.py
DEPENDENCIES: None
"""

import os
import time
from typing import Dict, List, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint


@service_metadata(
    name="Scanner",
    version="1.0.0",
    description="Recursively scans directories, filters junk, and detects binaries.",
    tags=["filesystem", "scanner", "tree"],
    capabilities=["filesystem:read"]
)
class ScannerMS:
    """
    The Scanner: Walks the file system, filters junk, and detects binary files.
    Generates the tree structure used by the UI.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}

        # Folders to completely ignore (Standard developer noise)
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env',
            '.idea', '.vscode', 'dist', 'build', 'coverage'
        }

        # Extensions that are explicitly binary/junk
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class',
            '.jpg', '.jpeg', '.png', '.gif', '.ico', '.svg',
            '.zip', '.tar', '.gz', '.pdf', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }

    def is_binary(self, file_path: str) -> bool:
        """
        Determines if a file is binary using two heuristics:
        1. Extension check (Fast)
        2. Content check for null bytes (Accurate)
        """
        # 1. Fast Fail on Extension
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS:
            return True

        # 2. Content Inspection (Read first 1KB)
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                # Text files shouldn't contain null bytes
                if b'\x00' in chunk:
                    return True
        except (IOError, OSError):
            # If we can't read it, treat as binary/unsafe
            return True

        return False

    @service_endpoint(
        inputs={"root_path": "str"},
        outputs={"tree": "Optional[Dict]"},
        description="Recursively scans a directory and returns a JSON-compatible tree.",
        tags=["filesystem", "scan"],
        side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Optional[Dict[str, Any]]:
        """
        Recursively scans a directory and returns a JSON-compatible tree.
        Returns None if path is invalid.
        """
        target = os.path.abspath(root_path)

        if not os.path.exists(target):
            return None

        if not os.path.isdir(target):
            # Handle single file case
            return self._create_node(target, is_dir=False)

        return self._scan_recursive(target)

    def _scan_recursive(self, current_path: str) -> Dict[str, Any]:
        """
        Internal recursive worker.
        """
        node = self._create_node(current_path, is_dir=True)
        node['children'] = []

        try:
            # os.scandir is faster than os.listdir as it returns file attributes
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))

                for entry in entries:
                    # Skip ignored directories
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS:
                        continue

                    # Skip hidden files (dotfiles)
                    if entry.name.startswith('.'):
                        continue

                    if entry.is_dir():
                        child_node = self._scan_recursive(entry.path)
                        if child_node:  # Only add if valid
                            node['children'].append(child_node)
                    else:
                        child_node = self._create_node(entry.path, is_dir=False)
                        node['children'].append(child_node)

        except PermissionError:
            node['error'] = "Access Denied"

        return node

    def _create_node(self, path: str, is_dir: bool) -> Dict[str, Any]:
        """
        Standardizes the node structure for the UI.
        """
        name = os.path.basename(path)
        node = {
            'text': name,
            'path': path,
            'type': 'folder' if is_dir else 'file',
            'checked': False,  # UI State
        }

        if not is_dir:
            if self.is_binary(path):
                node['type'] = 'binary'

        return node

    @service_endpoint(
        inputs={"tree_node": "Dict"},
        outputs={"files": "List[str]"},
        description="Flattens a tree node into a list of file paths.",
        tags=["filesystem", "utility"],
        side_effects=[]
    )
    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        """
        Helper to extract all valid file paths from a tree node 
        (e.g., when the user clicks 'Start Ingest').
        """
        files = []

        if tree_node['type'] == 'file':
            files.append(tree_node['path'])

        elif tree_node['type'] == 'folder' and 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))

        return files


# --- Independent Test Block ---
if __name__ == "__main__":
    scanner = ScannerMS()
    print("Service ready:", scanner)

    # Scan the current directory
    cwd = os.getcwd()
    print(f"Scanning: {cwd} ...")

    start_time = time.time()
    tree = scanner.scan_directory(cwd)
    duration = time.time() - start_time

    if tree:
        file_count = len(scanner.flatten_tree(tree))
        print(f"Scan complete in {duration:.4f}s")
        print(f"Found {file_count} files.")
        # Print top level children to verify
        print("Top Level Structure:")
        for child in tree.get('children', [])[:5]:
            print(f" - [{child['type'].upper()}] {child['text']}")
    else:
        print("Scan failed or path invalid.")

--------------------------------------------------------------------------------
FILE: __ScoutMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ScoutMS
ENTRY_POINT: __ScoutMS.py
DEPENDENCIES: None
"""

import os
import time
import requests
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Any, Optional

# Try imports for Web/PDF support
try:
    from bs4 import BeautifulSoup
except ImportError:
    BeautifulSoup = None

@service_metadata(
    name="ScoutMS",
    version="1.0.0",
    description="The Scout: A depth-aware utility for recursively walking local file systems or crawling websites.",
    tags=["utility", "scanner", "crawler"],
    capabilities=["filesystem:read", "web:crawl"]
)
class ScoutMS:
    """
    The Scanner: Walks file systems OR crawls websites (Depth-Aware).
    """
    
    def __init__(self):
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage', 'site-packages'
        }
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', 
            '.zip', '.tar', '.gz', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }
        self.visited_urls = set()

    def is_binary(self, file_path: str) -> bool:
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS: return True
        return False

    @service_endpoint(
        inputs={"root_path": "str", "web_depth": "int"},
        outputs={"tree": "dict"},
        description="Main entry point to perform a recursive scan of a directory or a web crawl.",
        tags=["discovery", "recursive"]
    )
    def scan_directory(self, root_path: str, web_depth: int = 0) -> Optional[Dict[str, Any]]:
        """
        Main Entry Point.
        :param root_path: File path or URL.
        :param web_depth: How many links deep to crawl (0 = single page).
        """
        # 1. Web Crawl Mode
        if root_path.startswith("http://") or root_path.startswith("https://"):
            self.visited_urls.clear()
            return self._crawl_web_recursive(root_path, depth=web_depth, origin_domain=urlparse(root_path).netloc)

        # 2. Local File System Mode
        target = os.path.abspath(root_path)
        if not os.path.exists(target): return None
        
        if not os.path.isdir(target): 
            return self._create_node(target, is_dir=False)
            
        return self._scan_fs_recursive(target)

    # --- Web Logic ---
    def _crawl_web_recursive(self, url: str, depth: int, origin_domain: str) -> Dict[str, Any]:
        """
        Recursively fetches links.
        """
        # Generate a nice VFS path: web/domain/path
        parsed = urlparse(url)
        clean_path = parsed.path.strip("/")
        if not clean_path: clean_path = "index.html"
        rel_path = f"web/{parsed.netloc}/{clean_path}"

        node = {
            'name': url,
            'path': url,
            'rel_path': rel_path,
            'type': 'web',
            'children': [],
            'checked': True
        }
        
        if depth < 0 or url in self.visited_urls: return node
        self.visited_urls.add(url)

        if depth > 0 and BeautifulSoup:
            try:
                # Polite Delay
                time.sleep(0.1)
                resp = requests.get(url, timeout=5)
                if resp.status_code == 200:
                    soup = BeautifulSoup(resp.content, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        full_url = urljoin(url, link['href'])
                        parsed = urlparse(full_url)
                        
                        # Filter: Only same domain, valid schemes
                        if parsed.netloc == origin_domain and parsed.scheme in ['http', 'https']:
                            if full_url not in self.visited_urls:
                                child_node = self._crawl_web_recursive(full_url, depth - 1, origin_domain)
                                node['children'].append(child_node)
            except Exception as e:
                node['error'] = str(e)
                
        return node

    # --- File System Logic ---
    def _scan_fs_recursive(self, current_path: str, root_path: str = None) -> Dict[str, Any]:
        if root_path is None: root_path = current_path
        
        node = self._create_node(current_path, is_dir=True, root_path=root_path)
        node['children'] = []
        try:
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                for entry in entries:
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS: continue
                    if entry.name.startswith('.'): continue

                    if entry.is_dir():
                        child = self._scan_fs_recursive(entry.path, root_path=root_path)
                        if child: node['children'].append(child)
                    else:
                        node['children'].append(self._create_node(entry.path, is_dir=False, root_path=root_path))
        except PermissionError:
            node['error'] = "Access Denied"
        return node

    def _create_node(self, path: str, is_dir: bool, root_path: str = None) -> Dict[str, Any]:
        name = os.path.basename(path)
        # Calculate relative path for VFS
        rel_path = name
        if root_path:
            try:
                rel_path = os.path.relpath(path, root_path).replace("\\", "/")
            except ValueError:
                pass

        node = {
            'name': name, 
            'path': path, 
            'rel_path': rel_path,
            'type': 'folder' if is_dir else 'file', 
            'children': [],
            'checked': False
        }
        return node

    @service_endpoint(
        inputs={"tree_node": "dict"},
        outputs={"file_list": "list"},
        description="Flattens a hierarchical tree node structure into a simple list of paths.",
        tags=["utility", "processing"]
    )
    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
        files = []
        if tree_node['type'] in ['file', 'web']:
            files.append(tree_node['path'])
        elif 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files

    if __name__ == "__main__":
        svc = ScoutMS()
        print("Service ready:", svc._service_info["name"])
    # Basic local test
    current_dir = os.path.dirname(os.path.abspath(__file__))
    tree = svc.scan_directory(current_dir)
    if tree:
        print(f"Scanned {len(svc.flatten_tree(tree))} files in current directory.")




--------------------------------------------------------------------------------
FILE: __SearchEngineMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SearchEngineMS
ENTRY_POINT: __SearchEngineMS.py
DEPENDENCIES: requests, sqlite-vec
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["requests", "sqlite-vec"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _SearchEngineMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import sqlite3
import json
import struct
import requests
import os
from typing import List, Dict, Any, Optional

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    
    Architecture:
    1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
    2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
    3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).
    """

    def __init__(self, model_name: str = "phi3:mini-128k"):
        self.model = model_name

    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict]:
        """
        Main entry point. Returns a list of results sorted by relevance.
        """
        if not os.path.exists(db_path):
            return []

        conn = sqlite3.connect(db_path)
        # Enable sqlite-vec extension if needed, though standard connect might miss it 
        # depending on system install. For now, we assume the DB is pre-populated 
        # and standard SQL queries work if the extension is loaded globally or unnecessary 
        # for simple selects (standard SQLite can read vec0 tables usually, just not query them efficiently without ext).
        # Note: If sqlite-vec is not loaded, the vec0 MATCH queries below will fail.
        # We try to load it here just in case.
        conn.enable_load_extension(True)
        try:
            import sqlite_vec
            sqlite_vec.load(conn)
        except:
            print("Warning: sqlite_vec not loaded in Search Engine. Vector search may fail.")

        cursor = conn.cursor()

        # 1. Vectorize the User Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails
            return self._keyword_search_only(cursor, query, limit)

        # Pack vector for sqlite-vec (Float32 Little Endian)
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)

        # 2. HYBRID QUERY (The "Magic" SQL)
        # We use CTEs to get top 50 from Vector and top 50 from Keyword, then merge.
        sql = """
        WITH 
        vec_matches AS (
            SELECT rowid, distance,
            row_number() OVER (ORDER BY distance) as rank
            FROM knowledge_vectors
            WHERE embedding MATCH ? 
            AND k = 50
        ),
        fts_matches AS (
            SELECT rowid, rank as fts_score,
            row_number() OVER (ORDER BY rank) as rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT 50
        )
        SELECT 
            kc.file_path,
            kc.content,
            (
                -- RRF Formula: 1 / (k + rank)
                COALESCE(1.0 / (60 + v.rank), 0.0) +
                COALESCE(1.0 / (60 + f.rank), 0.0)
            ) as rrf_score
        FROM knowledge_chunks kc
        LEFT JOIN vec_matches v ON kc.id = v.rowid
        LEFT JOIN fts_matches f ON kc.id = f.rowid
        WHERE v.rowid IS NOT NULL OR f.rowid IS NOT NULL
        ORDER BY rrf_score DESC
        LIMIT ?;
        """

        try:
            # Escape quotes for FTS
            fts_query = f'"{query}"' 
            rows = cursor.execute(sql, (vec_bytes, fts_query, limit)).fetchall()
        except sqlite3.OperationalError as e:
            print(f"Search Error (likely missing sqlite-vec): {e}")
            return []

        results = []
        for r in rows:
            path, content, score = r
            snippet = self._extract_snippet(content, query)
            results.append({
                "path": path,
                "score": round(score, 4),
                "snippet": snippet,
                "full_content": content # Keeping this for "Reconstruct" later
            })

        conn.close()
        return results

    def _keyword_search_only(self, cursor, query: str, limit: int) -> List[Dict]:
        """Fallback if embeddings are offline."""
        sql = """
            SELECT file_path, content
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        rows = cursor.execute(sql, (f'"{query}"', limit)).fetchall()
        return [{
            "path": r[0], 
            "score": 0.0, 
            "snippet": self._extract_snippet(r[1], query),
            "full_content": r[1]
        } for r in rows]

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        """Call Ollama to get the vector for the search query."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

    def _extract_snippet(self, content: str, query: str) -> str:
        """Finds the best window of text around the keyword."""
        lower_content = content.lower()
        lower_query = query.lower().split()[0] # Take first word for simple centering
        
        idx = lower_content.find(lower_query)
        if idx == -1:
            return content[:200].replace('\n', ' ') + "..."
            
        start = max(0, idx - 60)
        end = min(len(content), idx + 140)
        snippet = content[start:end].replace('\n', ' ')
        return f"...{snippet}..."

# --- Independent Test Block ---
if __name__ == "__main__":
    # Note: Requires a real DB path to work
    print("Initializing Search Engine...")
    engine = SearchEngineMS()
    # Test would go here

--------------------------------------------------------------------------------
FILE: __SemanticChunkerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SemanticChunkerMS
ENTRY_POINT: __SemanticChunkerMS.py
DEPENDENCIES: None
"""

import ast
from dataclasses import dataclass
from typing import List
from microservice_std_lib import service_metadata, service_endpoint

@dataclass SemanticChunkerMS CodeChunk:
    name: str          # e.g., "SemanticChunkerMS AuthMS"
    type: str          # "SemanticChunkerMS", "function", "text"
    content: str       # The raw source
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

@service_metadata(
    name="SmartChunkerMS",
    version="1.0.0",
    description="The Surgeon: Intelligent Code Splitter that parses source code into logical semantic units (Classes, Functions) using AST.",
    tags=["utility", "nlp", "parser"],
    capabilities=["python-ast", "semantic-chunking"]
)
SemanticChunkerMS SemanticChunker:
    """
    Intelligent Code Splitter.
    Parses source code into logical units (Classes, Functions) 
    rather than arbitrary text windows.
    """
    
    @service_endpoint(
        inputs={"content": "str", "filename": "str"},
        outputs={"chunks": "list"},
        description="Main entry point to split a file into semantic chunks based on its extension and content.",
        tags=["processing", "chunking"]
    )
    def chunk_file(self, content: str, filename: str) -> List[CodeChunk]:
        # 1. Python Code
        if filename.endswith(".py"):
            return self._chunk_python(content)
            
        # 2. Text / Prose Documents (Smaller semantic windows)
        lower = filename.lower()
        if lower.endswith(('.md', '.txt', '.pdf', '.html', '.htm', '.rst')):
            return self._chunk_generic(content, window_size=800)
            
        # 3. Fallback (Generic Code/Binary)
        return self._chunk_generic(content, window_size=1500)

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                end = node.end_lineno if hasattr(node, 'end_lineno') else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", type="function", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"SemanticChunkerMS {node.name}", type="SemanticChunkerMS", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))

            # Fallback: If no classes/functions found (e.g., script file), treat as generic
            if not chunks:
                return self._chunk_generic(source)
                
        except SyntaxError:
            return self._chunk_generic(source)
            
        return chunks

    def _chunk_generic(self, text: str, window_size: int = 1500) -> List[CodeChunk]:
        """Sliding window for non-code files."""
        chunks = []
        # normalize newlines to avoid massive single-line blobs
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        lines = text.splitlines(keepends=True)
        
        current_chunk = []
        current_size = 0
        chunk_idx = 1
        start_line = 1
        
        for i, line in enumerate(lines):
            current_chunk.append(line)
            current_size += len(line)
            
            if current_size >= window_size:
                chunks.append(CodeChunk(
                    name=f"Chunk {chunk_idx}", type="text_block",
                    content="".join(current_chunk), start_line=start_line, end_line=i + 1
                ))
                current_chunk = []
                current_size = 0
                chunk_idx += 1
                start_line = i + 2
                
        if current_chunk:
            chunks.append(CodeChunk(
                name=f"Chunk {chunk_idx}", type="text_block",
                content="".join(current_chunk), start_line=start_line, end_line=len(lines)
            ))
            
                    return chunks

            if __name__ == "__main__":
                svc = SemanticChunker()
                print("Service ready:", svc._service_info["name"])
                # Basic test on a snippet of Python code
                test_code = "def hello():\n    print('world')\n\nclass Test:\n    pass"
                chunks = svc.chunk_file(test_code, "test.py")
                print(f"Extracted {len(chunks)} semantic chunks.")
                for c in chunks:
                    print(f" - [{c.type}] {c.name} ({c.start_line}-{c.end_line})")



--------------------------------------------------------------------------------
FILE: __ServiceRegistryMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ServiceRegistryMS
ENTRY_POINT: __ServiceRegistryMS.py
DEPENDENCIES: None
"""

import ast
import json
import uuid
import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("ServiceRegistry")
# ==============================================================================

@service_metadata(
name="ServiceRegistry",
version="1.0.0",
description="Scans a library of Python microservices and generates standardized JSON Service Tokens.",
tags=["introspection", "registry", "parsing"],
capabilities=["filesystem:read", "filesystem:write"]
)
class ServiceRegistryMS:
    """
The Tokenizer (v2): Scans a library of Python microservices and generates
standardized JSON 'Service Tokens'.
Feature: Hybrid AST/Regex parsing for maximum robustness.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    self.root = Path(self.config.get("root_path", ".")).resolve()
    self.registry = []

    @service_endpoint(
    inputs={"save_to": "str"},
    outputs={"registry": "List[Dict]"},
    description="Scans the file system for microservices and builds a registry.",
    tags=["introspection", "scan"],
    side_effects=["filesystem:read", "filesystem:write"]
    )
    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict]:
    log.info(f"Scanning for microservices in: {self.root}")
        
        # 1. Walk directories
        for item in self.root.iterdir():
            if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                self._process_folder(item)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            log.info(f"âœ… Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            log.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_folder(self, folder: Path):
        # Find the main .py file (usually matches folder name, or is the only .py file)
        candidates = list(folder.glob("*.py"))
        for file in candidates:
            if file.name.startswith("__"): continue
            
            token = self._tokenize_file(file)
            if token:
                self.registry.append(token)
                log.info(f"  + Tokenized: {token['name']}")
                break 

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source = f.read()
            
            # Attempt 1: Strict AST Parsing (The "Right" Way)
            try:
                return self._ast_parse(source, file_path)
            except Exception:
                # Attempt 2: Regex Fallback (The "Survival" Way)
                return self._regex_parse(source, file_path)
                
        except Exception as e:
            log.warning(f"  - Failed to read {file_path.name}: {e}")
            return None

    def _ast_parse(self, source: str, file_path: Path):
tree = ast.parse(source)
        target_class = None
        
        # Find class ending in 'MS'
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name.endswith("MS"):
                target_class = node
                break
        
        if not target_class: return None

        # Extract Metadata
        return self._build_token(
            name=target_class.name,
            doc=ast.get_docstring(target_class) or "",
            methods=[
                (n.name, [a.arg for a in n.args.args if a.arg != 'self'], ast.get_docstring(n) or "")
                for n in target_class.body if isinstance(n, ast.FunctionDef) and not n.name.startswith("_")
            ],
            deps=self._extract_ast_imports(tree),
            file_path=file_path
        )

    def _regex_parse(self, source: str, file_path: Path):
        # Find class definition
        class_match = re.search(r'class\s+(\w+MS)', source)
        if not class_match: return None
        name = class_match.group(1)
        
        # Find methods (def name(args):)
        methods = []
        for match in re.finditer(r'def\s+(\w+)\s*\((.*?)\):', source):
            m_name = match.group(1)
            if not m_name.startswith("_"):
                args = [a.strip().split(':')[0] for a in match.group(2).split(',') if a.strip() != 'self']
                methods.append((m_name, args, "Regex extracted"))
                
        return self._build_token(name, "Parsed via Regex", methods, [], file_path)

    def _build_token(self, name, doc, methods, deps, file_path):
        # Generate deterministic ID
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, name).hex[:8].upper()}"
        
        method_dict = {
            m_name: {"args": m_args, "doc": m_doc.strip()} 
            for m_name, m_args, m_doc in methods
        }

        return {
            "token_id": token_id,
            "name": name,
            "path": str(file_path.relative_to(self.root)).replace('\\', '/'),
            "description": doc.strip(),
            "methods": method_dict,
            "dependencies": sorted(deps)
        }

    def _extract_ast_imports(self, tree):
        deps = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: deps.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: deps.add(node.module.split('.')[0])
        return list(deps)

if __name__ == "__main__":
svc = ServiceRegistryMS()
print("Service ready:", svc)
svc.scan()

--------------------------------------------------------------------------------
FILE: __SpinnerThingyMaBobberMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SpinnerThingyMaBobberMS
ENTRY_POINT: __SpinnerThingyMaBobberMS.py
DEPENDENCIES: None
"""

import tkinter as tk
import math
import colorsys
import time
from typing import Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SpinnerTHINGYMABOBBER",
version="1.0.0",
description="Interactive visual spinner widget for OBS/UI overlays.",
tags=["ui", "widget", "visuals"],
capabilities=["ui:gui"]
)
class SpinnerThingyMaBobberMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.root = tk.Tk()
self.root.title("OBS Interactive Spinner")
        self.root.configure(bg="black")
        
        # Default size
        self.root.geometry("600x600")
        
        # Canvas for drawing
        self.canvas = tk.Canvas(
            self.root, 
            bg="black", 
            highlightthickness=0
        )
        self.canvas.pack(fill="both", expand=True)

        # --- STATE VARIABLES ---
        self.angle_1 = 0
        self.angle_2 = 0
        self.angle_3 = 0
        self.hue = 0
        
        # Text Input State
        self.user_text = "PROCESSING"
        self.cursor_visible = True
        self.last_cursor_toggle = time.time()
        
        # Bind keyboard events to the window
        self.root.bind("<Key>", self.handle_keypress)
        
        # Start animation
        self.animate()

        @service_endpoint(
@service_endpoint(
    inputs={},
    outputs={},
    description="Launches the GUI main loop.",
    tags=["ui", "execution"],
    mode="sync",
    side_effects=["ui:block"]
)
def launch(self):
    self.root.mainloop()

def handle_keypress(self, event):
    # Handle Backspace
    if event.keysym == "BackSpace":
        self.user_text = self.user_text[:-1]
    # Handle Escape (Reset to default)
    elif event.keysym == "Escape":
        self.user_text = "PROCESSING"
    # Ignore special keys (Shift, Ctrl, Alt, F-keys, etc.)
    elif len(event.char) == 1 and ord(event.char) >= 32:
        # Limit length to prevent chaos (optional, but 20 is a safe max)
        if len(self.user_text) < 25: 
            self.user_text += event.char.upper()

def get_neon_color(self, offset=0):
    h = (self.hue + offset) % 1.0
    r, g, b = colorsys.hsv_to_rgb(h, 1.0, 1.0)
    return f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}'

def draw_arc(self, cx, cy, radius, width, start, extent, color):
    x0 = cx - radius
    y0 = cy - radius
    x1 = cx + radius
    y1 = cy + radius
    
    self.canvas.create_arc(
        x0, y0, x1, y1,
        start=start, extent=extent,
        outline=color, width=width, style="arc"
    )
def animate(self):
    self.canvas.delete("all")
    
    # Window Dimensions
    w = self.canvas.winfo_width()
    h = self.canvas.winfo_height()
    
    if w < 10 or h < 10:
        self.root.after(50, self.animate)
        return

    cx, cy = w / 2, h / 2
    base_size = min(w, h) / 2
    
    # Update Hue
    self.hue += 0.005
    if self.hue > 1: self.hue = 0
    c1 = self.get_neon_color(0.0)
    c2 = self.get_neon_color(0.3)
    c3 = self.get_neon_color(0.6)

    # --- RINGS ---
        
    # Ring 1
    r1 = base_size * 0.85
    self.angle_1 -= 3
    for i in range(3):
        self.draw_arc(cx, cy, r1, base_size*0.08, self.angle_1 + (i*120), 80, c1)

    # Ring 2
    r2 = base_size * 0.65
    self.angle_2 += 5
    self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2, 160, c2)
    self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2 + 180, 160, c2)

    # Ring 3
    r3 = base_size * 0.45
    self.angle_3 -= 8
    self.draw_arc(cx, cy, r3, base_size*0.04, self.angle_3, 300, c3)
        # --- TEXT LOGIC ---
        
        # Toggle cursor every 0.5 seconds
        if time.time() - self.last_cursor_toggle > 0.5:
            self.cursor_visible = not self.cursor_visible
            self.last_cursor_toggle = time.time()
            
        display_text = self.user_text + ("_" if self.cursor_visible else " ")

        # Dynamic Font Scaling
        # We start with a base size (0.15 of window).
        # If text is long (> 8 chars), we shrink it proportionally so it fits.
        text_len = max(len(self.user_text), 1)
        scaling_factor = 1.0
        if text_len > 8:
            scaling_factor = 8 / text_len
            
        font_size = int(base_size * 0.15 * scaling_factor)
        # Ensure font doesn't vanish
        font_size = max(font_size, 10) 

        self.canvas.create_text(
            cx, cy, 
            text=display_text, 
            fill="white", 
            font=("Courier", font_size, "bold")
        )

        self.root.after(30, self.animate)

if __name__ == "__main__":
svc = SpinnerThingyMaBobberMS()
svc.launch()

--------------------------------------------------------------------------------
FILE: __SysInspectorMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _SysInspectorMS
ENTRY_POINT: __SysInspectorMS.py
DEPENDENCIES: None
"""

import platform
import subprocess
import sys
import datetime
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SysInspector",
version="1.0.0",
description="Gathers hardware and environment statistics via shell commands.",
tags=["system", "audit", "hardware"],
capabilities=["os:shell", "compute"]
)
class SysInspectorMS:
    """
The Auditor: Gathers hardware and environment statistics.
Supports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={},
outputs={"report": "str"},
description="Runs the full audit and returns a formatted string report.",
tags=["system", "report"],
side_effects=["os:read"]
)
def generate_report(self) -> str:
        """
        Runs the full audit and returns a formatted string report.
        """
        system_os = platform.system()
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = [
            f"System Audit Report",
            f"Generated: {timestamp}",
            f"OS: {system_os} {platform.release()} ({platform.machine()})",
            "-" * 40,
            ""
        ]

        # 1. Hardware Section
        report.append("--- Hardware Information ---")
        if system_os == "Windows":
            report.extend(self._audit_windows())
        elif system_os == "Linux":
            report.extend(self._audit_linux())
        elif system_os == "Darwin":
            report.extend(self._audit_mac())
        else:
            report.append("Unsupported Operating System for detailed hardware audit.")

        # 2. Software Section
        report.append("\n--- Software Environment ---")
        report.append(f"Python Version: {platform.python_version()}")
        report.append(f"Python Executable: {sys.executable}")
        
        return "\n".join(report)

    def _run_cmd(self, cmd: str) -> str:
        """Helper to run shell commands safely."""
        try:
            # shell=True is often required for piped commands, specifically on Windows/Linux
            result = subprocess.run(
                cmd, 
                text=True, 
                capture_output=True, 
                check=False, 
                shell=True, 
                timeout=5
            )
            if result.returncode == 0 and result.stdout:
                return result.stdout.strip()
            elif result.stderr:
                return f"[Cmd Error]: {result.stderr.strip()}"
            return "[No Output]"
        except Exception as e:
            return f"[Execution Error]: {e}"

    # --- OS Specific Implementations ---

    def _audit_windows(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("wmic cpu get name"))
        # GPU
        data.append("GPU: " + self._run_cmd("wmic path win32_videocontroller get name"))
        # RAM
        try:
            mem_str = self._run_cmd("wmic computersystem get totalphysicalmemory").splitlines()[-1]
            mem_bytes = int(mem_str)
            data.append(f"Memory: {mem_bytes / (1024**3):.2f} GB")
        except:
            data.append("Memory: Could not retrieve total physical memory.")
        # Disk
        data.append("\nDisks:")
        data.append(self._run_cmd("wmic diskdrive get model,size"))
        return data

    def _audit_linux(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("lscpu | grep 'Model name'"))
        # GPU (Requires lspci, usually in pciutils)
        data.append("GPU: " + self._run_cmd("lspci | grep -i vga"))
        # RAM
        data.append("Memory:\n" + self._run_cmd("free -h"))
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("lsblk -o NAME,SIZE,MODEL"))
        return data

    def _audit_mac(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("sysctl -n machdep.cpu.brand_string"))
        # GPU
        data.append("GPU:\n" + self._run_cmd("system_profiler SPDisplaysDataType | grep -E 'Chipset Model|VRAM'"))
        # RAM
        data.append("Memory Details:\n" + self._run_cmd("system_profiler SPMemoryDataType | grep -E 'Size|Type|Speed'"))
        # RAM Total
        try:
            mem_bytes = int(self._run_cmd('sysctl -n hw.memsize'))
            data.append(f"Total Memory: {mem_bytes / (1024**3):.2f} GB")
        except: 
            pass
        # Disk
data.append("\nDisks:\n" + self._run_cmd("diskutil list physical"))
        return data

# --- Independent Test Block ---
if __name__ == "__main__":
    inspector = SysInspectorMS()
    print("Service ready:", inspector)
    print("Running System Inspector...")
    print("\n" + inspector.generate_report())

--------------------------------------------------------------------------------
FILE: __TasklistVaultMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TasklistVaultMS
ENTRY_POINT: __TasklistVaultMS.py
DEPENDENCIES: None
"""

import sqlite3
import uuid
import logging
import datetime
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "task_vault.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("TaskVault")

TaskStatus = Literal["Pending", "Running", "Complete", "Error", "Awaiting-Approval"]
# ==============================================================================

@service_metadata(
name="TaskVault",
version="1.0.0",
description="Persistent SQLite engine for hierarchical task management.",
tags=["tasks", "db", "project-management"],
capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class TasklistVaultMS:
    """
The Taskmaster: A persistent SQLite engine for hierarchical task management.
Supports infinite nesting of sub-tasks and status tracking.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", DB_PATH)
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. Task Lists (The containers)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_lists (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP
                )
            """)
            # 2. Tasks (The items, supporting hierarchy via parent_id)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id TEXT PRIMARY KEY,
                    list_id TEXT NOT NULL,
                    parent_id TEXT,
                    content TEXT NOT NULL,
                    status TEXT DEFAULT 'Pending',
                    result TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    FOREIGN KEY(list_id) REFERENCES task_lists(id) ON DELETE CASCADE,
                    FOREIGN KEY(parent_id) REFERENCES tasks(id) ON DELETE CASCADE
                )
            """)

    # --- List Management ---

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"list_id": "str"},
    description="Creates a new task list and returns its ID.",
    tags=["tasks", "create"],
    side_effects=["db:write"]
    )
    def create_list(self, name: str) -> str:
    """Creates a new task list and returns its ID."""
        list_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO task_lists (id, name, created_at) VALUES (?, ?, ?)",
                (list_id, name, now)
            )
        log.info(f"Created Task List: '{name}' ({list_id})")
        return list_id

    @service_endpoint(
    inputs={},
    outputs={"lists": "List[Dict]"},
    description="Returns metadata for all task lists.",
    tags=["tasks", "read"],
    side_effects=["db:read"]
    )
    def get_lists(self) -> List[Dict]:
    """Returns metadata for all task lists."""
        with self._get_conn() as conn:
            rows = conn.execute("SELECT * FROM task_lists ORDER BY created_at DESC").fetchall()
            return [dict(r) for r in rows]

    # --- Task Management ---

    @service_endpoint(
    inputs={"list_id": "str", "content": "str", "parent_id": "Optional[str]"},
    outputs={"task_id": "str"},
    description="Adds a task (or sub-task) to a list.",
    tags=["tasks", "write"],
    side_effects=["db:write"]
    )
    def add_task(self, list_id: str, content: str, parent_id: Optional[str] = None) -> str:
    """Adds a task (or sub-task) to a list."""
        task_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                """INSERT INTO tasks (id, list_id, parent_id, content, status, created_at, updated_at) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (task_id, list_id, parent_id, content, "Pending", now, now)
            )
        return task_id

    @service_endpoint(
    inputs={"task_id": "str", "content": "str", "status": "str", "result": "str"},
    outputs={},
    description="Updates a task's details.",
    tags=["tasks", "update"],
    side_effects=["db:write"]
    )
    def update_task(self, task_id: str, content: str = None, status: TaskStatus = None, result: str = None):
    """Updates a task's details."""
        updates = []
        params = []
        
        if content:
            updates.append("content = ?")
            params.append(content)
        if status:
            updates.append("status = ?")
            params.append(status)
        if result:
            updates.append("result = ?")
            params.append(result)
            
        if not updates: return

        updates.append("updated_at = ?")
        params.append(datetime.datetime.utcnow())
        params.append(task_id)

        sql = f"UPDATE tasks SET {', '.join(updates)} WHERE id = ?"
        
        with self._get_conn() as conn:
            conn.execute(sql, params)
        log.info(f"Updated task {task_id}")

    # --- Tree Reconstruction ---

    @service_endpoint(
    inputs={"list_id": "str"},
    outputs={"tree": "Dict[str, Any]"},
    description="Fetches a list and reconstructs the full hierarchy of tasks.",
    tags=["tasks", "read"],
    side_effects=["db:read"]
    )
    def get_full_tree(self, list_id: str) -> Dict[str, Any]:
    """
    Fetches a list and reconstructs the full hierarchy of tasks.
    """
        with self._get_conn() as conn:
# 1. Get List Info
            list_row = conn.execute("SELECT * FROM task_lists WHERE id = ?", (list_id,)).fetchone()
            if not list_row: return None
            
            # 2. Get All Tasks
            task_rows = conn.execute("SELECT * FROM tasks WHERE list_id = ?", (list_id,)).fetchall()
            
        # 3. Build Adjacency Map
        tasks_by_id = {}
        for r in task_rows:
            t = dict(r)
            t['sub_tasks'] = [] # Prepare children container
            tasks_by_id[t['id']] = t

        # 4. Link Parents and Children
        root_tasks = []
        for t_id, task in tasks_by_id.items():
            parent_id = task['parent_id']
            if parent_id and parent_id in tasks_by_id:
                tasks_by_id[parent_id]['sub_tasks'].append(task)
            else:
                root_tasks.append(task)

        return {
            "id": list_row['id'],
            "name": list_row['name'],
            "tasks": root_tasks
        }

    @service_endpoint(
    inputs={"list_id": "str"},
    outputs={},
    description="Deletes a task list and all its tasks.",
    tags=["tasks", "delete"],
    side_effects=["db:write"]
    )
    def delete_list(self, list_id: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM task_lists WHERE id = ?", (list_id,))
        log.info(f"Deleted list {list_id}")
# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    vault = TasklistVaultMS()
    print("Service ready:", vault)
    
    # 1. Create a Plan
    plan_id = vault.create_list("System Upgrade Plan")
    
    # 2. Add Root Tasks
    t1 = vault.add_task(plan_id, "Backup Database")
    t2 = vault.add_task(plan_id, "Update Server")
    
    # 3. Add Sub-Tasks
    t2_1 = vault.add_task(plan_id, "Stop Services", parent_id=t2)
    t2_2 = vault.add_task(plan_id, "Run Installer", parent_id=t2)
    
    # 4. Update Status
    vault.update_task(t1, status="Complete", result="Backup saved to /tmp/bk.tar")
    vault.update_task(t2_1, status="Running")
    
    # 5. Render Tree
    tree = vault.get_full_tree(plan_id)
    print(f"\n--- {tree['name']} ---")
    
    def print_node(node, indent=0):
        status_icon = "âœ“" if node['status'] == 'Complete' else "â—‹"
        print(f"{'  '*indent}{status_icon} {node['content']} [{node['status']}]")
        for child in node['sub_tasks']:
            print_node(child, indent + 1)

    for task in tree['tasks']:
        print_node(task)
        
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: __TelemetryServiceMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TelemetryServiceMS
ENTRY_POINT: __TelemetryServiceMS.py
DEPENDENCIES: None
"""

import logging
import queue
import time
from base_service import BaseService
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
    name="TelemetryServiceMS",
    version="1.0.0",
    description="The Nervous System: Watches the thread-safe LogQueue and updates GUI components with real-time status.",
    tags=["utility", "logging", "telemetry"],
    capabilities=["log-redirection", "real-time-updates"]
)
class TelemetryServiceMS(BaseService):
    """
    The Nervous System.
    Watches the thread-safe LogQueue and updates the GUI Panels.
    """
    def __init__(self, root, panels):
        super().__init__("TelemetryServiceMS")
        self.root = root
        self.panels = panels
        self.log_queue = queue.Queue()
        self.start_time = time.time()
        self._heartbeat_count = 0
        
        # We set up the global logging hook HERE, inside the service
        self._setup_logging_hook()

    @service_endpoint(
        inputs={},
        outputs={"status": "str", "uptime": "float", "queue_depth": "int"},
        description="Standardized health check to verify the operational state of the telemetry pipeline.",
        tags=["diagnostic", "health"]
    )
    def get_health(self) -> Dict[str, Any]:
        """Returns the operational status of the TelemetryServiceMS."""
        return {
            "status": "online",
            "uptime": time.time() - self.start_time,
            "queue_depth": self.log_queue.qsize()
        }

    def _setup_logging_hook(self):
        """Redirects Python's standard logging to our Queue."""
        logger = logging.getLogger()
        logger.setLevel(logging.INFO)
        
        # Create our custom handler that feeds the queue
        q_handler = QueueHandler(self.log_queue)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%H:%M:%S')
        q_handler.setFormatter(formatter)
        logger.addHandler(q_handler)

    @service_endpoint(
        inputs={},
        outputs={},
        description="Initiates the telemetry service and begins the asynchronous GUI log-polling loop.",
        tags=["lifecycle", "event-loop"],
        mode="async"
    )
    def start(self):
        """Begins the GUI update loop."""
        self.log_info("Telemetry Service starting...")
        self._poll_queue()

    @service_endpoint(
        inputs={},
        outputs={"alive": "bool", "heartbeat": "int"},
        description="Verifies that the GUI polling loop is actively processing the log queue.",
        tags=["diagnostic", "heartbeat"]
    )
    def ping(self) -> Dict[str, Any]:
        """Allows an agent to verify the pulse of the UI loop."""
        return {"alive": True, "heartbeat": self._heartbeat_count}

    def _poll_queue(self):
        """The heartbeat that drains the queue into the GUI."""
        self._heartbeat_count += 1
        try:
            while True:
                record = self.log_queue.get_nowait()
                msg = f"[{record.levelname}] {record.message}" # Removed \n because .log() adds it
                
                # Update the GUI
                self.panels.log(msg)
                
        except queue.Empty:
            pass
        finally:
            # Check again in 100ms
            self.root.after(100, self._poll_queue)

# Helper Class for the Queue
class QueueHandler(logging.Handler):
    def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.format(record)
        self.log_queue.put(record)

    if __name__ == "__main__":
    # Mock objects for independent test
    class MockRoot: 
        def after(self, ms, func): print(f"Loop scheduled for {ms}ms")
    class MockPanels:
        def log(self, msg): print(f"UI LOG: {msg}")
    
    svc = TelemetryServiceMS(MockRoot(), MockPanels())
    print("Service ready:", svc._service_info["name"])
    svc.log_info("Internal test message")
    svc._poll_queue()




--------------------------------------------------------------------------------
FILE: __TextChunkerMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TextChunkerMS
ENTRY_POINT: __TextChunkerMS.py
DEPENDENCIES: None
"""

from typing import Any, Dict, List, Optional, Tuple
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="TextChunker",
version="1.0.0",
description="Splits text into chunks using various strategies (chars, lines).",
tags=["chunking", "nlp", "rag"],
capabilities=["compute"]
)
class TextChunkerMS:
    """
The Butcher: A unified service for splitting text into digestible chunks
for RAG (Retrieval Augmented Generation).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@staticmethod
@service_endpoint(
inputs={"text": "str", "chunk_size": "int", "chunk_overlap": "int"},
outputs={"chunks": "List[str]"},
description="Standard sliding window split by character count.",
tags=["chunking", "chars"],
side_effects=[]
)
    def chunk_by_chars(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """
        Standard Sliding Window. Best for prose/documentation.
        Splits purely by character count.
        """
        if chunk_size <= 0: raise ValueError("chunk_size must be positive")
        
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            # Advance start, backing up by overlap
            start += chunk_size - chunk_overlap
            
        return chunks

    @staticmethod
    @service_endpoint(
    inputs={"text": "str", "max_lines": "int", "max_chars": "int"},
    outputs={"chunks": "List[Dict]"},
    description="Line-preserving chunker, best for code.",
    tags=["chunking", "lines", "code"],
    side_effects=[]
    )
    def chunk_by_lines(text: str, max_lines: int = 200, max_chars: int = 4000) -> List[Dict[str, Any]]:
    """
    Line-Preserving Chunker. Best for Code.
    Respects line boundaries and returns metadata about line numbers.
    """
        lines = text.splitlines()
        chunks = []
        start = 0
        
        while start < len(lines):
            end = min(start + max_lines, len(lines))
            chunk_str = "\n".join(lines[start:end])
            
            # If too big, shrink window (back off)
            while len(chunk_str) > max_chars and end > start + 1:
                end -= 1
                chunk_str = "\n".join(lines[start:end])
            
            chunks.append({
                "text": chunk_str,
                "start_line": start + 1,
                "end_line": end
            })
            start = end
            
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = TextChunkerMS()
print("Service ready:", chunker)
    
    # 1. Prose Test
    print("--- Prose Chunking ---")
    lorem = "A" * 100 # 100 chars
    result = chunker.chunk_by_chars(lorem, chunk_size=40, chunk_overlap=10)
for i, c in enumerate(result):
        print(f"Chunk {i}: len={len(c)}")

    # 2. Code Test
    print("\n--- Code Chunking ---")
    code = "\n".join([f"print('Line {i}')" for i in range(1, 10)])
    # Force splits small for testing
    result_code = chunker.chunk_by_lines(code, max_lines=3, max_chars=100)
    for i, c in enumerate(result_code):
        print(f"Chunk {i}: Lines {c['start_line']}-{c['end_line']}")

--------------------------------------------------------------------------------
FILE: __ThoughtStreamMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _ThoughtStreamMS
ENTRY_POINT: __ThoughtStreamMS.py
DEPENDENCIES: None
"""

import tkinter as tk
from tkinter import ttk
import datetime
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="ThoughtStream",
version="1.0.0",
description="A UI widget for displaying a stream of AI thoughts/logs.",
tags=["ui", "stream", "logs", "widget"],
capabilities=["ui:gui"]
)
class ThoughtStreamMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
        
        # Header
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        # The Stream Area (Canvas allows for custom drawing like sparklines)
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340) # Fixed width like React
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    @service_endpoint(
    inputs={"filename": "str", "chunk_id": "int", "content": "str", "vector_preview": "List[float]", "color": "str"},
    outputs={},
    description="Adds a new thought bubble to the visual stream.",
    tags=["ui", "update"],
    side_effects=["ui:update"]
    )
    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
    """
    Mimics the 'InspectorFrame' from your React code.
    """
# Bubble Container
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        # Header: File + Timestamp
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        header_lbl = tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", 
                              fg="#007ACC", bg="#1a1a25", font=("Consolas", 8))
        header_lbl.pack(anchor="w", padx=5, pady=2)
        
        # Content Snippet
        snippet = content[:400] + "..." if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", 
                               font=("Consolas", 8), justify="left", wraplength=300)
        content_lbl.pack(fill="x", padx=5, pady=2)
        
        # Vector Sparkline (The Custom Draw)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        
        bar_w = w / len(vector) if len(vector) > 0 else 0
        
        for i, val in enumerate(vector):
            # Normalize -1..1 to 0..1 for height
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            
            # Draw bar
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")

# --- Usage Example ---
if __name__ == "__main__":
    root = tk.Tk()
    root.geometry("400x600")
    
    stream = ThoughtStreamMS({"parent": root})
    print("Service ready:", stream)
    stream.pack(fill="both", expand=True)
    
    # Simulate an incoming "Microservice" event
    import random
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("ExplorerView.tsx", 1, "import React from 'react'...", fake_vector, "#FF00FF")
    
    root.mainloop()

--------------------------------------------------------------------------------
FILE: __TkinterUniButtonMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TkinterUniButtonMS
ENTRY_POINT: __TkinterUniButtonMS.py
DEPENDENCIES: None
"""

## Locking Dual Button - Instructions
# TO USE:
# In your main app file
# from components import UnifiedButtonGroup # assuming you saved the TkinterUniButtonMS there
# 
# def my_validation_logic():
#     # do pandas stuff, etc
#     pass
# 
# def my_apply_logic():
#    # do database stuff
#     pass
# 
# Drop the button group into your GUI
# my_buttons = UnifiedButtonGroup(
#     parent=my_frame, 
#     on_validate=my_validation_logic, 
#     on_apply=my_apply_logic
# )
# my_buttons.pack()
## 

import tkinter as tk
from dataclasses import dataclass, field
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

@dataclass TkinterUniButtonMS ButtonConfig:
    text: str
    command: callable
    bg_color: str
    active_bg_color: str
    fg_color: str = "#FFFFFF"

@dataclass
TkinterUniButtonMS LinkConfig:
    """Configuration for the 'Linked' state (The Trap)"""
    trap_bg: str = "#7C3AED"    # Deep Purple
    btn_bg: str = "#8B5CF6"     # Lighter Purple
    text_color: str = "#FFFFFF"

@service_metadata(
name="LockingDualBtn",
version="1.0.0",
description="A unified button group (Left/Right/Link) where linking merges the actions.",
tags=["ui", "widget", "button"],
capabilities=["ui:gui"]
)
TkinterUniButtonMS LockingDualBtnMS(tk.Frame):
"""
A generic button group that can merge ANY two actions.
Pass the visual/functional definitions in via the config objects.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
        
self.left_cfg = self.config.get("left_btn")
self.right_cfg = self.config.get("right_btn")
self.link_cfg = self.config.get("link_config") or LinkConfig()
        
        self.is_linked = False
        self.default_bg = parent.cget("bg") # Fallback to parent background

        self._setup_ui()
        self._update_state()

    def _setup_ui(self):
        self.config(padx=4, pady=4)
        
        common_style = {"relief": "flat", "font": ("Segoe UI", 10, "bold"), "bd": 0, "cursor": "hand2"}

        # 1. Left Button (Generic)
        self.btn_left = tk.Button(self, command=lambda: self._execute("left"), **common_style)
        self.btn_left.pack(side="left", fill="y", padx=(0, 2))

        # 2. Link Toggle (The Chain)
        self.btn_link = tk.Button(self, text="&", width=3, command=self._toggle_link, **common_style)
        self.btn_link.pack(side="left", fill="y", padx=(0, 2))

        # 3. Right Button (Generic)
        self.btn_right = tk.Button(self, command=lambda: self._execute("right"), **common_style)
        self.btn_right.pack(side="left", fill="y")

    def _toggle_link(self):
        self.is_linked = not self.is_linked
        self._update_state()

    def _update_state(self):
        if self.is_linked:
            # --- LINKED STATE (The Trap) ---
            self.config(bg=self.link_cfg.trap_bg)
            
            # Both buttons look identical in the "Trap"
            for btn in (self.btn_left, self.btn_right, self.btn_link):
                btn.config(bg=self.link_cfg.btn_bg, fg=self.link_cfg.text_color, activebackground=self.link_cfg.trap_bg)
            
            # Keep original text
            self.btn_left.config(text=self.left_cfg.text)
            self.btn_right.config(text=self.right_cfg.text)

        else:
            # --- INDEPENDENT STATE ---
            try: self.config(bg=self.default_bg)
            except: self.config(bg="#f0f0f0") 

            # Restore Left Button
            self.btn_left.config(
                text=self.left_cfg.text, 
                bg=self.left_cfg.bg_color, 
                fg=self.left_cfg.fg_color,
                activebackground=self.left_cfg.active_bg_color
            )

            # Restore Right Button
            self.btn_right.config(
                text=self.right_cfg.text, 
                bg=self.right_cfg.bg_color, 
                fg=self.right_cfg.fg_color,
                activebackground=self.right_cfg.active_bg_color
            )

            # Restore Link Button (Neutral Gray)
            self.btn_link.config(bg="#E5E7EB", fg="#374151", activebackground="#D1D5DB")

    def _execute(self, source):
        if self.is_linked:
            # Chain them: Left then Right
            self.left_cfg.command()
            self.right_cfg.command()
        else:
        if source == "left": self.left_cfg.command()
        elif source == "right": self.right_cfg.command()

        if __name__ == "__main__":
        root = tk.Tk()
        btn1 = ButtonConfig("Save", lambda: print("Save"), "#444", "#555")
        btn2 = ButtonConfig("Run", lambda: print("Run"), "#444", "#555")
        svc = LockingDualBtnMS({"parent": root, "left_btn": btn1, "right_btn": btn2})
        print("Service ready:", svc)
        svc.pack(pady=20)
        root.mainloop()

--------------------------------------------------------------------------------
FILE: __TreeMapperMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _TreeMapperMS
ENTRY_POINT: __TreeMapperMS.py
DEPENDENCIES: None
"""

import os
from pathlib import Path
from typing import Any, Dict, List, Set, Optional
from microservice_std_lib import service_metadata, service_endpoint
import datetime

# ==============================================================================
# USER CONFIGURATION: DEFAULT EXCLUSIONS
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    '.venv', 'env', 'venv', 'dist', 'build', '.DS_Store'
}
# ==============================================================================

@service_metadata(
name="TreeMapper",
version="1.0.0",
description="Generates ASCII-art style directory maps of the file system.",
tags=["filesystem", "map", "visualization"],
capabilities=["filesystem:read"]
)
class TreeMapperMS:
    """
The Cartographer: Generates ASCII-art style directory maps.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"root_path": "str", "additional_exclusions": "Set[str]", "use_default_exclusions": "bool"},
outputs={"tree_map": "str"},
description="Generates an ASCII tree map of the directory.",
tags=["filesystem", "visualization"],
side_effects=["filesystem:read"]
)
def generate_tree(self, 
                      root_path: str, 
                      additional_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> str:
        
        start_path = Path(root_path).resolve()
        if not start_path.exists(): return f"Error: Path '{root_path}' does not exist."

        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_EXCLUDES)
        if additional_exclusions:
            exclusions.update(additional_exclusions)

        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lines = [
            f"Project Map: {start_path.name}",
            f"Generated: {timestamp}",
            "-" * 40,
            f"ðŸ“ {start_path.name}/"
        ]

        self._walk(start_path, "", lines, exclusions)
        return "\n".join(lines)

    def _walk(self, directory: Path, prefix: str, lines: List[str], exclusions: Set[str]):
        try:
            children = sorted(
                [p for p in directory.iterdir() if p.name not in exclusions],
                key=lambda x: (x.is_file(), x.name.lower())
            )
        except PermissionError:
            lines.append(f"{prefix}â””â”€â”€ ðŸš« [Permission Denied]")
            return

        count = len(children)
        for index, path in enumerate(children):
            is_last = (index == count - 1)
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "
            
            if path.is_dir():
                lines.append(f"{prefix}{connector}ðŸ“ {path.name}/")
                extension = "    " if is_last else "â”‚   "
                self._walk(path, prefix + extension, lines, exclusions)
            else:
                lines.append(f"{prefix}{connector}ðŸ“„ {path.name}")

if __name__ == "__main__":
svc = TreeMapperMS()
print("Service ready:", svc)

--------------------------------------------------------------------------------
FILE: __VectorFactoryMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _VectorFactoryMS
ENTRY_POINT: __VectorFactoryMS.py
DEPENDENCIES: pip install chromadb faiss-cpu numpy
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pip install chromadb faiss-cpu numpy"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _VectorFactoryMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import os
import uuid
import logging
import shutil
from typing import List, Dict, Any, Optional, Protocol, Union
from pathlib import Path
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("VectorFactory")
# ==============================================================================

# --- Interface Definition ---

class VectorFactoryMS(Protocol):
    """The contract that all vector backends must fulfill."""
    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]) -> None:
        ...
    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        ...
    def count(self) -> int:
        ...
    def clear(self) -> None:
        ...

# --- Implementation 1: FAISS (Local, Fast, RAM-heavy) ---

class FaissVectorStore:
    def __init__(self, index_path: str, dimension: int):
        import numpy as np
        import faiss # Lazy import
        self.np = np
        self.faiss = faiss
        
        self.index_path = index_path
        self.dim = dimension
        self.metadata_store = []
        
        # Load or Create
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
            # Load metadata (simple JSON sidecar for this implementation)
            meta_path = index_path + ".meta.json"
            if os.path.exists(meta_path):
                import json
                with open(meta_path, 'r') as f:
                    self.metadata_store = json.load(f)
        else:
            self.index = faiss.IndexFlatL2(dimension)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        
        vecs = self.np.array(embeddings).astype("float32")
        self.index.add(vecs)
        self.metadata_store.extend(metadatas)
        self._save()

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        if self.index.ntotal == 0: return []
        
        q_vec = self.np.array([query_vector]).astype("float32")
        distances, indices = self.index.search(q_vec, k)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and idx < len(self.metadata_store):
                entry = self.metadata_store[idx].copy()
                entry['score'] = float(dist) # FAISS returns L2 distance (lower is better)
                results.append(entry)
        return results

    def count(self) -> int:
        return self.index.ntotal

    def clear(self):
        self.index.reset()
        self.metadata_store = []
        self._save()

    def _save(self):
        self.faiss.write_index(self.index, self.index_path)
        import json
        with open(self.index_path + ".meta.json", 'w') as f:
            json.dump(self.metadata_store, f)

# --- Implementation 2: ChromaDB (Persistent, Feature-rich) ---

class ChromaVectorStore:
    def __init__(self, persist_dir: str, collection_name: str):
        import chromadb # Lazy import
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(collection_name)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        # Chroma requires unique IDs
        ids = [str(uuid.uuid4()) for _ in embeddings]
        
        # Ensure metadata is flat (Chroma limitation on nested dicts)
        clean_metas = [{k: str(v) if isinstance(v, (list, dict)) else v for k, v in m.items()} for m in metadatas]
        
        # Chroma expects 'documents' usually, but we handle logic upstream. 
        # We pass empty strings for 'documents' if purely vector-based, 
        # or map content from metadata if available.
        docs = [m.get("content", "") for m in metadatas]

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            metadatas=clean_metas,
            documents=docs
        )

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        results = self.collection.query(
            query_embeddings=[query_vector],
            n_results=k
        )
        
        output = []
        if not results['ids']: return []

        # Unpack Chroma's columnar response format
        for i in range(len(results['ids'][0])):
            entry = results['metadatas'][0][i].copy()
            entry['score'] = results['distances'][0][i]
            entry['id'] = results['ids'][0][i]
            output.append(entry)
        return output

    def count(self) -> int:
        return self.collection.count()

    def clear(self):
        # Chroma doesn't have a truncate command, so we delete the collection
        name = self.collection.name
        self.client.delete_collection(name)
        self.collection = self.client.get_or_create_collection(name)

# --- The Factory ---

@service_metadata(
name="VectorFactory",
version="1.0.0",
description="Factory for creating VectorFactoryMS instances (FAISS, Chroma).",
tags=["vector", "factory", "db"],
capabilities=["filesystem:read", "filesystem:write"]
)
class VectorFactoryMS:
    """
The Switchboard: Returns the appropriate VectorFactoryMS implementation
based on configuration.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"backend": "str", "config": "Dict"},
outputs={"store": "VectorFactoryMS"},
description="Creates and returns a configured VectorFactoryMS instance.",
tags=["vector", "create"],
side_effects=[]
)
def create(self, backend: str, config: Dict[str, Any]) -> VectorFactoryMS:
"""
:param backend: 'faiss' or 'chroma'
        :param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)
        """
        log.info(f"Initializing Vector Store: {backend.upper()}")
        
        if backend == "faiss":
            path = config.get("path", "vector_index.bin")
            dim = config.get("dim", 384)
            return FaissVectorStore(path, dim)
            
        elif backend == "chroma":
            path = config.get("path", "./chroma_db")
            name = config.get("collection", "default_collection")
            return ChromaVectorStore(path, name)
            
        else:
            raise ValueError(f"Unknown backend: {backend}")

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing VectorFactoryMS ---")
    
    # 1. Mock Data (dim=4 for simplicity)
    mock_vec = [0.1, 0.2, 0.3, 0.4]
    mock_meta = {"text": "Hello World", "source": "test"}
    
    # 2. Test FAISS
    print("\n[Testing FAISS]")
    factory = VectorFactoryMS()
    print("Service ready:", factory)
    try:
    faiss_store = factory.create("faiss", {"path": "test_faiss.index", "dim": 4})
        faiss_store.add([mock_vec], [mock_meta])
        print(f"Count: {faiss_store.count()}")
        res = faiss_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("test_faiss.index"): os.remove("test_faiss.index")
        if os.path.exists("test_faiss.index.meta.json"): os.remove("test_faiss.index.meta.json")
    except ImportError:
        print("Skipping FAISS test (library not installed)")
# 3. Test Chroma
print("\n[Testing Chroma]")
try:
    chroma_store = factory.create("chroma", {"path": "./test_chroma_db", "collection": "test_col"})
    chroma_store.add([mock_vec], [mock_meta])
    print(f"Count: {chroma_store.count()}")
    res = chroma_store.search(mock_vec, 1)
    print(f"Search Result: {res[0]['text']}")
    # Cleanup
    if os.path.exists("./test_chroma_db"): shutil.rmtree("./test_chroma_db")
except ImportError:
    print("Skipping Chroma test (library not installed)")
except Exception as e:
    print(f"Chroma Error: {e}")

--------------------------------------------------------------------------------
FILE: __WebScraperMS.py
--------------------------------------------------------------------------------
"""
SERVICE_NAME: _WebScraperMS
ENTRY_POINT: __WebScraperMS.py
DEPENDENCIES: pip install httpx readability-lxml
"""

# --- RUNTIME DEPENDENCY CHECK ---
import importlib.util, sys
REQUIRED = ["pip install httpx readability-lxml"]
MISSING = []
for lib in REQUIRED:
    # Clean version numbers for check (e.g., pygame==2.0 -> pygame)
    clean_lib = lib.split('>=')[0].split('==')[0].split('>')[0].replace('-', '_')
    if importlib.util.find_spec(clean_lib) is None:
        if clean_lib == 'pywebview': clean_lib = 'webview' # Common alias
        if importlib.util.find_spec(clean_lib) is None:
            MISSING.append(lib)

if MISSING:
    print('\n' + '!'*60)
    print(f'MISSING DEPENDENCIES for _WebScraperMS:')
    print(f'Run:  pip install {" ".join(MISSING)}')
    print('!'*60 + '\n')
    # sys.exit(1) # Uncomment to force stop if missing

import httpx
import logging
import asyncio
from typing import Optional, Dict, Any, List
from readability import Document
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
TIMEOUT_SECONDS = 15.0

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("WebScraper")
# ==============================================================================

@service_metadata(
name="WebScraper",
version="1.0.0",
description="Fetches URLs and extracts main content using Readability (stripping ads/nav).",
tags=["scraper", "web", "readability"],
capabilities=["network:outbound", "compute"]
)
class WebScraperMS:
    """
The Reader: Fetches URLs and extracts the main content using Readability.
Strips ads, navbars, and boilerplate to return clean text for LLMs.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.headers = {"User-Agent": USER_AGENT}

    @service_endpoint(
    inputs={"url": "str"},
    outputs={"data": "Dict[str, Any]"},
    description="Fetches and cleans a URL.",
    tags=["scraper", "read"],
    side_effects=["network:outbound"]
    )
    def scrape(self, url: str) -> Dict[str, Any]:
    """
    Synchronous wrapper for fetching and cleaning a URL.
    Returns: {
            "url": str,
            "title": str,
            "content": str (The main body text),
            "html": str (The raw HTML of the main content area)
        }
        """
return asyncio.run(self._scrape_async(url))

    async def _scrape_async(self, url: str) -> Dict[str, Any]:
        log.info(f"Fetching: {url}")
        
        async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=TIMEOUT_SECONDS) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                log.error(f"HTTP Error {e.response.status_code}: {e}")
                raise
            except httpx.RequestError as e:
                log.error(f"Request failed: {e}")
                raise

        # Parse with Readability
        try:
            doc = Document(response.text)
            title = doc.title()
            # Summary() returns the HTML of the main content area
            clean_html = doc.summary() 
            
            # Convert HTML content to plain text for the LLM
            # (Simple strip tags implementation, for better results use BeautifulSoup)
            clean_text = self._strip_tags(clean_html)
            
            log.info(f"Successfully scraped '{title}' ({len(clean_text)} chars)")
            
            return {
                "url": url,
                "title": title,
                "content": clean_text,
                "html": clean_html
            }
        except Exception as e:
            log.error(f"Parsing failed: {e}")
            raise

    def _strip_tags(self, html: str) -> str:
def _strip_tags(self, html: str) -> str:
        """
        Removes HTML tags to leave only the readable text.
        """
        import re
        # Remove scripts and styles
        html = re.sub(r'<(script|style).*?>.*?</\1>', '', html, flags=re.DOTALL)
        # Remove tags
        text = re.sub(r'<[^>]+>', ' ', html)
        # Collapse whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# --- Independent Test Block ---
if __name__ == "__main__":
    scraper = WebScraperMS()
    print("Service ready:", scraper)
    
    # Test URL (Example: Python's PEP 8)
    target_url = "https://peps.python.org/pep-0008/"
    
    print(f"--- Scraping {target_url} ---")
    try:
        data = scraper.scrape(target_url)
        print(f"\nTitle: {data['title']}")
        print(f"Content Preview:\n{data['content'][:500]}...")
        print(f"\nTotal Length: {len(data['content'])} characters")
    except Exception as e:
        print(f"Scrape failed: {e}")
