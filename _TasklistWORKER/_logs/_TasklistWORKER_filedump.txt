Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_TasklistWORKER


--------------------------------------------------------------------------------
FILE: LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: requirements.txt
--------------------------------------------------------------------------------
tk
ollama
--------------------------------------------------------------------------------
FILE: setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: src\app - Copy.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import threading
import json
import os
import datetime

# --- MICROSERVICES CHECK ---
try:
    from src._microservices.ollama_client import OllamaClient
    from src._microservices.template_engine import resolve_template
except ImportError:
    try:
        from _microservices.ollama_client import OllamaClient
        from _microservices.template_engine import resolve_template
    except ImportError:
        raise ImportError("Microservices missing. Ensure src/_microservices exists.")

# ==============================================================================
# CUSTOM WIDGET: Smart Task Step (Expander)
# ==============================================================================
class TaskStepWidget(tk.Frame):
    def __init__(self, parent, index, app_ref, initial_data=None):
        super().__init__(parent, bg="#334155", bd=1, relief="flat")
        self.app = app_ref
        self.index = index
        self.expanded = False
        
        # Data Model
        self.data = initial_data or {
            "system": "You are a helpful AI assistant.",
            "prompt": "",
            "use_context": True
        }

        self._build_ui()
        self._refresh_summary()

    def _build_ui(self):
        # --- HEADER (Always Visible) ---
        self.header = tk.Frame(self, bg="#334155", height=30)
        self.header.pack(fill="x", padx=2, pady=2)
        
        # Click header to toggle
        self.header.bind("<Button-1>", self.toggle)
        
        # Step Number / Status Indicator
        self.lbl_idx = tk.Label(self.header, text=f"#{self.index+1}", bg="#334155", fg="#94a3b8", font=("Segoe UI", 10, "bold"), width=4)
        self.lbl_idx.pack(side="left")
        self.lbl_idx.bind("<Button-1>", self.toggle)

        # Summary Text (Read-only representation)
        self.lbl_summary = tk.Label(self.header, text="(New Step)", bg="#334155", fg="white", anchor="w", font=("Segoe UI", 10))
        self.lbl_summary.pack(side="left", fill="x", expand=True, padx=5)
        self.lbl_summary.bind("<Button-1>", self.toggle)

        # Delete Button (Small 'x')
        lbl_del = tk.Label(self.header, text="√ó", bg="#334155", fg="#ef4444", font=("Arial", 12, "bold"), cursor="hand2")
        lbl_del.pack(side="right", padx=5)
        lbl_del.bind("<Button-1>", lambda e: self.app.delete_step(self.index))

        # --- BODY (Hidden by default) ---
        self.body = tk.Frame(self, bg="#1e293b", padx=5, pady=5)
        
        # 1. System Prompt
        tk.Label(self.body, text="System Prompt:", bg="#1e293b", fg="#64748b", font=("Segoe UI", 8)).pack(anchor="w")
        self.ent_sys = tk.Entry(self.body, bg="#0f172a", fg="#94a3b8", borderwidth=0, insertbackground="white")
        self.ent_sys.pack(fill="x", pady=(0, 5))
        self.ent_sys.insert(0, self.data["system"])

        # 2. User Prompt (Auto-growing Text)
        tk.Label(self.body, text="User Prompt:", bg="#1e293b", fg="#cbd5e1", font=("Segoe UI", 9, "bold")).pack(anchor="w")
        self.txt_prompt = tk.Text(self.body, height=4, bg="#0f172a", fg="white", borderwidth=0, insertbackground="white", font=("Segoe UI", 10), wrap="word")
        self.txt_prompt.pack(fill="x", pady=(0, 5))
        self.txt_prompt.insert("1.0", self.data["prompt"])
        
        # Bindings for "Save on Type" / "Auto-size"
        self.txt_prompt.bind("<KeyRelease>", self._on_text_change)
        self.txt_prompt.bind("<FocusOut>", self._sync_data)

        # 3. Context Toggle
        self.var_ctx = tk.BooleanVar(value=self.data["use_context"])
        chk = tk.Checkbutton(self.body, text="Include Previous Response", variable=self.var_ctx, bg="#1e293b", fg="#94a3b8", selectcolor="#0f172a", activebackground="#1e293b", activeforeground="white")
        chk.pack(anchor="w")

    def toggle(self, event=None):
        if self.expanded:
            self.collapse()
        else:
            self.expand()

    def expand(self):
        self.app.collapse_all_steps() # Singleton expansion (optional, feels cleaner)
        self.body.pack(fill="x", expand=True)
        self.header.config(bg="#475569")
        self.lbl_idx.config(bg="#475569", fg="white")
        self.lbl_summary.config(bg="#475569")
        self.expanded = True
        self.txt_prompt.focus_set()

    def collapse(self):
        self._sync_data()
        self.body.pack_forget()
        self.header.config(bg="#334155")
        self.lbl_idx.config(bg="#334155", fg="#94a3b8")
        self.lbl_summary.config(bg="#334155")
        self.expanded = False
        self._refresh_summary()

    def _sync_data(self, event=None):
        """Update internal data dict from widgets."""
        self.data["system"] = self.ent_sys.get()
        self.data["prompt"] = self.txt_prompt.get("1.0", "end-1c")
        self.data["use_context"] = self.var_ctx.get()

    def _on_text_change(self, event=None):
        # Auto-grow logic
        lines = int(self.txt_prompt.index('end-1c').split('.')[0])
        new_height = min(max(4, lines + 1), 15)
        if int(self.txt_prompt.cget("height")) != new_height:
            self.txt_prompt.config(height=new_height)

    def _refresh_summary(self):
        """Update the collapsed header text."""
        raw = self.data["prompt"].replace("\n", " ").strip()
        if not raw: raw = "(Empty Step)"
        if len(raw) > 50: raw = raw[:47] + "..."
        self.lbl_summary.config(text=raw)

    def set_active(self, active: bool):
        color = "#2563eb" if active else ("#475569" if self.expanded else "#334155")
        self.config(bg=color)
        if not self.expanded:
            self.header.config(bg=color)
            self.lbl_idx.config(bg=color)
            self.lbl_summary.config(bg=color)

# ==============================================================================
# MAIN APPLICATION
# ==============================================================================
class PromptChainerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("_PromptCHAINER v3.1 [IDE Mode]")
        self.root.geometry("1600x900")
        self.root.configure(bg="#0f172a")

        self.client = OllamaClient()
        
        # State
        self.state = {
            "chat": {"history": [], "last_response": ""},
            "working": {"step_outputs": {}, "thoughts": []},
            "outputs": {}
        }
        self.steps = [] # List of TaskStepWidget
        self.current_step_idx = 0
        self.is_running = False
        
        # UI Vars
        self.selected_model = tk.StringVar()
        self.helper_model = tk.StringVar()
        self.auto_run = tk.BooleanVar(value=False)

        self._build_layout()
        self._refresh_models()
        self.log_system("System Initialized.")

    def _build_layout(self):
        # Main container (3 Columns)
        self.paned = tk.PanedWindow(self.root, orient=tk.HORIZONTAL, bg="#0f172a", sashwidth=4, sashrelief="flat")
        self.paned.pack(fill="both", expand=True, pady=(0, 30)) # Leave room for bottom log

        # --- COL 1: HISTORY (Left) ---
        f_left = tk.Frame(self.paned, bg="#0f172a")
        self.paned.add(f_left, minsize=350, stretch="always")
        
        tk.Label(f_left, text="Conversation History", bg="#0f172a", fg="#64748b", font=("Segoe UI", 9, "bold")).pack(anchor="w", padx=5, pady=5)
        self.txt_chat = scrolledtext.ScrolledText(f_left, bg="#020617", fg="#94a3b8", borderwidth=0, font=("Consolas", 10))
        self.txt_chat.pack(fill="both", expand=True, padx=5)

        # --- COL 2: CHAIN EDITOR (Center) ---
        f_center = tk.Frame(self.paned, bg="#1e293b")
        self.paned.add(f_center, minsize=450, stretch="always")

        # 1. Header (Top)
        c_head = tk.Frame(f_center, bg="#1e293b", pady=10, padx=10)
        c_head.pack(side="top", fill="x")
        tk.Label(c_head, text="Task Chain", font=("Segoe UI", 14, "bold"), bg="#1e293b", fg="white").pack(side="left")
        
        tk.Label(c_head, text="Model:", bg="#1e293b", fg="#94a3b8").pack(side="left", padx=(20, 5))
        self.cb_model = ttk.Combobox(c_head, textvariable=self.selected_model, state="readonly", width=18)
        self.cb_model.pack(side="left")
        
        tk.Button(c_head, text="üìÇ", command=self.load_chain, bg="#334155", fg="white", width=3, relief="flat").pack(side="right")
        tk.Button(c_head, text="üíæ", command=self.save_chain, bg="#334155", fg="white", width=3, relief="flat").pack(side="right", padx=5)

        # 2. Footer Buttons (Bottom - PACK FIRST TO ENSURE VISIBILITY)
        f_ctrl = tk.Frame(f_center, bg="#1e293b", pady=10, padx=10)
        f_ctrl.pack(side="bottom", fill="x")
        
        # "+ Add Step"
        tk.Button(f_ctrl, text="+ Add Step", command=self.add_step, bg="#334155", fg="white", relief="flat", font=("Segoe UI", 10)).pack(fill="x")

        # 3. Scrollable Task Area (Fills remaining space)
        self.canvas = tk.Canvas(f_center, bg="#1e293b", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(f_center, orient="vertical", command=self.canvas.yview)
        self.frame_tasks = tk.Frame(self.canvas, bg="#1e293b")
        
        self.canvas.create_window((0, 0), window=self.frame_tasks, anchor="nw")
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="top", fill="both", expand=True, padx=5)
        self.scrollbar.pack(side="right", fill="y")
        self.frame_tasks.bind("<Configure>", lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all")))


        # --- COL 3: ANALYSIS & OUTPUT (Right) ---
        f_right = tk.Frame(self.paned, bg="#0f172a")
        self.paned.add(f_right, minsize=400, stretch="always")

        # Split Right Col into Thoughts (Top) and Staging (Bottom)
        paned_right = tk.PanedWindow(f_right, orient=tk.VERTICAL, bg="#0f172a", sashwidth=4)
        paned_right.pack(fill="both", expand=True)

        # Thoughts Pane
        f_thoughts = tk.Frame(paned_right, bg="#0f172a")
        h_thoughts = tk.Frame(f_thoughts, bg="#0f172a")
        h_thoughts.pack(fill="x", pady=5)
        tk.Label(h_thoughts, text="Thought Stream", bg="#0f172a", fg="#a78bfa", font=("Segoe UI", 10, "bold")).pack(side="left", padx=5)
        
        # Helper Model Select
        self.cb_helper = ttk.Combobox(h_thoughts, textvariable=self.helper_model, state="readonly", width=15)
        self.cb_helper.pack(side="right", padx=5)
        tk.Label(h_thoughts, text="Helper:", bg="#0f172a", fg="#64748b", font=("Segoe UI", 8)).pack(side="right")

        self.txt_thoughts = scrolledtext.ScrolledText(f_thoughts, bg="#1e1e1e", fg="#a78bfa", borderwidth=0, font=("Segoe UI", 9))
        self.txt_thoughts.pack(fill="both", expand=True, padx=5)
        paned_right.add(f_thoughts, minsize=200, stretch="always")

        # Staging Pane (Bottom Right)
        f_stage = tk.Frame(paned_right, bg="#0f172a")
        
        tk.Label(f_stage, text="FINAL OUTPUT / STAGING (Edit before continuing)", bg="#0f172a", fg="#facc15", font=("Segoe UI", 10, "bold")).pack(anchor="w", padx=5, pady=(10, 5))
        
        self.txt_staging = scrolledtext.ScrolledText(f_stage, bg="#1e293b", fg="#e2e8f0", borderwidth=0, font=("Consolas", 11), insertbackground="white")
        self.txt_staging.pack(fill="both", expand=True, padx=5)
        
        # ACTION BAR (The Green Arrow)
        f_action = tk.Frame(f_stage, bg="#0f172a", pady=10)
        f_action.pack(fill="x", padx=5)
        
        # This is the Master Action Button
        self.btn_action = tk.Button(f_action, text="START CHAIN ‚û°", command=self.on_action_click, bg="#2563eb", fg="white", font=("Segoe UI", 12, "bold"), relief="flat", height=2)
        self.btn_action.pack(fill="x")
        
        tk.Checkbutton(f_action, text="Auto-Commit (No Pause)", variable=self.auto_run, bg="#0f172a", fg="#94a3b8", selectcolor="#0f172a", activebackground="#0f172a").pack(anchor="e")

        paned_right.add(f_stage, minsize=300, stretch="always")

        # --- BOTTOM: SYSTEM LOG ---
        self.txt_log = tk.Text(self.root, height=8, bg="#020617", fg="#475569", font=("Consolas", 9), borderwidth=0, state="disabled")
        self.txt_log.place(relx=0, rely=1, anchor="sw", relwidth=1.0, height=150) # Use place to anchor firmly to bottom
        self.txt_log.pack(side="bottom", fill="x")

    # --- LOGIC ---

    def log_system(self, msg):
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        self.txt_log.config(state="normal")
        self.txt_log.insert("end", f"[{timestamp}] {msg}\n")
        self.txt_log.see("end")
        self.txt_log.config(state="disabled")

    def _refresh_models(self):
        def worker():
            try:
                models = self.client.list_models()
                self.root.after(0, lambda: self._update_combos(models))
            except Exception as e:
                self.log_system(f"Error listing models: {e}")
        threading.Thread(target=worker, daemon=True).start()

    def _update_combos(self, models):
        self.cb_model.config(values=models)
        self.cb_helper.config(values=models)
        if models:
            self.cb_model.set(models[0])
            helpers = [m for m in models if "mini" in m or "7b" in m]
            self.cb_helper.set(helpers[0] if helpers else models[0])
        self.log_system(f"Found {len(models)} models.")

    def collapse_all_steps(self):
        for s in self.steps:
            if s.expanded: s.collapse()

    def add_step(self, data=None):
        idx = len(self.steps)
        # Create widget
        w = TaskStepWidget(self.frame_tasks, idx, self, initial_data=data)
        w.pack(fill="x", pady=2)
        self.steps.append(w)
        
        # If adding manually (no data), expand it for editing
        if not data:
            w.expand()

    def delete_step(self, index):
        if not (0 <= index < len(self.steps)): return
        
        # Remove widget
        self.steps[index].destroy()
        self.steps.pop(index)
        
        # Re-index remaining
        for i, s in enumerate(self.steps):
            s.index = i
            s.lbl_idx.config(text=f"#{i+1}")
        
        self.log_system(f"Deleted step #{index+1}")

    def on_action_click(self):
        # Determine context: Are we starting? confirming? finishing?
        
        # Case 1: Not running -> Start
        if not self.is_running:
            self.start_chain()
            return

        # Case 2: Running -> Commit Staging & Move Next
        self.commit_staging()

    def start_chain(self):
        if not self.steps:
            messagebox.showwarning("Empty", "Add some steps first.")
            return

        self.is_running = True
        self.current_step_idx = 0
        
        # Clear logs
        self.txt_chat.delete("1.0", "end")
        self.txt_thoughts.delete("1.0", "end")
        self.state["chat"]["history"] = []
        self.state["chat"]["last_response"] = ""
        
        self.log_system("--- STARTED NEW CHAIN ---")
        self.run_step(0)

    def run_step(self, idx):
        if idx >= len(self.steps):
            self.finish_chain()
            return

        self.current_step_idx = idx
        step_widget = self.steps[idx]
        step_widget._sync_data() # Ensure we have latest text
        data = step_widget.data

        # UI Update
        for s in self.steps: s.set_active(False)
        step_widget.set_active(True)
        
        self.btn_action.config(text="Running...", bg="#475569", state="disabled")
        self.log_system(f"Executing Step #{idx+1}")

        # Construct Prompt
        # 1. Template Resolution
        raw_prompt = data["prompt"]
        final_prompt = resolve_template(raw_prompt, self.state)
        
        # 2. Context Injection
        if data["use_context"] and idx > 0:
            final_prompt += f"\n\n[CONTEXT FROM PREVIOUS STEP]:\n{self.state['chat']['last_response']}"

        self.txt_chat.insert("end", f"\n[USER - Step {idx+1}]\n{final_prompt}\n" + "-"*30 + "\n")
        self.txt_chat.see("end")

        # Threaded Inference
        threading.Thread(target=self._inference_worker, args=(data["system"], final_prompt), daemon=True).start()

    def _inference_worker(self, sys_prompt, user_prompt):
        try:
            main_model = self.selected_model.get()
            resp = self.client.generate(main_model, sys_prompt, user_prompt)

            helper_model = self.helper_model.get()
            thought = "..."
            if helper_model:
                thought = self.client.generate(helper_model, "Summarize this interaction and suggest the next logical thought.", f"Q: {user_prompt}\nA: {resp}")

            self.root.after(0, lambda: self._on_step_complete(resp, thought))
        except Exception as e:
            self.root.after(0, lambda: self.log_system(f"Inference Error: {e}"))
            self.root.after(0, lambda: self.btn_action.config(state="normal", text="Retry"))

    def _on_step_complete(self, response, thought):
        # 1. Populate Staging
        self.txt_staging.delete("1.0", "end")
        self.txt_staging.insert("1.0", response)
        
        # 2. Populate Thoughts
        self.txt_thoughts.insert("end", f"Step {self.current_step_idx+1}: {thought.strip()}\n\n")
        self.txt_thoughts.see("end")

        # 3. Enable 'Next' Button
        is_last = (self.current_step_idx == len(self.steps) - 1)
        btn_text = "Finish & Save üèÅ" if is_last else "Confirm & Next ‚û°"
        btn_color = "#16a34a" if is_last else "#2563eb"
        
        self.btn_action.config(state="normal", text=btn_text, bg=btn_color)
        self.log_system(f"Step #{self.current_step_idx+1} complete. Waiting for confirmation.")

        if self.auto_run.get():
            self.commit_staging()

    def commit_staging(self):
        # User may have edited text in staging
        final_text = self.txt_staging.get("1.0", "end-1c")
        
        self.state["chat"]["last_response"] = final_text
        self.txt_chat.insert("end", f"\n[ASSISTANT]\n{final_text}\n" + "="*40 + "\n")
        self.txt_chat.see("end")
        
        self.run_step(self.current_step_idx + 1)

    def finish_chain(self):
        self.is_running = False
        self.btn_action.config(text="START CHAIN ‚û°", bg="#2563eb")
        self.log_system("Chain Completed.")
        
        # Save Log
        if not os.path.exists("_logs"): os.makedirs("_logs")
        ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        fn = f"_logs/run_{ts}.json"
        
        log_data = {
            "chat": self.txt_chat.get("1.0", "end-1c"),
            "thoughts": self.txt_thoughts.get("1.0", "end-1c"),
            "final_output": self.state["chat"]["last_response"]
        }
        with open(fn, "w", encoding="utf-8") as f:
            json.dump(log_data, f, indent=2)
        
        messagebox.showinfo("Success", f"Run saved to {fn}")

    # --- SAVE / LOAD ---
    def save_chain(self):
        # Force sync of all widgets
        for s in self.steps: s._sync_data()
        
        data = [s.data for s in self.steps]
        f = filedialog.asksaveasfilename(defaultextension=".json", filetypes=[("JSON", "*.json")])
        if f:
            with open(f, 'w') as file:
                json.dump(data, file, indent=2)
            self.log_system(f"Saved chain to {os.path.basename(f)}")

    def load_chain(self):
        f = filedialog.askopenfilename(filetypes=[("JSON", "*.json")])
        if f:
            with open(f, 'r') as file:
                data = json.load(file)
            
            # Clear UI
            for s in self.steps: s.destroy()
            self.steps = []
            
            # Rebuild
            for step_data in data:
                self.add_step(step_data)
            self.log_system(f"Loaded chain from {os.path.basename(f)}")

if __name__ == "__main__":
    root = tk.Tk()
    app = PromptChainerApp(root)
    root.mainloop()
--------------------------------------------------------------------------------
FILE: src\app.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import messagebox, ttk, simpledialog, filedialog
import threading
import json
import os
import datetime

from src.gui_layout import WorkbenchUI

try:
    from src._microservices.ollama_client import OllamaClient
    from src._microservices.template_engine import resolve_template
except ImportError:
    from _microservices.ollama_client import OllamaClient
    from _microservices.template_engine import resolve_template

# ==============================================================================
# DATA MANAGER: Roles
# ==============================================================================
class RoleManager:
    def __init__(self):
        # Default Roles
        self.roles = {
            "Helpful Assistant": "You are a helpful AI assistant.",
            "Python Expert": "You are a senior Python developer. Be concise and precise.",
            "Creative Writer": "You are a creative writer. Use vivid imagery.",
            "Strict Analyst": "You are a logic-first analyst. No fluff."
        }
    
    def get_names(self):
        return list(self.roles.keys())
    
    def get_prompt(self, name):
        return self.roles.get(name, "")
    
    def add_role(self, name, prompt):
        self.roles[name] = prompt

# ==============================================================================
# DIALOGS
# ==============================================================================
class AddRoleDialog(simpledialog.Dialog):
    def body(self, master):
        tk.Label(master, text="Role Name:").grid(row=0)
        tk.Label(master, text="System Prompt:").grid(row=1)
        self.e1 = tk.Entry(master)
        self.e2 = tk.Entry(master)
        self.e1.grid(row=0, column=1)
        self.e2.grid(row=1, column=1)
        return self.e1
    def apply(self):
        self.result = (self.e1.get(), self.e2.get())

class LogConfigDialog(simpledialog.Dialog):
    def __init__(self, parent, current_config):
        self.config = current_config
        super().__init__(parent, title="Log Configuration")

    def body(self, master):
        tk.Label(master, text="Log Subdirectory:").grid(row=0, sticky="w")
        self.e_dir = tk.Entry(master)
        self.e_dir.insert(0, self.config["dir"])
        self.e_dir.grid(row=0, column=1)

        self.var_ts = tk.BooleanVar(value=self.config["timestamp"])
        tk.Checkbutton(master, text="Include Timestamp in Filename", variable=self.var_ts).grid(row=1, columnspan=2, sticky="w")
        
        return self.e_dir

    def apply(self):
        self.result = {
            "dir": self.e_dir.get(),
            "timestamp": self.var_ts.get()
        }

# ==============================================================================
# CONTROLLER: Task Step Row
# ==============================================================================
class TaskStepController(tk.Frame):
    def __init__(self, parent, index, app_ref, role_mgr, initial_data=None):
        super().__init__(parent, bg="#334155", bd=1, relief="flat")
        self.app = app_ref
        self.index = index
        self.role_mgr = role_mgr
        self.expanded = False
        
        self.data = initial_data or {
            "role": "Helpful Assistant",
            "prompt": "",
            "isolated": False,
            "skip": False
        }
        
        self._build_row_ui()
        self._refresh_summary()

    def _build_row_ui(self):
        # Header
        self.header = tk.Frame(self, bg="#334155", height=30)
        self.header.pack(fill="x", padx=2, pady=2)
        self.header.bind("<Button-1>", self.toggle)

        # Controls Left
        self.lbl_idx = tk.Label(self.header, text=f"#{self.index+1}", bg="#334155", fg="#94a3b8", font=("Segoe UI", 10, "bold"), width=3)
        self.lbl_idx.pack(side="left")

        # Run Single Step Button
        self.btn_run_single = tk.Label(self.header, text="‚ñ∂", bg="#334155", fg="#4ade80", cursor="hand2", font=("Arial", 10))
        self.btn_run_single.pack(side="left", padx=5)
        self.btn_run_single.bind("<Button-1>", self.run_this_step_only)
        
        # Summary
        self.lbl_summary = tk.Label(self.header, text="(New Step)", bg="#334155", fg="white", anchor="w")
        self.lbl_summary.pack(side="left", fill="x", expand=True, padx=5)
        self.lbl_summary.bind("<Button-1>", self.toggle)
        
        # Status Icons
        self.lbl_skip = tk.Label(self.header, text="‚õî", bg="#334155", fg="#ef4444", font=("Segoe UI", 8)) # Hidden default
        self.lbl_iso = tk.Label(self.header, text="üîí", bg="#334155", fg="#facc15", font=("Segoe UI", 8)) # Hidden default

        # Delete
        lbl_del = tk.Label(self.header, text="√ó", bg="#334155", fg="#ef4444", cursor="hand2", font=("Arial", 12, "bold"))
        lbl_del.pack(side="right", padx=5)
        lbl_del.bind("<Button-1>", lambda e: self.app.delete_step(self.index))

        # Body
        self.body = tk.Frame(self, bg="#1e293b", padx=5, pady=5)
        
        # Role Row
        f_role = tk.Frame(self.body, bg="#1e293b")
        f_role.pack(fill="x", pady=(0, 5))
        tk.Label(f_role, text="Role:", bg="#1e293b", fg="#64748b").pack(side="left")
        
        self.cb_role = ttk.Combobox(f_role, values=self.role_mgr.get_names(), state="readonly")
        self.cb_role.pack(side="left", fill="x", expand=True, padx=5)
        self.cb_role.set(self.data["role"])
        self.cb_role.bind("<<ComboboxSelected>>", self._sync_data)

        tk.Button(f_role, text="+", command=self._add_role_local, bg="#334155", fg="white", width=2).pack(side="right")

        # Prompt
        tk.Label(self.body, text="Prompt:", bg="#1e293b", fg="#cbd5e1", font=("Segoe UI", 8, "bold")).pack(anchor="w")
        self.txt_prompt = tk.Text(self.body, height=5, bg="#0f172a", fg="white", borderwidth=0, font=("Segoe UI", 9), wrap="word")
        self.txt_prompt.pack(fill="x", pady=(0, 5))
        self.txt_prompt.insert("1.0", self.data["prompt"])
        self.txt_prompt.bind("<FocusOut>", self._sync_data)

        # Settings Row
        f_set = tk.Frame(self.body, bg="#1e293b")
        f_set.pack(fill="x")
        
        self.var_iso = tk.BooleanVar(value=self.data["isolated"])
        tk.Checkbutton(f_set, text="Run in Isolation", variable=self.var_iso, bg="#1e293b", fg="#facc15", selectcolor="#0f172a", activebackground="#1e293b", command=self._sync_data).pack(side="left")
        
        self.var_skip = tk.BooleanVar(value=self.data["skip"])
        tk.Checkbutton(f_set, text="Skip Step", variable=self.var_skip, bg="#1e293b", fg="#ef4444", selectcolor="#0f172a", activebackground="#1e293b", command=self._sync_data).pack(side="right")

    def toggle(self, event=None):
        if self.expanded: self.collapse()
        else: self.expand()

    def expand(self):
        self.app.collapse_all_steps()
        self.body.pack(fill="x", expand=True)
        self.expanded = True
    
    def collapse(self):
        self._sync_data()
        self.body.pack_forget()
        self.expanded = False
        self._refresh_summary()

    def _sync_data(self, event=None):
        self.data["role"] = self.cb_role.get()
        self.data["prompt"] = self.txt_prompt.get("1.0", "end-1c")
        self.data["isolated"] = self.var_iso.get()
        self.data["skip"] = self.var_skip.get()
        self._refresh_summary()

    def _refresh_summary(self):
        raw = self.data["prompt"].replace("\n", " ").strip()
        if not raw: raw = "(Empty Step)"
        if len(raw) > 35: raw = raw[:32] + "..."
        
        summary = f"[{self.data['role']}] {raw}"
        self.lbl_summary.config(text=summary)
        
        # Toggle Icons
        if self.data["isolated"]: self.lbl_iso.pack(side="right", padx=2)
        else: self.lbl_iso.pack_forget()
        
        if self.data["skip"]: 
            self.lbl_skip.pack(side="right", padx=2)
            self.lbl_summary.config(fg="#64748b") # Grey out text
        else: 
            self.lbl_skip.pack_forget()
            self.lbl_summary.config(fg="white")

    def _add_role_local(self):
        self.app.add_new_role()
        # Refresh combo
        self.cb_role.config(values=self.app.roles.get_names())

    def run_this_step_only(self, event=None):
        # Stop propagation so we don't toggle expand
        # Run this step via app controller
        self.app.run_manual_step(self.index)
        return "break"

    def set_active(self, state):
        # state: "active", "pending", "none"
        if state == "active":
            bg = "#2563eb"
        elif state == "pending":
            bg = "#475569" # Grey to show it needs rerunning
        else:
            bg = "#334155"
        
        self.header.config(bg=bg)
        self.lbl_idx.config(bg=bg)
        self.btn_run_single.config(bg=bg)
        self.lbl_summary.config(bg=bg)

# ==============================================================================
# MAIN CONTROLLER
# ==============================================================================
class WorkbenchApp:
    def __init__(self, root):
        self.ui = WorkbenchUI(root)
        self.client = OllamaClient()
        self.roles = RoleManager()
        
        # App Config
        self.log_config = {"dir": "_logs", "timestamp": True}
        
        # State
        self.state = {
            "chat": {"last_response": ""},
            "history": [] 
        }
        self.steps = []
        self.current_step_idx = 0
        self.is_running = False
        
        # Bindings
        self.ui.widgets["btn_add_step"].config(command=self.add_step)
        self.ui.widgets["btn_run"].config(command=self.on_run_click)
        self.ui.widgets["btn_inject"].config(command=self.inject_to_chat)
        self.ui.widgets["btn_send_chat"].config(command=self.send_chat_message)
        
        self.ui.widgets["btn_add_role_chat"].config(command=self.add_new_role)
        self.ui.widgets["cb_chat_role"].bind("<<ComboboxSelected>>", lambda e: None) # Just holds value
        
        # Log Bindings
        self.ui.widgets["btn_log_save"].config(command=self.save_log_file)
        self.ui.widgets["btn_log_config"].config(command=self.open_log_config)

        # Init Data
        self._refresh_roles()
        self.add_step()
        threading.Thread(target=self._fetch_models, daemon=True).start()
        self.log("Workbench initialized.")

    def log(self, msg):
        timestamp = datetime.datetime.now().strftime("%H:%M:%S")
        self.ui.widgets["log_console"].config(state="normal")
        self.ui.widgets["log_console"].insert("end", f"[{timestamp}] {msg}\n")
        self.ui.widgets["log_console"].see("end")
        self.ui.widgets["log_console"].config(state="disabled")

    def _fetch_models(self):
        try:
            models = self.client.list_models()
            self.ui.root.after(0, lambda: self._update_dropdowns(models))
        except Exception as e:
            self.ui.root.after(0, lambda: self.log(f"Model Error: {e}"))

    def _update_dropdowns(self, models):
        for w in ["cb_helper", "cb_chat_model", "cb_task_model"]:
            self.ui.widgets[w].config(values=models)
            if models: self.ui.widgets[w].set(models[0])

    def _refresh_roles(self):
        names = self.roles.get_names()
        self.ui.widgets["cb_chat_role"].config(values=names)
        self.ui.widgets["cb_chat_role"].set("Helpful Assistant")
        # Update steps if needed? No, they pull dynamically.

    def add_new_role(self):
        d = AddRoleDialog(self.ui.root)
        if d.result:
            name, prompt = d.result
            if name and prompt:
                self.roles.add_role(name, prompt)
                self._refresh_roles()
                self.log(f"Added role: {name}")

    # --- CHAT LOGIC ---
    def send_chat_message(self):
        # 1. Get Text
        txt = self.ui.widgets["chat_input"].get("1.0", "end-1c").strip()
        if not txt: return
        
        # 2. Clear Input
        self.ui.widgets["chat_input"].delete("1.0", "end")
        
        # 3. Log User
        self._append_session("USER", txt)
        
        # 4. Get Config
        model = self.ui.widgets["cb_chat_model"].get()
        role_name = self.ui.widgets["cb_chat_role"].get()
        sys_prompt = self.roles.get_prompt(role_name)
        
        self.log(f"Chatting with {model} as {role_name}...")
        
        # 5. Thread
        threading.Thread(target=self._chat_worker, args=(model, sys_prompt, txt), daemon=True).start()

    def _chat_worker(self, model, sys, prompt):
        try:
            # We don't maintain full history buffer in state yet for simplicity, 
            # we just send prompt. Ideally we'd send history.
            # V5 Update: Let's simple-chain.
            context = ""
            if self.state["chat"]["last_response"]:
                context = f"Previous Context: {self.state['chat']['last_response']}\n\n"
            
            resp = self.client.generate(model, sys, context + prompt)
            self.ui.root.after(0, lambda: self._on_chat_complete(resp))
        except Exception as e:
            self.ui.root.after(0, lambda: self.log(f"Chat Error: {e}"))

    def _on_chat_complete(self, resp):
        self._append_session("ASSISTANT", resp)
        self.state["chat"]["last_response"] = resp

    # --- TASK LOGIC ---
    def collapse_all_steps(self):
        for s in self.steps: 
            if s.expanded: s.collapse()

    def add_step(self, data=None):
        container = self.ui.widgets["task_container"]
        idx = len(self.steps)
        step = TaskStepController(container, idx, self, self.roles, initial_data=data)
        step.pack(fill="x", pady=2)
        self.steps.append(step)
        if not data: step.expand()

    def delete_step(self, idx):
        if 0 <= idx < len(self.steps):
            self.steps[idx].destroy()
            self.steps.pop(idx)
            for i, s in enumerate(self.steps):
                s.index = i
                s.lbl_idx.config(text=f"#{i+1}")

    def run_manual_step(self, idx):
        # Visual Reset logic
        # If we run step 2, steps 3, 4, 5 become "Pending" (dirty)
        for i in range(idx + 1, len(self.steps)):
            self.steps[i].set_active("pending")
        
        # Hijack the standard runner
        self.run_step(idx)

    def on_run_click(self):
        # Continue chain logic
        self.run_step(self.current_step_idx)

    def run_step(self, idx):
        if idx >= len(self.steps): return
        
        self.current_step_idx = idx
        step = self.steps[idx]
        step._sync_data()

        if step.data["skip"]:
            self.log(f"Skipping Step #{idx+1}...")
            # Auto-advance if running chain, but wait if manual?
            # For now, just stop and let user click next.
            self.current_step_idx += 1
            if self.current_step_idx < len(self.steps):
                self.run_step(self.current_step_idx) # Auto skip
            return

        # UI Update
        for s in self.steps: 
            if s != step and s.header.cget("bg") == "#2563eb": 
                s.set_active("none")
        step.set_active("active")

        self.ui.widgets["btn_run"].config(text="Running...", state="disabled", bg="#475569")
        self.ui.widgets["btn_inject"].config(state="disabled")

        # Build Prompt
        raw_p = step.data["prompt"]
        final_p = resolve_template(raw_p, self.state)
        
        if not step.data["isolated"] and idx > 0 and self.state["chat"]["last_response"]:
             final_p += f"\n\n[CONTEXT]:\n{self.state['chat']['last_response']}"
        
        model = self.ui.widgets["cb_task_model"].get() or "qwen2.5:7b-instruct"
        role_name = step.data["role"]
        sys_p = self.roles.get_prompt(role_name)

        self.log(f"Running Step #{idx+1} on {model}...")
        threading.Thread(target=self._task_worker, args=(model, sys_p, final_p), daemon=True).start()

    def _task_worker(self, model, sys, prompt):
        try:
            resp = self.client.generate(model, sys, prompt)
            
            helper = self.ui.widgets["cb_helper"].get()
            thought = self.client.generate(helper, "Analyze.", f"TASK: {prompt[:100]}\nRESULT: {resp[:100]}")
            
            self.ui.root.after(0, lambda: self._on_task_complete(resp, thought))
        except Exception as e:
            self.ui.root.after(0, lambda: self.log(f"Task Error: {e}"))

    def _on_task_complete(self, resp, thought):
        self.ui.widgets["staging"].delete("1.0", "end")
        self.ui.widgets["staging"].insert("1.0", resp)
        self.ui.widgets["thoughts"].insert("end", f"Step {self.current_step_idx+1}: {thought}\n\n")
        
        btn = self.ui.widgets["btn_run"]
        is_last = self.current_step_idx == len(self.steps) - 1
        btn.config(text="Finish üèÅ" if is_last else "Confirm & Next ‚û°", state="normal", bg="#16a34a" if is_last else "#2563eb")
        self.ui.widgets["btn_inject"].config(state="normal")

    def inject_to_chat(self):
        content = self.ui.widgets["staging"].get("1.0", "end-1c")
        self._append_session("INJECTED", content)
        self.state["chat"]["last_response"] = content
        
        if self.current_step_idx < len(self.steps) - 1:
            self.current_step_idx += 1
            self.steps[self.current_step_idx].set_active("active")

    def _append_session(self, role, text):
        w = self.ui.widgets["session_log"]
        w.insert("end", f"\n[{role}]\n{text}\n" + "-"*40 + "\n")
        w.see("end")

    # --- LOGGING UTILS ---
    def open_log_config(self):
        d = LogConfigDialog(self.ui.root, self.log_config)
        if d.result:
            self.log_config = d.result
            self.log(f"Log config updated: {self.log_config}")

    def save_log_file(self):
        d = self.log_config["dir"]
        if not os.path.exists(d): os.makedirs(d)
        
        name = "system_log"
        if self.log_config["timestamp"]:
            name += f"_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        name += ".txt"
        
        path = os.path.join(d, name)
        content = self.ui.widgets["log_console"].get("1.0", "end-1c")
        
        with open(path, "w", encoding="utf-8") as f:
            f.write(content)
        
        messagebox.showinfo("Saved", f"Log saved to {path}")

if __name__ == "__main__":
    root = tk.Tk()
    app = WorkbenchApp(root)
    root.mainloop()

--------------------------------------------------------------------------------
FILE: src\gui_layout.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk, scrolledtext

class WorkbenchUI:
    def __init__(self, root):
        self.root = root
        self.root.title("_CognitiveWORKBENCH v5.1 [Scaffolded]")
        self.root.geometry("1900x1000")
        self.root.configure(bg="#0f172a")
        
        self.widgets = {}
        self._setup_styles()
        self._build_layout()

    def _setup_styles(self):
        style = ttk.Style()
        style.theme_use('clam')
        style.configure("TFrame", background="#0f172a")
        style.configure("TLabel", background="#1e293b", foreground="#e2e8f0")
        style.configure("TButton", background="#334155", foreground="white", borderwidth=0)
        style.map("TButton", background=[('active', '#475569')])
        style.configure("Card.TFrame", background="#1e293b", relief="flat")
        
        # Log Panel
        style.configure("Log.TFrame", background="#020617", borderwidth=2, relief="sunken")
        
        # Notebook (Tabs)
        style.configure("TNotebook", background="#0f172a", borderwidth=0)
        style.configure("TNotebook.Tab", background="#334155", foreground="white", padding=[10, 5], font=("Segoe UI", 9))
        style.map("TNotebook.Tab", background=[("selected", "#1e293b")], foreground=[("selected", "#60a5fa")])

    def _build_layout(self):
        # Master Vertical Paned Window
        self.main_split = tk.PanedWindow(self.root, orient=tk.VERTICAL, bg="#0f172a", sashwidth=6)
        self.main_split.pack(fill="both", expand=True)

        # --- UPPER SECTION: 3-COLUMNS ---
        self.paned_cols = tk.PanedWindow(self.main_split, orient=tk.HORIZONTAL, bg="#0f172a", sashwidth=6)
        self.main_split.add(self.paned_cols, stretch="always", minsize=600)

        # === COL 1: SUBCONSCIOUS (Left) ===
        self.f_left = tk.Frame(self.paned_cols, bg="#020617")
        self.paned_cols.add(self.f_left, minsize=350, stretch="always")
        
        h_left = tk.Frame(self.f_left, bg="#020617", pady=5)
        h_left.pack(fill="x")
        tk.Label(h_left, text="‚ö° SUBCONSCIOUS", bg="#020617", fg="#64748b", font=("Segoe UI", 9, "bold")).pack(side="left", padx=5)
        
        self.cb_helper = ttk.Combobox(h_left, state="readonly", width=25)
        self.cb_helper.pack(side="right", padx=5)
        self.widgets["cb_helper"] = self.cb_helper
        
        self.txt_thoughts = scrolledtext.ScrolledText(self.f_left, bg="#020617", fg="#94a3b8", borderwidth=0, font=("Consolas", 9))
        self.txt_thoughts.pack(fill="both", expand=True, padx=5)
        self.widgets["thoughts"] = self.txt_thoughts

        f_left_foot = tk.Frame(self.f_left, bg="#1e293b", height=40)
        f_left_foot.pack(side="bottom", fill="x")
        self.btn_settings = tk.Button(f_left_foot, text="‚öô Settings / Embedder", bg="#1e293b", fg="#94a3b8", relief="flat")
        self.btn_settings.pack(side="left", padx=10, pady=5)
        self.widgets["btn_settings"] = self.btn_settings

        # === COL 2: SESSION LTM (Center) ===
        self.f_center = tk.Frame(self.paned_cols, bg="#0f172a")
        self.paned_cols.add(self.f_center, minsize=600, stretch="always")

        h_center = tk.Frame(self.f_center, bg="#1e293b", height=50)
        h_center.pack(fill="x")
        tk.Label(h_center, text="SESSION", bg="#1e293b", fg="white", font=("Segoe UI", 11, "bold")).pack(side="left", padx=10)
        
        tk.Label(h_center, text="Chat Model:", bg="#1e293b", fg="#94a3b8").pack(side="left", padx=(15, 5))
        self.cb_chat_model = ttk.Combobox(h_center, state="readonly", width=30)
        self.cb_chat_model.pack(side="left")
        self.widgets["cb_chat_model"] = self.cb_chat_model

        # Session Role Controls
        f_role_sess = tk.Frame(h_center, bg="#1e293b")
        f_role_sess.pack(side="left", padx=15)
        tk.Label(f_role_sess, text="Role:", bg="#1e293b", fg="#94a3b8").pack(side="left")
        self.cb_chat_role = ttk.Combobox(f_role_sess, state="readonly", width=20)
        self.cb_chat_role.pack(side="left", padx=2)
        self.widgets["cb_chat_role"] = self.cb_chat_role
        
        self.btn_add_role_chat = tk.Button(f_role_sess, text="+", bg="#334155", fg="white", width=3)
        self.btn_add_role_chat.pack(side="left")
        self.widgets["btn_add_role_chat"] = self.btn_add_role_chat

        self.btn_onboard = tk.Button(h_center, text="Catch Me Up", bg="#2563eb", fg="white", font=("Segoe UI", 9))
        self.btn_onboard.pack(side="right", padx=10)
        self.widgets["btn_onboard"] = self.btn_onboard

        self.txt_session = scrolledtext.ScrolledText(self.f_center, bg="#0f172a", fg="#e2e8f0", borderwidth=0, font=("Segoe UI", 10), insertbackground="white")
        self.txt_session.pack(fill="both", expand=True, padx=10, pady=(10, 0))
        self.widgets["session_log"] = self.txt_session

        f_chat_input = tk.Frame(self.f_center, bg="#1e293b", pady=5, padx=5)
        f_chat_input.pack(fill="x", padx=10, pady=10)
        
        self.txt_chat_input = tk.Text(f_chat_input, height=4, bg="#0f172a", fg="white", borderwidth=0, font=("Segoe UI", 10), insertbackground="white")
        self.txt_chat_input.pack(side="left", fill="x", expand=True, padx=(0, 5))
        self.widgets["chat_input"] = self.txt_chat_input
        
        self.btn_send_chat = tk.Button(f_chat_input, text="SEND ‚û§", bg="#16a34a", fg="white", font=("Segoe UI", 10, "bold"), width=10)
        self.btn_send_chat.pack(side="right", fill="y")
        self.widgets["btn_send_chat"] = self.btn_send_chat

        # === COL 3: WORKFLOW (Right) - NOW TABBED ===
        self.f_right_container = tk.Frame(self.paned_cols, bg="#1e293b")
        self.paned_cols.add(self.f_right_container, minsize=550, stretch="always")

        # The Notebook
        self.notebook = ttk.Notebook(self.f_right_container)
        self.notebook.pack(fill="both", expand=True, padx=5, pady=5)

        # -- TAB 1: EXECUTION (The Active Workbench) --
        self.tab_exec = tk.Frame(self.notebook, bg="#1e293b")
        self.notebook.add(self.tab_exec, text=" ‚ñ∂ Execution ")
        self._build_execution_tab(self.tab_exec)

        # -- TAB 2: TASKLISTS (Manager) --
        self.tab_tasks = tk.Frame(self.notebook, bg="#0f172a")
        self.notebook.add(self.tab_tasks, text=" Tasklists ")
        tk.Label(self.tab_tasks, text="Tasklist Manager / Engineering (Coming Soon)", bg="#0f172a", fg="#475569").pack(expand=True)

        # -- TAB 3: PROMPTS (Manager) --
        self.tab_prompts = tk.Frame(self.notebook, bg="#0f172a")
        self.notebook.add(self.tab_prompts, text=" Prompts ")
        tk.Label(self.tab_prompts, text="Prompt Engineering Library (Coming Soon)", bg="#0f172a", fg="#475569").pack(expand=True)

        # -- TAB 4: CARTRIDGES (Manager) --
        self.tab_carts = tk.Frame(self.notebook, bg="#0f172a")
        self.notebook.add(self.tab_carts, text=" Cartridges ")
        tk.Label(self.tab_carts, text="RAG Cartridge Manager (Coming Soon)", bg="#0f172a", fg="#475569").pack(expand=True)

        # --- LOWER SECTION: SYSTEM LOG ---
        self.f_log = ttk.Frame(self.main_split, style="Log.TFrame")
        self.main_split.add(self.f_log, minsize=150, stretch="never")
        
        # Log Content (Top)
        self.txt_log = scrolledtext.ScrolledText(self.f_log, height=8, bg="#020617", fg="#475569", font=("Consolas", 9), borderwidth=0)
        self.txt_log.pack(fill="both", expand=True, side="top")
        self.widgets["log_console"] = self.txt_log

        # Log Toolbar (Bottom)
        f_log_ctrl = tk.Frame(self.f_log, bg="#0f172a", height=25)
        f_log_ctrl.pack(side="bottom", fill="x")
        
        tk.Label(f_log_ctrl, text="SYSTEM LOG", bg="#0f172a", fg="#64748b", font=("Segoe UI", 8, "bold")).pack(side="left", padx=5)
        
        self.btn_log_save = tk.Button(f_log_ctrl, text="üíæ Save Log", bg="#334155", fg="white", font=("Segoe UI", 8))
        self.btn_log_save.pack(side="right", padx=2, pady=2)
        self.widgets["btn_log_save"] = self.btn_log_save
        
        self.btn_log_config = tk.Button(f_log_ctrl, text="‚öô Config", bg="#334155", fg="white", font=("Segoe UI", 8))
        self.btn_log_config.pack(side="right", padx=2, pady=2)
        self.widgets["btn_log_config"] = self.btn_log_config

    def _build_execution_tab(self, parent):
        # Header
        h_right = tk.Frame(parent, bg="#334155", height=45)
        h_right.pack(fill="x")
        tk.Label(h_right, text="WORKBENCH", bg="#334155", fg="white", font=("Segoe UI", 10, "bold")).pack(side="left", padx=10)
        
        tk.Label(h_right, text="Task Model:", bg="#334155", fg="#cbd5e1").pack(side="left", padx=(15, 5))
        self.cb_task_model = ttk.Combobox(h_right, state="readonly", width=30)
        self.cb_task_model.pack(side="left")
        self.widgets["cb_task_model"] = self.cb_task_model

        self.btn_cartridge = tk.Button(h_right, text="üì¶ Make Cartridge", bg="#475569", fg="white")
        self.btn_cartridge.pack(side="right", padx=5)
        self.widgets["btn_cartridge"] = self.btn_cartridge

        # Task Steps Editor
        self.canvas_tasks = tk.Canvas(parent, bg="#1e293b", highlightthickness=0)
        self.scroll_tasks = ttk.Scrollbar(parent, orient="vertical", command=self.canvas_tasks.yview)
        self.frame_tasks_inner = tk.Frame(self.canvas_tasks, bg="#1e293b")
        
        self.canvas_tasks.create_window((0, 0), window=self.frame_tasks_inner, anchor="nw")
        self.canvas_tasks.configure(yscrollcommand=self.scroll_tasks.set)
        
        self.canvas_tasks.pack(side="top", fill="both", expand=True, padx=5, pady=5)
        self.scroll_tasks.pack(side="right", fill="y", in_=parent)
        self.frame_tasks_inner.bind("<Configure>", lambda e: self.canvas_tasks.configure(scrollregion=self.canvas_tasks.bbox("all")))
        self.widgets["task_container"] = self.frame_tasks_inner

        # Action Area
        f_action = tk.Frame(parent, bg="#0f172a", pady=10, padx=10)
        f_action.pack(side="bottom", fill="x")

        tk.Label(f_action, text="STAGING (Output Buffer)", bg="#0f172a", fg="#facc15", font=("Segoe UI", 8, "bold")).pack(anchor="w")
        self.txt_staging = tk.Text(f_action, height=10, bg="#1e293b", fg="white", borderwidth=1, relief="solid", insertbackground="white")
        self.txt_staging.pack(fill="x", pady=(5, 5))
        self.widgets["staging"] = self.txt_staging

        f_btns = tk.Frame(f_action, bg="#0f172a")
        f_btns.pack(fill="x")
        
        self.btn_add_step = tk.Button(f_btns, text="+ Step", bg="#334155", fg="white")
        self.btn_add_step.pack(side="left", padx=(0, 5))
        self.widgets["btn_add_step"] = self.btn_add_step

        self.btn_inject = tk.Button(f_btns, text="‚Üô Inject to Chat", bg="#0f766e", fg="white", state="disabled")
        self.btn_inject.pack(side="left", padx=5)
        self.widgets["btn_inject"] = self.btn_inject
        
        tk.Frame(f_btns, bg="#0f172a").pack(side="left", expand=True)

        self.btn_run = tk.Button(f_btns, text="RUN STEP ‚û°", bg="#2563eb", fg="white", font=("Segoe UI", 10, "bold"), width=15)
        self.btn_run.pack(side="right")
        self.widgets["btn_run"] = self.btn_run

--------------------------------------------------------------------------------
FILE: src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: src\_microservices\ollama_client.py
--------------------------------------------------------------------------------
import json
import urllib.request
import urllib.error


class OllamaClient:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url.rstrip("/")

    # -----------------------------
    # Low-level helpers
    # -----------------------------

    def _read_json(self, resp) -> dict:
        raw = resp.read().decode("utf-8", errors="replace")
        try:
            return json.loads(raw)
        except Exception as e:
            raise RuntimeError(f"Ollama returned non-JSON response: {e}\nRAW:\n{raw}")

    def _request_json(self, method: str, path: str, payload: dict | None = None, timeout: int = 60) -> dict:
        url = f"{self.base_url}{path}"

        data = None
        headers = {}
        if payload is not None:
            data = json.dumps(payload).encode("utf-8")
            headers["Content-Type"] = "application/json"

        req = urllib.request.Request(url, data=data, headers=headers, method=method)

        try:
            with urllib.request.urlopen(req, timeout=timeout) as resp:
                return self._read_json(resp)
        except urllib.error.HTTPError as e:
            # HTTPError is also a file-like object; it may contain JSON
            try:
                body = e.read().decode("utf-8", errors="replace")
            except Exception:
                body = "<unable to read body>"
            raise RuntimeError(f"Ollama HTTPError {e.code} for {method} {path}: {e.reason}\nBODY:\n{body}")
        except urllib.error.URLError as e:
            raise RuntimeError(f"Ollama URLError for {method} {path}: {e}")

    # -----------------------------
    # Public API
    # -----------------------------

    def list_models(self) -> list[str]:
        """Return list of installed model names via Ollama /api/tags."""
        obj = self._request_json("GET", "/api/tags", payload=None, timeout=30)
        models = []
        for m in obj.get("models", []) or []:
            name = m.get("name")
            if name:
                models.append(name)
        return models

    def generate(self, model: str, system: str, prompt: str, options: dict | None = None) -> str:
        """Uses Ollama /api/generate with system+prompt. Returns response text."""
        payload = {
            "model": model,
            "prompt": prompt,
            "system": system,
            "stream": False
        }
        if options:
            payload["options"] = options

        obj = self._request_json("POST", "/api/generate", payload=payload, timeout=300)
        return obj.get("response", "")


--------------------------------------------------------------------------------
FILE: src\_microservices\runner.py
--------------------------------------------------------------------------------
import json
from typing import Any, Dict, List, Optional, Tuple

from src._microservices.ollama_client import OllamaClient
from src._microservices.template_engine import resolve_template

_client = OllamaClient()


def _set_by_path(state: dict, path: str, value):
    # path like "working.step_outputs.S1" or "chat.last_response"
    parts = path.split(".")
    cur = state
    for p in parts[:-1]:
        cur = cur.setdefault(p, {})
    cur[parts[-1]] = value


def _get_by_path(state: dict, path: str):
    parts = path.split(".")
    cur = state
    for p in parts:
        if not isinstance(cur, dict) or p not in cur:
            return None
        cur = cur[p]
    return cur


def _ensure_state_defaults(state: dict) -> dict:
    """
    Runner-friendly defaults so UI can pass a minimal state in.
    """
    state.setdefault("chat", {})
    state["chat"].setdefault("history", [])          # list[{role, content}]
    state["chat"].setdefault("last_user", "")
    state["chat"].setdefault("last_response", "")

    state.setdefault("working", {})
    state["working"].setdefault("step_outputs", {})  # dict step_id -> output
    state["working"].setdefault("thoughts", [])      # list[{step_id, name, summary, errors?}]
    state["working"].setdefault("notes", [])         # list[{step_id, name, errors}]

    state.setdefault("outputs", {})
    state["outputs"].setdefault("final", "")

    return state


def _chain_prompt(base_prompt: str, chain_mode: str, last: str) -> str:
    """
    chain_mode:
      - "none": base_prompt only
      - "last": base_prompt + "\n\n---\nLAST_OUTPUT:\n" + last
      - "replace": replace occurrences of "{{last}}" in base_prompt
    """
    last = last or ""
    chain_mode = (chain_mode or "none").strip().lower()

    if chain_mode == "none":
        return base_prompt
    if chain_mode == "last":
        if not last:
            return base_prompt
        return f"{base_prompt}\n\n---\nLAST_OUTPUT:\n{last}"
    if chain_mode == "replace":
        return base_prompt.replace("{{last}}", last)

    # unknown -> safe default
    return base_prompt


def _safe_json_loads(text: str) -> Tuple[Optional[Any], Optional[str]]:
    try:
        return json.loads(text), None
    except Exception as e:
        return None, str(e)


def _run_helper_summary(
    helper_model: str,
    helper_system: str,
    helper_template: str,
    step_name: str,
    step_output: str,
    state: dict,
    options: Optional[dict] = None
) -> str:
    """
    Summarize what happened in this step. The template can reference:
      - {{state....}} via template_engine
      - {{step_name}}
      - {{step_output}}
    """
    tmp_state = dict(state)
    tmp_state.setdefault("_step_ctx", {})
    tmp_state["_step_ctx"]["step_name"] = step_name
    tmp_state["_step_ctx"]["step_output"] = step_output

    # allow easy placeholders without complicating template_engine:
    prompt = helper_template.replace("{{step_name}}", step_name).replace("{{step_output}}", step_output)
    prompt = resolve_template(prompt, tmp_state)

    raw = _client.generate(
        model=helper_model,
        system=helper_system,
        prompt=prompt,
        options=options or {"temperature": 0.2}
    )
    return (raw or "").strip()


def run_tasklist(
    state: dict,
    tasklist: dict,
    chat_model_default: str,
    helper_model_default: str,
) -> dict:
    """
    Generic tasklist runner.

    Each step:
      - builds a prompt from user_prompt_template (+ optional chaining)
      - calls an Ollama model
      - stores output to output_key
      - optionally runs helper summary (thought bubble)

    Expected step fields (minimal):
      - id (str)
      - name (str)
      - enabled (bool)
      - model (optional; defaults to chat_model_default)
      - system_prompt (str)
      - user_prompt_template (str)
      - chain_mode ("none" | "last" | "replace") optional
      - output_key (defaults to "working.step_outputs.<id>")
      - expects ("text" | "json") optional
      - thought_enabled (bool) optional
      - thought_model (optional; defaults helper_model_default)
      - thought_system_prompt (optional)
      - thought_prompt_template (optional)
    """
    state = _ensure_state_defaults(state)

    steps = tasklist.get("steps", [])
    last = state["chat"].get("last_response", "")

    for step in steps:
        if not step.get("enabled", True):
            continue

        step_id = step.get("id", "STEP?")
        step_name = step.get("name", step_id)

        model = step.get("model") or chat_model_default
        system = step.get("system_prompt", "")
        template = step.get("user_prompt_template", "")
        options = step.get("ollama_options") or {}

        chain_mode = step.get("chain_mode", "none")
        expects = step.get("expects", "text").strip().lower()

        output_key = step.get("output_key") or f"working.step_outputs.{step_id}"

        # 1) build prompt
        base_prompt = resolve_template(template, state)
        prompt = _chain_prompt(base_prompt, chain_mode, last)

        # 2) call model
        raw = _client.generate(model=model, system=system, prompt=prompt, options=options)
        raw_stripped = (raw or "").strip()

        # 3) parse/store
        produced_obj = None
        if expects == "json":
            produced_obj, err = _safe_json_loads(raw_stripped)
            if err:
                state["working"]["notes"].append({
                    "step_id": step_id,
                    "name": step_name,
                    "errors": [f"json_parse failed: {err}"]
                })
                # fall back to raw text storage
                _set_by_path(state, output_key, raw_stripped)
                last = raw_stripped
            else:
                _set_by_path(state, output_key, produced_obj)
                last = json.dumps(produced_obj, ensure_ascii=False)
        else:
            _set_by_path(state, output_key, raw_stripped)
            last = raw_stripped

        # record runner-level last_response
        state["chat"]["last_response"] = last

        # 4) thought bubble (helper model)
        if step.get("thought_enabled", True):
            helper_model = step.get("thought_model") or helper_model_default
            helper_system = step.get("thought_system_prompt") or (
                "Summarize what happened in this step in 1-3 bullet points. "
                "Be concrete. No fluff. No preamble."
            )
            helper_template = step.get("thought_prompt_template") or (
                "STEP: {{step_name}}\n\nOUTPUT:\n{{step_output}}\n\n"
                "Return 1-3 bullets: what changed / what was decided / what to do next."
            )

            try:
                summary = _run_helper_summary(
                    helper_model=helper_model,
                    helper_system=helper_system,
                    helper_template=helper_template,
                    step_name=step_name,
                    step_output=last,
                    state=state,
                    options={"temperature": 0.2}
                )
            except Exception as e:
                summary = f"- Thought summary failed: {e}"

            state["working"]["thoughts"].append({
                "step_id": step_id,
                "name": step_name,
                "summary": summary
            })

    state["outputs"]["final"] = state["chat"].get("last_response", "")
    return state

--------------------------------------------------------------------------------
FILE: src\_microservices\state.py
--------------------------------------------------------------------------------
def new_state() -> dict:
    """Create a fresh state object for the Tasklist Chat Prototype.

    Structure notes:
      - chat: conversational inputs + last outputs
      - working: per-step artifacts (outputs, thought bubbles, errors)
      - outputs: final result produced by the tasklist runner
    """
    return {
        "chat": {
            "history": [],            # list of {"role": "user"|"assistant", "content": str}
            "last_user": "",          # last user message
            "last_response": ""       # last model output (string form)
        },
        "working": {
            "step_outputs": {},       # dict step_id -> output (text or parsed json)
            "thoughts": [],           # list of {step_id, name, summary}
            "notes": []               # list of {step_id, name, errors:[...]}
        },
        "outputs": {
            "final": ""              # final assistant message to display in chat
        }
    }


--------------------------------------------------------------------------------
FILE: src\_microservices\tasklists.py
--------------------------------------------------------------------------------
def get_tasklist_names(mode: str):
    tl = _TASKLISTS.get(mode, {})
    return list(tl.keys())

def load_tasklist_by_name(mode: str, name: str) -> dict:
    return _TASKLISTS[mode][name]

_TASKLISTS = {
    "validate_patch": {
        "validate_patch_v0": {
            "name": "validate_patch_v0",
            "mode": "validate_patch",
            "steps": [
                {
                    "id": "V1",
                    "name": "Normalize patch JSON",
                    "enabled": True,
                    "model": "qwen2.5:7b-coder",
                    "ollama_options": {"temperature": 0.0},
                    "system_prompt": "You are a strict JSON normalizer. Output ONLY valid JSON. No commentary.",
                    "user_prompt_template": "PATCH_JSON_INPUT:\n{{state.inputs.existing_patch_json}}\n\nReturn valid JSON only.",
                    "expects": "json",
                    "output_key": "working.candidate_patch",
                    "validators": ["json_parse"],
                    "on_fail": "retry"
                },
                {
                    "id": "V2",
                    "name": "Enforce schema and output final patch JSON",
                    "enabled": True,
                    "model": "qwen2.5:7b-coder",
                    "ollama_options": {"temperature": 0.0},
                    "system_prompt": "Output ONLY JSON that matches EXACT schema: {hunks:[{description,search_block,replace_block,use_patch_indent}]}. No extra keys. hunks non-empty. use_patch_indent boolean.",
                    "user_prompt_template": "CANDIDATE:\n{{state.working.candidate_patch}}\n\nReturn schema-valid patch JSON only.",
                    "expects": "patch_json",
                    "output_key": "outputs.final_patch",
                    "validators": ["schema_strict"],
                    "on_fail": "retry"
                }
            ]
        }
    },
    "repair_patch": {
        "repair_patch_v0": {
            "name": "repair_patch_v0",
            "mode": "repair_patch",
            "steps": [
                {
                    "id": "R1",
                    "name": "Rewrite hunks to match current file",
                    "enabled": True,
                    "model": "qwen2.5:7b-coder",
                    "ollama_options": {"temperature": 0.2},
                    "system_prompt": (
                        "You are a patch surgeon for TokenizingPATCHER.\n"
                        "MUST output ONLY schema-valid patch JSON.\n"
                        "Rules:\n"
                        "- Every search_block MUST appear verbatim in TARGET_FILE.\n"
                        "- replace_block must be concrete (no placeholders).\n"
                        "- No extra keys."
                    ),
                    "user_prompt_template": (
                        "TARGET_FILE:\n{{state.inputs.target_file_text}}\n\n"
                        "FAILING_PATCH_JSON:\n{{state.inputs.existing_patch_json}}\n\n"
                        "ERROR_LOG:\n{{state.inputs.error_log_text}}\n\n"
                        "Produce corrected patch JSON."
                    ),
                    "expects": "patch_json",
                    "output_key": "outputs.final_patch",
                    "validators": ["schema_strict", "match_search_blocks"],
                    "on_fail": "retry"
                }
            ]
        }
    },
    "create_patch": {
        "create_patch_v0": {
            "name": "create_patch_v0",
            "mode": "create_patch",
            "steps": [
                {
                    "id": "C1",
                    "name": "Generate patch from snippet + file",
                    "enabled": True,
                    "model": "qwen2.5:7b-coder",
                    "ollama_options": {"temperature": 0.2},
                    "system_prompt": (
                        "Generate TokenizingPATCHER patch JSON.\n"
                        "Hard rules:\n"
                        "- Output ONLY schema-valid patch JSON.\n"
                        "- Every search_block MUST be verbatim substring from TARGET_FILE.\n"
                        "- replace_block must be concrete and placeholder-free.\n"
                        "- No extra keys."
                    ),
                    "user_prompt_template": (
                        "TARGET_FILE:\n{{state.inputs.target_file_text}}\n\n"
                        "CHANGE_SNIPPET:\n{{state.inputs.snippet_text}}\n\n"
                        "Create patch JSON."
                    ),
                    "expects": "patch_json",
                    "output_key": "outputs.final_patch",
                    "validators": ["schema_strict", "match_search_blocks"],
                    "on_fail": "retry"
                }
            ]
        }
    }
}

--------------------------------------------------------------------------------
FILE: src\_microservices\template_engine.py
--------------------------------------------------------------------------------
def resolve_template(template: str, state: dict) -> str:
    """
    Minimal {{state.path.to.key}} templating.
    Only supports read access, no function calls.
    """
    out = template

    # very small & safe on purpose
    # pattern scanning without regex to keep it obvious
    while True:
        start = out.find("{{")
        if start == -1:
            break
        end = out.find("}}", start)
        if end == -1:
            break
        expr = out[start+2:end].strip()

        val = ""
        if expr.startswith("state."):
            path = expr[len("state."):].split(".")
            cur = state
            ok = True
            for p in path:
                if isinstance(cur, dict) and p in cur:
                    cur = cur[p]
                else:
                    ok = False
                    break
            if ok:
                val = cur if isinstance(cur, str) else str(cur)

        out = out[:start] + val + out[end+2:]
    return out
