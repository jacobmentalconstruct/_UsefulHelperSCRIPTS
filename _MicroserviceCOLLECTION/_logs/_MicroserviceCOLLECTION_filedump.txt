Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_MicroserviceCOLLECTION


--------------------------------------------------------------------------------
FILE: microservice_std_lib.py
--------------------------------------------------------------------------------
"""
LIBRARY: Microservice Standard Lib
VERSION: 2.0.0
ROLE: Provides decorators for tagging Python classes as AI-discoverable services.
"""

import functools
import inspect
from typing import Dict, List, Any, Optional, Type

# ==============================================================================
# DECORATORS (The "Writer" Tools)
# ==============================================================================

def service_metadata(name: str, version: str, description: str, tags: List[str], capabilities: List[str] = None):
    """
    Class Decorator.
    Labels a Microservice class with high-level metadata for the Catalog.
    """
    def decorator(cls):
        cls._is_microservice = True
        cls._service_info = {
            "name": name,
            "version": version,
            "description": description,
            "tags": tags,
            "capabilities": capabilities or []
        }
        return cls
    return decorator

def service_endpoint(inputs: Dict[str, str], outputs: Dict[str, str], description: str, tags: List[str] = None, side_effects: List[str] = None, mode: str = "sync"):
    """
    Method Decorator.
    Defines the 'Socket' that the AI Architect can plug into.
    
    :param inputs: Dict of {arg_name: type_string} (e.g. {"query": "str"})
    :param outputs: Dict of {return_name: type_string} (e.g. {"results": "List[Dict]"})
    :param description: What this specific function does.
    :param tags: Keywords for searching (e.g. ["search", "read-only"])
    :param side_effects: List of impact types (e.g. ["network:outbound", "disk:write"])
    :param mode: 'sync', 'async', or 'ui_event'
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            return func(*args, **kwargs)
        
        # Attach metadata to the function object itself
        wrapper._endpoint_info = {
            "name": func.__name__,
            "inputs": inputs,
            "outputs": outputs,
            "description": description,
            "tags": tags or [],
            "side_effects": side_effects or [],
            "mode": mode
        }
        return wrapper
    return decorator

# ==============================================================================
# INTROSPECTION (The "Reader" Tools)
# ==============================================================================

def extract_service_schema(service_cls: Type) -> Dict[str, Any]:
    """
    Scans a decorated Service Class and returns a JSON-serializable schema 
    of its metadata and all its exposed endpoints.
    
    This is what the AI Agent uses to 'read' the manual.
    """
    if not getattr(service_cls, "_is_microservice", False):
        raise ValueError(f"Class {service_cls.__name__} is not decorated with @service_metadata")

    schema = {
        "meta": getattr(service_cls, "_service_info", {}),
        "endpoints": []
    }

    # Inspect all methods of the class
    for name, method in inspect.getmembers(service_cls, predicate=inspect.isfunction):
        # Unwrap decorators if necessary to find our tags
        # (Though usually the wrapper has the tag attached)
        endpoint_info = getattr(method, "_endpoint_info", None)
        
        if endpoint_info:
            schema["endpoints"].append(endpoint_info)

    return schema
--------------------------------------------------------------------------------
FILE: README.md
--------------------------------------------------------------------------------


Include a README.md with each microservice in the format seen below (adhere to the format it is concise to save context window space with AI agents):
# [SERVICE_NAME] v[VERSION]
> [ONE_SENTENCE_ROLE]

## Setup
* **Deps:** `pip install -r requirements.txt`
* **Env:** `PYTHONPATH=..` (Must find `microservice_std_lib.py`)
* **Run:** `python app.py`

## Specs
* **Port:** [DEFAULT_PORT]
* **Input:** [PRIMARY_INPUT]
* **Output:** [PRIMARY_OUTPUT]
--------------------------------------------------------------------------------
FILE: rename-main-apps.bat
--------------------------------------------------------------------------------
@echo off
setlocal enabledelayedexpansion

:: Set the script to work in the directory where it is located
cd /d "%~dp0"

echo ========================================================
echo      MICROSERVICE CANONICAL NAMING UTILITY
echo ========================================================
echo.

for /d %%D in (*) do (
    set "folder=%%D"
    
    :: Skip special folders if any
    if not "!folder!"=="_logs" (
        pushd "!folder!"
        
        :: Count .py files in this folder
        set count=0
        for %%f in (*.py) do set /a count+=1
        
        if !count! EQU 1 (
            :: EXACTLY ONE FILE: Safe to rename
            for %%f in (*.py) do (
                if /i "%%f" neq "app.py" (
                    echo [RENAME] !folder!\%%f -^> app.py
                    ren "%%f" "app.py"
                ) else (
                    echo [OK]     !folder! is already canonical.
                )
            )
        ) else (
            :: MULTIPLE FILES: Unsafe, user must handle
            echo [SKIP]   !folder! has !count! Python files. Manual check required.
        )
        
        popd
    )
)

echo.
echo ========================================================
echo  RECOMMENDED MANUAL ACTIONS:
echo  1. Go to _GraphEngine/
echo  2. Rename 'app.py' (Physics) -^> 'graph_engine.py'
echo  3. Rename 'graph_view.py' (Service) -^> 'app.py'
echo ========================================================
pause
--------------------------------------------------------------------------------
FILE: _APIGatewayMS\app.py
--------------------------------------------------------------------------------
"""
SERVICE: APIGateway
ROLE: Expose a local Python object as an HTTP API surface.
INPUTS:
- backend_core: Arbitrary Python object providing callable methods.
OUTPUTS:
- Running FastAPI+Uvicorn gateway.
NOTES:
This service dynamically binds Python callables to REST endpoints and handles
inbound API routing, health checks, and CORS configuration.
"""

import logging
import sys
import threading
import asyncio
from typing import Any, Dict, List, Optional, Callable

from microservice_std_lib import service_metadata, service_endpoint

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("APIGateway")

# ==============================================================================
# CONFIGURATION
# ==============================================================================
API_TITLE = "Microservice Gateway"
API_VERSION = "2.0.0"
DEFAULT_HOST = "0.0.0.0"
DEFAULT_PORT = 8099
# ==============================================================================

@service_metadata(
    name="APIGateway",
    version="2.0.0",
    description="Exposes local Python objects as REST APIs via FastAPI.",
    tags=["networking", "api", "gateway"],
    capabilities=["network:inbound"],
)
class APIGatewayMS:
    """
    ROLE: Expose arbitrary Python objects as REST API endpoints.
    INPUTS:
    - config: Optional dict containing backend_core and runtime settings.
    OUTPUTS:
    - FastAPI application instance with dynamically attached endpoints.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.core = self.config.get("backend_core")
        self.server_thread: Optional[threading.Thread] = None
        self.server_instance = None # Handle for the uvicorn server object
        self._available = False

        # Lazy import to avoid hard crash if libs are missing
        try:
            from fastapi import FastAPI
            from fastapi.middleware.cors import CORSMiddleware
            from pydantic import BaseModel
            import uvicorn

            self.FastAPI = FastAPI
            self.CORSMiddleware = CORSMiddleware
            self.BaseModel = BaseModel
            self.uvicorn = uvicorn
            self._available = True
        except ImportError as e:
            logger.critical(f"Missing dependency: {e}")
            logger.error("Run: pip install -r requirements.txt")
            return

        # Create FastAPI app
        self.app = self.FastAPI(title=API_TITLE, version=API_VERSION)

        # Enable CORS
        self.app.add_middleware(
            self.CORSMiddleware,
            allow_origins=["*"],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

        # Setup base routes
        self._setup_system_routes()

    # ------------------------------------------------------------------ #
    # System routes
    # ------------------------------------------------------------------ #
    def _setup_system_routes(self) -> None:
        @self.app.get("/")
        def root():
            return {"status": "online", "service": API_TITLE, "version": API_VERSION}

        @self.app.get("/health")
        def health():
            backend_type = type(self.core).__name__ if self.core is not None else "None"
            return {"status": "healthy", "backend_type": backend_type}

    # ------------------------------------------------------------------ #
    # Dynamic endpoints
    # ------------------------------------------------------------------ #
    @service_endpoint(
        inputs={"path": "str", "method": "str", "handler": "Callable"},
        outputs={},
        description="Dynamically adds a route to the API.",
        tags=["configuration", "routing"],
        side_effects=["runtime:state_change"],
    )
    def add_endpoint(self, path: str, method: str, handler: Callable) -> None:
        """
        Dynamically adds a route to the API.

        :param path: URL path (e.g., "/search")
        :param method: "GET" or "POST"
        :param handler: The function to run
        """
        if not self._available:
            logger.error("APIGatewayMS is not available; cannot add endpoint.")
            return

        method_upper = method.upper()
        # Note: We apply the handler directly. FastAPI introspects the handler
        # signature for Pydantic models.
        if method_upper == "POST":
            self.app.post(path)(handler)
        elif method_upper == "GET":
            self.app.get(path)(handler)
        elif method_upper == "PUT":
            self.app.put(path)(handler)
        elif method_upper == "DELETE":
            self.app.delete(path)(handler)
        else:
            raise ValueError(f"Unsupported method: {method}")
        
        logger.debug(f"Attached endpoint [{method_upper}] {path}")

    # ------------------------------------------------------------------ #
    # Server control
    # ------------------------------------------------------------------ #
    @service_endpoint(
        inputs={"host": "str", "port": "int", "blocking": "bool"},
        outputs={},
        description="Starts the Uvicorn server.",
        tags=["execution", "server"],
        mode="sync",
        side_effects=["network:inbound", "process:blocking"],
    )
    def start(
        self,
        host: str = DEFAULT_HOST,
        port: int = DEFAULT_PORT,
        blocking: bool = True,
    ) -> None:
        """
        Starts the Uvicorn server.
        """
        if not self._available:
            logger.error("APIGatewayMS not available; FastAPI/uvicorn not installed.")
            return

        # Configure Uvicorn programmatically
        config = self.uvicorn.Config(
            app=self.app, 
            host=host, 
            port=port, 
            log_level="info"
        )
        self.server_instance = self.uvicorn.Server(config)

        if blocking:
            logger.info(f"Starting API Gateway (Blocking) at http://{host}:{port}")
            self.server_instance.run()
        else:
            logger.info(f"Starting API Gateway (Threaded) at http://{host}:{port}")
            self.server_thread = threading.Thread(target=self.server_instance.run, daemon=True)
            self.server_thread.start()

    def stop(self):
        """Stops the server if running in threaded mode."""
        if self.server_instance:
            self.server_instance.should_exit = True
            if self.server_thread:
                self.server_thread.join(timeout=5)
            logger.info("Server stopped.")


# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Define a Mock Backend (The "Core" Logic)
    class MockBackend:
        def search(self, query: str):
            return [f"Result for {query} 1", f"Result for {query} 2"]

        def echo(self, msg: str):
            return f"Echo: {msg}"

    backend = MockBackend()

    # 2. Init Gateway
    gateway = APIGatewayMS({"backend_core": backend})

    if gateway._available:
        # 3. Define Request Models (Pydantic) for strong typing in Swagger
        class SearchReq(gateway.BaseModel):
            query: str
            limit: int = 10

        class EchoReq(gateway.BaseModel):
            message: str

        # 4. Map Backend Methods to API Endpoints
        def search_endpoint(req: SearchReq):
            """Searches the mock backend."""
            return {"results": backend.search(req.query), "limit": req.limit}

        def echo_endpoint(req: EchoReq):
            """Echoes a message."""
            return {"response": backend.echo(req.message)}

        gateway.add_endpoint("/v1/search", "POST", search_endpoint)
        gateway.add_endpoint("/v1/echo", "POST", echo_endpoint)

        # 5. Run
        try:
            gateway.start(port=8099, blocking=True)
        except KeyboardInterrupt:
            gateway.stop()
--------------------------------------------------------------------------------
FILE: _APIGatewayMS\README.md
--------------------------------------------------------------------------------
# APIGateway v2.0.0
> Dynamic HTTP Gateway that turns Python objects into REST endpoints.

## Setup
* **Deps:** `pip install -r requirements.txt`
* **Env:** `PYTHONPATH=..` (Requires `microservice_std_lib.py` in root)
* **Run:** `python app.py`

## Specs
* **Port:** 8099
* **Input:** Python Object (`backend_core`)
* **Output:** HTTP Server (Blocking or Threaded)
--------------------------------------------------------------------------------
FILE: _APIGatewayMS\requirements.txt
--------------------------------------------------------------------------------
fastapi>=0.95.0
uvicorn>=0.22.0
pydantic>=1.10.0

--------------------------------------------------------------------------------
FILE: _ArchiveBotMS\app.py
--------------------------------------------------------------------------------
"""
SERVICE: ArchiveBot
VERSION: 1.1.0
ROLE: Create timestamped compressed .tar.gz backups of directory trees.
INPUTS:
- source_path: (str) The root folder to backup.
- output_dir: (str) Where to save the resulting file.
OUTPUTS:
- archive_path: (str) Absolute path to the created file.
- file_count: (int) Total files compressed.
DEPENDENCIES: None (Standard Library)
NOTES:
Includes default ignore lists for common dev artifacts (node_modules, venv, etc).
"""

import datetime
import fnmatch
import logging
import os
import tarfile
from pathlib import Path
from typing import Any, Dict, Optional, Set, Tuple

from microservice_std_lib import service_metadata, service_endpoint

# === [ CONFIGURATION ] ========================================================
SERVICE_TITLE = "ArchiveBot"
SERVICE_VERSION = "1.1.0"
LOG_LEVEL = logging.INFO

# Default exclusions (Dev artifacts, caches, system files)
DEFAULT_IGNORE_DIRS: Set[str] = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", "dist",
    "build", "coverage", "target", "out", "bin", "obj"
}

DEFAULT_IGNORE_FILES: Set[str] = {
    ".DS_Store", "Thumbs.db", "*.pyc", "*.pyo", "*.log", "*.tmp"
}

logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(SERVICE_TITLE)

# === [ SERVICE DEFINITION ] ===================================================
@service_metadata(
    name=SERVICE_TITLE,
    version=SERVICE_VERSION,
    description="Creates timestamped .tar.gz backups of directory trees.",
    tags=["utility", "backup", "filesystem"],
    capabilities=["filesystem:read", "filesystem:write"]
)
class ArchiveBotMS:
    
    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}

    # === [ CORE ENDPOINTS ] ===================================================
    @service_endpoint(
        inputs={
            "source_path": "str",
            "output_dir": "str",
            "extra_exclusions": "List[str] | None",
            "use_default_exclusions": "bool"
        },
        outputs={
            "archive_path": "str",
            "file_count": "int"
        },
        description="Compresses a directory into a .tar.gz archive.",
        tags=["action", "backup"],
        side_effects=["filesystem:write"]
    )
    def create_backup(
        self,
        source_path: str,
        output_dir: str,
        extra_exclusions: Optional[Set[str]] = None,
        use_default_exclusions: bool = True,
    ) -> Dict[str, Any]:
        
        src = Path(source_path).resolve()
        out = Path(output_dir).resolve()

        if not src.exists():
            logger.error(f"Source not found: {src}")
            raise FileNotFoundError(f"Source path does not exist: {src}")

        out.mkdir(parents=True, exist_ok=True)

        # Build exclusion set
        exclude_patterns: Set[str] = set()
        if use_default_exclusions:
            exclude_patterns.update(DEFAULT_IGNORE_DIRS)
            exclude_patterns.update(DEFAULT_IGNORE_FILES)
        if extra_exclusions:
            exclude_patterns.update(extra_exclusions)

        # Generate filename: backup_FOLDERNAME_YYYY-MM-DD_HH-MM-SS.tar.gz
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        archive_name = f"backup_{src.name}_{timestamp}.tar.gz"
        archive_path = out / archive_name

        file_count = 0

        try:
            with tarfile.open(archive_path, "w:gz") as tar:
                for root, dirs, files in os.walk(src):
                    # Filter directories in-place to prevent walking into them
                    dirs[:] = [d for d in dirs if not self._is_excluded(d, exclude_patterns)]

                    for file_name in files:
                        if self._is_excluded(file_name, exclude_patterns):
                            continue

                        full_path = Path(root) / file_name
                        # Don't zip the file if we are writing it inside the source folder
                        if full_path == archive_path: continue

                        rel_path = full_path.relative_to(src)
                        tar.add(full_path, arcname=rel_path)
                        file_count += 1

            logger.info(f"Archive created: {archive_path} ({file_count} files)")
            return {
                "archive_path": str(archive_path),
                "file_count": file_count
            }

        except Exception as exc:
            logger.exception(f"Backup failed: {exc}")
            if archive_path.exists():
                try:
                    archive_path.unlink()
                except Exception: pass
            raise exc

    # === [ HELPERS ] ==========================================================
    def _is_excluded(self, name: str, patterns: Set[str]) -> bool:
        for pattern in patterns:
            if name == pattern or fnmatch.fnmatch(name, pattern):
                return True
        return False

# === [ SELF-TEST / RUNNER ] ===================================================
if __name__ == "__main__":
    # Create a dummy file to backup for testing
    import tempfile
    
    bot = ArchiveBotMS()
    
    with tempfile.TemporaryDirectory() as tmp_source:
        with tempfile.TemporaryDirectory() as tmp_out:
            # Create a test file
            p = Path(tmp_source) / "test_file.txt"
            p.write_text("Hello Archive")
            
            print(f"Backing up {tmp_source} to {tmp_out}...")
            result = bot.create_backup(tmp_source, tmp_out)
            print(f"Result: {result}")
--------------------------------------------------------------------------------
FILE: _AuthMS\app.py
--------------------------------------------------------------------------------
import base64
import hashlib
import json
import logging
import time
from typing import Any, Dict, Optional

from microservice_std_lib import service_metadata, service_endpoint

"""
SERVICE: Auth
ROLE: Manage user authentication and signed session tokens.
INPUTS:
  - username: Login identifier.
  - password: Secret credential.
  - token: Serialized session token string.
OUTPUTS:
  - token: Signed session token (str) or None on failure.
  - is_valid: Boolean indicating whether a token is valid and not expired.
NOTES:
  This is a simplified in-memory auth system intended for local tools and
  pipelines, not production-grade security.
"""

logger = logging.getLogger(__name__)

# ==============================================================================
# CONFIGURATION
# ==============================================================================

DEFAULT_SECRET_KEY = "super_secret_cortex_key"
DEFAULT_SALT = "cortex_salt"


# ==============================================================================


@service_metadata(
    name="Auth",
    version="1.0.0",
    description="Manages user authentication and session tokens.",
    tags=["auth", "security", "crypto"],
    capabilities=["crypto"],
)
class AuthMS:
    """
    ROLE: Simple authentication microservice providing username/password login
          and signed session tokens.

    INPUTS:
      - config: Optional configuration dict. Recognized keys:
          - 'secret_key': Secret used to sign tokens.

    OUTPUTS:
      - Exposes `login` and `validate_session` endpoints for use in pipelines.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None) -> None:
        self.config = config or {}
        self.secret_key: str = self.config.get("secret_key", DEFAULT_SECRET_KEY)

        # In a real scenario, this might load from a secure config file or DB.
        # For now, we keep a minimal in-memory user database.
        self.users_db: Dict[str, str] = {
            "admin": self._hash_password("admin123"),
        }

    # --------------------------------------------------------------------- #
    # Public endpoints
    # --------------------------------------------------------------------- #

    @service_endpoint(
        inputs={"username": "str", "password": "str"},
        outputs={"token": "Optional[str]"},
        description="Attempts to log in and returns a signed session token.",
        tags=["auth", "security", "session"],
    )
    def login(self, username: str, password: str) -> Optional[str]:
        """
        Attempt to log in with the provided username and password.

        :param username: Login identifier.
        :param password: Plain-text password.
        :returns: Signed session token if successful, None otherwise.
        """
        if username not in self.users_db:
            return None

        stored_hash = self.users_db[username]
        if self._verify_password(password, stored_hash):
            return self._create_token(username)

        return None

    @service_endpoint(
        inputs={"token": "str"},
        outputs={"is_valid": "bool"},
        description="Checks whether a token is valid and not expired.",
        tags=["auth", "security"],
    )
    def validate_session(self, token: str) -> bool:
        """
        Check if a serialized token is valid and not expired.

        :param token: Session token string.
        :returns: True if token is valid and not expired, False otherwise.
        """
        payload = self._decode_token(token)
        return payload is not None

    # --------------------------------------------------------------------- #
    # Internal helpers
    # --------------------------------------------------------------------- #

    def _hash_password(self, password: str) -> str:
        """
        Securely hashes a password using SHA-256 with a static salt.
        """
        return hashlib.sha256((password + DEFAULT_SALT).encode("utf-8")).hexdigest()

    def _verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """
        Verifies a provided password against the stored hash.
        """
        return self._hash_password(plain_password) == hashed_password

    def _create_token(self, user_id: str, expires_in: int = 3600) -> str:
        """
        Generates a signed session token.

        Payload includes:
          - 'sub' (subject)
          - 'exp' (expiration time)
          - 'iat' (issued-at time)
          - 'scope' (authorization scope)
        """
        now = int(time.time())
        payload = {
            "sub": user_id,
            "exp": now + expires_in,
            "iat": now,
            "scope": "admin",
        }

        json_payload = json.dumps(payload).encode("utf-8")
        token_part = base64.b64encode(json_payload).decode("utf-8")

        signature = hashlib.sha256((token_part + self.secret_key).encode("utf-8")).hexdigest()
        return f"{token_part}.{signature}"

    def _decode_token(self, token: str) -> Optional[Dict[str, Any]]:
        """
        Parses and validates the incoming token.

        Returns the payload if valid, None otherwise.
        """
        try:
            if not token or "." not in token:
                return None

            token_part, signature = token.split(".", 1)

            # Recalculate signature to verify integrity
            recalc_signature = hashlib.sha256(
                (token_part + self.secret_key).encode("utf-8")
            ).hexdigest()

            if signature != recalc_signature:
                return None  # Invalid signature

            # Decode payload
            payload_json = base64.b64decode(token_part).decode("utf-8")
            payload: Dict[str, Any] = json.loads(payload_json)

            # Check expiration
            if payload.get("exp", 0) < time.time():
                return None  # Expired

            return payload

        except Exception:
            # Intentionally swallow details here and just treat token as invalid.
            logger.exception("Failed to decode or validate auth token.")
            return None


if __name__ == "__main__":
    svc = AuthMS()
    print("Service ready:", svc)
    # Example (manual) usage:
    # token = svc.login("admin", "admin123")
    # print("Token:", token)
    # print("Valid:", svc.validate_session(token) if token else False)

--------------------------------------------------------------------------------
FILE: _ChalkBoardMS\app.py
--------------------------------------------------------------------------------
import webview # pip install pywebview
import threading
import time
import json
from typing import Optional, Dict, Any

# Mocking your microservice lib if missing
try:
    from microservice_std_lib import service_metadata, service_endpoint
except ImportError:
    def service_metadata(**kwargs): return lambda c: c
    def service_endpoint(**kwargs): return lambda f: f

@service_metadata(
    name="ChalkboardWeb",
    version="2.0.0",
    description="HTML5/CSS3 powered Digital Signage",
    tags=["ui", "webview", "obs"],
    capabilities=["ui:gui"]
)
class ChalkboardAPI:
    def __init__(self):
        self._window = None
        self.state = {"text": "ON AIR", "theme": "neon"}

    # --- JAVASCRIPT CALLS THIS ---
    def loaded(self):
        """Called by JS when the page is ready."""
        print("Frontend loaded!")
        return self.state

    def log_action(self, action_name):
        """Called by JS when a button is clicked."""
        print(f"User triggered: {action_name}")
        # Here you could trigger external hardware, lights, etc.

    # --- PYTHON CALLS THIS (Your Agent) ---
    @service_endpoint(inputs={"text": "str", "theme": "str"}, outputs={})
    def update_sign(self, text: str, theme: str = "neon"):
        """Updates the HTML via JavaScript injection."""
        self.state["text"] = text
        self.state["theme"] = theme
        
        if self._window:
            # Execute JS directly from Python!
            sanitized_text = json.dumps(text) # Safety
            self._window.evaluate_js(f"updateDisplay({sanitized_text}, '{theme}')")

    @service_endpoint(inputs={"effect": "str"}, outputs={})
    def trigger_effect(self, effect: str):
        """Triggers a CSS animation (boom, pow, etc)."""
        if self._window:
            self._window.evaluate_js(f"triggerEffect('{effect}')")

def start_app():
    api = ChalkboardAPI()
    
    # Load your HTML file directly
    window = webview.create_window(
        'OBS Rad-IO Signboard', 
        url='_ChalkBOARD.html', # Your HTML file path
        js_api=api, # Expose the class to JS
        width=900, 
        height=600,
        background_color='#000000'
    )
    api._window = window
    webview.start(debug=True) # debug=True lets you inspect element with right click

if __name__ == "__main__":
    start_app()
--------------------------------------------------------------------------------
FILE: _ChalkBoardMS\app_v1.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import font, ttk, colorchooser
import colorsys
import math
import random
import time
from typing import Any, Dict, Optional

# Assuming these exist in your environment. 
# If testing standalone without the lib, comment out the decorator and import.
try:
    from microservice_std_lib import service_metadata, service_endpoint
except ImportError:
    # Mocking for standalone functionality if lib is missing
    def service_metadata(**kwargs):
        def decorator(cls):
            return cls
        return decorator

    def service_endpoint(**kwargs):
        def decorator(func):
            return func
        return decorator

@service_metadata(
    name="Chalkboard",
    version="1.1.0",
    description="Interactive digital signage/chalkboard widget with retro themes.",
    tags=["ui", "visuals", "widget"],
    capabilities=["ui:gui"]
)
class ChalkboardMS(tk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        super().__init__(parent)
        
        # --- CONFIGURATION ---
        self.bg_color = "#050505"
        self.configure(bg=self.bg_color)
        
        # Resolving root for bindings
        self.root = self.winfo_toplevel()
        if parent is None: 
            self.root.title("OBS Rad-IO Signboard")
            self.root.geometry("900x600")
            self.root.configure(bg="#111")
        
        # --- STATE ---
        self.text_content = "ON AIR"
        self.saved_state = {} 
        
        # Cursor State
        self.cursor_visible = True
        self.cursor_blink_state = True
        self.last_activity_time = time.time()
        
        # Animation / Action State
        self.action_active = False
        self.action_start_time = 0
        self.action_type = None 
        self.restore_timer = None 
        
        # Window State
        self.settings_window = None 
        
        # Appearance Defaults
        self.text_color = "#f4f4f4" 
        self.shadow_color = "#000000"
        self.neon_stroke_color = "#FF00FF" # Magenta default
        self.neon_fill_color = "#FFFFFF"
        
        # Font State
        self.base_font_size = 80
        self.current_font_size = 80
        self.scale_factor = 1.0
        self.font_family = "Arial"
        self.is_bold = True
        self.is_italic = False
        self.is_underline = False
        
        # Style Mode
        self.render_style = "neon" # Default to the coolest one
        
        # Spinner State
        self.spinner_active = False
        self.spinner_hue = 0.0
        self.spin_angle_1 = 0
        self.spin_angle_2 = 0
        self.spin_angle_3 = 0

        self._detect_system_fonts()

        # --- UI LAYOUT ---
        self.canvas = tk.Canvas(self, bg=self.bg_color, highlightthickness=0)
        self.canvas.pack(fill="both", expand=True)
        
        # Control Deck (The Mixer Board Look)
        self.frm_actions = tk.Frame(self, bg="#1a1a1a", height=50, bd=2, relief="raised")
        self.frm_actions.pack(fill="x", side="bottom")
        
        self.btn_settings = tk.Button(
            self, text="⚙", font=("Segoe UI", 12), 
            bg=self.bg_color, fg="#444", borderwidth=0, 
            command=self.open_settings, cursor="hand2"
        )
        self.btn_settings.place(relx=1.0, rely=0.0, anchor="ne", x=-5, y=5)

        # --- INITIALIZE ACTIONS ---
        self._build_action_buttons()

        # --- BINDINGS ---
        self.root.bind("<Key>", self.handle_keypress)
        self.root.bind("<Button-1>", self.handle_click)
        self.root.bind("<Configure>", self.on_resize)
        
        self.root.bind("<Shift-MouseWheel>", self.on_scale_scroll)
        self.root.bind("<Shift-Button-4>", lambda e: self.on_scale_scroll(e, 1))
        self.root.bind("<Shift-Button-5>", lambda e: self.on_scale_scroll(e, -1))
        
        # Theme Hotkeys (From your HTML file)
        self.root.bind("<F1>", lambda e: self.apply_theme("neon"))
        self.root.bind("<F2>", lambda e: self.apply_theme("terminal"))
        self.root.bind("<F3>", lambda e: self.apply_theme("chalk"))

        # Start Loops
        self.blink_cursor_loop()
        self.animate_loop()
        
        # Apply default theme
        self.apply_theme("neon")

    def _detect_system_fonts(self):
        system_fonts = font.families()
        self.font_family = "Arial"
        # Prioritize retro/display fonts
        preferred = ["Impact", "Press Start 2P", "Consolas", "Courier New", "Comic Sans MS", "Segoe UI Black"]
        for p in preferred:
            if p in system_fonts:
                self.font_family = p
                # Don't break immediately, we might find a better one later in the list? 
                # Actually let's just grab the first hit for now.
                break

    def _build_action_buttons(self):
        # Create a "Bank" of buttons style
        lbl = tk.Label(self.frm_actions, text="FX BANK:", bg="#1a1a1a", fg="#666", font=("Arial", 8, "bold"))
        lbl.pack(side="left", padx=(10, 5))

        def create_fx_btn(text, color, cmd):
            btn = tk.Button(
                self.frm_actions, 
                text=text, font=("Impact", 12),
                bg="#333", fg=color, 
                activebackground=color, activeforeground="#000",
                relief="raised", bd=1,
                command=lambda: [self.handle_click(None), cmd()],
                cursor="hand2", width=6
            )
            btn.pack(side="left", padx=5, pady=8)
            return btn

        self.btn_spin = create_fx_btn("◎ LOOP", "#00FF00", self.toggle_spinner)
        create_fx_btn("BOOM", "#FF4400", lambda: self.trigger_action("BOOM!", "#FF4400", "boom"))
        create_fx_btn("POW", "#FFFF00", lambda: self.trigger_action("POW!", "#FFFF00", "pow"))
        create_fx_btn("ZAP", "#00FFFF", lambda: self.trigger_action("ZAP!", "#00FFFF", "zap"))
        
        # Helper text
        info = tk.Label(self.frm_actions, text="[F1:Neon | F2:Hack | F3:Chalk]", bg="#1a1a1a", fg="#444", font=("Consolas", 8))
        info.pack(side="right", padx=10)

    # --- THEME ENGINE ---
    def apply_theme(self, theme_name):
        if theme_name == "neon":
            self.bg_color = "#050505"
            self.text_color = "#FFFFFF"
            self.neon_stroke_color = "#bc13fe" # Purple/Pink
            self.render_style = "neon"
            self.font_family = "Impact" if "Impact" in font.families() else "Arial"
            
        elif theme_name == "terminal":
            self.bg_color = "#000000"
            self.text_color = "#00ff41" # Matrix Green
            self.render_style = "normal"
            self.font_family = "Courier New"
            # Simulate scanlines in bg color? Hard in tk, keeping it black.
            
        elif theme_name == "chalk":
            self.bg_color = "#2b3a28" # Dark Green
            self.text_color = "#f4f4f4"
            self.render_style = "shaky" # Rough look
            self.font_family = "Comic Sans MS" if "Comic Sans MS" in font.families() else "Arial"

        self.canvas.config(bg=self.bg_color)
        self.frm_actions.config(bg="#1a1a1a")
        
    # --- LOGIC ---

    def toggle_spinner(self):
        self.spinner_active = not self.spinner_active
        if self.spinner_active:
            self.btn_spin.config(fg="#000", bg="#00FF00", text="ON")
        else:
            self.btn_spin.config(fg="#00FF00", bg="#333", text="◎ LOOP")

    @service_endpoint(
        inputs={"text": "str", "color": "str", "action_type": "str"},
        outputs={},
        description="Triggers a visual effect (BOOM, POW, ZAP).",
        tags=["ui", "effect"],
        side_effects=["ui:update"]
    )
    def trigger_action(self, text, color, action_type):
        if self.restore_timer:
            self.root.after_cancel(self.restore_timer)
            self.restore_timer = None
        else:
            self.saved_state = {
                "text": self.text_content,
                "color": self.text_color,
                "style": self.render_style,
                "font": self.font_family,
                "bold": self.is_bold
            }
        
        self.action_active = True
        self.action_start_time = time.time()
        self.action_type = action_type
        self.text_content = text
        self.text_color = color
        self.render_style = "comic"
        
        sys_fonts = font.families()
        if action_type == "boom" and "Impact" in sys_fonts: self.font_family = "Impact"
        elif action_type == "pow" and "Comic Sans MS" in sys_fonts: self.font_family = "Comic Sans MS"
        elif action_type == "zap": self.font_family = "Courier New"
        
        self.restore_timer = self.root.after(1500, self._restore_state)
        
    def _restore_state(self):
        if not self.saved_state: return
        self.text_content = self.saved_state["text"]
        self.text_color = self.saved_state["color"]
        self.render_style = self.saved_state["style"]
        self.font_family = self.saved_state["font"]
        self.is_bold = self.saved_state["bold"]
        
        self.action_active = False
        self.restore_timer = None
        self.current_font_size = self.base_font_size

    def update_animation_physics(self):
        if not self.action_active:
            self.current_font_size = self.base_font_size
            return

        elapsed = time.time() - self.action_start_time
        
        if self.action_type == "boom":
            progress = min(elapsed * 3, 1.5)
            scale = 0.5 + progress
            self.current_font_size = int(self.base_font_size * scale)
            self.is_bold = True
            
        elif self.action_type == "pow":
            pulse = math.sin(elapsed * 10) * 0.3
            scale = 1.2 + pulse
            self.current_font_size = int(self.base_font_size * scale)
            self.is_bold = (int(elapsed * 10) % 2 == 0)
            
        elif self.action_type == "zap":
            jitter = random.uniform(0.8, 1.4)
            self.current_font_size = int(self.base_font_size * jitter)
            self.is_bold = True

    # --- INPUT HANDLING ---

    def handle_click(self, event):
        self.last_activity_time = time.time()
        self.cursor_blink_state = True
        self.cursor_visible = True
        self.root.focus_set()

    def handle_keypress(self, event):
        self.last_activity_time = time.time()
        # Ignore function keys
        if event.keysym in ["F1", "F2", "F3", "F4", "F5", "Shift_L", "Shift_R", "Control_L", "Alt_L"]: return
        
        if isinstance(event.widget, (tk.Entry, tk.Listbox, ttk.Combobox)): return
        
        if event.keysym == "BackSpace": 
            self.text_content = self.text_content[:-1]
        elif event.keysym == "Escape": 
            self.text_content = ""
        elif event.keysym == "Return":
             # Optional: Clear on enter or add newline? Let's clear for now as it's a signboard
             pass 
        elif len(event.char) == 1 and ord(event.char) >= 32: 
            self.text_content += event.char

    def on_scale_scroll(self, event, direction=None):
        self.last_activity_time = time.time()
        delta = direction if direction else (1 if event.delta > 0 else -1)
        if delta > 0: self.base_font_size += 5
        else: self.base_font_size = max(10, self.base_font_size - 5)

    # --- RENDERING ENGINE ---

    def get_font_tuple(self):
        style_list = []
        if self.is_bold: style_list.append("bold")
        if self.is_italic: style_list.append("italic")
        if self.is_underline: style_list.append("underline")
        size = int(self.current_font_size * self.scale_factor)
        return (self.font_family, size, " ".join(style_list))

    def blink_cursor_loop(self):
        self.cursor_blink_state = not self.cursor_blink_state
        self.root.after(600, self.blink_cursor_loop)

    def animate_loop(self):
        # 1. Activity Check
        time_since_activity = time.time() - self.last_activity_time
        
        # --- SAFE FOCUS CHECK ---
        try:
            is_focused = self.root.focus_displayof() is not None
        except Exception:
            is_focused = False
            
        self.cursor_visible = (is_focused and time_since_activity < 5.0 and self.cursor_blink_state)

        # 2. Physics Update
        self.update_animation_physics()

        # 3. Draw
        self.draw_frame()
        self.root.after(16, self.animate_loop)

    def draw_frame(self):
        self.canvas.delete("all")
        w = self.canvas.winfo_width()
        h = self.canvas.winfo_height()
        cx, cy = w / 2, h / 2

        # SPINNER
        if self.spinner_active:
            self._draw_spinner(cx, cy, min(w, h))

        # TEXT
        display_text = self.text_content + ("_" if self.cursor_visible and not self.action_active else "")
        current_font = self.get_font_tuple()

        if self.render_style == "neon":
            # Simulate Glow with stacked offsets
            glow_color = self.neon_stroke_color
            
            # Deep glow (blurry)
            for i in range(1, 4):
                 self.canvas.create_text(cx, cy, text=display_text, fill=glow_color, font=current_font)
            
            # Offset Glitch Effect (subtle vibration)
            if random.random() > 0.95:
                ox = random.randint(-3, 3)
                self.canvas.create_text(cx+ox, cy, text=display_text, fill="#FFFFFF", font=current_font)
            else:
                self.canvas.create_text(cx, cy, text=display_text, fill="#FFFFFF", font=current_font)

        elif self.render_style == "comic":
            # Pop Art Shadow
            depth = max(3, self.current_font_size // 10)
            for i in range(depth, 0, -1):
                self.canvas.create_text(cx+i, cy+i, text=display_text, fill="#000", font=current_font)
            self.canvas.create_text(cx, cy, text=display_text, fill=self.text_color, font=current_font)
            self.canvas.create_text(cx-2, cy-2, text=display_text, fill="#FFFFFF", font=current_font)
            self.canvas.create_text(cx-2, cy-2, text=display_text, fill=self.text_color, font=current_font)
            
        elif self.render_style == "shaky":
            # Nervous / Chalk style
            ox = random.randint(-1, 1)
            oy = random.randint(-1, 1)
            self.canvas.create_text(cx+ox, cy+oy, text=display_text, fill=self.text_color, font=current_font)
            
        else: # Normal / Terminal
            self.canvas.create_text(cx, cy, text=display_text, fill=self.text_color, font=current_font)

    def _draw_spinner(self, cx, cy, min_dim):
        base_size = min_dim / 2
        self.spinner_hue = (self.spinner_hue + 0.005) % 1.0
        def get_col(offset):
            r, g, b = colorsys.hsv_to_rgb((self.spinner_hue + offset)%1.0, 1.0, 1.0)
            return f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}'

        c1 = get_col(0.0)
        c2 = get_col(0.3)
        
        # Draw dynamic rings
        self.spin_angle_1 -= 3
        self.canvas.create_arc(cx-base_size*0.8, cy-base_size*0.8, cx+base_size*0.8, cy+base_size*0.8, 
                               start=self.spin_angle_1, extent=120, outline=c1, width=10, style="arc")
        self.canvas.create_arc(cx-base_size*0.8, cy-base_size*0.8, cx+base_size*0.8, cy+base_size*0.8, 
                               start=self.spin_angle_1+180, extent=120, outline=c1, width=10, style="arc")

        self.spin_angle_2 += 5
        self.canvas.create_arc(cx-base_size*0.6, cy-base_size*0.6, cx+base_size*0.6, cy+base_size*0.6, 
                               start=self.spin_angle_2, extent=250, outline=c2, width=6, style="arc")

    def on_resize(self, event):
        # Redraw background if needed
        pass

    # --- SETTINGS WINDOW ---
    def open_settings(self):
        if self.settings_window and tk.Toplevel.winfo_exists(self.settings_window):
            self.settings_window.lift()
            return

        win = tk.Toplevel(self.root)
        self.settings_window = win
        win.title("Settings")
        win.geometry("300x400")
        win.configure(bg="#222")
        
        def add_btn(txt, cmd):
            tk.Button(win, text=txt, command=cmd, bg="#333", fg="#FFF").pack(fill="x", padx=20, pady=5)

        tk.Label(win, text="Settings", font=("Arial", 14, "bold"), bg="#222", fg="#FFF").pack(pady=10)
        
        add_btn("Pick Neon Color", self._pick_neon_color)
        add_btn("Close", win.destroy)

    def _pick_neon_color(self):
        c = colorchooser.askcolor(self.neon_stroke_color)[1]
        if c: 
            self.neon_stroke_color = c
            self.apply_theme("neon")

if __name__ == "__main__":
    root = tk.Tk()
    app = ChalkboardMS({"parent": root})
    app.pack(fill="both", expand=True)
    root.mainloop()
--------------------------------------------------------------------------------
FILE: _ChalkBoardMS\_ChalkBoard.html
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: _CodeChunkerMS\app.py
--------------------------------------------------------------------------------
import re
from typing import List, Dict, Any, Optional
from pathlib import Path
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="CodeChunker",
version="1.0.0",
description="Splits code into semantic blocks (Classes, Functions) using indentation and regex heuristics.",
tags=["parsing", "chunking", "code"],
capabilities=["filesystem:read"]
)
class CodeChunkerMS:
    """
The Surgeon (Pure Python Edition): Splits code into semantic blocks
    (Classes, Functions) using indentation and regex heuristics.
    
    Advantages: Zero dependencies. Works on any machine.
    Disadvantages: Slightly less precise than Tree-Sitter for messy code.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    # Regex to find definitions. Capture group 1 is the indentation.
    # Supports Python, JS, TS, Go signatures loosely.
        self.def_pattern = re.compile(
            r'^(\s*)(?:async\s+)?(?:class|def|function|func|var|const)\s+([a-zA-Z0-9_]+)', 
            re.MULTILINE
        )

    @service_endpoint(
    inputs={"file_path": "str", "max_chars": "int"},
    outputs={"chunks": "List[Dict]"},
    description="Reads a file and breaks it into logical blocks based on indentation.",
    tags=["parsing", "chunking"],
    side_effects=["filesystem:read"]
    )
    def chunk_file(self, file_path: str, max_chars: int = 1500) -> List[Dict[str, Any]]:
    """
    Reads a file and breaks it into logical blocks based on indentation.
    """
        path = Path(file_path)
        try:
            code = path.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return []

        return self._chunk_by_indentation(code, max_chars)

    def _chunk_by_indentation(self, code: str, max_chars: int) -> List[Dict]:
lines = code.splitlines()
        chunks = []
        
        current_chunk_lines = []
        current_start_line = 0
        current_indent = 0
        in_block = False
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # 1. Skip empty lines if we aren't in a block
            if not stripped and not in_block:
                continue

            # 2. Calculate Indentation
            indent_match = re.match(r'^(\s*)', line)
            indent_level = len(indent_match.group(1)) if indent_match else 0

            # 3. Check for Block Start (def/class at root level or low indent)
            # We allow indent < 4 spaces to catch top-level stuff or slight nesting
            match = self.def_pattern.match(line)
            is_def = match is not None and indent_level <= 4
            
            # IF we hit a new definition AND we have a chunk pending:
            if is_def and current_chunk_lines:
                # Save previous chunk
                self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                # Reset
                current_chunk_lines = []
                current_start_line = i + 1
                in_block = True
                current_indent = indent_level

            # IF we hit a line with LESS indentation than the current block start,
            # the block has ended. (Python/Yaml logic, mostly holds for C-style too if formatted)
            if in_block and stripped and indent_level <= current_indent and not is_def:
                # Special case: Closing braces '}' often have same indent as start
                if not stripped.startswith('}'):
                    self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                    current_chunk_lines = []
                    current_start_line = i + 1
                    in_block = False

            current_chunk_lines.append(line)

        # Flush remaining
        if current_chunk_lines:
            self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)

        return chunks

    def _finalize_chunk(self, chunks, lines, start_line, max_chars):
        """Recursively splits huge chunks if they exceed max_chars."""
        full_text = "\n".join(lines)
        if not full_text.strip(): return

        # If chunk is too big, split it by lines (naive fallback for massive functions)
        if len(full_text) > max_chars:
            self._split_large_block(chunks, lines, start_line, max_chars)
        else:
            chunks.append({
                "type": "block", # Generic type since we aren't parsing AST
                "text": full_text,
                "start_line": start_line,
                "end_line": start_line + len(lines)
            })

    def _split_large_block(self, chunks, lines, start_line, max_chars):
        """Force split a large block while keeping line boundaries."""
        current_sub = []
        current_len = 0
        sub_start = start_line
        
        for i, line in enumerate(lines):
            if current_len + len(line) > max_chars:
                if current_sub:
                    chunks.append({
                        "type": "fragment",
                        "text": "\n".join(current_sub),
                        "start_line": sub_start,
                        "end_line": sub_start + len(current_sub)
                    })
                current_sub = []
                current_len = 0
                sub_start = start_line + i
            
            current_sub.append(line)
            current_len += len(line)
            
        if current_sub:
            chunks.append({
                "type": "fragment",
                "text": "\n".join(current_sub),
                "start_line": sub_start,
                "end_line": sub_start + len(current_sub)
            })

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = CodeChunkerMS()
print("Service ready:", chunker)
    
# Test Python Code
    py_code = """
import os

def small_helper():
    return True

class DataProcessor:
    def __init__(self):
        self.data = []

    def process(self, raw_input):
        # This is a comment inside the function
        if raw_input:
            self.data.append(raw_input)
        return True
    """
    
    # Write temp file
    import tempfile
    with tempfile.NamedTemporaryFile(suffix=".py", mode="w+", delete=False) as tmp:
        tmp.write(py_code)
        tmp_path = tmp.name
        
    print(f"--- Chunking {tmp_path} (Pure Python) ---")
    chunks = chunker.chunk_file(tmp_path)
    
    for i, c in enumerate(chunks):
        print(f"\n[Chunk {i}] Lines {c['start_line']}-{c['end_line']}")
        print(f"{'-'*20}\n{c['text'].strip()}\n{'-'*20}")
        
    os.remove(tmp_path)

--------------------------------------------------------------------------------
FILE: _CodeChunkerMS\app.py.bak
--------------------------------------------------------------------------------
import re
from typing import List, Dict, Any, Optional
from pathlib import Path
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="CodeChunker",
version="1.0.0",
description="Splits code into semantic blocks (Classes, Functions) using indentation and regex heuristics.",
tags=["parsing", "chunking", "code"],
capabilities=["filesystem:read"]
)
class CodeChunkerMS:
    """
The Surgeon (Pure Python Edition): Splits code into semantic blocks
    (Classes, Functions) using indentation and regex heuristics.
    
    Advantages: Zero dependencies. Works on any machine.
    Disadvantages: Slightly less precise than Tree-Sitter for messy code.
    """
    def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    # Regex to find definitions. Capture group 1 is the indentation.
    # Supports Python, JS, TS, Go signatures loosely.
        self.def_pattern = re.compile(
            r'^(\s*)(?:async\s+)?(?:class|def|function|func|var|const)\s+([a-zA-Z0-9_]+)', 
            re.MULTILINE
        )

    @service_endpoint(
    inputs={"file_path": "str", "max_chars": "int"},
    outputs={"chunks": "List[Dict]"},
    description="Reads a file and breaks it into logical blocks based on indentation.",
    tags=["parsing", "chunking"],
    side_effects=["filesystem:read"]
    )
    def chunk_file(self, file_path: str, max_chars: int = 1500) -> List[Dict[str, Any]]:
    """
    Reads a file and breaks it into logical blocks based on indentation.
    """
        path = Path(file_path)
        try:
            code = path.read_text(encoding="utf-8", errors="ignore")
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return []

        return self._chunk_by_indentation(code, max_chars)

    def _chunk_by_indentation(self, code: str, max_chars: int) -> List[Dict]:
        lines = code.splitlines()
        chunks = []
        
        current_chunk_lines = []
        current_start_line = 0
        current_indent = 0
        in_block = False
        
        for i, line in enumerate(lines):
            stripped = line.strip()
            
            # 1. Skip empty lines if we aren't in a block
            if not stripped and not in_block:
                continue

            # 2. Calculate Indentation
            indent_match = re.match(r'^(\s*)', line)
            indent_level = len(indent_match.group(1)) if indent_match else 0

            # 3. Check for Block Start (def/class at root level or low indent)
            # We allow indent < 4 spaces to catch top-level stuff or slight nesting
            match = self.def_pattern.match(line)
            is_def = match is not None and indent_level <= 4
            
            # IF we hit a new definition AND we have a chunk pending:
            if is_def and current_chunk_lines:
                # Save previous chunk
                self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                # Reset
                current_chunk_lines = []
                current_start_line = i + 1
                in_block = True
                current_indent = indent_level

            # IF we hit a line with LESS indentation than the current block start,
            # the block has ended. (Python/Yaml logic, mostly holds for C-style too if formatted)
            if in_block and stripped and indent_level <= current_indent and not is_def:
                # Special case: Closing braces '}' often have same indent as start
                if not stripped.startswith('}'):
                    self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)
                    current_chunk_lines = []
                    current_start_line = i + 1
                    in_block = False

            current_chunk_lines.append(line)

        # Flush remaining
        if current_chunk_lines:
            self._finalize_chunk(chunks, current_chunk_lines, current_start_line, max_chars)

        return chunks

    def _finalize_chunk(self, chunks, lines, start_line, max_chars):
        """Recursively splits huge chunks if they exceed max_chars."""
        full_text = "\n".join(lines)
        if not full_text.strip(): return

        # If chunk is too big, split it by lines (naive fallback for massive functions)
        if len(full_text) > max_chars:
            self._split_large_block(chunks, lines, start_line, max_chars)
        else:
            chunks.append({
                "type": "block", # Generic type since we aren't parsing AST
                "text": full_text,
                "start_line": start_line,
                "end_line": start_line + len(lines)
            })

    def _split_large_block(self, chunks, lines, start_line, max_chars):
        """Force split a large block while keeping line boundaries."""
        current_sub = []
        current_len = 0
        sub_start = start_line
        
        for i, line in enumerate(lines):
            if current_len + len(line) > max_chars:
                if current_sub:
                    chunks.append({
                        "type": "fragment",
                        "text": "\n".join(current_sub),
                        "start_line": sub_start,
                        "end_line": sub_start + len(current_sub)
                    })
                current_sub = []
                current_len = 0
                sub_start = start_line + i
            
            current_sub.append(line)
            current_len += len(line)
            
        if current_sub:
            chunks.append({
                "type": "fragment",
                "text": "\n".join(current_sub),
                "start_line": sub_start,
                "end_line": sub_start + len(current_sub)
            })

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = CodeChunkerMS()
print("Service ready:", chunker)
    
# Test Python Code
    py_code = """
import os

def small_helper():
    return True

class DataProcessor:
    def __init__(self):
        self.data = []

    def process(self, raw_input):
        # This is a comment inside the function
        if raw_input:
            self.data.append(raw_input)
        return True
    """
    
    # Write temp file
    import tempfile
    with tempfile.NamedTemporaryFile(suffix=".py", mode="w+", delete=False) as tmp:
        tmp.write(py_code)
        tmp_path = tmp.name
        
    print(f"--- Chunking {tmp_path} (Pure Python) ---")
    chunks = chunker.chunk_file(tmp_path)
    
    for i, c in enumerate(chunks):
        print(f"\n[Chunk {i}] Lines {c['start_line']}-{c['end_line']}")
        print(f"{'-'*20}\n{c['text'].strip()}\n{'-'*20}")
        
    os.remove(tmp_path)

--------------------------------------------------------------------------------
FILE: _CodeGrapherMS\app.py
--------------------------------------------------------------------------------
import ast
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="CodeGrapher",
version="1.0.0",
description="Parses Python code to extract symbols (nodes) and call relationships (edges).",
tags=["parsing", "graph", "analysis"],
capabilities=["filesystem:read"]
)
class CodeGrapherMS:
    """
    The Cartographer of Logic: Parses Python code to extract high-level 
    symbols (classes, functions) and maps their 'Call' relationships.
    
    Output: A graph structure (Nodes + Edges) suitable for visualization 
    or dependency analysis.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
    self.nodes = [] # List of symbols (functions, classes)
    self.edges = [] # List of relationships (source -> target)

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"graph_data": "Dict[str, Any]"},
    description="Recursively scans a directory for .py files and builds the graph.",
    tags=["parsing", "graph"],
    side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Dict[str, Any]:
    """
    Recursively scans a directory for .py files and builds the graph.
    """
        root = Path(root_path).resolve()
        self.nodes = []
        self.edges = []
        
        if not root.exists():
            return {"error": f"Path {root} does not exist"}

        # 1. Parsing Pass (Create Nodes)
        for path in root.rglob("*.py"):
            try:
                # Skip hidden/venv folders
                if any(p.startswith('.') for p in path.parts) or "venv" in path.parts:
                    continue
                    
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    source = f.read()
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_symbols = self._parse_source(source, rel_path)
                self.nodes.extend(file_symbols)
                
            except Exception as e:
                print(f"Failed to parse {path.name}: {e}")

        # 2. Linking Pass (Create Edges)
        self._build_edges()

        return {
            "root": str(root),
            "node_count": len(self.nodes),
            "edge_count": len(self.edges),
            "nodes": self.nodes,
            "edges": self.edges
        }

    def _parse_source(self, source: str, file_path: str) -> List[Dict]:
        """
        Uses Python's AST to extract surgical symbol info.
        """
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return []

        visitor = SurgicalVisitor(file_path)
        visitor.visit(tree)
        return visitor.symbols

    def _build_edges(self):
        """
        Resolves 'calls' strings into explicit graph edges.
        """
        # Create a quick lookup map: "my_function" -> NodeID
        # Note: This is a naive lookup (name collision possible). 
        # A robust version would use "module.class.method" fully qualified names.
        name_map = {n['name']: n['id'] for n in self.nodes}

        for node in self.nodes:
            source_id = node['id']
            calls = node.get('calls', [])
            
            for target_name in calls:
                if target_name in name_map:
                    target_id = name_map[target_name]
                    
                    # Avoid self-loops for cleanliness
                    if source_id != target_id:
                        self.edges.append({
                            "source": source_id,
                            "target": target_id,
                            "type": "calls"
                        })

# --- Helper Class: The AST Walker ---

class SurgicalVisitor(ast.NodeVisitor):
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.symbols = []

    def visit_FunctionDef(self, node):
        self._handle_func(node, "function")

    def visit_AsyncFunctionDef(self, node):
        self._handle_func(node, "async_function")

    def visit_ClassDef(self, node):
        # Record the class
        class_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": class_id,
            "file": self.file_path,
            "name": node.name,
            "type": "class",
            "line": node.lineno,
            "calls": [] # Classes don't 'call' things directly usually, their methods do
        })
        # Visit children (methods)
        self.generic_visit(node)

    def _handle_func(self, node, type_name):
        # Extract outgoing calls from the function body
        calls = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.append(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.append(child.func.attr)
        
        unique_calls = list(set(calls))
        
        node_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": node_id,
            "file": self.file_path,
            "name": node.name,
            "type": type_name,
            "line": node.lineno,
            "calls": unique_calls
        })

# --- Independent Test Block ---
if __name__ == "__main__":
    import sys
    
    # Defaults to current directory
    target_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    
    print(f"Mapping Logic in: {target_dir}")
    grapher = CodeGrapherMS()
    print("Service ready:", grapher)
    graph_data = grapher.scan_directory(target_dir)
    
    print(f"\n--- Scan Complete ---")
    print(f"Nodes Found: {graph_data['node_count']}")
    print(f"Edges Built: {graph_data['edge_count']}")
    
    # Save to JSON for inspection
    out_file = "code_graph_dump.json"
    with open(out_file, "w") as f:
        json.dump(graph_data, f, indent=2)
    print(f"Graph saved to {out_file}")

--------------------------------------------------------------------------------
FILE: _CodeGrapherMS\app.py.bak
--------------------------------------------------------------------------------
import ast
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="CodeGrapher",
version="1.0.0",
description="Parses Python code to extract symbols (nodes) and call relationships (edges).",
tags=["parsing", "graph", "analysis"],
capabilities=["filesystem:read"]
)
class CodeGrapherMS:
    """
    The Cartographer of Logic: Parses Python code to extract high-level 
    symbols (classes, functions) and maps their 'Call' relationships.
    
    Output: A graph structure (Nodes + Edges) suitable for visualization 
    or dependency analysis.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
    self.nodes = [] # List of symbols (functions, classes)
    self.edges = [] # List of relationships (source -> target)

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"graph_data": "Dict[str, Any]"},
    description="Recursively scans a directory for .py files and builds the graph.",
    tags=["parsing", "graph"],
    side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Dict[str, Any]:
    """
    Recursively scans a directory for .py files and builds the graph.
    """
        root = Path(root_path).resolve()
        self.nodes = []
        self.edges = []
        
        if not root.exists():
            return {"error": f"Path {root} does not exist"}

        # 1. Parsing Pass (Create Nodes)
        for path in root.rglob("*.py"):
            try:
                # Skip hidden/venv folders
                if any(p.startswith('.') for p in path.parts) or "venv" in path.parts:
                    continue
                    
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    source = f.read()
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_symbols = self._parse_source(source, rel_path)
                self.nodes.extend(file_symbols)
                
            except Exception as e:
                print(f"Failed to parse {path.name}: {e}")

        # 2. Linking Pass (Create Edges)
        self._build_edges()

        return {
            "root": str(root),
            "node_count": len(self.nodes),
            "edge_count": len(self.edges),
            "nodes": self.nodes,
            "edges": self.edges
        }

    def _parse_source(self, source: str, file_path: str) -> List[Dict]:
        """
        Uses Python's AST to extract surgical symbol info.
        """
        try:
            tree = ast.parse(source)
        except SyntaxError:
            return []

        visitor = SurgicalVisitor(file_path)
        visitor.visit(tree)
        return visitor.symbols

    def _build_edges(self):
        """
        Resolves 'calls' strings into explicit graph edges.
        """
        # Create a quick lookup map: "my_function" -> NodeID
        # Note: This is a naive lookup (name collision possible). 
        # A robust version would use "module.class.method" fully qualified names.
        name_map = {n['name']: n['id'] for n in self.nodes}

        for node in self.nodes:
            source_id = node['id']
            calls = node.get('calls', [])
            
            for target_name in calls:
                if target_name in name_map:
                    target_id = name_map[target_name]
                    
                    # Avoid self-loops for cleanliness
                    if source_id != target_id:
                        self.edges.append({
                            "source": source_id,
                            "target": target_id,
                            "type": "calls"
                        })

# --- Helper Class: The AST Walker ---

class SurgicalVisitor(ast.NodeVisitor):
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.symbols = []

    def visit_FunctionDef(self, node):
        self._handle_func(node, "function")

    def visit_AsyncFunctionDef(self, node):
        self._handle_func(node, "async_function")

    def visit_ClassDef(self, node):
        # Record the class
        class_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": class_id,
            "file": self.file_path,
            "name": node.name,
            "type": "class",
            "line": node.lineno,
            "calls": [] # Classes don't 'call' things directly usually, their methods do
        })
        # Visit children (methods)
        self.generic_visit(node)

    def _handle_func(self, node, type_name):
        # Extract outgoing calls from the function body
        calls = []
        for child in ast.walk(node):
            if isinstance(child, ast.Call):
                if isinstance(child.func, ast.Name):
                    calls.append(child.func.id)
                elif isinstance(child.func, ast.Attribute):
                    calls.append(child.func.attr)
        
        unique_calls = list(set(calls))
        
        node_id = f"{self.file_path}::{node.name}"
        self.symbols.append({
            "id": node_id,
            "file": self.file_path,
            "name": node.name,
            "type": type_name,
            "line": node.lineno,
            "calls": unique_calls
        })

# --- Independent Test Block ---
if __name__ == "__main__":
    import sys
    
    # Defaults to current directory
    target_dir = sys.argv[1] if len(sys.argv) > 1 else "."
    
    print(f"Mapping Logic in: {target_dir}")
    grapher = CodeGrapherMS()
    print("Service ready:", grapher)
    graph_data = grapher.scan_directory(target_dir)
    
    print(f"\n--- Scan Complete ---")
    print(f"Nodes Found: {graph_data['node_count']}")
    print(f"Edges Built: {graph_data['edge_count']}")
    
    # Save to JSON for inspection
    out_file = "code_graph_dump.json"
    with open(out_file, "w") as f:
        json.dump(graph_data, f, indent=2)
    print(f"Graph saved to {out_file}")

--------------------------------------------------------------------------------
FILE: _CognitiveMemoryMS\app.py
--------------------------------------------------------------------------------
import uuid
import json
import logging
import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_MEMORY_FILE = Path("working_memory.jsonl")
FLUSH_THRESHOLD = 5  # Number of turns before summarizing to Long Term Memory
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("CognitiveMem")
# ==============================================================================

class MemoryEntry(BaseModel):
    """Atomic unit of memory."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)
    role: str # 'user', 'assistant', 'system', 'tool'
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

@service_metadata(
name="CognitiveMemory",
version="1.0.0",
description="Manages Short-Term (Working) Memory and orchestrates flushing to Long-Term Memory.",
tags=["memory", "history", "context"],
capabilities=["filesystem:read", "filesystem:write"]
)
class CognitiveMemoryMS:
    """
The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
flushing to Long-Term Memory (Vector Store).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.file_path = Path(self.config.get("persistence_path", DEFAULT_MEMORY_FILE))
self.summarizer = self.config.get("summarizer_func")
self.ingestor = self.config.get("long_term_ingest_func")
        
self.working_memory: List[MemoryEntry] = []
self._load_working_memory()

    # --- Working Memory Operations ---

    @service_endpoint(
    inputs={"role": "str", "content": "str", "metadata": "Dict"},
    outputs={"entry": "MemoryEntry"},
    description="Adds an item to working memory and persists it.",
    tags=["memory", "write"],
    side_effects=["filesystem:write"]
    )
    def add_entry(self, role: str, content: str, metadata: Dict = None) -> MemoryEntry:
    """Adds an item to working memory and persists it."""
        entry = MemoryEntry(role=role, content=content, metadata=metadata or {})
        self.working_memory.append(entry)
        self._append_to_file(entry)
        log.info(f"Added memory: [{role}] {content[:30]}...")
        return entry

    @service_endpoint(
    inputs={"limit": "int"},
    outputs={"context": "str"},
    description="Returns the most recent conversation history formatted for an LLM.",
    tags=["memory", "read", "llm"],
    side_effects=["filesystem:read"]
    )
    def get_context(self, limit: int = 10) -> str:
    """
    Returns the most recent conversation history formatted for an LLM.
    """
        recent = self.working_memory[-limit:]
        return "\n".join([f"{e.role.upper()}: {e.content}" for e in recent])

    def get_full_history(self) -> List[Dict]:
        """Returns the raw list of memory objects."""
        return [e.dict() for e in self.working_memory]

    # --- Consolidation (The "Sleep" Cycle) ---

    @service_endpoint(
    inputs={},
    outputs={},
    description="Signals that a turn is complete; checks if memory flush is needed.",
    tags=["memory", "maintenance"],
    side_effects=["filesystem:write"]
def commit_turn(self):
        """
        Signal that a "Turn" (User + AI response) is complete.
        Checks if memory is full and triggers a flush if needed.
        """
        if len(self.working_memory) >= FLUSH_THRESHOLD:
            self._flush_to_long_term()

    def _flush_to_long_term(self):
        """
        Compresses working memory into a summary and moves it to Long-Term storage.
        """
        if not self.summarizer or not self.ingestor:
            log.warning("Flush triggered but Summarizer/Ingestor not configured. Skipping.")
            return

        log.info("🌀 Flushing Working Memory to Long-Term Storage...")
        
        # 1. Combine Text
        full_text = "\n".join([f"{e.role}: {e.content}" for e in self.working_memory])
        
        # 2. Summarize
        try:
            summary = self.summarizer(full_text)
            log.info(f"Summary generated: {summary[:50]}...")
        except Exception as e:
            log.error(f"Summarization failed: {e}")
            return

        # 3. Ingest into Vector DB
        try:
            meta = {
                "source": "cognitive_memory_flush", 
                "date": datetime.datetime.utcnow().isoformat(),
                "original_entry_count": len(self.working_memory)
            }
            self.ingestor(summary, meta)
            log.info("✅ Saved to Long-Term Memory.")
        except Exception as e:
            log.error(f"Ingestion failed: {e}")
            return

        # 4. Clear Working Memory (but keep file history or archive it?)
        # For this pattern, we clear the 'Active' RAM, and maybe rotate the log file.
        self.working_memory.clear()
        self._rotate_log_file()

    # --- Persistence Helpers ---

    def _load_working_memory(self):
        """Rehydrates memory from the JSONL file."""
        if not self.file_path.exists():
            return
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.working_memory.append(MemoryEntry.parse_raw(line))
            log.info(f"Loaded {len(self.working_memory)} items from {self.file_path}")
        except Exception as e:
            log.error(f"Corrupt memory file: {e}")

    def _append_to_file(self, entry: MemoryEntry):
        """Appends a single entry to the JSONL log."""
        with open(self.file_path, 'a', encoding='utf-8') as f:
            f.write(entry.json() + "\n")

    def _rotate_log_file(self):
        """Renames the current log to an archive timestamp."""
        if self.file_path.exists():
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = self.file_path.with_name(f"memory_archive_{timestamp}.jsonl")
            self.file_path.rename(archive_name)
            log.info(f"Rotated memory log to {archive_name}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
# 1. Setup Mock Dependencies
def mock_summarizer(text):
    return f"SUMMARY OF {len(text)} CHARS: The user and AI discussed AI architecture."

def mock_ingest(text, metadata):
    print(f"\n[VectorDB] Indexing: '{text}'\n[VectorDB] Meta: {metadata}")

# 2. Initialize
print("--- Initializing Cognitive Memory ---")
mem = CognitiveMemoryMS({
    "summarizer_func": mock_summarizer,
    "long_term_ingest_func": mock_ingest
})
print("Service ready:", mem)

# 3. Simulate Conversation
print("\n--- Simulating Conversation ---")
mem.add_entry("user", "Hello, who are you?")
mem.add_entry("assistant", "I am a Cognitive Agent.")
mem.add_entry("user", "What is your memory capacity?")
mem.add_entry("assistant", "I have a tiered memory system.")
mem.add_entry("user", "That sounds complex.")

print(f"\nCurrent Context:\n{mem.get_context()}")

# 4. Trigger Flush (Threshold is 5)
print("\n--- Triggering Memory Flush ---")
mem.commit_turn() # Should trigger flush because count is 5

print(f"\nWorking Memory after flush: {len(mem.working_memory)} items")

# Cleanup
if Path("working_memory.jsonl").exists():
    os.remove("working_memory.jsonl")
# Clean up archives if any were made
for p in Path(".").glob("memory_archive_*.jsonl"):
    os.remove(p)

--------------------------------------------------------------------------------
FILE: _CognitiveMemoryMS\app.py.bak
--------------------------------------------------------------------------------
import uuid
import json
import logging
import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DEFAULT_MEMORY_FILE = Path("working_memory.jsonl")
FLUSH_THRESHOLD = 5  # Number of turns before summarizing to Long Term Memory
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("CognitiveMem")
# ==============================================================================

class MemoryEntry(BaseModel):
    """Atomic unit of memory."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)
    role: str # 'user', 'assistant', 'system', 'tool'
    content: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

@service_metadata(
name="CognitiveMemory",
version="1.0.0",
description="Manages Short-Term (Working) Memory and orchestrates flushing to Long-Term Memory.",
tags=["memory", "history", "context"],
capabilities=["filesystem:read", "filesystem:write"]
)
class CognitiveMemoryMS:
    """
The Hippocampus: Manages Short-Term (Working) Memory and orchestrates 
flushing to Long-Term Memory (Vector Store).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.file_path = Path(self.config.get("persistence_path", DEFAULT_MEMORY_FILE))
self.summarizer = self.config.get("summarizer_func")
self.ingestor = self.config.get("long_term_ingest_func")
        
self.working_memory: List[MemoryEntry] = []
self._load_working_memory()

    # --- Working Memory Operations ---

    @service_endpoint(
    inputs={"role": "str", "content": "str", "metadata": "Dict"},
    outputs={"entry": "MemoryEntry"},
    description="Adds an item to working memory and persists it.",
    tags=["memory", "write"],
    side_effects=["filesystem:write"]
    )
    def add_entry(self, role: str, content: str, metadata: Dict = None) -> MemoryEntry:
    """Adds an item to working memory and persists it."""
        entry = MemoryEntry(role=role, content=content, metadata=metadata or {})
        self.working_memory.append(entry)
        self._append_to_file(entry)
        log.info(f"Added memory: [{role}] {content[:30]}...")
        return entry

    @service_endpoint(
    inputs={"limit": "int"},
    outputs={"context": "str"},
    description="Returns the most recent conversation history formatted for an LLM.",
    tags=["memory", "read", "llm"],
    side_effects=["filesystem:read"]
    )
    def get_context(self, limit: int = 10) -> str:
    """
    Returns the most recent conversation history formatted for an LLM.
    """
        recent = self.working_memory[-limit:]
        return "\n".join([f"{e.role.upper()}: {e.content}" for e in recent])

    def get_full_history(self) -> List[Dict]:
        """Returns the raw list of memory objects."""
        return [e.dict() for e in self.working_memory]

    # --- Consolidation (The "Sleep" Cycle) ---

    @service_endpoint(
    inputs={},
    outputs={},
    description="Signals that a turn is complete; checks if memory flush is needed.",
    tags=["memory", "maintenance"],
    side_effects=["filesystem:write"]
    )
    def commit_turn(self):
    """
    Signal that a "Turn" (User + AI response) is complete.
    Checks if memory is full and triggers a flush if needed.
    """
        if len(self.working_memory) >= FLUSH_THRESHOLD:
            self._flush_to_long_term()

    def _flush_to_long_term(self):
        """
        Compresses working memory into a summary and moves it to Long-Term storage.
        """
        if not self.summarizer or not self.ingestor:
            log.warning("Flush triggered but Summarizer/Ingestor not configured. Skipping.")
            return

        log.info("🌀 Flushing Working Memory to Long-Term Storage...")
        
        # 1. Combine Text
        full_text = "\n".join([f"{e.role}: {e.content}" for e in self.working_memory])
        
        # 2. Summarize
        try:
            summary = self.summarizer(full_text)
            log.info(f"Summary generated: {summary[:50]}...")
        except Exception as e:
            log.error(f"Summarization failed: {e}")
            return

        # 3. Ingest into Vector DB
        try:
            meta = {
                "source": "cognitive_memory_flush", 
                "date": datetime.datetime.utcnow().isoformat(),
                "original_entry_count": len(self.working_memory)
            }
            self.ingestor(summary, meta)
            log.info("✅ Saved to Long-Term Memory.")
        except Exception as e:
            log.error(f"Ingestion failed: {e}")
            return

        # 4. Clear Working Memory (but keep file history or archive it?)
        # For this pattern, we clear the 'Active' RAM, and maybe rotate the log file.
        self.working_memory.clear()
        self._rotate_log_file()

    # --- Persistence Helpers ---

    def _load_working_memory(self):
        """Rehydrates memory from the JSONL file."""
        if not self.file_path.exists():
            return
        
        try:
            with open(self.file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        self.working_memory.append(MemoryEntry.parse_raw(line))
            log.info(f"Loaded {len(self.working_memory)} items from {self.file_path}")
        except Exception as e:
            log.error(f"Corrupt memory file: {e}")

    def _append_to_file(self, entry: MemoryEntry):
        """Appends a single entry to the JSONL log."""
        with open(self.file_path, 'a', encoding='utf-8') as f:
            f.write(entry.json() + "\n")

    def _rotate_log_file(self):
        """Renames the current log to an archive timestamp."""
        if self.file_path.exists():
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = self.file_path.with_name(f"memory_archive_{timestamp}.jsonl")
            self.file_path.rename(archive_name)
            log.info(f"Rotated memory log to {archive_name}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup Mock Dependencies
    def mock_summarizer(text):
        return f"SUMMARY OF {len(text)} CHARS: The user and AI discussed AI architecture."

    def mock_ingest(text, metadata):
        print(f"\n[VectorDB] Indexing: '{text}'\n[VectorDB] Meta: {metadata}")

    # 2. Initialize
    print("--- Initializing Cognitive Memory ---")
    mem = CognitiveMemoryMS({
    "summarizer_func": mock_summarizer,
    "long_term_ingest_func": mock_ingest
    })
    print("Service ready:", mem)

    # 3. Simulate Conversation
    print("\n--- Simulating Conversation ---")
    mem.add_entry("user", "Hello, who are you?")
    mem.add_entry("assistant", "I am a Cognitive Agent.")
    mem.add_entry("user", "What is your memory capacity?")
    mem.add_entry("assistant", "I have a tiered memory system.")
    mem.add_entry("user", "That sounds complex.")
    
    print(f"\nCurrent Context:\n{mem.get_context()}")

    # 4. Trigger Flush (Threshold is 5)
    print("\n--- Triggering Memory Flush ---")
    mem.commit_turn() # Should trigger flush because count is 5
    
    print(f"\nWorking Memory after flush: {len(mem.working_memory)} items")
    
    # Cleanup
    if Path("working_memory.jsonl").exists():
        os.remove("working_memory.jsonl")
    # Clean up archives if any were made
    for p in Path(".").glob("memory_archive_*.jsonl"):
        os.remove(p)

--------------------------------------------------------------------------------
FILE: _CognitiveMemoryMS\requirements.txt
--------------------------------------------------------------------------------
pip install pydantic
--------------------------------------------------------------------------------
FILE: _ContextAggregatorMS\app.py
--------------------------------------------------------------------------------
import os
import fnmatch
import datetime
from pathlib import Path
from typing import Set, Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
# Extensions known to be binary/non-text (Images, Archives, Executables)
DEFAULT_BINARY_EXTENSIONS = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".mp4", ".mkv", ".avi", ".mov", ".webm",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".exe", ".dll", ".so",
    ".db", ".sqlite", ".mdb", ".pyc", ".pyo", ".class", ".jar", ".wasm"
}

# Folders to ignore by default
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", ".env", 
    "dist", "build", "coverage", ".idea", ".vscode"
}
# ==============================================================================

@service_metadata(
name="ContextAggregator",
version="1.0.0",
description="Flattens a project folder into a single readable text file.",
tags=["filesystem", "context", "compilation"],
capabilities=["filesystem:read", "filesystem:write"]
)
class ContextAggregatorMS:
    """
The Context Builder: Flattens a project folder into a single readable text file.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
max_file_size_mb = self.config.get("max_file_size_mb", 1)
self.max_file_size_bytes = max_file_size_mb * 1024 * 1024

    @service_endpoint(
    inputs={"root_path": "str", "output_file": "str", "extra_exclusions": "Set[str]", "use_default_exclusions": "bool"},
    outputs={"file_count": "int"},
    description="Aggregates project files into a single text dump.",
    tags=["filesystem", "dump"],
    side_effects=["filesystem:read", "filesystem:write"]
    )
def aggregate(self, 
    root_path: str, 
    output_file: str, 
    extra_exclusions: Optional[Set[str]] = None,
    use_default_exclusions: bool = True) -> int:
        
        project_root = Path(root_path).resolve()
        out_path = Path(output_file).resolve()
        
        # Build Exclusions
        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_IGNORE_DIRS)
        if extra_exclusions:
            exclusions.update(extra_exclusions)

        # Build Binary List
        binary_exts = DEFAULT_BINARY_EXTENSIONS.copy() # Always keep binaries as base unless manually cleared
        
        file_count = 0
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.write(f"File Dump from Project: {project_root.name}\nGenerated: {timestamp}\n{'='*60}\n\n")

                for root, dirs, files in os.walk(project_root):
                    dirs[:] = [d for d in dirs if d not in exclusions]
                    
                    for filename in files:
                        if self._should_exclude(filename, exclusions): continue

                        file_path = Path(root) / filename
                        if file_path.resolve() == out_path: continue

                        if self._is_safe_to_dump(file_path, binary_exts):
                            self._write_file_content(out_f, file_path, project_root)
                            file_count += 1                            
        except IOError as e: print(f"Error writing dump: {e}")
return file_count

    def _should_exclude(self, filename: str, exclusions: Set[str]) -> bool:
        return any(fnmatch.fnmatch(filename, pattern) for pattern in exclusions)

    def _is_safe_to_dump(self, file_path: Path, binary_exts: Set[str]) -> bool:
        if "".join(file_path.suffixes).lower() in binary_exts: return False
        try:
            if file_path.stat().st_size > self.max_file_size_bytes: return False
            with open(file_path, 'rb') as f:
                if b'\0' in f.read(1024): return False
        except (IOError, OSError): return False
        return True

    def _write_file_content(self, out_f, file_path: Path, project_root: Path):
        relative_path = file_path.relative_to(project_root)
        header = f"\n{'-'*20} FILE: {relative_path} {'-'*20}\n"
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as in_f:
                out_f.write(header + in_f.read() + f"\n{'-'*60}\n")
        except Exception as e:
            out_f.write(f"\n[Error reading file: {e}]\n")

if __name__ == "__main__":
    svc = ContextAggregatorMS()
    print("Service ready:", svc)

--------------------------------------------------------------------------------
FILE: _ContextAggregatorMS\app.py.bak
--------------------------------------------------------------------------------
import os
import fnmatch
import datetime
from pathlib import Path
from typing import Set, Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
# Extensions known to be binary/non-text (Images, Archives, Executables)
DEFAULT_BINARY_EXTENSIONS = {
    ".tar.gz", ".gz", ".zip", ".rar", ".7z", ".bz2", ".xz", ".tgz",
    ".png", ".jpg", ".jpeg", ".gif", ".bmp", ".ico", ".webp", ".tif", ".tiff",
    ".mp3", ".wav", ".ogg", ".flac", ".mp4", ".mkv", ".avi", ".mov", ".webm",
    ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx", ".exe", ".dll", ".so",
    ".db", ".sqlite", ".mdb", ".pyc", ".pyo", ".class", ".jar", ".wasm"
}

# Folders to ignore by default
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", ".env", 
    "dist", "build", "coverage", ".idea", ".vscode"
}
# ==============================================================================

@service_metadata(
name="ContextAggregator",
version="1.0.0",
description="Flattens a project folder into a single readable text file.",
tags=["filesystem", "context", "compilation"],
capabilities=["filesystem:read", "filesystem:write"]
)
class ContextAggregatorMS:
    """
The Context Builder: Flattens a project folder into a single readable text file.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
max_file_size_mb = self.config.get("max_file_size_mb", 1)
self.max_file_size_bytes = max_file_size_mb * 1024 * 1024

    @service_endpoint(
    inputs={"root_path": "str", "output_file": "str", "extra_exclusions": "Set[str]", "use_default_exclusions": "bool"},
    outputs={"file_count": "int"},
    description="Aggregates project files into a single text dump.",
    tags=["filesystem", "dump"],
    side_effects=["filesystem:read", "filesystem:write"]
    )
    def aggregate(self, 
    root_path: str, 
    output_file: str, 
    extra_exclusions: Optional[Set[str]] = None,
    use_default_exclusions: bool = True) -> int:
        
        project_root = Path(root_path).resolve()
        out_path = Path(output_file).resolve()
        
        # Build Exclusions
        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_IGNORE_DIRS)
        if extra_exclusions:
            exclusions.update(extra_exclusions)

        # Build Binary List
        binary_exts = DEFAULT_BINARY_EXTENSIONS.copy() # Always keep binaries as base unless manually cleared
        
        file_count = 0
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        try:
            with open(out_path, "w", encoding="utf-8") as out_f:
                out_f.write(f"File Dump from Project: {project_root.name}\nGenerated: {timestamp}\n{'='*60}\n\n")

                for root, dirs, files in os.walk(project_root):
                    dirs[:] = [d for d in dirs if d not in exclusions]
                    
                    for filename in files:
                        if self._should_exclude(filename, exclusions): continue

                        file_path = Path(root) / filename
                        if file_path.resolve() == out_path: continue

                        if self._is_safe_to_dump(file_path, binary_exts):
                            self._write_file_content(out_f, file_path, project_root)
                            file_count += 1
                            
        except IOError as e: print(f"Error writing dump: {e}")
        return file_count

    def _should_exclude(self, filename: str, exclusions: Set[str]) -> bool:
        return any(fnmatch.fnmatch(filename, pattern) for pattern in exclusions)

    def _is_safe_to_dump(self, file_path: Path, binary_exts: Set[str]) -> bool:
        if "".join(file_path.suffixes).lower() in binary_exts: return False
        try:
            if file_path.stat().st_size > self.max_file_size_bytes: return False
            with open(file_path, 'rb') as f:
                if b'\0' in f.read(1024): return False
        except (IOError, OSError): return False
        return True

    def _write_file_content(self, out_f, file_path: Path, project_root: Path):
        relative_path = file_path.relative_to(project_root)
        header = f"\n{'-'*20} FILE: {relative_path} {'-'*20}\n"
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as in_f:
                out_f.write(header + in_f.read() + f"\n{'-'*60}\n")
        except Exception as e:
            out_f.write(f"\n[Error reading file: {e}]\n")

if __name__ == "__main__":
svc = ContextAggregatorMS()
print("Service ready:", svc)

--------------------------------------------------------------------------------
FILE: _DiffEngineMS\app.py
--------------------------------------------------------------------------------
import sqlite3
import difflib
import datetime
import uuid
import logging
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "diff_engine.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("DiffEngine")
# ==============================================================================

@service_metadata(
name="DiffEngine",
version="1.0.0",
description="Implements hybrid versioning (Head + Diff History) for file content.",
tags=["version-control", "diff", "db"],
capabilities=["db:sqlite", "filesystem:write"]
)
class DiffEngineMS:
    """
The Timekeeper: Implements a 'Hybrid' versioning architecture.
1. HEAD: Stores full current content for fast read access (UI/RAG).
2. HISTORY: Stores diff deltas using difflib for audit trails.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = Path(self.config.get("db_path", DB_PATH))
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. The Head (Fast Access Cache)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id TEXT PRIMARY KEY,
                    path TEXT UNIQUE NOT NULL,
                    content TEXT,
                    last_updated TIMESTAMP
                )
            """)
            
            # 2. The Rising Edge (Diff History)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS diff_log (
                    id TEXT PRIMARY KEY,
                    file_id TEXT NOT NULL,
                    timestamp TIMESTAMP,
                    change_type TEXT,  -- 'CREATE', 'EDIT', 'DELETE'
                    diff_blob TEXT,    -- The text output of difflib
                    author TEXT,
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)

    # --- Core Workflow ---

    @service_endpoint(
    inputs={"path": "str", "new_content": "str", "author": "str"},
    outputs={"status": "str", "file_id": "str"},
    description="Updates a file, creating a diff history entry and updating the head state.",
    tags=["version-control", "write"],
    side_effects=["db:write"]
    )
    def update_file(self, path: str, new_content: str, author: str = "agent") -> Dict[str, Any]:
    """
    The Atomic Update Operation:
    1. Checks current state.
        2. Calculates Diff.
        3. Writes Diff to History.
        4. Updates Head to New Content.
        
        Returns: Dict with status (changed/unchanged), file_id, and diff_summary.
        """
        path = str(Path(path).as_posix()) # Normalize path
        now = datetime.datetime.utcnow()
        
        with self._get_conn() as conn:
            # 1. Fetch Head
            row = conn.execute("SELECT id, content FROM files WHERE path = ?", (path,)).fetchone()
if not row:
                # --- CASE: NEW FILE ---
                file_id = str(uuid.uuid4())
                conn.execute(
                    "INSERT INTO files (id, path, content, last_updated) VALUES (?, ?, ?, ?)",
                    (file_id, path, new_content, now)
                )
                self._log_diff(conn, file_id, "CREATE", "[New File Created]", author, now)
                log.info(f"Created new file: {path}")
                return {"status": "created", "file_id": file_id}

            # --- CASE: EXISTING FILE ---
            file_id = row['id']
            old_content = row['content'] or ""

            # 2. Calculate Diff
            # difflib needs lists of lines
            old_lines = old_content.splitlines(keepends=True)
            new_lines = new_content.splitlines(keepends=True)

            # Standard unified diff
            diff_gen = difflib.unified_diff(
                old_lines, new_lines, 
                fromfile=f"a/{path}", tofile=f"b/{path}",
                lineterm=''
            )
            diff_text = "".join(diff_gen)

            if not diff_text:
                return {"status": "unchanged", "file_id": file_id}

            # 3. Write History
            self._log_diff(conn, file_id, "EDIT", diff_text, author, now)

            # 4. Update Head
            conn.execute(
                "UPDATE files SET content = ?, last_updated = ? WHERE id = ?",
                (new_content, now, file_id)
            )
            log.info(f"Updated file: {path}")
            return {"status": "updated", "file_id": file_id, "diff_size": len(diff_text)}

    def _log_diff(self, conn, file_id, change_type, diff_text, author, timestamp):
        diff_id = str(uuid.uuid4())
        conn.execute(
            "INSERT INTO diff_log (id, file_id, timestamp, change_type, diff_blob, author) VALUES (?, ?, ?, ?, ?, ?)",
            (diff_id, file_id, timestamp, change_type, diff_text, author)
        )

    # --- Retrieval ---

    @service_endpoint(
    inputs={"path": "str"},
    outputs={"content": "Optional[str]"},
    description="Fast retrieval of current content.",
    tags=["version-control", "read"],
    side_effects=["db:read"]
    )
    def get_head(self, path: str) -> Optional[str]:
    """Fast retrieval of current content."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT content FROM files WHERE path = ?", (path,)).fetchone()
            return row['content'] if row else None

    @service_endpoint(
    inputs={"path": "str"},
    outputs={"history": "List[Dict]"},
    description="Retrieves the full evolution history of a file.",
    tags=["version-control", "read"],
    side_effects=["db:read"]
    )
    def get_history(self, path: str) -> List[Dict]:
    """Retrieves the full evolution history of a file."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT id FROM files WHERE path = ?", (path,)).fetchone()
            if not row: return []
            
            rows = conn.execute(
                "SELECT timestamp, change_type, diff_blob, author FROM diff_log WHERE file_id = ? ORDER BY timestamp DESC",
                (row['id'],)
            ).fetchall()
            
            return [dict(r) for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
import os
if DB_PATH.exists(): os.remove(DB_PATH)
    
engine = DiffEngineMS()
print("Service ready:", engine)
    
    print("--- 1. Creating File ---")
    engine.update_file("notes.txt", "Todo List:\n1. Buy Milk\n")
    
    print("\n--- 2. Updating File (The Rising Edge) ---")
    # Change: Add 'Buy Eggs', Remove 'Buy Milk' (Simulating a replacement)
    new_text = "Todo List:\n1. Buy Eggs\n2. Code Python\n"
    res = engine.update_file("notes.txt", new_text, author="Jacob")
    
    print(f"Update Result: {res['status']}")
    
    print("\n--- 3. Inspecting History ---")
    history = engine.get_history("notes.txt")
    for event in history:
        print(f"\n[{event['timestamp']}] {event['change_type']} by {event['author']}")
        print(f"Diff Preview:\n{event['diff_blob'].strip()}")

    print("\n--- 4. Inspecting Head (Cache) ---")
    print(engine.get_head("notes.txt"))
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)


--------------------------------------------------------------------------------
FILE: _DiffEngineMS\app.py.bak
--------------------------------------------------------------------------------
import sqlite3
import difflib
import datetime
import uuid
import logging
from pathlib import Path
from typing import Optional, Dict, List, Tuple, Any
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "diff_engine.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("DiffEngine")
# ==============================================================================

@service_metadata(
name="DiffEngine",
version="1.0.0",
description="Implements hybrid versioning (Head + Diff History) for file content.",
tags=["version-control", "diff", "db"],
capabilities=["db:sqlite", "filesystem:write"]
)
class DiffEngineMS:
    """
The Timekeeper: Implements a 'Hybrid' versioning architecture.
1. HEAD: Stores full current content for fast read access (UI/RAG).
2. HISTORY: Stores diff deltas using difflib for audit trails.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = Path(self.config.get("db_path", DB_PATH))
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. The Head (Fast Access Cache)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id TEXT PRIMARY KEY,
                    path TEXT UNIQUE NOT NULL,
                    content TEXT,
                    last_updated TIMESTAMP
                )
            """)
            
            # 2. The Rising Edge (Diff History)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS diff_log (
                    id TEXT PRIMARY KEY,
                    file_id TEXT NOT NULL,
                    timestamp TIMESTAMP,
                    change_type TEXT,  -- 'CREATE', 'EDIT', 'DELETE'
                    diff_blob TEXT,    -- The text output of difflib
                    author TEXT,
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)

    # --- Core Workflow ---

    @service_endpoint(
    inputs={"path": "str", "new_content": "str", "author": "str"},
    outputs={"status": "str", "file_id": "str"},
    description="Updates a file, creating a diff history entry and updating the head state.",
    tags=["version-control", "write"],
    side_effects=["db:write"]
    )
    def update_file(self, path: str, new_content: str, author: str = "agent") -> Dict[str, Any]:
    """
    The Atomic Update Operation:
    1. Checks current state.
        2. Calculates Diff.
        3. Writes Diff to History.
        4. Updates Head to New Content.
        
        Returns: Dict with status (changed/unchanged), file_id, and diff_summary.
        """
        path = str(Path(path).as_posix()) # Normalize path
        now = datetime.datetime.utcnow()
        
        with self._get_conn() as conn:
            # 1. Fetch Head
            row = conn.execute("SELECT id, content FROM files WHERE path = ?", (path,)).fetchone()
            
            if not row:
                # --- CASE: NEW FILE ---
                file_id = str(uuid.uuid4())
                conn.execute(
                    "INSERT INTO files (id, path, content, last_updated) VALUES (?, ?, ?, ?)",
                    (file_id, path, new_content, now)
                )
                self._log_diff(conn, file_id, "CREATE", "[New File Created]", author, now)
                log.info(f"Created new file: {path}")
                return {"status": "created", "file_id": file_id}

            # --- CASE: EXISTING FILE ---
            file_id = row['id']
            old_content = row['content'] or ""
            
            # 2. Calculate Diff
            # difflib needs lists of lines
            old_lines = old_content.splitlines(keepends=True)
            new_lines = new_content.splitlines(keepends=True)
            
            # Standard unified diff
            diff_gen = difflib.unified_diff(
                old_lines, new_lines, 
                fromfile=f"a/{path}", tofile=f"b/{path}",
                lineterm=''
            )
            diff_text = "".join(diff_gen)

            if not diff_text:
                return {"status": "unchanged", "file_id": file_id}

            # 3. Write History
            self._log_diff(conn, file_id, "EDIT", diff_text, author, now)

            # 4. Update Head
            conn.execute(
                "UPDATE files SET content = ?, last_updated = ? WHERE id = ?",
                (new_content, now, file_id)
            )
            log.info(f"Updated file: {path}")
            return {"status": "updated", "file_id": file_id, "diff_size": len(diff_text)}

    def _log_diff(self, conn, file_id, change_type, diff_text, author, timestamp):
        diff_id = str(uuid.uuid4())
        conn.execute(
            "INSERT INTO diff_log (id, file_id, timestamp, change_type, diff_blob, author) VALUES (?, ?, ?, ?, ?, ?)",
            (diff_id, file_id, timestamp, change_type, diff_text, author)
        )

    # --- Retrieval ---

    @service_endpoint(
    inputs={"path": "str"},
    outputs={"content": "Optional[str]"},
    description="Fast retrieval of current content.",
    tags=["version-control", "read"],
    side_effects=["db:read"]
    )
    def get_head(self, path: str) -> Optional[str]:
    """Fast retrieval of current content."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT content FROM files WHERE path = ?", (path,)).fetchone()
            return row['content'] if row else None

    @service_endpoint(
    inputs={"path": "str"},
    outputs={"history": "List[Dict]"},
    description="Retrieves the full evolution history of a file.",
    tags=["version-control", "read"],
    side_effects=["db:read"]
    )
    def get_history(self, path: str) -> List[Dict]:
    """Retrieves the full evolution history of a file."""
        with self._get_conn() as conn:
            row = conn.execute("SELECT id FROM files WHERE path = ?", (path,)).fetchone()
            if not row: return []
            
            rows = conn.execute(
                "SELECT timestamp, change_type, diff_blob, author FROM diff_log WHERE file_id = ? ORDER BY timestamp DESC",
                (row['id'],)
            ).fetchall()
            
            return [dict(r) for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
import os
if DB_PATH.exists(): os.remove(DB_PATH)
    
engine = DiffEngineMS()
print("Service ready:", engine)
    
    print("--- 1. Creating File ---")
    engine.update_file("notes.txt", "Todo List:\n1. Buy Milk\n")
    
    print("\n--- 2. Updating File (The Rising Edge) ---")
    # Change: Add 'Buy Eggs', Remove 'Buy Milk' (Simulating a replacement)
    new_text = "Todo List:\n1. Buy Eggs\n2. Code Python\n"
    res = engine.update_file("notes.txt", new_text, author="Jacob")
    
    print(f"Update Result: {res['status']}")
    
    print("\n--- 3. Inspecting History ---")
    history = engine.get_history("notes.txt")
    for event in history:
        print(f"\n[{event['timestamp']}] {event['change_type']} by {event['author']}")
        print(f"Diff Preview:\n{event['diff_blob'].strip()}")

    print("\n--- 4. Inspecting Head (Cache) ---")
    print(engine.get_head("notes.txt"))
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)


--------------------------------------------------------------------------------
FILE: _ExplorerWidgetMS\app.py
--------------------------------------------------------------------------------
import os
import queue
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional

import tkinter as tk
from tkinter import ttk

from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# USER CONFIGURATION: DEFAULTS
# ==============================================================================
DEFAULT_EXCLUDED_FOLDERS = {
    "node_modules",
    ".git",
    "__pycache__",
    ".venv",
    ".mypy_cache",
    "_logs",
    "dist",
    "build",
    ".vscode",
    ".idea",
    "target",
    "out",
    "bin",
    "obj",
    "Debug",
    "Release",
    "logs",
}
# ==============================================================================


@service_metadata(
    name="ExplorerWidget",
    version="1.0.0",
    description="A standalone file system tree viewer widget.",
    tags=["ui", "filesystem", "widget"],
    capabilities=["ui:gui", "filesystem:read"],
)
class ExplorerWidgetMS(ttk.Frame):
    """
    A standalone file system tree viewer.
    """

    GLYPH_CHECKED = "[X]"
    GLYPH_UNCHECKED = "[ ]"

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config_data: Dict[str, Any] = config or {}
        parent = self.config_data.get("parent")
        super().__init__(parent)

        self.root_path: Path = Path(
            self.config_data.get("root_path", ".")
        ).resolve()
        self.use_defaults: bool = self.config_data.get(
            "use_default_exclusions", True
        )

        # GUI coordination
        self.gui_queue: queue.Queue = queue.Queue()
        self.folder_item_states: Dict[str, str] = {}
        self.state_lock = threading.RLock()

        self._setup_styles()
        self._build_ui()
        self.process_gui_queue()
        self.refresh_tree()

    # ------------------------------------------------------------------ UI SETUP

    def _setup_styles(self) -> None:
        style = ttk.Style()
        if "clam" in style.theme_names():
            style.theme_use("clam")

        style.configure(
            "Explorer.Treeview",
            background="#252526",
            foreground="lightgray",
            fieldbackground="#252526",
            borderwidth=0,
            font=("Consolas", 10),
        )
        style.map(
            "Explorer.Treeview",
            background=[("selected", "#007ACC")],
            foreground=[("selected", "white")],
        )

    def _build_ui(self) -> None:
        self.columnconfigure(0, weight=1)
        self.rowconfigure(0, weight=1)

        self.tree = ttk.Treeview(
            self,
            show="tree",
            columns=("size",),
            selectmode="none",
            style="Explorer.Treeview",
        )
        self.tree.column("size", width=80, anchor="e")

        ysb = ttk.Scrollbar(self, orient="vertical", command=self.tree.yview)
        xsb = ttk.Scrollbar(self, orient="horizontal", command=self.tree.xview)

        self.tree.configure(yscrollcommand=ysb.set, xscrollcommand=xsb.set)

        self.tree.grid(row=0, column=0, sticky="nsew")
        ysb.grid(row=0, column=1, sticky="ns")
        xsb.grid(row=1, column=0, sticky="ew")

        self.tree.bind("<ButtonRelease-1>", self._on_click)

    # ----------------------------------------------------------------- SERVICE API

    @service_endpoint(
        inputs={},
        outputs={},
        description="Rescans the directory and refreshes the tree view.",
        tags=["ui", "refresh"],
        side_effects=["filesystem:read", "ui:update"],
    )
    def refresh_tree(self) -> None:
        # Clear the tree view
        for item in self.tree.get_children():
            self.tree.delete(item)

        # Reset state
        with self.state_lock:
            self.folder_item_states.clear()
            self.folder_item_states[str(self.root_path)] = "checked"

        # Build a flat list of tree items for insertion
        root_id = str(self.root_path)
        tree_data: List[Dict[str, Any]] = [
            {
                "parent": "",
                "iid": root_id,
                "text": f" {self.root_path.name} (Root)",
                "open": True,
            }
        ]

        self._scan_recursive(self.root_path, root_id, tree_data)

        # Insert into Treeview
        for item in tree_data:
            self.tree.insert(
                item["parent"],
                "end",
                iid=item["iid"],
                text=item["text"],
                open=item.get("open", False),
            )
            self.tree.set(item["iid"], "size", "...")

        # Update glyphs
        self._refresh_visuals(root_id)

        # Kick off background size calculation (currently stubbed)
        threading.Thread(
            target=self._calc_sizes_thread,
            args=(root_id,),
            daemon=True,
        ).start()

    # ----------------------------------------------------------------- INTERNALS

    def _scan_recursive(
        self, current_path: Path, parent_id: str, data_list: List[Dict[str, Any]]
    ) -> None:
        try:
            items = sorted(
                current_path.iterdir(),
                key=lambda x: (not x.is_dir(), x.name.lower()),
            )
            for item in items:
                if not item.is_dir():
                    continue

                path_str = str(item.resolve())

                state = "checked"
                if self.use_defaults and item.name in DEFAULT_EXCLUDED_FOLDERS:
                    state = "unchecked"

                with self.state_lock:
                    self.folder_item_states[path_str] = state

                data_list.append(
                    {"parent": parent_id, "iid": path_str, "text": f" {item.name}"}
                )
                self._scan_recursive(item, path_str, data_list)
        except (PermissionError, OSError):
            # Skip directories we can't read
            pass

    def _on_click(self, event: tk.Event) -> None:
        item_id = self.tree.identify_row(event.y)
        if not item_id:
            return

        with self.state_lock:
            curr = self.folder_item_states.get(item_id, "unchecked")
            self.folder_item_states[item_id] = (
                "checked" if curr == "unchecked" else "unchecked"
            )

        self._refresh_visuals(str(self.root_path))

    def _refresh_visuals(self, start_node: str) -> None:
        def _update(node_id: str) -> None:
            if not self.tree.exists(node_id):
                return

            with self.state_lock:
                state = self.folder_item_states.get(node_id, "unchecked")

            glyph = (
                self.GLYPH_CHECKED if state == "checked" else self.GLYPH_UNCHECKED
            )

            name = Path(node_id).name
            if node_id == str(self.root_path):
                name += " (Root)"

            self.tree.item(node_id, text=f"{glyph} {name}")

            for child in self.tree.get_children(node_id):
                _update(child)

        _update(start_node)

    def _calc_sizes_thread(self, root_id: str) -> None:
        """
        Background worker for calculating folder sizes.

        Currently a stub so that the thread exits cleanly without errors.
        You can later extend this to walk the filesystem and push
        size updates via self.gui_queue.
        """
        return

    # ----------------------------------------------------------------- SERVICE API

    @service_endpoint(
        inputs={},
        outputs={"selected_paths": "List[str]"},
        description="Returns a list of currently checked folder paths.",
        tags=["ui", "read"],
        side_effects=["ui:read"],
    )
    def get_selected_paths(self) -> List[str]:
        selected: List[str] = []
        with self.state_lock:
            for path, state in self.folder_item_states.items():
                if state == "checked":
                    selected.append(path)
        return selected

    # ------------------------------------------------------------------ GUI QUEUE

    def process_gui_queue(self) -> None:
        while not self.gui_queue.empty():
            try:
                callback = self.gui_queue.get_nowait()
            except queue.Empty:
                break
            else:
                try:
                    callback()
                except Exception:
                    # In production you might want logging here.
                    pass

        # Schedule next pump
        self.after(100, self.process_gui_queue)


if __name__ == "__main__":
    # Simple harness for manual testing.
    root = tk.Tk()
    root.title("ExplorerWidgetMS Test Harness")

    widget = ExplorerWidgetMS({"parent": root, "root_path": os.getcwd()})
    widget.pack(fill="both", expand=True)

    root.mainloop()

--------------------------------------------------------------------------------
FILE: _ExplorerWidgetMS\README.md
--------------------------------------------------------------------------------
# ExplorerWidgetMS

A Tk-based folder explorer microservice.  
Provides a selectable directory tree, checked-folder state, and service endpoints.  
Use `get_selected_paths()` to retrieve active folders.  
Embed as a widget or call via microservice runtime.

--------------------------------------------------------------------------------
FILE: _ExplorerWidgetMS\requirements.txt
--------------------------------------------------------------------------------
microservice-std-lib>=1.0.0

--------------------------------------------------------------------------------
FILE: _FingerprintScannerMS\app.py
--------------------------------------------------------------------------------
import hashlib
import os
import logging
from pathlib import Path
from typing import Any, Dict, List, Set, Optional, Tuple
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Folders to ignore during the scan (Standard developer noise)
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", 
    "dist", "build", "coverage", "target", "out", "bin", "obj",
    "_project_library", "_sandbox", "_logs"
}

# Files to ignore
DEFAULT_IGNORE_FILES = {
    ".DS_Store", "Thumbs.db", "*.log", "*.tmp", "*.lock"
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("Fingerprint")
# ==============================================================================

@service_metadata(
name="FingerprintScanner",
version="1.0.0",
description="Scans a directory tree and generates a deterministic SHA-256 fingerprint.",
tags=["scanning", "integrity", "hashing"],
capabilities=["filesystem:read"]
)
class FingerprintScannerMS:
    """
The Detective: Scans a directory tree and generates a deterministic
'Fingerprint' (SHA-256 Merkle Root) representing its exact state.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"state": "Dict[str, Any]"},
    description="Scans the project and returns a comprehensive state object (hashes + Merkle root).",
    tags=["scanning", "read"],
    side_effects=["filesystem:read"]
    )
def scan_project(self, root_path: str) -> Dict[str, Any]:
        """
        Scans the project and returns a comprehensive state object.
            output = {
                "root": str,
                "project_fingerprint": str (The global hash),
                "file_hashes": {rel_path: sha256},
                "file_count": int
            }
        """
        root = Path(root_path).resolve()
        if not root.exists():
            raise FileNotFoundError(f"Path not found: {root}")

        file_map = {}
        
        # 1. Walk and Hash
        # Use sorted() to ensure iteration order doesn't affect the final hash
        for path in sorted(root.rglob("*")):
            if path.is_file():
                if self._should_ignore(path, root):
                    continue
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_hash = self._hash_file(path)
                
                if file_hash:
                    file_map[rel_path] = file_hash

        # 2. Calculate Merkle Root (Global Fingerprint)
        # We sort by relative path to ensure deterministic ordering
        sorted_hashes = [file_map[p] for p in sorted(file_map.keys())]
        combined_data = "".join(sorted_hashes).encode('utf-8')
        project_fingerprint = hashlib.sha256(combined_data).hexdigest()

        log.info(f"Scanned {len(file_map)} files. Fingerprint: {project_fingerprint[:8]}...")

        return {
            "root": str(root),
            "project_fingerprint": project_fingerprint,
            "file_hashes": file_map,
            "file_count": len(file_map)
        }

    def _should_ignore(self, path: Path, root: Path) -> bool:
        """Checks path against exclusion lists."""
        try:
            rel_parts = path.relative_to(root).parts
            
            # Check directories
            # If any parent directory is in the ignore list, skip
            for part in rel_parts[:-1]: 
                if part in DEFAULT_IGNORE_DIRS:
                    return True
            
            # Check filename
            import fnmatch
            name = path.name
            if name in DEFAULT_IGNORE_FILES:
                return True
            if any(fnmatch.fnmatch(name, pat) for pat in DEFAULT_IGNORE_FILES):
                return True
                
            return False
        except ValueError:
            return True

    def _hash_file(self, path: Path) -> Optional[str]:
        """Reads file bytes and returns SHA256 hash."""
        try:
            # Read binary to avoid encoding issues and to hash exact content
            content = path.read_bytes()
            return hashlib.sha256(content).hexdigest()
        except (PermissionError, OSError):
            log.warning(f"Could not read/hash: {path}")
            return None

# --- Independent Test Block ---
if __name__ == "__main__":
    import time
    
    # 1. Create a dummy project
    test_dir = Path("test_fingerprint_proj")
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
    test_dir.mkdir()
    
    (test_dir / "main.py").write_text("print('hello')")
    (test_dir / "utils.py").write_text("def add(a,b): return a+b")
    
    scanner = FingerprintScannerMS()
    print("Service ready:", scanner)
    
    # 2. Initial Scan
    print("--- Scan 1 (Initial) ---")
    state_1 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 1: {state_1['project_fingerprint']}")
    
    # 3. Modify a file
    print("\n--- Modifying 'main.py' ---")
    time.sleep(0.1) # Ensure filesystem timestamp tick (though we hash content)
    (test_dir / "main.py").write_text("print('hello world')")
    
    # 4. Scan again
    print("--- Scan 2 (After Modification) ---")
    state_2 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 2: {state_2['project_fingerprint']}")
    
    # 5. Compare
    if state_1['project_fingerprint'] != state_2['project_fingerprint']:
        print("\n✅ SUCCESS: Fingerprint changed as expected.")
        # Find the diff
        for path, h in state_2['file_hashes'].items():
            if state_1['file_hashes'].get(path) != h:
                print(f"   Changed File: {path}")
    else:
        print("\n❌ FAILURE: Fingerprint did not change.")

    # Cleanup
if test_dir.exists():
    import shutil
    shutil.rmtree(test_dir)

--------------------------------------------------------------------------------
FILE: _FingerprintScannerMS\app.py.bak
--------------------------------------------------------------------------------
import hashlib
import os
import logging
from pathlib import Path
from typing import Any, Dict, List, Set, Optional, Tuple
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Folders to ignore during the scan (Standard developer noise)
DEFAULT_IGNORE_DIRS = {
    "node_modules", ".git", "__pycache__", ".venv", "venv", "env",
    ".mypy_cache", ".pytest_cache", ".idea", ".vscode", 
    "dist", "build", "coverage", "target", "out", "bin", "obj",
    "_project_library", "_sandbox", "_logs"
}

# Files to ignore
DEFAULT_IGNORE_FILES = {
    ".DS_Store", "Thumbs.db", "*.log", "*.tmp", "*.lock"
}

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("Fingerprint")
# ==============================================================================

@service_metadata(
name="FingerprintScanner",
version="1.0.0",
description="Scans a directory tree and generates a deterministic SHA-256 fingerprint.",
tags=["scanning", "integrity", "hashing"],
capabilities=["filesystem:read"]
)
class FingerprintScannerMS:
    """
The Detective: Scans a directory tree and generates a deterministic
'Fingerprint' (SHA-256 Merkle Root) representing its exact state.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"state": "Dict[str, Any]"},
    description="Scans the project and returns a comprehensive state object (hashes + Merkle root).",
    tags=["scanning", "read"],
    side_effects=["filesystem:read"]
    )
    def scan_project(self, root_path: str) -> Dict[str, Any]:
    """
    Scans the project and returns a comprehensive state object.
        output = {
            "root": str,
            "project_fingerprint": str (The global hash),
            "file_hashes": {rel_path: sha256},
            "file_count": int
        }
        """
        root = Path(root_path).resolve()
        if not root.exists():
            raise FileNotFoundError(f"Path not found: {root}")

        file_map = {}
        
        # 1. Walk and Hash
        # Use sorted() to ensure iteration order doesn't affect the final hash
        for path in sorted(root.rglob("*")):
            if path.is_file():
                if self._should_ignore(path, root):
                    continue
                
                rel_path = str(path.relative_to(root)).replace("\\", "/")
                file_hash = self._hash_file(path)
                
                if file_hash:
                    file_map[rel_path] = file_hash

        # 2. Calculate Merkle Root (Global Fingerprint)
        # We sort by relative path to ensure deterministic ordering
        sorted_hashes = [file_map[p] for p in sorted(file_map.keys())]
        combined_data = "".join(sorted_hashes).encode('utf-8')
        project_fingerprint = hashlib.sha256(combined_data).hexdigest()

        log.info(f"Scanned {len(file_map)} files. Fingerprint: {project_fingerprint[:8]}...")

        return {
            "root": str(root),
            "project_fingerprint": project_fingerprint,
            "file_hashes": file_map,
            "file_count": len(file_map)
        }

    def _should_ignore(self, path: Path, root: Path) -> bool:
        """Checks path against exclusion lists."""
        try:
            rel_parts = path.relative_to(root).parts
            
            # Check directories
            # If any parent directory is in the ignore list, skip
            for part in rel_parts[:-1]: 
                if part in DEFAULT_IGNORE_DIRS:
                    return True
            
            # Check filename
            import fnmatch
            name = path.name
            if name in DEFAULT_IGNORE_FILES:
                return True
            if any(fnmatch.fnmatch(name, pat) for pat in DEFAULT_IGNORE_FILES):
                return True
                
            return False
        except ValueError:
            return True

    def _hash_file(self, path: Path) -> Optional[str]:
        """Reads file bytes and returns SHA256 hash."""
        try:
            # Read binary to avoid encoding issues and to hash exact content
            content = path.read_bytes()
            return hashlib.sha256(content).hexdigest()
        except (PermissionError, OSError):
            log.warning(f"Could not read/hash: {path}")
            return None

# --- Independent Test Block ---
if __name__ == "__main__":
    import time
    
    # 1. Create a dummy project
    test_dir = Path("test_fingerprint_proj")
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)
    test_dir.mkdir()
    
    (test_dir / "main.py").write_text("print('hello')")
    (test_dir / "utils.py").write_text("def add(a,b): return a+b")
    
    scanner = FingerprintScannerMS()
    print("Service ready:", scanner)
    
    # 2. Initial Scan
    print("--- Scan 1 (Initial) ---")
    state_1 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 1: {state_1['project_fingerprint']}")
    
    # 3. Modify a file
    print("\n--- Modifying 'main.py' ---")
    time.sleep(0.1) # Ensure filesystem timestamp tick (though we hash content)
    (test_dir / "main.py").write_text("print('hello world')")
    
    # 4. Scan again
    print("--- Scan 2 (After Modification) ---")
    state_2 = scanner.scan_project(str(test_dir))
    print(f"Fingerprint 2: {state_2['project_fingerprint']}")
    
    # 5. Compare
    if state_1['project_fingerprint'] != state_2['project_fingerprint']:
        print("\n✅ SUCCESS: Fingerprint changed as expected.")
        # Find the diff
        for path, h in state_2['file_hashes'].items():
            if state_1['file_hashes'].get(path) != h:
                print(f"   Changed File: {path}")
    else:
        print("\n❌ FAILURE: Fingerprint did not change.")

    # Cleanup
    if test_dir.exists():
        import shutil
        shutil.rmtree(test_dir)

--------------------------------------------------------------------------------
FILE: _GitPilotMS\app.py
--------------------------------------------------------------------------------
import os
import subprocess
import threading
import queue
import time
import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple, Any, Callable, Dict
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Detect if GitHub CLI is available
def which(cmd: str) -> Optional[str]:
    for p in os.environ.get("PATH", "").split(os.pathsep):
        f = Path(p) / cmd
        if os.name == 'nt':
            for ext in (".exe", ".cmd", ".bat"): 
                if (f.with_suffix(ext)).exists(): return str(f.with_suffix(ext))
        if f.exists() and os.access(f, os.X_OK): return str(f)
    return None

USE_GH = which("gh") is not None
# ==============================================================================

@dataclass
class GitStatusEntry:
    path: str
    index: str
    workdir: str

@dataclass
class GitStatus:
    repo_path: str
    branch: Optional[str]
    ahead: int
    behind: int
    entries: List[GitStatusEntry]

# --- Backend: The Git Wrapper ---
class GitCLI:
    """
    A robust wrapper around the git command line executable.
    """
    def __init__(self, repo_path: Path):
        self.root = self._resolve_repo_root(repo_path)
def _run(self, args: List[str], *, cwd: Optional[Path] = None) -> Tuple[str, str]:
        cmd = ["git", *args]
        # Prevent console window popping up on Windows
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        proc = subprocess.run(
            cmd,
            cwd=str(cwd or self.root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
            startupinfo=startupinfo
        )
        if proc.returncode != 0:
            raise RuntimeError(proc.stderr.strip() or f"git {' '.join(args)} failed")
        return proc.stdout, proc.stderr

    @staticmethod
    def _resolve_repo_root(path: Path) -> Path:
        path = path.resolve()
        if (path / ".git").exists(): return path
        p = path
        while True:
            if (p / ".git").exists(): return p
            if p.parent == p: break
            p = p.parent
        return path

    def init(self) -> None:
        self._run(["init"])

    def status(self) -> GitStatus:
        try:
            out, _ = self._run(["rev-parse", "--abbrev-ref", "HEAD"])
            branch = out.strip()
        except Exception: branch = None
        
        ahead = behind = 0
        try:
            out, _ = self._run(["rev-list", "--left-right", "--count", "@{upstream}...HEAD"])
            left, right = out.strip().split()
            behind, ahead = int(left), int(right)
        except Exception: pass
        
        out, _ = self._run(["status", "--porcelain=v1"])
        entries = []
        for line in out.splitlines():
            if not line.strip(): continue
            xy = line[:2]
            path = line[3:]
            index, work = xy[0], xy[1]
            entries.append(GitStatusEntry(path=path, index=index, workdir=work))
        return GitStatus(str(self.root), branch, ahead, behind, entries)

    def stage(self, paths: List[str]) -> None:
        if paths: self._run(["add", "--"] + paths)

    def unstage(self, paths: List[str]) -> None:
        if paths: self._run(["reset", "HEAD", "--"] + paths)

    def diff(self, file: Optional[str] = None) -> str:
        args = ["diff"]
        if file: args += ["--", file]
        out, _ = self._run(args)
        return out

    def commit(self, message: str, author_name: str, author_email: str) -> str:
        env = os.environ.copy()
        if author_name: 
            env["GIT_AUTHOR_NAME"] = author_name
            env["GIT_COMMITTER_NAME"] = author_name
        if author_email:
            env["GIT_AUTHOR_EMAIL"] = author_email
            env["GIT_COMMITTER_EMAIL"] = author_email
            
        proc = subprocess.run(
            ["git", "commit", "-m", message], 
            cwd=str(self.root), 
            capture_output=True, 
            text=True, 
            env=env
        )
        if proc.returncode != 0: raise RuntimeError(proc.stderr.strip() or proc.stdout.strip())
        out, _ = self._run(["rev-parse", "HEAD"])
        return out.strip()

    def log(self, limit: int = 100) -> List[Tuple[str, str, str, int]]:
        fmt = "%H%x1f%s%x1f%an%x1f%at"
        try:
            out, _ = self._run(["log", f"-n{limit}", f"--pretty=format:{fmt}"])
            items = []
            for line in out.splitlines():
                commit, summary, author, at = line.split("\x1f")
                items.append((commit, summary, author, int(at)))
            return items
        except Exception: return []

    def branches(self) -> List[Tuple[str, bool]]:
        try:
            out, _ = self._run(["branch"])
            res = []
            for line in out.splitlines():
                is_head = line.strip().startswith("*")
                name = line.replace("*", "", 1).strip()
                res.append((name, is_head))
            return res
        except Exception: return []

    def checkout(self, name: str, create: bool = False) -> None:
        if create: self._run(["checkout", "-B", name])
        else: self._run(["checkout", name])

    def push(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        args = ["push", remote]
        if branch: args.append(branch)
        out, _ = self._run(args)
return out

    def pull(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        if branch: out, _ = self._run(["pull", remote, branch])
        else: out, _ = self._run(["pull", remote])
        return out

# --- Threading Helper ---
class Worker:
    def __init__(self, ui_callback):
        self.q = queue.Queue()
        self.ui_callback = ui_callback
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()

    def submit(self, op: str, func, *args, **kwargs):
        self.q.put((op, func, args, kwargs))

    def _loop(self):
        while True:
            op, func, args, kwargs = self.q.get()
            try:
                result = op, True, func(*args, **kwargs)
            except Exception as e:
                result = op, False, e
            finally:
                self.ui_callback(result)

# --- Frontend: The GUI Panel ---
@service_metadata(
name="GitPilot",
version="1.0.0",
description="A Tkinter GUI panel for Git operations (Stage, Commit, Push, Pull).",
tags=["ui", "git", "version-control", "widget"],
capabilities=["ui:gui", "filesystem:read", "filesystem:write", "network:outbound"]
)
class GitPilotMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
super().__init__(parent)

initial_path = self.config.get("initial_path")
self.repo_path = None
self.git = None
self.worker = Worker(self._on_worker_done)

self._build_ui()
if initial_path:
    self.set_repo(initial_path)

@service_endpoint(
inputs={"path": "Path"},
outputs={},
description="Sets the active repository path and refreshes status.",
tags=["git", "config"],
side_effects=["filesystem:read", "ui:update"]
)
def set_repo(self, path: Path):
    try:
        self.git = GitCLI(path)
        self.repo_path = self.git.root
        self.path_var.set(f"Repo: {self.repo_path}")
        self._refresh()
    except Exception as e:
        self.path_var.set(f"Error: {e}")

def _build_ui(self):
    self.columnconfigure(0, weight=1)
    self.rowconfigure(1, weight=1)

    # Status Bar
    bar = ttk.Frame(self)
    bar.grid(row=0, column=0, sticky="ew")
    self.path_var = tk.StringVar(value="No Repo Selected")
    self.busy_var = tk.StringVar()
    ttk.Label(bar, textvariable=self.path_var).pack(side="left", padx=5)
    ttk.Label(bar, textvariable=self.busy_var, foreground="blue").pack(side="right", padx=5)

    # Tabs
        self.nb = ttk.Notebook(self)
        self.nb.grid(row=1, column=0, sticky="nsew")
        
        self.tab_changes = self._build_changes_tab(self.nb)
        self.tab_log = self._build_log_tab(self.nb)
        
        self.nb.add(self.tab_changes, text="Changes")
        self.nb.add(self.tab_log, text="History")

    def _build_changes_tab(self, parent):
        frame = ttk.Frame(parent)
        paned = ttk.PanedWindow(frame, orient=tk.VERTICAL)
        paned.pack(fill="both", expand=True)

        # File List
        top = ttk.Frame(paned)
        top.rowconfigure(1, weight=1)
        top.columnconfigure(0, weight=1)
        
        # Toolbar
        tb = ttk.Frame(top)
        tb.grid(row=0, column=0, sticky="ew")
        ttk.Button(tb, text="Refresh", command=self._refresh).pack(side="left")
        ttk.Button(tb, text="Stage", command=self._stage).pack(side="left")
        ttk.Button(tb, text="Unstage", command=self._unstage).pack(side="left")
        ttk.Button(tb, text="Diff", command=self._show_diff).pack(side="left")
        ttk.Button(tb, text="Push", command=self._push).pack(side="left", padx=10)
        ttk.Button(tb, text="Pull", command=self._pull).pack(side="left")

        # Treeview
        self.tree = ttk.Treeview(top, columns=("path", "idx", "wd"), show="headings", selectmode="extended")
        self.tree.heading("path", text="Path")
        self.tree.heading("idx", text="Index")
        self.tree.heading("wd", text="Workdir")
        self.tree.column("path", width=400)
        self.tree.column("idx", width=50, anchor="center")
        self.tree.column("wd", width=50, anchor="center")
        self.tree.grid(row=1, column=0, sticky="nsew")
        
        paned.add(top, weight=3)

        # Commit Area
        bot = ttk.Frame(paned)
        bot.columnconfigure(1, weight=1)
        ttk.Label(bot, text="Message:").grid(row=0, column=0, sticky="nw")
        self.msg_text = tk.Text(bot, height=4)
        self.msg_text.grid(row=0, column=1, sticky="nsew")
        ttk.Button(bot, text="Commit", command=self._commit).grid(row=1, column=1, sticky="e", pady=5)
        
        paned.add(bot, weight=1)
        return frame

    def _build_log_tab(self, parent):
        frame = ttk.Frame(parent)
        self.log_tree = ttk.Treeview(frame, columns=("sha", "msg", "auth", "time"), show="headings")
        self.log_tree.heading("sha", text="SHA")
        self.log_tree.heading("msg", text="Message")
        self.log_tree.heading("auth", text="Author")
        self.log_tree.heading("time", text="Time")
        self.log_tree.column("sha", width=80)
        self.log_tree.column("msg", width=400)
        self.log_tree.pack(fill="both", expand=True)
        return frame

    # --- Actions ---

    def _submit(self, label, func, *args):
        self.busy_var.set(f"{label}...")
        self.worker.submit(label, func, *args)

    def _on_worker_done(self, result):
        self.after(0, self._handle_result, result)

    def _handle_result(self, result):
        label, ok, data = result
        self.busy_var.set("")
        if not ok:
            messagebox.showerror("Error", str(data))
            return
        
        if label == "refresh":
            status, logs = data
            self.tree.delete(*self.tree.get_children())
            for e in status.entries:
                self.tree.insert("", "end", values=(e.path, e.index, e.workdir))
            
            self.log_tree.delete(*self.log_tree.get_children())
            for sha, msg, auth, ts in logs:
                t_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(ts))
                self.log_tree.insert("", "end", values=(sha[:7], msg, auth, t_str))
        
        if label == "diff":
            top = tk.Toplevel(self)
            top.title("Diff")
            txt = tk.Text(top, font=("Consolas", 10))
            txt.pack(fill="both", expand=True)
            txt.insert("1.0", data)

        if label in ["stage", "unstage", "commit", "push", "pull"]:
            self._refresh()

    def _refresh(self):
        if not self.git: return
        self._submit("refresh", lambda: (self.git.status(), self.git.log()))

    def _get_selection(self):
        return [self.tree.item(i)['values'][0] for i in self.tree.selection()]

    def _stage(self):
        paths = self._get_selection()
        if paths: self._submit("stage", self.git.stage, paths)

    def _unstage(self):
        paths = self._get_selection()
        if paths: self._submit("unstage", self.git.unstage, paths)

    def _commit(self):
        msg = self.msg_text.get("1.0", "end").strip()
        if not msg: return
        self._submit("commit", self.git.commit, msg, "GitPilot", "pilot@local")
        self.msg_text.delete("1.0", "end")

    def _push(self):
        self._submit("push", self.git.push)

    def _pull(self):
        self._submit("pull", self.git.pull)

    def _show_diff(self):
        sel = self._get_selection()
        file = sel[0] if sel else None
        self._submit("diff", self.git.diff, file)

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Git Pilot Test")
    root.geometry("800x600")
    
    # Use current directory
    cwd = Path(os.getcwd())
    
    panel = GitPilotMS({"parent": root, "initial_path": cwd})
    print("Service ready:", panel)
    panel.pack(fill="both", expand=True)
    
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _GitPilotMS\app.py.bak
--------------------------------------------------------------------------------
import os
import subprocess
import threading
import queue
import time
import tkinter as tk
from tkinter import ttk, messagebox, simpledialog
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Tuple, Any, Callable, Dict
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Detect if GitHub CLI is available
def which(cmd: str) -> Optional[str]:
    for p in os.environ.get("PATH", "").split(os.pathsep):
        f = Path(p) / cmd
        if os.name == 'nt':
            for ext in (".exe", ".cmd", ".bat"): 
                if (f.with_suffix(ext)).exists(): return str(f.with_suffix(ext))
        if f.exists() and os.access(f, os.X_OK): return str(f)
    return None

USE_GH = which("gh") is not None
# ==============================================================================

@dataclass
class GitStatusEntry:
    path: str
    index: str
    workdir: str

@dataclass
class GitStatus:
    repo_path: str
    branch: Optional[str]
    ahead: int
    behind: int
    entries: List[GitStatusEntry]

# --- Backend: The Git Wrapper ---
class GitCLI:
    """
    A robust wrapper around the git command line executable.
    """
    def __init__(self, repo_path: Path):
        self.root = self._resolve_repo_root(repo_path)

    def _run(self, args: List[str], *, cwd: Optional[Path] = None) -> Tuple[str, str]:
        cmd = ["git", *args]
        # Prevent console window popping up on Windows
        startupinfo = None
        if os.name == 'nt':
            startupinfo = subprocess.STARTUPINFO()
            startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW

        proc = subprocess.run(
            cmd,
            cwd=str(cwd or self.root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding="utf-8",
            startupinfo=startupinfo
        )
        if proc.returncode != 0:
            raise RuntimeError(proc.stderr.strip() or f"git {' '.join(args)} failed")
        return proc.stdout, proc.stderr

    @staticmethod
    def _resolve_repo_root(path: Path) -> Path:
        path = path.resolve()
        if (path / ".git").exists(): return path
        p = path
        while True:
            if (p / ".git").exists(): return p
            if p.parent == p: break
            p = p.parent
        return path

    def init(self) -> None:
        self._run(["init"])

    def status(self) -> GitStatus:
        try:
            out, _ = self._run(["rev-parse", "--abbrev-ref", "HEAD"])
            branch = out.strip()
        except Exception: branch = None
        
        ahead = behind = 0
        try:
            out, _ = self._run(["rev-list", "--left-right", "--count", "@{upstream}...HEAD"])
            left, right = out.strip().split()
            behind, ahead = int(left), int(right)
        except Exception: pass
        
        out, _ = self._run(["status", "--porcelain=v1"])
        entries = []
        for line in out.splitlines():
            if not line.strip(): continue
            xy = line[:2]
            path = line[3:]
            index, work = xy[0], xy[1]
            entries.append(GitStatusEntry(path=path, index=index, workdir=work))
        return GitStatus(str(self.root), branch, ahead, behind, entries)

    def stage(self, paths: List[str]) -> None:
        if paths: self._run(["add", "--"] + paths)

    def unstage(self, paths: List[str]) -> None:
        if paths: self._run(["reset", "HEAD", "--"] + paths)

    def diff(self, file: Optional[str] = None) -> str:
        args = ["diff"]
        if file: args += ["--", file]
        out, _ = self._run(args)
        return out

    def commit(self, message: str, author_name: str, author_email: str) -> str:
        env = os.environ.copy()
        if author_name: 
            env["GIT_AUTHOR_NAME"] = author_name
            env["GIT_COMMITTER_NAME"] = author_name
        if author_email:
            env["GIT_AUTHOR_EMAIL"] = author_email
            env["GIT_COMMITTER_EMAIL"] = author_email
            
        proc = subprocess.run(
            ["git", "commit", "-m", message], 
            cwd=str(self.root), 
            capture_output=True, 
            text=True, 
            env=env
        )
        if proc.returncode != 0: raise RuntimeError(proc.stderr.strip() or proc.stdout.strip())
        out, _ = self._run(["rev-parse", "HEAD"])
        return out.strip()

    def log(self, limit: int = 100) -> List[Tuple[str, str, str, int]]:
        fmt = "%H%x1f%s%x1f%an%x1f%at"
        try:
            out, _ = self._run(["log", f"-n{limit}", f"--pretty=format:{fmt}"])
            items = []
            for line in out.splitlines():
                commit, summary, author, at = line.split("\x1f")
                items.append((commit, summary, author, int(at)))
            return items
        except Exception: return []

    def branches(self) -> List[Tuple[str, bool]]:
        try:
            out, _ = self._run(["branch"])
            res = []
            for line in out.splitlines():
                is_head = line.strip().startswith("*")
                name = line.replace("*", "", 1).strip()
                res.append((name, is_head))
            return res
        except Exception: return []

    def checkout(self, name: str, create: bool = False) -> None:
        if create: self._run(["checkout", "-B", name])
        else: self._run(["checkout", name])

    def push(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        args = ["push", remote]
        if branch: args.append(branch)
        out, _ = self._run(args)
        return out

    def pull(self, remote: str = "origin", branch: Optional[str] = None) -> str:
        if branch: out, _ = self._run(["pull", remote, branch])
        else: out, _ = self._run(["pull", remote])
        return out

# --- Threading Helper ---
class Worker:
    def __init__(self, ui_callback):
        self.q = queue.Queue()
        self.ui_callback = ui_callback
        self.thread = threading.Thread(target=self._loop, daemon=True)
        self.thread.start()

    def submit(self, op: str, func, *args, **kwargs):
        self.q.put((op, func, args, kwargs))

    def _loop(self):
        while True:
            op, func, args, kwargs = self.q.get()
            try:
                result = op, True, func(*args, **kwargs)
            except Exception as e:
                result = op, False, e
            finally:
                self.ui_callback(result)

# --- Frontend: The GUI Panel ---
@service_metadata(
name="GitPilot",
version="1.0.0",
description="A Tkinter GUI panel for Git operations (Stage, Commit, Push, Pull).",
tags=["ui", "git", "version-control", "widget"],
capabilities=["ui:gui", "filesystem:read", "filesystem:write", "network:outbound"]
)
class GitPilotMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
        
initial_path = self.config.get("initial_path")
self.repo_path = None
self.git = None
        self.worker = Worker(self._on_worker_done)

        self._build_ui()
        if initial_path:
        self.set_repo(initial_path)

        @service_endpoint(
        inputs={"path": "Path"},
        outputs={},
        description="Sets the active repository path and refreshes status.",
        tags=["git", "config"],
        side_effects=["filesystem:read", "ui:update"]
        )
        def set_repo(self, path: Path):
        try:
            self.git = GitCLI(path)
            self.repo_path = self.git.root
            self.path_var.set(f"Repo: {self.repo_path}")
            self._refresh()
        except Exception as e:
            self.path_var.set(f"Error: {e}")

    def _build_ui(self):
        self.columnconfigure(0, weight=1)
        self.rowconfigure(1, weight=1)

        # Status Bar
        bar = ttk.Frame(self)
        bar.grid(row=0, column=0, sticky="ew")
        self.path_var = tk.StringVar(value="No Repo Selected")
        self.busy_var = tk.StringVar()
        ttk.Label(bar, textvariable=self.path_var).pack(side="left", padx=5)
        ttk.Label(bar, textvariable=self.busy_var, foreground="blue").pack(side="right", padx=5)

        # Tabs
        self.nb = ttk.Notebook(self)
        self.nb.grid(row=1, column=0, sticky="nsew")
        
        self.tab_changes = self._build_changes_tab(self.nb)
        self.tab_log = self._build_log_tab(self.nb)
        
        self.nb.add(self.tab_changes, text="Changes")
        self.nb.add(self.tab_log, text="History")

    def _build_changes_tab(self, parent):
        frame = ttk.Frame(parent)
        paned = ttk.PanedWindow(frame, orient=tk.VERTICAL)
        paned.pack(fill="both", expand=True)

        # File List
        top = ttk.Frame(paned)
        top.rowconfigure(1, weight=1)
        top.columnconfigure(0, weight=1)
        
        # Toolbar
        tb = ttk.Frame(top)
        tb.grid(row=0, column=0, sticky="ew")
        ttk.Button(tb, text="Refresh", command=self._refresh).pack(side="left")
        ttk.Button(tb, text="Stage", command=self._stage).pack(side="left")
        ttk.Button(tb, text="Unstage", command=self._unstage).pack(side="left")
        ttk.Button(tb, text="Diff", command=self._show_diff).pack(side="left")
        ttk.Button(tb, text="Push", command=self._push).pack(side="left", padx=10)
        ttk.Button(tb, text="Pull", command=self._pull).pack(side="left")

        # Treeview
        self.tree = ttk.Treeview(top, columns=("path", "idx", "wd"), show="headings", selectmode="extended")
        self.tree.heading("path", text="Path")
        self.tree.heading("idx", text="Index")
        self.tree.heading("wd", text="Workdir")
        self.tree.column("path", width=400)
        self.tree.column("idx", width=50, anchor="center")
        self.tree.column("wd", width=50, anchor="center")
        self.tree.grid(row=1, column=0, sticky="nsew")
        
        paned.add(top, weight=3)

        # Commit Area
        bot = ttk.Frame(paned)
        bot.columnconfigure(1, weight=1)
        ttk.Label(bot, text="Message:").grid(row=0, column=0, sticky="nw")
        self.msg_text = tk.Text(bot, height=4)
        self.msg_text.grid(row=0, column=1, sticky="nsew")
        ttk.Button(bot, text="Commit", command=self._commit).grid(row=1, column=1, sticky="e", pady=5)
        
        paned.add(bot, weight=1)
        return frame

    def _build_log_tab(self, parent):
        frame = ttk.Frame(parent)
        self.log_tree = ttk.Treeview(frame, columns=("sha", "msg", "auth", "time"), show="headings")
        self.log_tree.heading("sha", text="SHA")
        self.log_tree.heading("msg", text="Message")
        self.log_tree.heading("auth", text="Author")
        self.log_tree.heading("time", text="Time")
        self.log_tree.column("sha", width=80)
        self.log_tree.column("msg", width=400)
        self.log_tree.pack(fill="both", expand=True)
        return frame

    # --- Actions ---

    def _submit(self, label, func, *args):
        self.busy_var.set(f"{label}...")
        self.worker.submit(label, func, *args)

    def _on_worker_done(self, result):
        self.after(0, self._handle_result, result)

    def _handle_result(self, result):
        label, ok, data = result
        self.busy_var.set("")
        if not ok:
            messagebox.showerror("Error", str(data))
            return
        
        if label == "refresh":
            status, logs = data
            self.tree.delete(*self.tree.get_children())
            for e in status.entries:
                self.tree.insert("", "end", values=(e.path, e.index, e.workdir))
            
            self.log_tree.delete(*self.log_tree.get_children())
            for sha, msg, auth, ts in logs:
                t_str = time.strftime('%Y-%m-%d %H:%M', time.localtime(ts))
                self.log_tree.insert("", "end", values=(sha[:7], msg, auth, t_str))
        
        if label == "diff":
            top = tk.Toplevel(self)
            top.title("Diff")
            txt = tk.Text(top, font=("Consolas", 10))
            txt.pack(fill="both", expand=True)
            txt.insert("1.0", data)

        if label in ["stage", "unstage", "commit", "push", "pull"]:
            self._refresh()

    def _refresh(self):
        if not self.git: return
        self._submit("refresh", lambda: (self.git.status(), self.git.log()))

    def _get_selection(self):
        return [self.tree.item(i)['values'][0] for i in self.tree.selection()]

    def _stage(self):
        paths = self._get_selection()
        if paths: self._submit("stage", self.git.stage, paths)

    def _unstage(self):
        paths = self._get_selection()
        if paths: self._submit("unstage", self.git.unstage, paths)

    def _commit(self):
        msg = self.msg_text.get("1.0", "end").strip()
        if not msg: return
        self._submit("commit", self.git.commit, msg, "GitPilot", "pilot@local")
        self.msg_text.delete("1.0", "end")

    def _push(self):
        self._submit("push", self.git.push)

    def _pull(self):
        self._submit("pull", self.git.pull)

    def _show_diff(self):
        sel = self._get_selection()
        file = sel[0] if sel else None
        self._submit("diff", self.git.diff, file)

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Git Pilot Test")
    root.geometry("800x600")
    
    # Use current directory
    cwd = Path(os.getcwd())
    
    panel = GitPilotMS({"parent": root, "initial_path": cwd})
    print("Service ready:", panel)
    panel.pack(fill="both", expand=True)
    
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _GraphEngineMS\app.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import os
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

# Use relative import for sibling module
from .graph_engine import GraphRenderer
@service_metadata(
name="GraphEngine",
version="1.0.0",
description="Interactive 2D Force-Directed Graph Visualizer (Pygame + Tkinter).",
tags=["graph", "visualization", "physics", "ui"],
capabilities=["ui:gui", "compute", "db:sqlite"]
)
class GraphEngineMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        parent = self.config.get("parent")
        super().__init__(parent)
        self.pack(fill="both", expand=True)

        # Widget logic
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)

        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None  # Keep reference to avoid GC

        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)

        # Zoom bindings
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1))  # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9))  # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)

        # Logic State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False

        # Start Loop
        self.animate()

    @service_endpoint(
    inputs={"db_path": "str"},
    outputs={},
    description="Loads graph data from SQLite and initializes the physics engine.",
    tags=["graph", "load", "db"],
    side_effects=["db:read", "ui:update"]
    )
    def load_from_db(self, db_path):
    """
    Fetches Nodes and Edges from the SQLite DB and formats them 
    for the Pygame Physics Engine.
    """
        if not os.path.exists(db_path):
            print(f"GraphView Error: DB not found at {db_path}")
            return

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # 1. Fetch Nodes
        # Schema: id, type, label, data_json
        try:
            db_nodes = cursor.execute("SELECT id, type, label FROM graph_nodes").fetchall()
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
        except sqlite3.OperationalError:
            print("Graph tables missing. Run Ingest first.")
            conn.close()
            return

        conn.close()

        # 2. Map String IDs -> Integer Indices
        # The physics engine uses list indices (0, 1, 2) for speed.
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label = row
            id_to_index[node_id] = idx
            formatted_nodes.append({
                'id': node_id,
                'type': n_type,
                'label': label
            })

        # 3. Translate Edges
        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                u = id_to_index[src]
                v = id_to_index[tgt]
                formatted_links.append((u, v))

        # 4. Push to Engine
        print(f"Graph Loaded: {len(formatted_nodes)} Nodes, {len(formatted_links)} Edges")
        self.engine.set_data(formatted_nodes, formatted_links)

    # --- EVENT HANDLERS ---
    
    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        self.is_dragging_node = hit

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        else:
            # Pan Camera
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        # Just update hover state for aesthetics
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)

    def on_windows_scroll(self, event):
        # Windows typically gives 120 or -120
        if event.delta > 0:
            self.on_zoom(1.1)
        else:
            self.on_zoom(0.9)

    # --- RENDER LOOP ---

    def animate(self):
    # 1. Step Physics
    self.engine.step_physics()
        
    # 2. Render to Pygame Surface & Convert to Tkinter
    # (We assume 800x600 or current engine size)
    w, h = self.engine.width, self.engine.height
    raw_data = self.engine.get_image_bytes()
        
    image = Image.frombytes('RGB', (w, h), raw_data)
    self.photo = ImageTk.PhotoImage(image=image)
    self.canvas_lbl.configure(image=self.photo)
        
    # 3. Loop (approx 60 FPS)
    self.after(16, self.animate)


--------------------------------------------------------------------------------
FILE: _GraphEngineMS\app.py.bak
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
from PIL import Image, ImageTk
import sqlite3
import os
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

# Use relative import for sibling module
from .graph_engine import GraphRenderer

@service_metadata(
name="GraphEngine",
version="1.0.0",
description="Interactive 2D Force-Directed Graph Visualizer (Pygame + Tkinter).",
tags=["graph", "visualization", "physics", "ui"],
capabilities=["ui:gui", "compute", "db:sqlite"]
)
class GraphEngineMS(ttk.Frame):
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
self.pack(fill="both", expand=True)
        
        # Widget logic
        self.canvas_lbl = tk.Label(self, bg="#101018", cursor="crosshair")
        self.canvas_lbl.pack(fill="both", expand=True)
        
        # Engine Init
        self.engine = GraphRenderer(800, 600)
        self.photo = None # Keep reference to avoid GC
        
        # Bindings
        self.canvas_lbl.bind('<Button-1>', self.on_click)
        self.canvas_lbl.bind('<ButtonRelease-1>', self.on_release)
        self.canvas_lbl.bind('<B1-Motion>', self.on_drag)
        self.canvas_lbl.bind('<Motion>', self.on_hover)
        # Zoom bindings
        self.canvas_lbl.bind('<Button-4>', lambda e: self.on_zoom(1.1)) # Linux Scroll Up
        self.canvas_lbl.bind('<Button-5>', lambda e: self.on_zoom(0.9)) # Linux Scroll Down
        self.canvas_lbl.bind('<MouseWheel>', self.on_windows_scroll)    # Windows Scroll
        self.canvas_lbl.bind('<Configure>', self.on_resize)
        
        # Logic State
        self.last_mouse_x = 0
        self.last_mouse_y = 0
        self.is_dragging_node = False
        
        # Start Loop
        self.animate()

    @service_endpoint(
    inputs={"db_path": "str"},
    outputs={},
    description="Loads graph data from SQLite and initializes the physics engine.",
    tags=["graph", "load", "db"],
    side_effects=["db:read", "ui:update"]
    )
    def load_from_db(self, db_path):
    """
    Fetches Nodes and Edges from the SQLite DB and formats them 
    for the Pygame Physics Engine.
    """
        if not os.path.exists(db_path):
            print(f"GraphView Error: DB not found at {db_path}")
            return

        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # 1. Fetch Nodes
        # Schema: id, type, label, data_json
        try:
            db_nodes = cursor.execute("SELECT id, type, label FROM graph_nodes").fetchall()
            db_edges = cursor.execute("SELECT source, target FROM graph_edges").fetchall()
        except sqlite3.OperationalError:
            print("Graph tables missing. Run Ingest first.")
            conn.close()
            return

        conn.close()

        # 2. Map String IDs -> Integer Indices
        # The physics engine uses list indices (0, 1, 2) for speed.
        id_to_index = {}
        formatted_nodes = []
        
        for idx, row in enumerate(db_nodes):
            node_id, n_type, label = row
            id_to_index[node_id] = idx
            formatted_nodes.append({
                'id': node_id,
                'type': n_type,
                'label': label
            })

        # 3. Translate Edges
        formatted_links = []
        for src, tgt in db_edges:
            if src in id_to_index and tgt in id_to_index:
                u = id_to_index[src]
                v = id_to_index[tgt]
                formatted_links.append((u, v))

        # 4. Push to Engine
        print(f"Graph Loaded: {len(formatted_nodes)} Nodes, {len(formatted_links)} Edges")
        self.engine.set_data(formatted_nodes, formatted_links)

    # --- EVENT HANDLERS ---
    
    def on_resize(self, event):
        if event.width > 1 and event.height > 1:
            self.engine.resize(event.width, event.height)

    def on_click(self, event):
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y
        # Check if we clicked a node
        hit = self.engine.handle_mouse_down(event.x, event.y)
        self.is_dragging_node = hit

    def on_release(self, event):
        self.engine.handle_mouse_up()
        self.is_dragging_node = False

    def on_drag(self, event):
        if self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, True)
        else:
            # Pan Camera
            dx = event.x - self.last_mouse_x
            dy = event.y - self.last_mouse_y
            self.engine.pan(dx, dy)
            
        self.last_mouse_x = event.x
        self.last_mouse_y = event.y

    def on_hover(self, event):
        # Just update hover state for aesthetics
        if not self.is_dragging_node:
            self.engine.handle_mouse_move(event.x, event.y, False)

    def on_zoom(self, amount):
        self.engine.zoom_camera(amount, 0, 0)

    def on_windows_scroll(self, event):
        # Windows typically gives 120 or -120
        if event.delta > 0:
            self.on_zoom(1.1)
        else:
            self.on_zoom(0.9)

    # --- RENDER LOOP ---

    def animate(self):
    # 1. Step Physics
    self.engine.step_physics()
        
    # 2. Render to Pygame Surface & Convert to Tkinter
    # (We assume 800x600 or current engine size)
    w, h = self.engine.width, self.engine.height
    raw_data = self.engine.get_image_bytes()
        
    image = Image.frombytes('RGB', (w, h), raw_data)
    self.photo = ImageTk.PhotoImage(image=image)
    self.canvas_lbl.configure(image=self.photo)
        
    # 3. Loop (approx 60 FPS)
    self.after(16, self.animate)


--------------------------------------------------------------------------------
FILE: _GraphEngineMS\graph_engine.py
--------------------------------------------------------------------------------
import pygame
import math
import random

# Initialize font module globally once
pygame.font.init()

class GraphRenderer:
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        self.width = width
        self.height = height
        self.bg_color = bg_color
        
        # Surface for drawing
        self.surface = pygame.Surface((width, height))
        
             # Camera State
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction State
        self.dragged_node_idx = None
        self.hovered_node_idx = None

    def resize(self, width, height):
        """Re-initialize surface on window resize"""
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    def set_data(self, nodes, links):
        """
        Expects nodes to have: id, type ('file'|'concept'), label
        Expects links to be tuples of indices: (source_idx, target_idx)
        """
        self.nodes = nodes
        self.links = links
        
        # Initialize physics state for new nodes
        for n in self.nodes:
            if 'x' not in n:
                n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Cache visual properties based on type
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # #007ACC (Blue)
                n['_radius'] = 6
            else:
                n['_color'] = (160, 32, 240) # #A020F0 (Purple)
                n['_radius'] = 8

    # --- INPUT HANDLING (Coordinate Transforms) ---
    
    def screen_to_world(self, sx, sy):
        """Convert Tkinter screen coordinates to Physics world coordinates"""
        # (Screen - Center) / Zoom + Center - Camera
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        # Find clicked node
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2: # Generous hit box
                self.dragged_node_idx = i
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            # Move the node directly
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
        else:
            # Hover check
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            
            return prev_hover != self.hovered_node_idx # Return True if redraw needed

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        # Zoom towards mouse pointer logic could go here
        # For now, simple center zoom
        old_zoom = self.zoom
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    # --- PHYSICS ---

    def step_physics(self):
        if not self.nodes: return

        # Constants matching D3 feel
        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.9
        
        cx, cy = self.width / 2, self.height / 2
# 1. Repulsion (Nodes push apart)
        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue # Don't move dragged node
            
            fx, fy = 0, 0
            
            # Center Gravity (Pull lightly to middle so they don't drift away)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # Node-Node Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Force = k / dist^2
                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 2. Attraction (Links pull together)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            
            # Spring force
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 3. Apply Velocity
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']

    # --- RENDERING ---

    def get_image_bytes(self):
        """ Renders the scene and returns raw RGB bytes + size """
        self.surface.fill(self.bg_color)
        
        # Pre-calculate center offset
        cx, cy = self.width / 2, self.height / 2
        
        # Helper for transforms
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # 1. Draw Links
        for u, v in self.links:
            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # 2. Draw Nodes
        for i, n in enumerate(self.nodes):
            sx, sy = to_screen(n['x'], n['y'])
            
            # Culling: Don't draw if off screen
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20:
if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20:
                continue

            rad = int(n['_radius'] * self.zoom)
            col = n['_color']

            # Highlight hovered
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)

            pygame.draw.circle(self.surface, col, (sx, sy), rad)

            # Draw Labels (only if zoomed in enough or hovered)
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

--------------------------------------------------------------------------------
FILE: _GraphEngineMS\graph_engine.py.bak
--------------------------------------------------------------------------------
import pygame
import math
import random

# Initialize font module globally once
pygame.font.init()

class GraphRenderer:
    def __init__(self, width, height, bg_color=(16, 16, 24)):
        self.width = width
        self.height = height
        self.bg_color = bg_color
        
        # Surface for drawing
        self.surface = pygame.Surface((width, height))
        
             # Camera State
        self.cam_x = 0
        self.cam_y = 0
        self.zoom = 1.0
        
        # Assets
        self.font = pygame.font.SysFont("Consolas", 12)
        
        # Data
        self.nodes = [] 
        self.links = []
        
        # Interaction State
        self.dragged_node_idx = None
        self.hovered_node_idx = None

    def resize(self, width, height):
        """Re-initialize surface on window resize"""
        self.width = width
        self.height = height
        self.surface = pygame.Surface((width, height))

    def set_data(self, nodes, links):
        """
        Expects nodes to have: id, type ('file'|'concept'), label
        Expects links to be tuples of indices: (source_idx, target_idx)
        """
        self.nodes = nodes
        self.links = links
        
        # Initialize physics state for new nodes
        for n in self.nodes:
            if 'x' not in n:
                n['x'] = random.randint(int(self.width*0.2), int(self.width*0.8))
                n['y'] = random.randint(int(self.height*0.2), int(self.height*0.8))
            if 'vx' not in n: n['vx'] = 0
            if 'vy' not in n: n['vy'] = 0
            
            # Cache visual properties based on type
            if n.get('type') == 'file':
                n['_color'] = (0, 122, 204) # #007ACC (Blue)
                n['_radius'] = 6
            else:
                n['_color'] = (160, 32, 240) # #A020F0 (Purple)
                n['_radius'] = 8

    # --- INPUT HANDLING (Coordinate Transforms) ---
    
    def screen_to_world(self, sx, sy):
        """Convert Tkinter screen coordinates to Physics world coordinates"""
        # (Screen - Center) / Zoom + Center - Camera
        cx, cy = self.width / 2, self.height / 2
        wx = (sx - cx) / self.zoom + cx - self.cam_x
        wy = (sy - cy) / self.zoom + cy - self.cam_y
        return wx, wy

    def handle_mouse_down(self, x, y):
        wx, wy = self.screen_to_world(x, y)
        # Find clicked node
        for i, n in enumerate(self.nodes):
            dist = math.hypot(n['x'] - wx, n['y'] - wy)
            if dist < n['_radius'] * 2: # Generous hit box
                self.dragged_node_idx = i
                return True
        return False

    def handle_mouse_move(self, x, y, is_dragging):
        wx, wy = self.screen_to_world(x, y)
        
        if is_dragging and self.dragged_node_idx is not None:
            # Move the node directly
            node = self.nodes[self.dragged_node_idx]
            node['x'] = wx
            node['y'] = wy
            node['vx'] = 0
            node['vy'] = 0
        else:
            # Hover check
            prev_hover = self.hovered_node_idx
            self.hovered_node_idx = None
            for i, n in enumerate(self.nodes):
                dist = math.hypot(n['x'] - wx, n['y'] - wy)
                if dist < n['_radius'] * 2:
                    self.hovered_node_idx = i
                    break
            
            return prev_hover != self.hovered_node_idx # Return True if redraw needed

    def handle_mouse_up(self):
        self.dragged_node_idx = None

    def pan(self, dx, dy):
        self.cam_x += dx / self.zoom
        self.cam_y += dy / self.zoom

    def zoom_camera(self, amount, mouse_x, mouse_y):
        # Zoom towards mouse pointer logic could go here
        # For now, simple center zoom
        old_zoom = self.zoom
        self.zoom *= amount
        self.zoom = max(0.1, min(self.zoom, 5.0))

    # --- PHYSICS ---

    def step_physics(self):
        if not self.nodes: return

        # Constants matching D3 feel
        REPULSION = 1000
        ATTRACTION = 0.01
        CENTER_GRAVITY = 0.01
        DAMPING = 0.9
        
        cx, cy = self.width / 2, self.height / 2

        # 1. Repulsion (Nodes push apart)
        for i, a in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue # Don't move dragged node
            
            fx, fy = 0, 0
            
            # Center Gravity (Pull lightly to middle so they don't drift away)
            fx += (cx - a['x']) * CENTER_GRAVITY
            fy += (cy - a['y']) * CENTER_GRAVITY

            # Node-Node Repulsion
            for j, b in enumerate(self.nodes):
                if i == j: continue
                dx = a['x'] - b['x']
                dy = a['y'] - b['y']
                dist_sq = dx*dx + dy*dy
                if dist_sq < 0.1: dist_sq = 0.1
                
                # Force = k / dist^2
                f = REPULSION / dist_sq
                dist = math.sqrt(dist_sq)
                fx += (dx / dist) * f
                fy += (dy / dist) * f

            a['vx'] = (a['vx'] + fx) * DAMPING
            a['vy'] = (a['vy'] + fy) * DAMPING

        # 2. Attraction (Links pull together)
        for u, v in self.links:
            a = self.nodes[u]
            b = self.nodes[v]
            
            dx = b['x'] - a['x']
            dy = b['y'] - a['y']
            
            # Spring force
            fx = dx * ATTRACTION
            fy = dy * ATTRACTION
            
            if u != self.dragged_node_idx:
                a['vx'] += fx
                a['vy'] += fy
            if v != self.dragged_node_idx:
                b['vx'] -= fx
                b['vy'] -= fy

        # 3. Apply Velocity
        for i, n in enumerate(self.nodes):
            if i == self.dragged_node_idx: continue
            n['x'] += n['vx']
            n['y'] += n['vy']

    # --- RENDERING ---

    def get_image_bytes(self):
        """ Renders the scene and returns raw RGB bytes + size """
        self.surface.fill(self.bg_color)
        
        # Pre-calculate center offset
        cx, cy = self.width / 2, self.height / 2
        
        # Helper for transforms
        def to_screen(x, y):
            sx = (x - cx + self.cam_x) * self.zoom + cx
            sy = (y - cy + self.cam_y) * self.zoom + cy
            return int(sx), int(sy)

        # 1. Draw Links
        for u, v in self.links:
            start = to_screen(self.nodes[u]['x'], self.nodes[u]['y'])
            end = to_screen(self.nodes[v]['x'], self.nodes[v]['y'])
            pygame.draw.line(self.surface, (60, 60, 80), start, end, 1)

        # 2. Draw Nodes
        for i, n in enumerate(self.nodes):
            sx, sy = to_screen(n['x'], n['y'])
            
            # Culling: Don't draw if off screen
            if sx < -20 or sx > self.width + 20 or sy < -20 or sy > self.height + 20:
                continue
                
            rad = int(n['_radius'] * self.zoom)
            col = n['_color']
            
            # Highlight hovered
            if i == self.hovered_node_idx or i == self.dragged_node_idx:
                pygame.draw.circle(self.surface, (255, 255, 255), (sx, sy), rad + 2)
            
            pygame.draw.circle(self.surface, col, (sx, sy), rad)
            
            # Draw Labels (only if zoomed in enough or hovered)
            if self.zoom > 0.8 or i == self.hovered_node_idx:
                text = self.font.render(n['label'], True, (200, 200, 200))
                self.surface.blit(text, (sx + rad + 4, sy - 6))

        return pygame.image.tostring(self.surface, 'RGB')

--------------------------------------------------------------------------------
FILE: _GraphEngineMS\requirements.txt
--------------------------------------------------------------------------------
pygame
Pillow
--------------------------------------------------------------------------------
FILE: _HeuristicSumMS\app.py
--------------------------------------------------------------------------------
import re
import os
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: REGEX PATTERNS
# ==============================================================================
# Captures: def my_func, class MyClass, function myFunc, interface MyInterface
SIG_RE = re.compile(r'^\s*(def|class|function|interface|struct|impl|func)\s+([A-Za-z_][A-Za-z0-9_]*)')

# Captures: # Heading, ## Subheading
MD_HDR_RE = re.compile(r'^\s{0,3}(#{1,3})\s+(.+)')

# Captures: """ Docstring """ or ''' Docstring ''' (Start of block)
DOC_RE = re.compile(r'^\s*("{3}|\'{3})(.*)', re.DOTALL)
# ==============================================================================

@service_metadata(
name="HeuristicSum",
version="1.0.0",
description="Generates quick summaries of code/text files using regex heuristics (No AI).",
tags=["parsing", "summary", "heuristics"],
capabilities=["compute"]
)
class HeuristicSumMS:
    """
The Skimmer: Generates quick summaries of code/text files without AI.
Scans for high-value lines (headers, signatures, docstrings) and concatenates them.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"text": "str", "filename": "str", "max_chars": "int"},
outputs={"summary": "str"},
description="Generates a summary string from the provided text.",
tags=["summary", "parsing"]
)
def summarize(self, text: str, filename: str = "", max_chars: int = 480) -> str:
        """
        Generates a summary string from the provided text.
        """
        lines = text.splitlines()
        picks = []

        # 1. Scan top 20 lines for Markdown Headers
        for ln in lines[:20]:
            m = MD_HDR_RE.match(ln)
            if m:
                picks.append(f"Heading: {m.group(2).strip()}")

        # 2. Scan top 40 lines for Code Signatures (Functions/Classes)
        for ln in lines[:40]:
            m = SIG_RE.match(ln)
            if m:
                picks.append(f"{m.group(1)} {m.group(2)}")

        # 3. Check for Docstrings / Preamble
        if lines:
            # Join first 80 lines to check for multi-line docstrings
            joined = "\n".join(lines[:80])
            m = DOC_RE.match(joined)
            if m:
                # Grab the first few lines of the docstring content
                after = joined.splitlines()[1:3]
                if after:
                    clean_doc = " ".join(s.strip() for s in after).strip()
                    picks.append(f"Doc: {clean_doc}")

        # 4. Fallback: First non-empty line if nothing else found
        if not picks:
            head = " ".join(l.strip() for l in lines[:2] if l.strip())
            if head:
                picks.append(head)

        # 5. Add Filename Context
        if filename:
            picks.append(f"[{os.path.basename(filename)}]")

        # 6. Deduplicate and Format
        seen = set()
        uniq = []
        for p in picks:
            if p and p not in seen:
                uniq.append(p)
                seen.add(p)

        summary = " | ".join(uniq)
        
        # 7. Truncate
        if len(summary) > max_chars:
            summary = summary[:max_chars-3] + "..."
            
        return summary.strip() if summary else "[No summary available]"

# --- Independent Test Block ---
if __name__ == "__main__":
skimmer = HeuristicSumMS()
print("Service ready:", skimmer)
    
# Test 1: Python Code
    py_code = """
    class DataProcessor:
        '''
        Handles the transformation of raw input data into structured formats.
        '''
        def process(self, data):
            pass
    """
    print(f"Python Summary: {skimmer.summarize(py_code, 'processor.py')}")

    # Test 2: Markdown
    md_text = """
    # Project Roadmap
    ## Phase 1
    We begin with ingestion.
    """
    print(f"Markdown Summary: {skimmer.summarize(md_text, 'README.md')}")

--------------------------------------------------------------------------------
FILE: _HeuristicSumMS\app.py.bak
--------------------------------------------------------------------------------
import re
import os
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: REGEX PATTERNS
# ==============================================================================
# Captures: def my_func, class MyClass, function myFunc, interface MyInterface
SIG_RE = re.compile(r'^\s*(def|class|function|interface|struct|impl|func)\s+([A-Za-z_][A-Za-z0-9_]*)')

# Captures: # Heading, ## Subheading
MD_HDR_RE = re.compile(r'^\s{0,3}(#{1,3})\s+(.+)')

# Captures: """ Docstring """ or ''' Docstring ''' (Start of block)
DOC_RE = re.compile(r'^\s*("{3}|\'{3})(.*)', re.DOTALL)
# ==============================================================================

@service_metadata(
name="HeuristicSum",
version="1.0.0",
description="Generates quick summaries of code/text files using regex heuristics (No AI).",
tags=["parsing", "summary", "heuristics"],
capabilities=["compute"]
)
class HeuristicSumMS:
    """
The Skimmer: Generates quick summaries of code/text files without AI.
Scans for high-value lines (headers, signatures, docstrings) and concatenates them.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"text": "str", "filename": "str", "max_chars": "int"},
outputs={"summary": "str"},
description="Generates a summary string from the provided text.",
tags=["summary", "parsing"]
)
def summarize(self, text: str, filename: str = "", max_chars: int = 480) -> str:
        """
        Generates a summary string from the provided text.
        """
        lines = text.splitlines()
        picks = []

        # 1. Scan top 20 lines for Markdown Headers
        for ln in lines[:20]:
            m = MD_HDR_RE.match(ln)
            if m:
                picks.append(f"Heading: {m.group(2).strip()}")

        # 2. Scan top 40 lines for Code Signatures (Functions/Classes)
        for ln in lines[:40]:
            m = SIG_RE.match(ln)
            if m:
                picks.append(f"{m.group(1)} {m.group(2)}")

        # 3. Check for Docstrings / Preamble
        if lines:
            # Join first 80 lines to check for multi-line docstrings
            joined = "\n".join(lines[:80])
            m = DOC_RE.match(joined)
            if m:
                # Grab the first few lines of the docstring content
                after = joined.splitlines()[1:3]
                if after:
                    clean_doc = " ".join(s.strip() for s in after).strip()
                    picks.append(f"Doc: {clean_doc}")

        # 4. Fallback: First non-empty line if nothing else found
        if not picks:
            head = " ".join(l.strip() for l in lines[:2] if l.strip())
            if head:
                picks.append(head)

        # 5. Add Filename Context
        if filename:
            picks.append(f"[{os.path.basename(filename)}]")

        # 6. Deduplicate and Format
        seen = set()
        uniq = []
        for p in picks:
            if p and p not in seen:
                uniq.append(p)
                seen.add(p)

        summary = " | ".join(uniq)
        
        # 7. Truncate
        if len(summary) > max_chars:
            summary = summary[:max_chars-3] + "..."
            
        return summary.strip() if summary else "[No summary available]"

# --- Independent Test Block ---
if __name__ == "__main__":
skimmer = HeuristicSumMS()
print("Service ready:", skimmer)
    
# Test 1: Python Code
    py_code = """
    class DataProcessor:
        '''
        Handles the transformation of raw input data into structured formats.
        '''
        def process(self, data):
            pass
    """
    print(f"Python Summary: {skimmer.summarize(py_code, 'processor.py')}")

    # Test 2: Markdown
    md_text = """
    # Project Roadmap
    ## Phase 1
    We begin with ingestion.
    """
    print(f"Markdown Summary: {skimmer.summarize(md_text, 'README.md')}")

--------------------------------------------------------------------------------
FILE: _IngestEngineMS\app.py
--------------------------------------------------------------------------------
import os
import time
import re
import sqlite3
import requests
import json
from typing import List, Generator, Dict, Any, Optional, Set
from dataclasses import dataclass
from microservice_std_lib import service_metadata, service_endpoint

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass
class IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

class SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    Used to generate the 'DEPENDS_ON' edges in the Knowledge Graph.
    """
    def __init__(self):
        # Python: "from x import y", "import x"
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        # JS/TS: "import ... from 'x'", "require('x')"
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        ext = os.path.splitext(file_path)[1].lower()
        
        lines = content.split('\n')
        for line in lines:
            match = None
            if ext == '.py':
                match = self.py_pattern.match(line)
            elif ext in ['.js', '.ts', '.tsx', '.jsx']:
                match = self.js_pattern.search(line)
            
            if match:
                # Clean up the module name (e.g., "backend.database" -> "database")
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        
        return dependencies

@service_metadata(
name="IngestEngine",
version="1.0.0",
description="Reads files, chunks text, fetches embeddings, and weaves graph edges.",
tags=["ingest", "rag", "parsing", "embedding"],
capabilities=["filesystem:read", "network:outbound", "db:sqlite"]
)
class IngestEngineMS:
    """
The Heavy Lifter: Reads files, chunks text, fetches embeddings,
populates the Graph Nodes, and weaves Graph Edges.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", "knowledge.db")
self.stop_signal = False
self.weaver = SynapseWeaver()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    @service_endpoint(
    inputs={"file_paths": "List[str]", "model_name": "str"},
    outputs={"status": "IngestStatus"},
    description="Processes a list of files, ingesting them into the knowledge graph.",
    tags=["ingest", "processing"],
    mode="generator",
    side_effects=["db:write", "network:outbound"]
    )
    def process_files(self, file_paths: List[str], model_name: str = "none") -> Generator[IngestStatus, None, None]:
    total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Optimization settings
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # Memory for graph weaving (Node Name -> Node ID)
        node_registry = {}
        file_contents = {} # Cache content for the weaving pass

        # --- PHASE 1: INGESTION (Files, Chunks, Nodes) ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)

            # 1. Read
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                file_contents[filename] = content # Cache for Phase 2
            except Exception as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"Error: {e}")
                continue

            # 2. Track File
            try:
                cursor.execute("INSERT OR REPLACE INTO files (path, last_updated) VALUES (?, ?)", 
                              (file_path, time.time()))
                file_id = cursor.lastrowid
            except sqlite3.Error:
                continue

            # 3. Create Graph Node (for Visualization)
            # We use the filename as the unique ID for the graph to make linking easier
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (filename, 'file', filename, json.dumps({"path": file_path})))
            
            node_registry[filename] = filename

            # 4. Chunking & Embedding
            chunks = self._chunk_text(content)
            
            for i, chunk_text in enumerate(chunks):
                if self.stop_signal: break
                
                embedding = None
                if model_name != "none":
                    embedding = self._get_embedding(model_name, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                    INSERT INTO chunks (file_id, chunk_index, content, embedding)
                    VALUES (?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": filename,
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            # Checkpoint per file
            conn.commit()

        # --- PHASE 2: WEAVING (Edges) ---
        yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
        
        edge_count = 0
        for filename, content in file_contents.items():
            if self.stop_signal: break
            
            # Find imports
            deps = self.weaver.extract_dependencies(content, filename)
            
            for dep in deps:
                # Naive matching: if 'database' is imported, look for 'database.py' or 'database.ts'
                # in our registry.
                target_id = None
                for potential_match in node_registry.keys():
                    if potential_match.startswith(dep + '.') or potential_match == dep:
                        target_id = potential_match
                        break
                
                if target_id and target_id != filename:
                    try:
                        cursor.execute("""
                            INSERT OR IGNORE INTO graph_edges (source, target, weight)
VALUES (?, ?, 1.0)
                        """, (filename, target_id))
                        edge_count += 1
                    except:
                        pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. Created {edge_count} dependency edges."
        )

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        if len(text) < chunk_size: return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += (chunk_size - overlap)
        return chunks

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": model, "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
# --- Independent Test Block ---
if __name__ == "__main__":
    TEST_DB = "test_ingest_v2.db"
    
    # Init DB Schema manually for test
    conn = sqlite3.connect(TEST_DB)
    conn.execute("CREATE TABLE IF NOT EXISTS files (id INTEGER PRIMARY KEY, path TEXT, last_updated REAL)")
    conn.execute("CREATE TABLE IF NOT EXISTS chunks (id INTEGER PRIMARY KEY, file_id INT, chunk_index INT, content TEXT, embedding BLOB)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)")
    conn.close()

    engine = IngestEngineMS({"db_path": TEST_DB})
    # Self-ingest to test dependency parsing
    files = ["_IngestEngineMS.py"] 
    
    print("Running Ingest V2...")
    for status in engine.process_files(files, "none"):
        print(f"[{status.progress_percent:.0f}%] {status.log_message}")
    
    # Verify Edges
    conn = sqlite3.connect(TEST_DB)
    edges = conn.execute("SELECT * FROM graph_edges").fetchall()
    nodes = conn.execute("SELECT * FROM graph_nodes").fetchall()
    print(f"\nResult: {len(nodes)} Nodes, {len(edges)} Edges.")
    conn.close()
    
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)

--------------------------------------------------------------------------------
FILE: _IngestEngineMS\app.py.bak
--------------------------------------------------------------------------------
import os
import time
import re
import sqlite3
import requests
import json
from typing import List, Generator, Dict, Any, Optional, Set
from dataclasses import dataclass
from microservice_std_lib import service_metadata, service_endpoint

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

@dataclass
class IngestStatus:
    current_file: str
    progress_percent: float
    processed_files: int
    total_files: int
    log_message: str
    thought_frame: Optional[Dict] = None

class SynapseWeaver:
    """
    Parses source code to extract import dependencies.
    Used to generate the 'DEPENDS_ON' edges in the Knowledge Graph.
    """
    def __init__(self):
        # Python: "from x import y", "import x"
        self.py_pattern = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')
        # JS/TS: "import ... from 'x'", "require('x')"
        self.js_pattern = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

    def extract_dependencies(self, content: str, file_path: str) -> List[str]:
        dependencies = []
        ext = os.path.splitext(file_path)[1].lower()
        
        lines = content.split('\n')
        for line in lines:
            match = None
            if ext == '.py':
                match = self.py_pattern.match(line)
            elif ext in ['.js', '.ts', '.tsx', '.jsx']:
                match = self.js_pattern.search(line)
            
            if match:
                # Clean up the module name (e.g., "backend.database" -> "database")
                raw_dep = match.group(1)
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                if clean_dep not in dependencies:
                    dependencies.append(clean_dep)
        
        return dependencies

@service_metadata(
name="IngestEngine",
version="1.0.0",
description="Reads files, chunks text, fetches embeddings, and weaves graph edges.",
tags=["ingest", "rag", "parsing", "embedding"],
capabilities=["filesystem:read", "network:outbound", "db:sqlite"]
)
class IngestEngineMS:
    """
The Heavy Lifter: Reads files, chunks text, fetches embeddings,
populates the Graph Nodes, and weaves Graph Edges.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", "knowledge.db")
self.stop_signal = False
self.weaver = SynapseWeaver()

    def abort(self):
        self.stop_signal = True

    def check_ollama_connection(self) -> bool:
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except:
            return False

    def get_available_models(self) -> List[str]:
        try:
            res = requests.get(f"{OLLAMA_API_URL}/tags")
            if res.status_code == 200:
                data = res.json()
                return [m['name'] for m in data.get('models', [])]
        except:
            pass
        return []

    @service_endpoint(
    inputs={"file_paths": "List[str]", "model_name": "str"},
    outputs={"status": "IngestStatus"},
    description="Processes a list of files, ingesting them into the knowledge graph.",
    tags=["ingest", "processing"],
    mode="generator",
    side_effects=["db:write", "network:outbound"]
    )
    def process_files(self, file_paths: List[str], model_name: str = "none") -> Generator[IngestStatus, None, None]:
    total = len(file_paths)
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Optimization settings
        cursor.execute("PRAGMA synchronous = OFF")
        cursor.execute("PRAGMA journal_mode = MEMORY")

        # Memory for graph weaving (Node Name -> Node ID)
        node_registry = {}
        file_contents = {} # Cache content for the weaving pass

        # --- PHASE 1: INGESTION (Files, Chunks, Nodes) ---
        for idx, file_path in enumerate(file_paths):
            if self.stop_signal:
                yield IngestStatus(file_path, 0, idx, total, "Ingestion Aborted.")
                break

            filename = os.path.basename(file_path)

            # 1. Read
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                file_contents[filename] = content # Cache for Phase 2
            except Exception as e:
                yield IngestStatus(file_path, (idx/total)*100, idx, total, f"Error: {e}")
                continue

            # 2. Track File
            try:
                cursor.execute("INSERT OR REPLACE INTO files (path, last_updated) VALUES (?, ?)", 
                              (file_path, time.time()))
                file_id = cursor.lastrowid
            except sqlite3.Error:
                continue

            # 3. Create Graph Node (for Visualization)
            # We use the filename as the unique ID for the graph to make linking easier
            cursor.execute("""
                INSERT OR REPLACE INTO graph_nodes (id, type, label, data_json)
                VALUES (?, ?, ?, ?)
            """, (filename, 'file', filename, json.dumps({"path": file_path})))
            
            node_registry[filename] = filename

            # 4. Chunking & Embedding
            chunks = self._chunk_text(content)
            
            for i, chunk_text in enumerate(chunks):
                if self.stop_signal: break
                
                embedding = None
                if model_name != "none":
                    embedding = self._get_embedding(model_name, chunk_text)
                
                emb_blob = json.dumps(embedding).encode('utf-8') if embedding else None
                
                cursor.execute("""
                    INSERT INTO chunks (file_id, chunk_index, content, embedding)
                    VALUES (?, ?, ?, ?)
                """, (file_id, i, chunk_text, emb_blob))

                # Visual Feedback
                thought_frame = {
                    "id": f"{file_id}_{i}",
                    "file": filename,
                    "chunk_index": i,
                    "content": chunk_text,
                    "vector_preview": embedding[:20] if embedding else [],
                    "concept_color": "#007ACC"
                }
                
                yield IngestStatus(
                    current_file=filename,
                    progress_percent=((idx + (i/len(chunks))) / total) * 100,
                    processed_files=idx,
                    total_files=total,
                    log_message=f"Processing {filename}...",
                    thought_frame=thought_frame
                )

            # Checkpoint per file
            conn.commit()

        # --- PHASE 2: WEAVING (Edges) ---
        yield IngestStatus("Graph", 100, total, total, "Weaving Knowledge Graph...")
        
        edge_count = 0
        for filename, content in file_contents.items():
            if self.stop_signal: break
            
            # Find imports
            deps = self.weaver.extract_dependencies(content, filename)
            
            for dep in deps:
                # Naive matching: if 'database' is imported, look for 'database.py' or 'database.ts'
                # in our registry.
                target_id = None
                for potential_match in node_registry.keys():
                    if potential_match.startswith(dep + '.') or potential_match == dep:
                        target_id = potential_match
                        break
                
                if target_id and target_id != filename:
                    try:
                        cursor.execute("""
                            INSERT OR IGNORE INTO graph_edges (source, target, weight)
                            VALUES (?, ?, 1.0)
                        """, (filename, target_id))
                        edge_count += 1
                    except:
                        pass

        conn.commit()
        conn.close()

        yield IngestStatus(
            current_file="Complete",
            progress_percent=100,
            processed_files=total,
            total_files=total,
            log_message=f"Ingestion Complete. Created {edge_count} dependency edges."
        )

    def _chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
        if len(text) < chunk_size: return [text]
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += (chunk_size - overlap)
        return chunks

    def _get_embedding(self, model: str, text: str) -> Optional[List[float]]:
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": model, "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

# --- Independent Test Block ---
if __name__ == "__main__":
    TEST_DB = "test_ingest_v2.db"
    
    # Init DB Schema manually for test
    conn = sqlite3.connect(TEST_DB)
    conn.execute("CREATE TABLE IF NOT EXISTS files (id INTEGER PRIMARY KEY, path TEXT, last_updated REAL)")
    conn.execute("CREATE TABLE IF NOT EXISTS chunks (id INTEGER PRIMARY KEY, file_id INT, chunk_index INT, content TEXT, embedding BLOB)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_nodes (id TEXT PRIMARY KEY, type TEXT, label TEXT, data_json TEXT)")
    conn.execute("CREATE TABLE IF NOT EXISTS graph_edges (source TEXT, target TEXT, weight REAL)")
    conn.close()

    engine = IngestEngineMS({"db_path": TEST_DB})
    # Self-ingest to test dependency parsing
    files = ["_IngestEngineMS.py"] 
    
    print("Running Ingest V2...")
    for status in engine.process_files(files, "none"):
        print(f"[{status.progress_percent:.0f}%] {status.log_message}")
    
    # Verify Edges
    conn = sqlite3.connect(TEST_DB)
    edges = conn.execute("SELECT * FROM graph_edges").fetchall()
    nodes = conn.execute("SELECT * FROM graph_nodes").fetchall()
    print(f"\nResult: {len(nodes)} Nodes, {len(edges)} Edges.")
    conn.close()
    
    if os.path.exists(TEST_DB):
        os.remove(TEST_DB)

--------------------------------------------------------------------------------
FILE: _IngestEngineMS\requirements.txt
--------------------------------------------------------------------------------
requests
--------------------------------------------------------------------------------
FILE: _IsoProcessMS\app.py
--------------------------------------------------------------------------------
import multiprocessing as mp
import logging
import logging.handlers
import time
import queue
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# WORKER LOGIC (Runs in Child Process)
# ==============================================================================
def _isolated_worker(result_queue: mp.Queue, log_queue: mp.Queue, payload: Any, config: Dict[str, Any]):
    """
    Entry point for the child process.
    Configures a logging handler to send records back to the parent.
    """
    # 1. Setup Logging Bridge
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    # Clear default handlers to avoid duplicate prints in child
    for h in root.handlers[:]:
        root.removeHandler(h)
    
    # Send all logs to the parent via the queue
    qh = logging.handlers.QueueHandler(log_queue)
    root.addHandler(qh)
    
    log = logging.getLogger("IsoWorker")

    try:
        log.info(f"Worker PID {mp.current_process().pid} started.")
        
        # --- 2. Heavy Imports (Simulated) ---
        log.info("Loading heavy libraries (Torch/Transformers)...")
        # from transformers import pipeline
        time.sleep(0.2) # Simulate import time

        # --- 3. The Logic ---
        model_name = config.get("model_name", "default-model")
        log.info(f"Initializing model '{model_name}'...")
        
        # Simulate processing steps with progress reporting
        for i in range(1, 4):
            time.sleep(0.3)
            log.info(f"Processing chunk {i}/3...")
        
        processed_data = f"Processed({payload}) via {model_name}"
        
        # --- 4. Return Result ---
        log.info("Work complete. Returning result.")
        result_queue.put({"success": True, "data": processed_data})

    except Exception as e:
        log.exception("Critical failure in worker process.")
        result_queue.put({"success": False, "error": str(e)})

# ==============================================================================
# PARENT CONTROLLER (Runs in Main Process)
# ==============================================================================
@service_metadata(
name="IsoProcess",
version="1.0.0",
description="Spawns isolated processes with real-time logging feedback.",
tags=["process", "isolation", "safety"],
capabilities=["process:spawn"]
)
class IsoProcessMS:
    """
The Safety Valve: Spawns isolated processes with real-time logging feedback.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.timeout = self.config.get("timeout_seconds", 60)
        
# Setup main logger
        self.log = logging.getLogger("IsoParent")
        if not self.log.handlers:
            logging.basicConfig(
                level=logging.INFO, 
                format='%(asctime)s [%(name)s] %(message)s',
                datefmt='%H:%M:%S'
            )

    @service_endpoint(
    inputs={"payload": "Any", "config": "Dict"},
    outputs={"result": "Any"},
    description="Executes a payload in an isolated child process.",
    tags=["process", "execution"],
    side_effects=["process:spawn"]
    )
def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
        config = config or {}
        
        # 1. Setup Queues
        ctx = mp.get_context("spawn")
        result_queue = ctx.Queue()
        log_queue = ctx.Queue()

        # 2. Setup Log Listener (The "Ear" of the parent)
        # This thread pulls logs from the queue and handles them in the main process
        listener = logging.handlers.QueueListener(log_queue, *logging.getLogger().handlers)
        listener.start()

        # 3. Launch Process
        process = ctx.Process(
            target=_isolated_worker,
            args=(result_queue, log_queue, payload, config)
        )
        
        self.log.info("🚀 Spawning isolated process...")
        process.start()
        
        try:
            # 4. Wait for Result
            result_packet = result_queue.get(timeout=self.timeout)
            process.join()

            if result_packet["success"]:
                return result_packet["data"]
            else:
                raise RuntimeError(f"Worker Error: {result_packet['error']}")

        except queue.Empty:
            self.log.error("⏳ Worker timed out! Terminating...")
            process.terminate()
            process.join()
            raise TimeoutError(f"Task exceeded {self.timeout}s limit.")
            
        finally:
            # Clean up the log listener so it doesn't hang
listener.stop()

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing IsoProcessMS with Live Logging ---")
    iso = IsoProcessMS({"timeout_seconds": 5})
    print("Service ready:", iso)
    
try:
        result = iso.execute("Sensitive Data", {"model_name": "DeepSeek-V3"})
        print(f"\n[Parent] Final Result: {result}")
    except Exception as e:
        print(f"\n[Parent] Failed: {e}")

--------------------------------------------------------------------------------
FILE: _IsoProcessMS\app.py.bak
--------------------------------------------------------------------------------
import multiprocessing as mp
import logging
import logging.handlers
import time
import queue
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# WORKER LOGIC (Runs in Child Process)
# ==============================================================================
def _isolated_worker(result_queue: mp.Queue, log_queue: mp.Queue, payload: Any, config: Dict[str, Any]):
    """
    Entry point for the child process.
    Configures a logging handler to send records back to the parent.
    """
    # 1. Setup Logging Bridge
    root = logging.getLogger()
    root.setLevel(logging.INFO)
    # Clear default handlers to avoid duplicate prints in child
    for h in root.handlers[:]:
        root.removeHandler(h)
    
    # Send all logs to the parent via the queue
    qh = logging.handlers.QueueHandler(log_queue)
    root.addHandler(qh)
    
    log = logging.getLogger("IsoWorker")

    try:
        log.info(f"Worker PID {mp.current_process().pid} started.")
        
        # --- 2. Heavy Imports (Simulated) ---
        log.info("Loading heavy libraries (Torch/Transformers)...")
        # from transformers import pipeline
        time.sleep(0.2) # Simulate import time

        # --- 3. The Logic ---
        model_name = config.get("model_name", "default-model")
        log.info(f"Initializing model '{model_name}'...")
        
        # Simulate processing steps with progress reporting
        for i in range(1, 4):
            time.sleep(0.3)
            log.info(f"Processing chunk {i}/3...")
        
        processed_data = f"Processed({payload}) via {model_name}"
        
        # --- 4. Return Result ---
        log.info("Work complete. Returning result.")
        result_queue.put({"success": True, "data": processed_data})

    except Exception as e:
        log.exception("Critical failure in worker process.")
        result_queue.put({"success": False, "error": str(e)})

# ==============================================================================
# PARENT CONTROLLER (Runs in Main Process)
# ==============================================================================
@service_metadata(
name="IsoProcess",
version="1.0.0",
description="Spawns isolated processes with real-time logging feedback.",
tags=["process", "isolation", "safety"],
capabilities=["process:spawn"]
)
class IsoProcessMS:
    """
The Safety Valve: Spawns isolated processes with real-time logging feedback.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.timeout = self.config.get("timeout_seconds", 60)
        
# Setup main logger
        self.log = logging.getLogger("IsoParent")
        if not self.log.handlers:
            logging.basicConfig(
                level=logging.INFO, 
                format='%(asctime)s [%(name)s] %(message)s',
                datefmt='%H:%M:%S'
            )

    @service_endpoint(
    inputs={"payload": "Any", "config": "Dict"},
    outputs={"result": "Any"},
    description="Executes a payload in an isolated child process.",
    tags=["process", "execution"],
    side_effects=["process:spawn"]
    )
    def execute(self, payload: Any, config: Optional[Dict[str, Any]] = None) -> Any:
    config = config or {}
        
    # 1. Setup Queues
        ctx = mp.get_context("spawn")
        result_queue = ctx.Queue()
        log_queue = ctx.Queue()

        # 2. Setup Log Listener (The "Ear" of the parent)
        # This thread pulls logs from the queue and handles them in the main process
        listener = logging.handlers.QueueListener(log_queue, *logging.getLogger().handlers)
        listener.start()

        # 3. Launch Process
        process = ctx.Process(
            target=_isolated_worker,
            args=(result_queue, log_queue, payload, config)
        )
        
        self.log.info("🚀 Spawning isolated process...")
        process.start()
        
        try:
            # 4. Wait for Result
            result_packet = result_queue.get(timeout=self.timeout)
            process.join()

            if result_packet["success"]:
                return result_packet["data"]
            else:
                raise RuntimeError(f"Worker Error: {result_packet['error']}")

        except queue.Empty:
            self.log.error("⏳ Worker timed out! Terminating...")
            process.terminate()
            process.join()
            raise TimeoutError(f"Task exceeded {self.timeout}s limit.")
            
        finally:
            # Clean up the log listener so it doesn't hang
            listener.stop()

# --- Independent Test Block ---
if __name__ == "__main__":
print("--- Testing IsoProcessMS with Live Logging ---")
iso = IsoProcessMS({"timeout_seconds": 5})
print("Service ready:", iso)
    
try:
        result = iso.execute("Sensitive Data", {"model_name": "DeepSeek-V3"})
        print(f"\n[Parent] Final Result: {result}")
    except Exception as e:
        print(f"\n[Parent] Failed: {e}")

--------------------------------------------------------------------------------
FILE: _LexicalSearchMS\app.py
--------------------------------------------------------------------------------
import sqlite3
import json
import os
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="LexicalSearch",
version="1.0.0",
description="Lightweight BM25 keyword search using SQLite FTS5 (No AI required).",
tags=["search", "index", "sqlite"],
capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class LexicalSearchMS:
    """
The Librarian's Index: A lightweight, AI-free search engine.
    
Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
Ideal for environments where installing PyTorch/Transformers is impossible
or overkill.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
default_db = str(Path(__file__).parent / "lexical_index.db")
self.db_path = self.config.get("db_path", default_db)
self._init_db()

    def _init_db(self):
        """
        Sets up the schema. 
        Uses Triggers to automatically keep the FTS index in sync with the main table.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1. Main Content Table (Stores the actual data)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                metadata TEXT  -- JSON blob for extra info (path, author, etc)
            );
        """)
        
        # 2. Virtual FTS Table (The Search Index)
        # content='documents' means it references the table above (saves space)
        cur.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                content,
                content='documents',
                content_rowid='rowid'  -- Internal SQLite mapping
            );
        """)

        # 3. Triggers (The "Magic" - Auto-sync index on Insert/Delete/Update)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_au AFTER UPDATE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        
        conn.commit()
        conn.close()

    @service_endpoint(
    inputs={"doc_id": "str", "text": "str", "metadata": "Dict"},
    outputs={},
    description="Adds or updates a document in the FTS index.",
    tags=["search", "write"],
    side_effects=["db:write"]
    )
    def add_document(self, doc_id: str, text: str, metadata: Dict[str, Any] = None):
    """
    Adds or updates a document in the index.
    """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        meta_json = json.dumps(metadata or {})
        
        # Upsert logic (Replace if ID exists)
        cur.execute("""
            INSERT OR REPLACE INTO documents (id, content, metadata)
            VALUES (?, ?, ?)
        """, (doc_id, text, meta_json))
        
        conn.commit()
        conn.close()

    @service_endpoint(
    inputs={"query": "str", "top_k": "int"},
    outputs={"results": "List[Dict]"},
    description="Performs a BM25 ranked keyword search.",
    tags=["search", "read"],
    side_effects=["db:read"]
    )
    def search(self, query: str, top_k: int = 20) -> List[Dict]:
    """
    Performs a BM25 Ranked Search.
    """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row # Allows dict-like access
        cur = conn.cursor()
        
        try:
            # The SQL Magic: 'bm25(documents_fts)' calculates relevance score
            sql = """
                SELECT 
                    d.id, 
                    d.content, 
                    d.metadata,
                    snippet(documents_fts, 0, '<b>', '</b>', '...', 15) as preview,
                    bm25(documents_fts) as score
                FROM documents_fts 
                JOIN documents d ON d.rowid = documents_fts.rowid
                WHERE documents_fts MATCH ? 
                ORDER BY score ASC
                LIMIT ?
            """
            # FTS5 query syntax: quotes typically help with special chars
            safe_query = f'"{query}"'
            rows = cur.execute(sql, (safe_query, top_k)).fetchall()
            
            results = []
            for r in rows:
                results.append({
                    "id": r['id'],
                    "score": round(r['score'], 4),
                    "preview": r['preview'], # FTS5 auto-generates snippets!
                    "metadata": json.loads(r['metadata']),
                    "full_content": r['content']
                })
            
            return results
            
        except sqlite3.OperationalError as e:
            # Usually happens if query syntax is bad (e.g. unmatched quotes)
            print(f"Search syntax error: {e}")
            return []
        finally:
            conn.close()

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_name = "test_lexical.db"
    
    # 1. Init
    engine = LexicalSearchMS({"db_path": db_name})
    print("Service ready:", engine)
    
    # 2. Ingest Data
    print("Ingesting test data...")
    engine.add_document("doc1", "Python is a great language for data science.", {"category": "coding"})
    engine.add_document("doc2", "The snake python is a reptile found in jungles.", {"category": "biology"})
    engine.add_document("doc3", "Data science involves python, pandas, and SQL.", {"category": "coding"})
    
# 3. Search
query = "python data"
print(f"\nSearching for: '{query}'")
hits = engine.search(query)
    
for hit in hits:
    print(f"[{hit['score']:.4f}] {hit['id']} ({hit['metadata']['category']})")
    print(f"   Preview: {hit['preview']}")
        
# Cleanup
if os.path.exists(db_name):
    os.remove(db_name)

--------------------------------------------------------------------------------
FILE: _LexicalSearchMS\app.py.bak
--------------------------------------------------------------------------------
import sqlite3
import json
import os
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="LexicalSearch",
version="1.0.0",
description="Lightweight BM25 keyword search using SQLite FTS5 (No AI required).",
tags=["search", "index", "sqlite"],
capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class LexicalSearchMS:
    """
The Librarian's Index: A lightweight, AI-free search engine.
    
Uses SQLite's FTS5 extension to provide fast, ranked keyword search (BM25).
Ideal for environments where installing PyTorch/Transformers is impossible
or overkill.
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
default_db = str(Path(__file__).parent / "lexical_index.db")
self.db_path = self.config.get("db_path", default_db)
self._init_db()

    def _init_db(self):
        """
        Sets up the schema. 
        Uses Triggers to automatically keep the FTS index in sync with the main table.
        """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        # 1. Main Content Table (Stores the actual data)
        cur.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT,
                metadata TEXT  -- JSON blob for extra info (path, author, etc)
            );
        """)
        
        # 2. Virtual FTS Table (The Search Index)
        # content='documents' means it references the table above (saves space)
        cur.execute("""
            CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
                content,
                content='documents',
                content_rowid='rowid'  -- Internal SQLite mapping
            );
        """)

        # 3. Triggers (The "Magic" - Auto-sync index on Insert/Delete/Update)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ai AFTER INSERT ON documents BEGIN
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_ad AFTER DELETE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
            END;
        """)
        cur.execute("""
            CREATE TRIGGER IF NOT EXISTS doc_au AFTER UPDATE ON documents BEGIN
                INSERT INTO documents_fts(documents_fts, rowid, content) VALUES('delete', old.rowid, old.content);
                INSERT INTO documents_fts(rowid, content) VALUES (new.rowid, new.content);
            END;
        """)
        
        conn.commit()
        conn.close()

    @service_endpoint(
    inputs={"doc_id": "str", "text": "str", "metadata": "Dict"},
    outputs={},
    description="Adds or updates a document in the FTS index.",
    tags=["search", "write"],
    side_effects=["db:write"]
    )
    def add_document(self, doc_id: str, text: str, metadata: Dict[str, Any] = None):
    """
    Adds or updates a document in the index.
    """
        conn = sqlite3.connect(self.db_path)
        cur = conn.cursor()
        
        meta_json = json.dumps(metadata or {})
        
        # Upsert logic (Replace if ID exists)
        cur.execute("""
            INSERT OR REPLACE INTO documents (id, content, metadata)
            VALUES (?, ?, ?)
        """, (doc_id, text, meta_json))
        
        conn.commit()
        conn.close()

    @service_endpoint(
    inputs={"query": "str", "top_k": "int"},
    outputs={"results": "List[Dict]"},
    description="Performs a BM25 ranked keyword search.",
    tags=["search", "read"],
    side_effects=["db:read"]
    )
    def search(self, query: str, top_k: int = 20) -> List[Dict]:
    """
    Performs a BM25 Ranked Search.
    """
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row # Allows dict-like access
        cur = conn.cursor()
        
        try:
            # The SQL Magic: 'bm25(documents_fts)' calculates relevance score
            sql = """
                SELECT 
                    d.id, 
                    d.content, 
                    d.metadata,
                    snippet(documents_fts, 0, '<b>', '</b>', '...', 15) as preview,
                    bm25(documents_fts) as score
                FROM documents_fts 
                JOIN documents d ON d.rowid = documents_fts.rowid
                WHERE documents_fts MATCH ? 
                ORDER BY score ASC
                LIMIT ?
            """
            # FTS5 query syntax: quotes typically help with special chars
            safe_query = f'"{query}"'
            rows = cur.execute(sql, (safe_query, top_k)).fetchall()
            
            results = []
            for r in rows:
                results.append({
                    "id": r['id'],
                    "score": round(r['score'], 4),
                    "preview": r['preview'], # FTS5 auto-generates snippets!
                    "metadata": json.loads(r['metadata']),
                    "full_content": r['content']
                })
            
            return results
            
        except sqlite3.OperationalError as e:
            # Usually happens if query syntax is bad (e.g. unmatched quotes)
            print(f"Search syntax error: {e}")
            return []
        finally:
            conn.close()

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    db_name = "test_lexical.db"
    
    # 1. Init
    engine = LexicalSearchMS({"db_path": db_name})
    print("Service ready:", engine)
    
    # 2. Ingest Data
    print("Ingesting test data...")
    engine.add_document("doc1", "Python is a great language for data science.", {"category": "coding"})
    engine.add_document("doc2", "The snake python is a reptile found in jungles.", {"category": "biology"})
    engine.add_document("doc3", "Data science involves python, pandas, and SQL.", {"category": "coding"})
    
    # 3. Search
    query = "python data"
    print(f"\nSearching for: '{query}'")
    hits = engine.search(query)
    
    for hit in hits:
        print(f"[{hit['score']:.4f}] {hit['id']} ({hit['metadata']['category']})")
        print(f"   Preview: {hit['preview']}")
        
    # Cleanup
    if os.path.exists(db_name):
        os.remove(db_name)


--------------------------------------------------------------------------------
FILE: _LibrarianServiceMS\app.py
--------------------------------------------------------------------------------
import os
import shutil
import sqlite3
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="LibrarianService",
version="1.0.0",
description="Manages the lifecycle (create, list, delete) of Knowledge Base files.",
tags=["kb", "management", "filesystem"],
capabilities=["filesystem:read", "filesystem:write", "db:sqlite"]
)
class LibrarianMS:
    """
The Librarian: Manages the physical creation, deletion, and listing
of Knowledge Base (KB) files.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
storage_dir = self.config.get("storage_dir", "./cortex_dbs")
self.storage_dir = Path(storage_dir)
self.storage_dir.mkdir(parents=True, exist_ok=True)

    @service_endpoint(
    inputs={},
    outputs={"kbs": "List[str]"},
    description="Lists available Knowledge Base files.",
    tags=["kb", "read"],
    side_effects=["filesystem:read"]
    )
    def list_kbs(self) -> List[str]:
    """
    Scans the storage directory for .db files.
    Equivalent to api.listKBs() in Sidebar.tsx.
    """
        if not self.storage_dir.exists():
            return []
        
        # Return simple filenames sorted by modification time (newest first)
        files = list(self.storage_dir.glob("*.db"))
        files.sort(key=os.path.getmtime, reverse=True)
        return [f.name for f in files]

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"status": "Dict"},
    description="Creates a new Knowledge Base with the standard schema.",
    tags=["kb", "create"],
    side_effects=["filesystem:write", "db:write"]
    )
    def create_kb(self, name: str) -> Dict[str, str]:
    """
    Creates a new SQLite database and initializes the Cortex Schema.
    """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            raise FileExistsError(f"Knowledge Base '{safe_name}' already exists.")

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # --- THE CORTEX SCHEMA ---
            # 1. System Config: Stores version and global metadata
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS config (
                    key TEXT PRIMARY KEY,
                    value TEXT
                )
            """)
            
            # 2. Files: Tracks scanned files to avoid re-ingesting unchanged ones
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    path TEXT UNIQUE NOT NULL,
                    checksum TEXT,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'indexed'
                )
            """)
            
            # 3. Chunks: The actual atomic units of knowledge
            # Note: 'embedding' is stored as a BLOB (bytes) for raw vector data
cursor.execute("""
                CREATE TABLE IF NOT EXISTS chunks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_id INTEGER,
                    chunk_index INTEGER,
                    content TEXT,
                    embedding BLOB, 
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)
            
            # 4. Graph Nodes: For the GraphView visualization
            # Distinguishes between 'file' nodes and 'concept' nodes
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_nodes (
                    id TEXT PRIMARY KEY,
                    type TEXT,  -- 'file' or 'concept'
                    label TEXT,
                    data_json TEXT -- Flexible JSON for positions/colors
                )
            """)
            
            # 5. Graph Edges: The connections
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_edges (
                    source TEXT,
                    target TEXT,
                    weight REAL DEFAULT 1.0,
                    FOREIGN KEY(source) REFERENCES graph_nodes(id),
                    FOREIGN KEY(target) REFERENCES graph_nodes(id)
                )
            """)
            
            # Timestamp creation
            cursor.execute("INSERT INTO config (key, value) VALUES (?, ?)", 
                           ("created_at", str(time.time())))
            
            conn.commit()
            conn.close()
            return {"status": "success", "path": str(db_path), "name": safe_name}
            
        except Exception as e:
            # Cleanup on failure
            if db_path.exists():
                os.remove(db_path)
            raise e

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"success": "bool"},
    description="Deletes a Knowledge Base file.",
    tags=["kb", "delete"],
    side_effects=["filesystem:write"]
    )
    def delete_kb(self, name: str) -> bool:
    """
    Physically removes the database file.
    """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            os.remove(db_path)
            return True
        return False

    @service_endpoint(
    inputs={"source_name": "str"},
    outputs={"status": "Dict"},
    description="Creates a copy of an existing KB.",
    tags=["kb", "copy"],
    side_effects=["filesystem:write"]
    )
    def duplicate_kb(self, source_name: str) -> Dict[str, str]:
    """
    Creates a copy of an existing KB.
    """
        safe_source = self._sanitize_name(source_name)
        source_path = self.storage_dir / safe_source
        
if not source_path.exists():
            raise FileNotFoundError(f"Source KB '{safe_source}' not found.")

# Generate new name
base = safe_source.replace('.db', '')
new_name = f"{base}_copy.db"
dest_path = self.storage_dir / new_name

# Handle collision if copy already exists
counter = 1
while dest_path.exists():
    new_name = f"{base}_copy_{counter}.db"
    dest_path = self.storage_dir / new_name
    counter += 1

shutil.copy2(source_path, dest_path)
return {"status": "success", "name": new_name}

def _sanitize_name(self, name: str) -> str:
    """Ensures the filename ends in .db and has no illegal chars."""
    clean = "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).strip()
    clean = clean.replace(' ', '_')
    if not clean.endswith('.db'):
        clean += '.db'
    return clean

# --- Independent Test Block ---
if __name__ == "__main__":
print("Initializing Librarian Service...")
lib = LibrarianMS({"storage_dir": "./test_brains"})
print("Service ready:", lib)
    
    # 1. Create
    print("Creating 'Project_Alpha'...")
    try:
        lib.create_kb("Project Alpha")
    except FileExistsError:
        print("Project Alpha already exists.")
        
    # 2. List
kbs = lib.list_kbs()
print(f"Available Brains: {kbs}")

# 3. Duplicate
if "Project_Alpha.db" in kbs:
    print("Duplicating Alpha...")
    lib.duplicate_kb("Project_Alpha.db")

# 4. Final List
print(f"Final Brains: {lib.list_kbs()}")

--------------------------------------------------------------------------------
FILE: _LibrarianServiceMS\app.py.bak
--------------------------------------------------------------------------------
import os
import shutil
import sqlite3
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="LibrarianService",
version="1.0.0",
description="Manages the lifecycle (create, list, delete) of Knowledge Base files.",
tags=["kb", "management", "filesystem"],
capabilities=["filesystem:read", "filesystem:write", "db:sqlite"]
)
class LibrarianMS:
    """
The Librarian: Manages the physical creation, deletion, and listing
of Knowledge Base (KB) files.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
storage_dir = self.config.get("storage_dir", "./cortex_dbs")
self.storage_dir = Path(storage_dir)
self.storage_dir.mkdir(parents=True, exist_ok=True)

    @service_endpoint(
    inputs={},
    outputs={"kbs": "List[str]"},
    description="Lists available Knowledge Base files.",
    tags=["kb", "read"],
    side_effects=["filesystem:read"]
    )
    def list_kbs(self) -> List[str]:
    """
    Scans the storage directory for .db files.
    Equivalent to api.listKBs() in Sidebar.tsx.
    """
        if not self.storage_dir.exists():
            return []
        
        # Return simple filenames sorted by modification time (newest first)
        files = list(self.storage_dir.glob("*.db"))
        files.sort(key=os.path.getmtime, reverse=True)
        return [f.name for f in files]

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"status": "Dict"},
    description="Creates a new Knowledge Base with the standard schema.",
    tags=["kb", "create"],
    side_effects=["filesystem:write", "db:write"]
    )
    def create_kb(self, name: str) -> Dict[str, str]:
    """
    Creates a new SQLite database and initializes the Cortex Schema.
    """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            raise FileExistsError(f"Knowledge Base '{safe_name}' already exists.")

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # --- THE CORTEX SCHEMA ---
            # 1. System Config: Stores version and global metadata
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS config (
                    key TEXT PRIMARY KEY,
                    value TEXT
                )
            """)
            
            # 2. Files: Tracks scanned files to avoid re-ingesting unchanged ones
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS files (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    path TEXT UNIQUE NOT NULL,
                    checksum TEXT,
                    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    status TEXT DEFAULT 'indexed'
                )
            """)
            
            # 3. Chunks: The actual atomic units of knowledge
            # Note: 'embedding' is stored as a BLOB (bytes) for raw vector data
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS chunks (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_id INTEGER,
                    chunk_index INTEGER,
                    content TEXT,
                    embedding BLOB, 
                    FOREIGN KEY(file_id) REFERENCES files(id)
                )
            """)
            
            # 4. Graph Nodes: For the GraphView visualization
            # Distinguishes between 'file' nodes and 'concept' nodes
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_nodes (
                    id TEXT PRIMARY KEY,
                    type TEXT,  -- 'file' or 'concept'
                    label TEXT,
                    data_json TEXT -- Flexible JSON for positions/colors
                )
            """)
            
            # 5. Graph Edges: The connections
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS graph_edges (
                    source TEXT,
                    target TEXT,
                    weight REAL DEFAULT 1.0,
                    FOREIGN KEY(source) REFERENCES graph_nodes(id),
                    FOREIGN KEY(target) REFERENCES graph_nodes(id)
                )
            """)
            
            # Timestamp creation
            cursor.execute("INSERT INTO config (key, value) VALUES (?, ?)", 
                           ("created_at", str(time.time())))
            
            conn.commit()
            conn.close()
            return {"status": "success", "path": str(db_path), "name": safe_name}
            
        except Exception as e:
            # Cleanup on failure
            if db_path.exists():
                os.remove(db_path)
            raise e

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"success": "bool"},
    description="Deletes a Knowledge Base file.",
    tags=["kb", "delete"],
    side_effects=["filesystem:write"]
    )
    def delete_kb(self, name: str) -> bool:
    """
    Physically removes the database file.
    """
        safe_name = self._sanitize_name(name)
        db_path = self.storage_dir / safe_name
        
        if db_path.exists():
            os.remove(db_path)
            return True
        return False

    @service_endpoint(
    inputs={"source_name": "str"},
    outputs={"status": "Dict"},
    description="Creates a copy of an existing KB.",
    tags=["kb", "copy"],
    side_effects=["filesystem:write"]
    )
    def duplicate_kb(self, source_name: str) -> Dict[str, str]:
    """
    Creates a copy of an existing KB.
    """
        safe_source = self._sanitize_name(source_name)
        source_path = self.storage_dir / safe_source
        
        if not source_path.exists():
            raise FileNotFoundError(f"Source KB '{safe_source}' not found.")
            
        # Generate new name
        base = safe_source.replace('.db', '')
        new_name = f"{base}_copy.db"
        dest_path = self.storage_dir / new_name
        
        # Handle collision if copy already exists
        counter = 1
        while dest_path.exists():
            new_name = f"{base}_copy_{counter}.db"
            dest_path = self.storage_dir / new_name
            counter += 1
            
        shutil.copy2(source_path, dest_path)
        return {"status": "success", "name": new_name}

    def _sanitize_name(self, name: str) -> str:
        """Ensures the filename ends in .db and has no illegal chars."""
        clean = "".join(c for c in name if c.isalnum() or c in (' ', '_', '-')).strip()
        clean = clean.replace(' ', '_')
        if not clean.endswith('.db'):
            clean += '.db'
        return clean

# --- Independent Test Block ---
if __name__ == "__main__":
print("Initializing Librarian Service...")
lib = LibrarianMS({"storage_dir": "./test_brains"})
print("Service ready:", lib)
    
    # 1. Create
    print("Creating 'Project_Alpha'...")
    try:
        lib.create_kb("Project Alpha")
    except FileExistsError:
        print("Project Alpha already exists.")
        
    # 2. List
    kbs = lib.list_kbs()
    print(f"Available Brains: {kbs}")
    
    # 3. Duplicate
    if "Project_Alpha.db" in kbs:
        print("Duplicating Alpha...")
        lib.duplicate_kb("Project_Alpha.db")
        
    # 4. Final List
    print(f"Final Brains: {lib.list_kbs()}")

--------------------------------------------------------------------------------
FILE: _LogViewMS\app.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import scrolledtext, filedialog
import queue
import logging
import datetime
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

class QueueHandler(logging.Handler):
    """Sends log records to a Tkinter-safe queue."""
def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.log_queue.put(record)

@service_metadata(
name="LogView",
version="1.0.0",
description="A thread-safe log viewer widget for Tkinter.",
tags=["ui", "logs", "widget"],
capabilities=["ui:gui", "filesystem:write"]
)
class LogViewMS(tk.Frame):
    """
The Console: A professional log viewer widget.
Features:
- Thread-safe (consumes from a Queue).
- Message Consolidation ("Error occurred (x5)").
- Level Filtering (Toggle INFO/DEBUG/ERROR).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    parent = self.config.get("parent")
    super().__init__(parent)
    self.log_queue = self.config.get("log_queue")

    # State for consolidation
    self.last_msg = None
    self.last_count = 0
    self.last_line_index = None

    self._build_ui()
    self._poll_queue()

def _build_ui(self):
    # Toolbar
    toolbar = tk.Frame(self, bg="#2d2d2d", height=30)
    toolbar.pack(fill="x", side="top")
# Filters
        self.filters = {
            "INFO": tk.BooleanVar(value=True),
            "DEBUG": tk.BooleanVar(value=True),
            "WARNING": tk.BooleanVar(value=True),
            "ERROR": tk.BooleanVar(value=True)
        }
        
        for level, var in self.filters.items():
            cb = tk.Checkbutton(
                toolbar, text=level, variable=var, 
                bg="#2d2d2d", fg="white", selectcolor="#444",
                activebackground="#2d2d2d", activeforeground="white"
            )
            cb.pack(side="left", padx=5)

        tk.Button(toolbar, text="Clear", command=self.clear, bg="#444", fg="white", relief="flat").pack(side="right", padx=5)
        tk.Button(toolbar, text="Save", command=self.save, bg="#444", fg="white", relief="flat").pack(side="right")

        # Text Area
        self.text = scrolledtext.ScrolledText(
            self, state="disabled", bg="#1e1e1e", fg="#d4d4d4", 
            font=("Consolas", 10), insertbackground="white"
        )
        self.text.pack(fill="both", expand=True)
        
        # Color Tags
        self.text.tag_config("INFO", foreground="#d4d4d4")
        self.text.tag_config("DEBUG", foreground="#569cd6")
        self.text.tag_config("WARNING", foreground="#ce9178")
        self.text.tag_config("ERROR", foreground="#f44747")
        self.text.tag_config("timestamp", foreground="#608b4e")

    def _poll_queue(self):
        """Pulls logs from the queue and updates UI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                self._display(record)
except queue.Empty:
            pass
        finally:
            self.after(100, self._poll_queue)

    def _display(self, record):
        level = record.levelname
        if not self.filters.get(level, tk.BooleanVar(value=True)).get():
            return

        msg = record.getMessage()
        ts = datetime.datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        
        self.text.config(state="normal")
        
        # Consolidation Logic
        if msg == self.last_msg:
            self.last_count += 1
            # Delete previous line content (keep timestamp)
            # This is complex in Tk text, simplified approach:
            # We just append (xN) if we can, otherwise standard print
            # For simplicity in this microservice, we will just append standard lines 
            # to ensure stability, or implement simple dedup:
            pass 
        else:
            self.last_msg = msg
            self.last_count = 1
        
        self.text.insert("end", f"[{ts}] ", "timestamp")
        self.text.insert("end", f"{msg}\n", level)
        self.text.see("end")
        self.text.config(state="disabled")

    @service_endpoint(
    inputs={},
    outputs={},
    description="Clears the log console.",
    tags=["ui", "logs"],
    side_effects=["ui:update"]
    )
    def clear(self):
    self.text.config(state="normal")
    self.text.delete("1.0", "end")
    self.text.config(state="disabled")

    @service_endpoint(
    inputs={},
    outputs={},
    description="Opens a dialog to save logs to a file.",
    tags=["ui", "filesystem"],
    side_effects=["filesystem:write", "ui:dialog"]
    )
    def save(self):
    path = filedialog.asksaveasfilename(defaultextension=".log", filetypes=[("Log Files", "*.log")])
        if path:
            try:
                with open(path, "w", encoding="utf-8") as f:
                    f.write(self.text.get("1.0", "end"))
            except Exception as e:
                print(f"Save failed: {e}")

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Log View Test")
    root.geometry("600x400")
    
    # 1. Setup Queue
    q = queue.Queue()
    
    # 2. Setup Logger
    logger = logging.getLogger("TestApp")
    logger.setLevel(logging.DEBUG)
    logger.addHandler(QueueHandler(q))
    
    # 3. Mount View
    log_view = LogViewMS({"parent": root, "log_queue": q})
    print("Service ready:", log_view)
    log_view.pack(fill="both", expand=True)
    
    # 4. Generate Logs
    def generate_noise():
        logger.info("System initializing...")
        logger.debug("Checking sensors...")
        logger.warning("Sensor 4 response slow.")
        logger.error("Connection failed!")
        root.after(2000, generate_noise)
        
    generate_noise()
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _LogViewMS\app.py.bak
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import scrolledtext, filedialog
import queue
import logging
import datetime
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

class QueueHandler(logging.Handler):
    """Sends log records to a Tkinter-safe queue."""
    def __init__(self, log_queue):
        super().__init__()
        self.log_queue = log_queue

    def emit(self, record):
        self.log_queue.put(record)

@service_metadata(
name="LogView",
version="1.0.0",
description="A thread-safe log viewer widget for Tkinter.",
tags=["ui", "logs", "widget"],
capabilities=["ui:gui", "filesystem:write"]
)
class LogViewMS(tk.Frame):
    """
The Console: A professional log viewer widget.
Features:
- Thread-safe (consumes from a Queue).
- Message Consolidation ("Error occurred (x5)").
- Level Filtering (Toggle INFO/DEBUG/ERROR).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
self.log_queue = self.config.get("log_queue")
        
        # State for consolidation
        self.last_msg = None
        self.last_count = 0
        self.last_line_index = None
        
        self._build_ui()
        self._poll_queue()

    def _build_ui(self):
        # Toolbar
        toolbar = tk.Frame(self, bg="#2d2d2d", height=30)
        toolbar.pack(fill="x", side="top")
        
        # Filters
        self.filters = {
            "INFO": tk.BooleanVar(value=True),
            "DEBUG": tk.BooleanVar(value=True),
            "WARNING": tk.BooleanVar(value=True),
            "ERROR": tk.BooleanVar(value=True)
        }
        
        for level, var in self.filters.items():
            cb = tk.Checkbutton(
                toolbar, text=level, variable=var, 
                bg="#2d2d2d", fg="white", selectcolor="#444",
                activebackground="#2d2d2d", activeforeground="white"
            )
            cb.pack(side="left", padx=5)

        tk.Button(toolbar, text="Clear", command=self.clear, bg="#444", fg="white", relief="flat").pack(side="right", padx=5)
        tk.Button(toolbar, text="Save", command=self.save, bg="#444", fg="white", relief="flat").pack(side="right")

        # Text Area
        self.text = scrolledtext.ScrolledText(
            self, state="disabled", bg="#1e1e1e", fg="#d4d4d4", 
            font=("Consolas", 10), insertbackground="white"
        )
        self.text.pack(fill="both", expand=True)
        
        # Color Tags
        self.text.tag_config("INFO", foreground="#d4d4d4")
        self.text.tag_config("DEBUG", foreground="#569cd6")
        self.text.tag_config("WARNING", foreground="#ce9178")
        self.text.tag_config("ERROR", foreground="#f44747")
        self.text.tag_config("timestamp", foreground="#608b4e")

    def _poll_queue(self):
        """Pulls logs from the queue and updates UI."""
        try:
            while True:
                record = self.log_queue.get_nowait()
                self._display(record)
        except queue.Empty:
            pass
        finally:
            self.after(100, self._poll_queue)

    def _display(self, record):
        level = record.levelname
        if not self.filters.get(level, tk.BooleanVar(value=True)).get():
            return

        msg = record.getMessage()
        ts = datetime.datetime.fromtimestamp(record.created).strftime("%H:%M:%S")
        
        self.text.config(state="normal")
        
        # Consolidation Logic
        if msg == self.last_msg:
            self.last_count += 1
            # Delete previous line content (keep timestamp)
            # This is complex in Tk text, simplified approach:
            # We just append (xN) if we can, otherwise standard print
            # For simplicity in this microservice, we will just append standard lines 
            # to ensure stability, or implement simple dedup:
            pass 
        else:
            self.last_msg = msg
            self.last_count = 1
        
        self.text.insert("end", f"[{ts}] ", "timestamp")
        self.text.insert("end", f"{msg}\n", level)
        self.text.see("end")
        self.text.config(state="disabled")

    @service_endpoint(
    inputs={},
    outputs={},
    description="Clears the log console.",
    tags=["ui", "logs"],
    side_effects=["ui:update"]
    )
    def clear(self):
    self.text.config(state="normal")
    self.text.delete("1.0", "end")
    self.text.config(state="disabled")

    @service_endpoint(
    inputs={},
    outputs={},
    description="Opens a dialog to save logs to a file.",
    tags=["ui", "filesystem"],
    side_effects=["filesystem:write", "ui:dialog"]
    )
    def save(self):
    path = filedialog.asksaveasfilename(defaultextension=".log", filetypes=[("Log Files", "*.log")])
        if path:
            try:
                with open(path, "w", encoding="utf-8") as f:
                    f.write(self.text.get("1.0", "end"))
            except Exception as e:
                print(f"Save failed: {e}")

# --- Independent Test Block ---
if __name__ == "__main__":
    root = tk.Tk()
    root.title("Log View Test")
    root.geometry("600x400")
    
    # 1. Setup Queue
    q = queue.Queue()
    
    # 2. Setup Logger
    logger = logging.getLogger("TestApp")
    logger.setLevel(logging.DEBUG)
    logger.addHandler(QueueHandler(q))
    
    # 3. Mount View
    log_view = LogViewMS({"parent": root, "log_queue": q})
    print("Service ready:", log_view)
    log_view.pack(fill="both", expand=True)
    
    # 4. Generate Logs
    def generate_noise():
        logger.info("System initializing...")
        logger.debug("Checking sensors...")
        logger.warning("Sensor 4 response slow.")
        logger.error("Connection failed!")
        root.after(2000, generate_noise)
        
    generate_noise()
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _MonacoHostMS\app.py
--------------------------------------------------------------------------------
import webview
import threading
import json
import os
from pathlib import Path
from typing import Any, Dict, Optional, Callable
from microservice_std_lib import service_metadata, service_endpoint

class MonacoBridge:
    """
    The Bridge: Handles bidirectional communication between Python and the Monaco Editor.
    """
    def __init__(self):
        self._window = None
        self._ready_event = threading.Event()
        self.on_save_callback: Optional[Callable[[str, str], None]] = None

    def set_window(self, window):
        self._window = window

    # --- JS -> Python (Called from Editor) ---
    
    def signal_editor_ready(self):
        """Called by JS when Monaco is fully loaded."""
        self._ready_event.set()
        print("Monaco Editor is ready.")

    def save_file(self, filepath: str, content: str):
        """Called by JS when Ctrl+S is pressed."""
        if self.on_save_callback:
            self.on_save_callback(filepath, content)
        else:
            print(f"Saved {filepath} (No callback registered)")

    def log(self, message: str):
        """Called by JS to print to Python console."""
        print(f"[Monaco JS]: {message}")

    # --- Python -> JS (Called from App) ---

    def open_file(self, filepath: str, content: str):
        """Opens a file in a new tab in the editor."""
        self._ready_event.wait(timeout=5)
        if not self._window: return
        
        safe_path = filepath.replace('\\', '\\\\').replace("'", "\\'")
        safe_content = json.dumps(content)
        
        js = f"window.pywebview.api.open_in_tab('{safe_path}', {safe_content})"
        self._window.evaluate_js(js)

    def highlight_range(self, filepath: str, start_line: int, end_line: int):
        """Scrolls to and highlights a specific line range."""
        self._ready_event.wait(timeout=5)
        if not self._window: return
        
        safe_path = filepath.replace('\\', '\\\\')
        js = f"window.pywebview.api.reveal_range('{safe_path}', {start_line}, {end_line})"
        self._window.evaluate_js(js)

@service_metadata(
name="MonacoHost",
version="1.0.0",
description="Hosts a Monaco Editor instance via PyWebView.",
tags=["ui", "editor", "webview"],
capabilities=["ui:gui", "filesystem:read"]
)
class MonacoHostMS:
    """
The Host: Manages the PyWebView window lifecycle.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.api = MonacoBridge()
default_html = str(Path(__file__).parent / "editor.html")
html_path = self.config.get("html_path", default_html)
self.html_path = Path(html_path).resolve()
self.window = None

    @service_endpoint(
    inputs={"title": "str", "width": "int", "height": "int", "func": "Callable"},
    outputs={},
    description="Launches the PyWebView editor window.",
    tags=["ui", "launch"],
    mode="sync",
    side_effects=["ui:gui"]
    )
    def launch(self, title="Monaco Editor", width=800, height=600, func=None):
    """
    Starts the editor window.
    :param func: Optional function to run in a background thread after launch.
    """
        self.window = webview.create_window(
            title, 
            str(self.html_path), 
            js_api=self.api,
            width=width, 
            height=height
        )
        self.api.set_window(self.window)
        
        if func:
            webview.start(func, debug=True)
        else:
            webview.start(debug=True)

# --- Independent Test Block ---
if __name__ == "__main__":
# 1. Setup
host = MonacoHostMS()
print("Service ready:", host)
    
    # 2. Define a background task to simulate "Opening a file" after 2 seconds
    def background_actions():
        import time
        print("Waiting for editor...")
        host.api._ready_event.wait()
        
        time.sleep(1)
        print("Opening test file...")
        
        dummy_code = """def hello_world():
    print("Hello from Python!")
    return True
"""
        host.api.open_file("test_script.py", dummy_code)
        
        # Define what happens on save
        host.api.on_save_callback = lambda path, content: print(f"SAVED TO DISK: {path}\nContent Size: {len(content)}")

# 3. Launch
print("Launching Editor...")
host.launch(func=background_actions)

--------------------------------------------------------------------------------
FILE: _MonacoHostMS\editor.html
--------------------------------------------------------------------------------
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Monaco Host</title>
    <style>
        html, body { margin: 0; padding: 0; width: 100%; height: 100%; overflow: hidden; background-color: #1e1e1e; font-family: sans-serif; }
        #container { display: flex; flex-direction: column; height: 100%; }
        #tabs { background: #252526; display: flex; overflow-x: auto; height: 35px; }
        .tab { 
            padding: 8px 15px; color: #969696; background: #2d2d2d; cursor: pointer; border-right: 1px solid #1e1e1e; font-size: 13px;
            display: flex; align-items: center;
        }
        .tab.active { background: #1e1e1e; color: #fff; }
        .tab:hover { background: #2d2d2d; color: #fff; }
        #editor { flex-grow: 1; }
    </style>
</head>
<body>
    <div id="container">
        <div id="tabs"></div>
        <div id="editor"></div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs/loader.js"></script>
    <script>
        require.config({ paths: { 'vs': 'https://cdn.jsdelivr.net/npm/monaco-editor@0.41.0/min/vs' }});

        let editor;
        let models = {}; // filepath -> model
        let currentPath = null;

        require(['vs/editor/editor.main'], function() {
            editor = monaco.editor.create(document.getElementById('editor'), {
                value: "# Ready.\n",
                language: 'python',
                theme: 'vs-dark',
                automaticLayout: true
            });

            // Signal Python that we are ready
            if (window.pywebview) window.pywebview.api.signal_editor_ready();

            // Keybindings
            editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.KeyS, function() {
                if (currentPath) {
                    const content = editor.getValue();
                    window.pywebview.api.save_file(currentPath, content);
                }
            });
        });

        // --- API exposed to Python ---
        window.pywebview = window.pywebview || {};
        window.pywebview.api = window.pywebview.api || {};

        window.pywebview.api.open_in_tab = function(filepath, content) {
            // Determine language
            let lang = 'plaintext';
            if (filepath.endsWith('.py')) lang = 'python';
            if (filepath.endsWith('.js')) lang = 'javascript';
            if (filepath.endsWith('.html')) lang = 'html';
            if (filepath.endsWith('.json')) lang = 'json';

            // Create or switch model
            if (!models[filepath]) {
                const uri = monaco.Uri.file(filepath);
                models[filepath] = monaco.editor.createModel(content, lang, uri);
                addTab(filepath);
            }
            
            switchTo(filepath);
        };

        window.pywebview.api.reveal_range = function(filepath, startLine, endLine) {
            if (filepath !== currentPath) switchTo(filepath);
            editor.revealLineInCenter(startLine);
            editor.setSelection({
                startLineNumber: startLine,
                startColumn: 1,
                endLineNumber: endLine,
                endColumn: 1000
            });
        };

        // --- Internal Helpers ---
        function addTab(filepath) {
            const tabs = document.getElementById('tabs');
            const tab = document.createElement('div');
            tab.className = 'tab';
            tab.innerText = filepath;
            tab.onclick = () => switchTo(filepath);
            tab.dataset.path = filepath;
            tabs.appendChild(tab);
        }

        function switchTo(filepath) {
            if (!models[filepath]) return;
            editor.setModel(models[filepath]);
            currentPath = filepath;

            // UI update
            document.querySelectorAll('.tab').forEach(t => t.classList.remove('active'));
            const activeTab = document.querySelector(`.tab[data-path="${filepath}"]`);
            if (activeTab) activeTab.classList.add('active');
        }
    </script>
</body>
</html>
--------------------------------------------------------------------------------
FILE: _MonacoHostMS\requirements.txt
--------------------------------------------------------------------------------
pywebview
--------------------------------------------------------------------------------
FILE: _NetworkLayoutMS\app.py
--------------------------------------------------------------------------------
import networkx as nx
import logging
from typing import List, Dict, Any, Tuple, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("NetLayout")
# ==============================================================================

@service_metadata(
name="NetworkLayout",
version="1.0.0",
description="Calculates visual (x,y) coordinates for graph nodes using NetworkX.",
tags=["graph", "layout", "visualization"],
capabilities=["compute"]
)
class NetworkLayoutMS:
    """
The Topologist: Calculates visual coordinates for graph nodes using
server-side algorithms (NetworkX). 
Useful for generating static map snapshots or pre-calculating positions 
to offload client-side rendering.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"nodes": "List[str]", "edges": "List[Tuple]", "algorithm": "str"},
    outputs={"positions": "Dict[str, Tuple]"},
    description="Computes (x, y) coordinates for the given graph nodes and edges.",
    tags=["graph", "compute"],
    side_effects=[]
    )
    def calculate_layout(self, nodes: List[str], edges: List[Tuple[str, str]], 
    algorithm: str = "spring", **kwargs) -> Dict[str, Tuple[float, float]]:
    """
    Computes (x, y) coordinates for the given graph.
        
    :param nodes: List of node IDs.
        :param edges: List of (source, target) tuples.
        :param algorithm: 'spring' (Force-directed) or 'circular'.
        :return: Dictionary {node_id: (x, y)}
        """
        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        G.add_edges_from(edges)
        
        log.info(f"Computing layout for {len(nodes)} nodes, {len(edges)} edges...")
        
        try:
            if algorithm == "circular":
                pos = nx.circular_layout(G)
            else:
                # Spring layout (Fruchterman-Reingold) is standard for knowledge graphs
                k_val = kwargs.get('k', 0.15) # Optimal distance between nodes
                iter_val = kwargs.get('iterations', 50)
                pos = nx.spring_layout(G, k=k_val, iterations=iter_val, seed=42)
                
            # Convert numpy arrays to simple lists/tuples for JSON serialization
            return {n: (float(p[0]), float(p[1])) for n, p in pos.items()}
            
        except Exception as e:
            log.error(f"Layout calculation failed: {e}")
            return {}

# --- Independent Test Block ---
if __name__ == "__main__":
layout = NetworkLayoutMS()
print("Service ready:", layout)
    
# 1. Define a simple graph
    test_nodes = ["Main", "Utils", "Config", "DB", "Auth"]
    test_edges = [
        ("Main", "Utils"),
        ("Main", "Config"),
        ("Main", "DB"),
        ("Main", "Auth"),
        ("DB", "Config"),
        ("Auth", "DB")
    ]
    
    # 2. Compute Layout
    positions = layout.calculate_layout(test_nodes, test_edges, k=0.5)
    
    print("--- Calculated Positions ---")
    for node, (x, y) in positions.items():
        print(f"{node:<10}: ({x: .4f}, {y: .4f})")

--------------------------------------------------------------------------------
FILE: _NetworkLayoutMS\app.py.bak
--------------------------------------------------------------------------------
import networkx as nx
import logging
from typing import List, Dict, Any, Tuple, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("NetLayout")
# ==============================================================================

@service_metadata(
name="NetworkLayout",
version="1.0.0",
description="Calculates visual (x,y) coordinates for graph nodes using NetworkX.",
tags=["graph", "layout", "visualization"],
capabilities=["compute"]
)
class NetworkLayoutMS:
    """
The Topologist: Calculates visual coordinates for graph nodes using
server-side algorithms (NetworkX). 
Useful for generating static map snapshots or pre-calculating positions 
to offload client-side rendering.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"nodes": "List[str]", "edges": "List[Tuple]", "algorithm": "str"},
    outputs={"positions": "Dict[str, Tuple]"},
    description="Computes (x, y) coordinates for the given graph nodes and edges.",
    tags=["graph", "compute"],
    side_effects=[]
    )
    def calculate_layout(self, nodes: List[str], edges: List[Tuple[str, str]], 
    algorithm: str = "spring", **kwargs) -> Dict[str, Tuple[float, float]]:
    """
    Computes (x, y) coordinates for the given graph.
        
    :param nodes: List of node IDs.
        :param edges: List of (source, target) tuples.
        :param algorithm: 'spring' (Force-directed) or 'circular'.
        :return: Dictionary {node_id: (x, y)}
        """
        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        G.add_edges_from(edges)
        
        log.info(f"Computing layout for {len(nodes)} nodes, {len(edges)} edges...")
        
        try:
            if algorithm == "circular":
                pos = nx.circular_layout(G)
            else:
                # Spring layout (Fruchterman-Reingold) is standard for knowledge graphs
                k_val = kwargs.get('k', 0.15) # Optimal distance between nodes
                iter_val = kwargs.get('iterations', 50)
                pos = nx.spring_layout(G, k=k_val, iterations=iter_val, seed=42)
                
            # Convert numpy arrays to simple lists/tuples for JSON serialization
            return {n: (float(p[0]), float(p[1])) for n, p in pos.items()}
            
        except Exception as e:
            log.error(f"Layout calculation failed: {e}")
            return {}

# --- Independent Test Block ---
if __name__ == "__main__":
layout = NetworkLayoutMS()
print("Service ready:", layout)
    
# 1. Define a simple graph
    test_nodes = ["Main", "Utils", "Config", "DB", "Auth"]
    test_edges = [
        ("Main", "Utils"),
        ("Main", "Config"),
        ("Main", "DB"),
        ("Main", "Auth"),
        ("DB", "Config"),
        ("Auth", "DB")
    ]
    
    # 2. Compute Layout
    positions = layout.calculate_layout(test_nodes, test_edges, k=0.5)
    
    print("--- Calculated Positions ---")
    for node, (x, y) in positions.items():
        print(f"{node:<10}: ({x: .4f}, {y: .4f})")

--------------------------------------------------------------------------------
FILE: _NetworkLayoutMS\requirements.txt
--------------------------------------------------------------------------------
pip install networkx
--------------------------------------------------------------------------------
FILE: _PromptOptimizerMS\app.py
--------------------------------------------------------------------------------
import json
import logging
from typing import List, Dict, Any, Callable, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: META-PROMPTS
# ==============================================================================
# The system prompt used to turn the LLM into a Prompt Engineer
REFINE_SYSTEM_PROMPT = (
    "You are a world-class prompt engineer. "
    "Given an original prompt and specific feedback, "
    "provide an improved, refined version of the prompt that incorporates the feedback. "
    "Return ONLY the refined prompt text, no preamble."
)

# The system prompt used to generate A/B test variations
VARIATION_SYSTEM_PROMPT = (
    "You are a creative AI assistant. "
    "Generate {num} innovative and diverse variations of the following prompt. "
    "Return the result as a valid JSON array of strings. "
    "Example: [\"variation 1\", \"variation 2\"]"
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptOpt")
# ==============================================================================

@service_metadata(
name="PromptOptimizer",
version="1.0.0",
description="Uses an LLM to refine prompts or generate variations.",
tags=["llm", "prompt-engineering", "optimization"],
capabilities=["network:outbound"]
)
class PromptOptimizerMS:
    """
The Tuner: Uses an LLM to refine prompts or generate variations.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.infer = self.config.get("inference_func")

    @service_endpoint(
    inputs={"draft_prompt": "str", "feedback": "str"},
    outputs={"refined_prompt": "str"},
    description="Rewrites a prompt based on feedback.",
    tags=["llm", "refine"],
    side_effects=["network:outbound"]
    )
def refine_prompt(self, draft_prompt: str, feedback: str) -> str:
    """
    Rewrites a prompt based on feedback.
    """
        full_prompt = (
            f"{REFINE_SYSTEM_PROMPT}\n\n"
            f"[Original Prompt]:\n{draft_prompt}\n\n"
            f"[Feedback]:\n{feedback}\n\n"
            f"[Refined Prompt]:"
        )
        
        log.info("Refining prompt...")
        try:
            result = self.infer(full_prompt)
            return result.strip()
        except Exception as e:
            log.error(f"Refinement failed: {e}")
            return draft_prompt # Fallback to original

@service_endpoint(
    inputs={"draft_prompt": "str", "num_variations": "int", "context_data": "Dict"},
    outputs={"variations": "List[str]"},
    description="Generates multiple versions of a prompt for testing.",
    tags=["llm", "variations"],
    side_effects=["network:outbound"]
)
def generate_variations(self, draft_prompt: str, num_variations: int = 3, context_data: Optional[Dict] = None) -> List[str]:
    """
    Generates multiple versions of a prompt for testing.
    """
        meta_prompt = VARIATION_SYSTEM_PROMPT.format(num=num_variations)
        
        prompt_content = draft_prompt
        if context_data:
            prompt_content += f"\n\n--- Context ---\n{json.dumps(context_data, indent=2)}"

        full_prompt = (
            f"{meta_prompt}\n\n"
            f"[Original Prompt]:\n{prompt_content}\n\n"
            f"[JSON Array of Variations]:"
        )

        log.info(f"Generating {num_variations} variations...")
        try:
            # We explicitly ask for JSON, but LLMs are chatty, so we might need cleaning logic here
            raw_response = self.infer(full_prompt)
            
            # Simple cleanup to find the JSON array if the LLM added text around it
            start = raw_response.find('[')
            end = raw_response.rfind(']') + 1
            if start == -1 or end == 0:
                raise ValueError("No JSON array found in response")
                
            clean_json = raw_response[start:end]
            variations = json.loads(clean_json)
            
            if isinstance(variations, list):
                return [str(v) for v in variations]
            return []
            
        except Exception as e:
            log.error(f"Variation generation failed: {e}")
            return []

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Mock Inference Engine (Simulating an LLM)
    def mock_llm(prompt: str) -> str:
        if "[Refined Prompt]" in prompt:
            return "You are a helpful assistant who speaks like a pirate. How may I help ye?"
        if "[JSON Array]" in prompt:
            return '["Variation A: Pirate Mode", "Variation B: Formal Mode", "Variation C: Concise Mode"]'
        return "Error"

    optimizer = PromptOptimizerMS({"inference_func": mock_llm})
    print("Service ready:", optimizer)

    # 2. Test Refine
    print("--- Test: Refine ---")
    draft = "Help me."
feedback = "Make it sound like a pirate."
    refined = optimizer.refine_prompt(draft, feedback)
    print(f"Original: {draft}")
    print(f"Refined:  {refined}")

    # 3. Test Variations
    print("\n--- Test: Variations ---")
    vars = optimizer.generate_variations(draft, num_variations=3)
    for i, v in enumerate(vars):
        print(f" {i+1}. {v}")

--------------------------------------------------------------------------------
FILE: _PromptOptimizerMS\app.py.bak
--------------------------------------------------------------------------------
import json
import logging
from typing import List, Dict, Any, Callable, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: META-PROMPTS
# ==============================================================================
# The system prompt used to turn the LLM into a Prompt Engineer
REFINE_SYSTEM_PROMPT = (
    "You are a world-class prompt engineer. "
    "Given an original prompt and specific feedback, "
    "provide an improved, refined version of the prompt that incorporates the feedback. "
    "Return ONLY the refined prompt text, no preamble."
)

# The system prompt used to generate A/B test variations
VARIATION_SYSTEM_PROMPT = (
    "You are a creative AI assistant. "
    "Generate {num} innovative and diverse variations of the following prompt. "
    "Return the result as a valid JSON array of strings. "
    "Example: [\"variation 1\", \"variation 2\"]"
)

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptOpt")
# ==============================================================================

@service_metadata(
name="PromptOptimizer",
version="1.0.0",
description="Uses an LLM to refine prompts or generate variations.",
tags=["llm", "prompt-engineering", "optimization"],
capabilities=["network:outbound"]
)
class PromptOptimizerMS:
    """
The Tuner: Uses an LLM to refine prompts or generate variations.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.infer = self.config.get("inference_func")

    @service_endpoint(
    inputs={"draft_prompt": "str", "feedback": "str"},
    outputs={"refined_prompt": "str"},
    description="Rewrites a prompt based on feedback.",
    tags=["llm", "refine"],
    side_effects=["network:outbound"]
    )
    def refine_prompt(self, draft_prompt: str, feedback: str) -> str:
    """
    Rewrites a prompt based on feedback.
    """
        full_prompt = (
            f"{REFINE_SYSTEM_PROMPT}\n\n"
            f"[Original Prompt]:\n{draft_prompt}\n\n"
            f"[Feedback]:\n{feedback}\n\n"
            f"[Refined Prompt]:"
        )
        
        log.info("Refining prompt...")
        try:
            result = self.infer(full_prompt)
            return result.strip()
        except Exception as e:
            log.error(f"Refinement failed: {e}")
            return draft_prompt # Fallback to original

    @service_endpoint(
    inputs={"draft_prompt": "str", "num_variations": "int", "context_data": "Dict"},
    outputs={"variations": "List[str]"},
    description="Generates multiple versions of a prompt for testing.",
    tags=["llm", "variations"],
    side_effects=["network:outbound"]
    )
    def generate_variations(self, draft_prompt: str, num_variations: int = 3, context_data: Optional[Dict] = None) -> List[str]:
    """
    Generates multiple versions of a prompt for testing.
    """
        meta_prompt = VARIATION_SYSTEM_PROMPT.format(num=num_variations)
        
        prompt_content = draft_prompt
        if context_data:
            prompt_content += f"\n\n--- Context ---\n{json.dumps(context_data, indent=2)}"

        full_prompt = (
            f"{meta_prompt}\n\n"
            f"[Original Prompt]:\n{prompt_content}\n\n"
            f"[JSON Array of Variations]:"
        )

        log.info(f"Generating {num_variations} variations...")
        try:
            # We explicitly ask for JSON, but LLMs are chatty, so we might need cleaning logic here
            raw_response = self.infer(full_prompt)
            
            # Simple cleanup to find the JSON array if the LLM added text around it
            start = raw_response.find('[')
            end = raw_response.rfind(']') + 1
            if start == -1 or end == 0:
                raise ValueError("No JSON array found in response")
                
            clean_json = raw_response[start:end]
            variations = json.loads(clean_json)
            
            if isinstance(variations, list):
                return [str(v) for v in variations]
            return []
            
        except Exception as e:
            log.error(f"Variation generation failed: {e}")
            return []

# --- Independent Test Block ---
if __name__ == "__main__":
    # 1. Mock Inference Engine (Simulating an LLM)
    def mock_llm(prompt: str) -> str:
        if "[Refined Prompt]" in prompt:
            return "You are a helpful assistant who speaks like a pirate. How may I help ye?"
        if "[JSON Array]" in prompt:
            return '["Variation A: Pirate Mode", "Variation B: Formal Mode", "Variation C: Concise Mode"]'
        return "Error"

    optimizer = PromptOptimizerMS({"inference_func": mock_llm})
    print("Service ready:", optimizer)

    # 2. Test Refine
    print("--- Test: Refine ---")
    draft = "Help me."
    feedback = "Make it sound like a pirate."
    refined = optimizer.refine_prompt(draft, feedback)
    print(f"Original: {draft}")
    print(f"Refined:  {refined}")

    # 3. Test Variations
    print("\n--- Test: Variations ---")
    vars = optimizer.generate_variations(draft, num_variations=3)
    for i, v in enumerate(vars):
        print(f" {i+1}. {v}")

--------------------------------------------------------------------------------
FILE: _PromptVaultMS\app.py
--------------------------------------------------------------------------------
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Callable
from pydantic import BaseModel, Field, ValidationError
from jinja2 import Environment, BaseLoader

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "prompt_vault.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptVault")
# ==============================================================================

# --- Data Models ---

class PromptVersion(BaseModel):
    """A specific historical version of a prompt."""
    version_num: int
    content: str
    author: str
    timestamp: datetime.datetime
    embedding: Optional[List[float]] = None

class PromptTemplate(BaseModel):
    """The master record for a prompt."""
    id: str
    slug: str
    title: str
    description: Optional[str] = ""
    tags: List[str] = []
    latest_version_num: int
    versions: List[PromptVersion] = []
    
    @property
    def latest(self) -> PromptVersion:
        """Helper to get the most recent content."""
        if not self.versions:
            raise ValueError("No versions found.")
        # versions are stored sorted by DB insertion usually, but let's be safe
        return sorted(self.versions, key=lambda v: v.version_num)[-1]

# --- Database Management ---

class PromptVaultMS:
    """
    The Vault: A persistent SQLite store for managing, versioning, 
    and rendering AI prompts.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()
        self.jinja_env = Environment(loader=BaseLoader())

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        """Bootstraps the schema."""
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    slug TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    tags_json TEXT,
                    latest_version INTEGER DEFAULT 1,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS versions (
                    id TEXT PRIMARY KEY,
                    template_id TEXT,
                    version_num INTEGER,
                    content TEXT,
                    author TEXT,
                    timestamp TIMESTAMP,
                    embedding_json TEXT,
                    FOREIGN KEY(template_id) REFERENCES templates(id)
                )
            """)
# --- CRUD Operations ---

    def create_template(self, slug: str, title: str, content: str, author: str = "system", tags: List[str] = None) -> PromptTemplate:
        """Creates a new prompt template with an initial version 1."""
        tags = tags or []
        now = datetime.datetime.utcnow()
        t_id = str(uuid.uuid4())
        v_id = str(uuid.uuid4())

        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO templates (id, slug, title, description, tags_json, latest_version, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                    (t_id, slug, title, "", json.dumps(tags), 1, now, now)
                )
                conn.execute(
                    "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                    (v_id, t_id, 1, content, author, now)
                )
            log.info(f"Created template: {slug}")
            return self.get_template(slug)
        except sqlite3.IntegrityError:
            raise ValueError(f"Template '{slug}' already exists.")

    def add_version(self, slug: str, content: str, author: str = "user") -> PromptTemplate:
        """Adds a new version to an existing template."""
        current = self.get_template(slug)
        if not current:
            raise ValueError(f"Template '{slug}' not found.")

        new_ver = current.latest_version_num + 1
        now = datetime.datetime.utcnow()
        v_id = str(uuid.uuid4())

        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                (v_id, current.id, new_ver, content, author, now)
            )
conn.execute(
                "UPDATE templates SET latest_version = ?, updated_at = ? WHERE id = ?",
                (new_ver, now, current.id)
            )
        log.info(f"Updated {slug} to v{new_ver}")
        return self.get_template(slug)

    def get_template(self, slug: str) -> Optional[PromptTemplate]:
        """Retrieves a full template with all history."""
        with self._get_conn() as conn:
            # 1. Fetch Template
            row = conn.execute("SELECT * FROM templates WHERE slug = ?", (slug,)).fetchone()
            if not row: return None

            # 2. Fetch Versions
            v_rows = conn.execute("SELECT * FROM versions WHERE template_id = ? ORDER BY version_num ASC", (row['id'],)).fetchall()

            versions = []
            for v in v_rows:
                versions.append(PromptVersion(
                    version_num=v['version_num'],
                    content=v['content'],
                    author=v['author'],
                    timestamp=v['timestamp']
                    # embedding logic skipped for brevity
                ))

            return PromptTemplate(
                id=row['id'],
                slug=row['slug'],
                title=row['title'],
                description=row['description'],
                tags=json.loads(row['tags_json']),
                latest_version_num=row['latest_version'],
                versions=versions
            )

    def render(self, slug: str, context: Dict[str, Any] = None) -> str:
        """Fetches the latest version and renders it with Jinja2."""
        template = self.get_template(slug)
if not template:
            raise ValueError(f"Template '{slug}' not found.")
        
        raw_text = template.latest.content
        jinja_template = self.jinja_env.from_string(raw_text)
        return jinja_template.render(**(context or {}))

    def list_slugs(self) -> List[str]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT slug FROM templates").fetchall()
            return [r[0] for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup
    if DB_PATH.exists(): os.remove(DB_PATH)
    vault = PromptVaultMS()
    
    # 2. Create
    print("--- Creating Prompt ---")
    vault.create_template(
        slug="greet_user",
        title="Greeting Protocol",
        content="Hello {{ name }}, welcome to the {{ system_name }}!",
        tags=["ui", "onboarding"]
    )
    
    # 3. Versioning
    print("--- Updating Prompt ---")
    vault.add_version("greet_user", "Greetings, {{ name }}. System {{ system_name }} is online.")
    
    # 4. Retrieval & Rendering
    print("--- Rendering ---")
    final_text = vault.render("greet_user", {"name": "Alice", "system_name": "Nexus"})
    print(f"Rendered Output: {final_text}")
    
    # 5. Inspection
    tpl = vault.get_template("greet_user")
print(f"Current Version: v{tpl.latest_version_num}")
print(f"History: {[v.content for v in tpl.versions]}")
# Cleanup
if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: _PromptVaultMS\app.py.bak
--------------------------------------------------------------------------------
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any, Callable
from pydantic import BaseModel, Field, ValidationError
from jinja2 import Environment, BaseLoader

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "prompt_vault.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("PromptVault")
# ==============================================================================

# --- Data Models ---

class PromptVersion(BaseModel):
    """A specific historical version of a prompt."""
    version_num: int
    content: str
    author: str
    timestamp: datetime.datetime
    embedding: Optional[List[float]] = None

class PromptTemplate(BaseModel):
    """The master record for a prompt."""
    id: str
    slug: str
    title: str
    description: Optional[str] = ""
    tags: List[str] = []
    latest_version_num: int
    versions: List[PromptVersion] = []
    
    @property
    def latest(self) -> PromptVersion:
        """Helper to get the most recent content."""
        if not self.versions:
            raise ValueError("No versions found.")
        # versions are stored sorted by DB insertion usually, but let's be safe
        return sorted(self.versions, key=lambda v: v.version_num)[-1]

# --- Database Management ---

class PromptVaultMS:
    """
    The Vault: A persistent SQLite store for managing, versioning, 
    and rendering AI prompts.
    """
    def __init__(self, db_path: Path = DB_PATH):
        self.db_path = db_path
        self._init_db()
        self.jinja_env = Environment(loader=BaseLoader())

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        """Bootstraps the schema."""
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS templates (
                    id TEXT PRIMARY KEY,
                    slug TEXT UNIQUE NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT,
                    tags_json TEXT,
                    latest_version INTEGER DEFAULT 1,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS versions (
                    id TEXT PRIMARY KEY,
                    template_id TEXT,
                    version_num INTEGER,
                    content TEXT,
                    author TEXT,
                    timestamp TIMESTAMP,
                    embedding_json TEXT,
                    FOREIGN KEY(template_id) REFERENCES templates(id)
                )
            """)

    # --- CRUD Operations ---

    def create_template(self, slug: str, title: str, content: str, author: str = "system", tags: List[str] = None) -> PromptTemplate:
        """Creates a new prompt template with an initial version 1."""
        tags = tags or []
        now = datetime.datetime.utcnow()
        t_id = str(uuid.uuid4())
        v_id = str(uuid.uuid4())

        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO templates (id, slug, title, description, tags_json, latest_version, created_at, updated_at) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                    (t_id, slug, title, "", json.dumps(tags), 1, now, now)
                )
                conn.execute(
                    "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                    (v_id, t_id, 1, content, author, now)
                )
            log.info(f"Created template: {slug}")
            return self.get_template(slug)
        except sqlite3.IntegrityError:
            raise ValueError(f"Template '{slug}' already exists.")

    def add_version(self, slug: str, content: str, author: str = "user") -> PromptTemplate:
        """Adds a new version to an existing template."""
        current = self.get_template(slug)
        if not current:
            raise ValueError(f"Template '{slug}' not found.")

        new_ver = current.latest_version_num + 1
        now = datetime.datetime.utcnow()
        v_id = str(uuid.uuid4())

        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO versions (id, template_id, version_num, content, author, timestamp) VALUES (?, ?, ?, ?, ?, ?)",
                (v_id, current.id, new_ver, content, author, now)
            )
            conn.execute(
                "UPDATE templates SET latest_version = ?, updated_at = ? WHERE id = ?",
                (new_ver, now, current.id)
            )
        log.info(f"Updated {slug} to v{new_ver}")
        return self.get_template(slug)

    def get_template(self, slug: str) -> Optional[PromptTemplate]:
        """Retrieves a full template with all history."""
        with self._get_conn() as conn:
            # 1. Fetch Template
            row = conn.execute("SELECT * FROM templates WHERE slug = ?", (slug,)).fetchone()
            if not row: return None

            # 2. Fetch Versions
            v_rows = conn.execute("SELECT * FROM versions WHERE template_id = ? ORDER BY version_num ASC", (row['id'],)).fetchall()
            
            versions = []
            for v in v_rows:
                versions.append(PromptVersion(
                    version_num=v['version_num'],
                    content=v['content'],
                    author=v['author'],
                    timestamp=v['timestamp']
                    # embedding logic skipped for brevity
                ))

            return PromptTemplate(
                id=row['id'],
                slug=row['slug'],
                title=row['title'],
                description=row['description'],
                tags=json.loads(row['tags_json']),
                latest_version_num=row['latest_version'],
                versions=versions
            )

    def render(self, slug: str, context: Dict[str, Any] = None) -> str:
        """Fetches the latest version and renders it with Jinja2."""
        template = self.get_template(slug)
        if not template:
            raise ValueError(f"Template '{slug}' not found.")
        
        raw_text = template.latest.content
        jinja_template = self.jinja_env.from_string(raw_text)
        return jinja_template.render(**(context or {}))

    def list_slugs(self) -> List[str]:
        with self._get_conn() as conn:
            rows = conn.execute("SELECT slug FROM templates").fetchall()
            return [r[0] for r in rows]

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    
    # 1. Setup
    if DB_PATH.exists(): os.remove(DB_PATH)
    vault = PromptVaultMS()
    
    # 2. Create
    print("--- Creating Prompt ---")
    vault.create_template(
        slug="greet_user",
        title="Greeting Protocol",
        content="Hello {{ name }}, welcome to the {{ system_name }}!",
        tags=["ui", "onboarding"]
    )
    
    # 3. Versioning
    print("--- Updating Prompt ---")
    vault.add_version("greet_user", "Greetings, {{ name }}. System {{ system_name }} is online.")
    
    # 4. Retrieval & Rendering
    print("--- Rendering ---")
    final_text = vault.render("greet_user", {"name": "Alice", "system_name": "Nexus"})
    print(f"Rendered Output: {final_text}")
    
    # 5. Inspection
    tpl = vault.get_template("greet_user")
    print(f"Current Version: v{tpl.latest_version_num}")
    print(f"History: {[v.content for v in tpl.versions]}")
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: _PromptVaultMS\requirements.txt
--------------------------------------------------------------------------------
pydantic
jinja2
--------------------------------------------------------------------------------
FILE: _RegexWeaverMS\app.py
--------------------------------------------------------------------------------
import re
import logging
from typing import Any, Dict, List, Optional, Set
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: PATTERNS
# ==============================================================================
# Python: "import x", "from x import y"
PY_IMPORT = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')

# JS/TS: "import ... from 'x'", "require('x')"
JS_IMPORT = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RegexWeaver")
# ==============================================================================

@service_metadata(
name="RegexWeaver",
version="1.0.0",
description="Fault-tolerant dependency extractor using Regex.",
tags=["parsing", "dependencies", "regex"],
capabilities=["compute"]
)
class RegexWeaverMS:
    """
The Weaver: A fault-tolerant dependency extractor.
Uses Regex to find imports, making it faster and more permissive
than AST parsers (works on broken code).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"content": "str", "language": "str"},
    outputs={"dependencies": "List[str]"},
    description="Scans code content for import statements.",
    tags=["parsing", "dependencies"],
    side_effects=[]
    )
    def extract_dependencies(self, content: str, language: str) -> List[str]:
    """
    Scans code content for import statements.
    :param language: 'python' or 'javascript' (includes ts/jsx).
    """
        dependencies: Set[str] = set()
        lines = content.splitlines()
        
        pattern = PY_IMPORT if language == 'python' else JS_IMPORT
        
        for line in lines:
            # Skip comments roughly
            if line.strip().startswith(('#', '//')):
                continue
                
            if language == 'python':
                match = pattern.match(line)
            else:
                match = pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                # Clean up: "backend.database" -> "database"
                # We usually want the leaf name for simple linking
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                dependencies.add(clean_dep)
                
        return sorted(list(dependencies))

# --- Independent Test Block ---
if __name__ == "__main__":
weaver = RegexWeaverMS()
print("Service ready:", weaver)
    
    # 1. Python Test
    py_code = """
    import os
    from backend.utils import helper
    # from commented.out import ignore_me
    import pandas as pd
    """
    print(f"Python Deps: {weaver.extract_dependencies(py_code, 'python')}")
    
    # 2. JS Test
    js_code = """
    import React from 'react';
    const utils = require('./lib/utils');
    // import hidden from 'hidden';
    """

print(f"JS Deps:     {weaver.extract_dependencies(js_code, 'javascript')}")

--------------------------------------------------------------------------------
FILE: _RegexWeaverMS\app.py.bak
--------------------------------------------------------------------------------
import re
import logging
from typing import Any, Dict, List, Optional, Set
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION: PATTERNS
# ==============================================================================
# Python: "import x", "from x import y"
PY_IMPORT = re.compile(r'^\s*(?:from|import)\s+([\w\.]+)')

# JS/TS: "import ... from 'x'", "require('x')"
JS_IMPORT = re.compile(r'(?:import\s+.*?from\s+[\'"]|require\([\'"])([\.\/\w\-_]+)[\'"]')

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RegexWeaver")
# ==============================================================================

@service_metadata(
name="RegexWeaver",
version="1.0.0",
description="Fault-tolerant dependency extractor using Regex.",
tags=["parsing", "dependencies", "regex"],
capabilities=["compute"]
)
class RegexWeaverMS:
    """
The Weaver: A fault-tolerant dependency extractor.
Uses Regex to find imports, making it faster and more permissive
than AST parsers (works on broken code).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

    @service_endpoint(
    inputs={"content": "str", "language": "str"},
    outputs={"dependencies": "List[str]"},
    description="Scans code content for import statements.",
    tags=["parsing", "dependencies"],
    side_effects=[]
    )
    def extract_dependencies(self, content: str, language: str) -> List[str]:
    """
    Scans code content for import statements.
    :param language: 'python' or 'javascript' (includes ts/jsx).
    """
        dependencies: Set[str] = set()
        lines = content.splitlines()
        
        pattern = PY_IMPORT if language == 'python' else JS_IMPORT
        
        for line in lines:
            # Skip comments roughly
            if line.strip().startswith(('#', '//')):
                continue
                
            if language == 'python':
                match = pattern.match(line)
            else:
                match = pattern.search(line)
            
            if match:
                raw_dep = match.group(1)
                # Clean up: "backend.database" -> "database"
                # We usually want the leaf name for simple linking
                clean_dep = raw_dep.split('.')[-1].split('/')[-1]
                dependencies.add(clean_dep)
                
        return sorted(list(dependencies))

# --- Independent Test Block ---
if __name__ == "__main__":
weaver = RegexWeaverMS()
print("Service ready:", weaver)
    
    # 1. Python Test
    py_code = """
    import os
    from backend.utils import helper
    # from commented.out import ignore_me
    import pandas as pd
    """
    print(f"Python Deps: {weaver.extract_dependencies(py_code, 'python')}")
    
    # 2. JS Test
    js_code = """
    import React from 'react';
    const utils = require('./lib/utils');
    // import hidden from 'hidden';
    """
    print(f"JS Deps:     {weaver.extract_dependencies(js_code, 'javascript')}")

--------------------------------------------------------------------------------
FILE: _RoleManagerMS\app.py
--------------------------------------------------------------------------------
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("roles.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RoleManager")
# ==============================================================================

class Role(BaseModel):
    id: str
    name: str
    description: Optional[str] = ""
    system_prompt: str
    knowledge_bases: List[str] = []
    memory_policy: str = "scratchpad" # or 'auto_commit'
    created_at: datetime.datetime

@service_metadata(
name="RoleManager",
version="1.0.0",
description="Manages Agent Personas (Roles), including System Prompts and Memory Settings.",
tags=["roles", "personas", "db"],
capabilities=["db:sqlite"]
)
class RoleManagerMS:
    """
The Casting Director: Manages Agent Personas (Roles).
Persists configuration for System Prompts, Attached KBs, and Memory Settings.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", DB_PATH)
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS roles (
                    id TEXT PRIMARY KEY,
                    name TEXT UNIQUE NOT NULL,
                    description TEXT,
                    system_prompt TEXT NOT NULL,
                    knowledge_bases_json TEXT,
                    memory_policy TEXT,
                    created_at TIMESTAMP
                )
            """)

    @service_endpoint(
    inputs={"name": "str", "system_prompt": "str", "description": "str", "kbs": "List[str]"},
    outputs={"role": "Role"},
    description="Creates a new Agent Persona.",
    tags=["roles", "create"],
    side_effects=["db:write"]
    )
    def create_role(self, name: str, system_prompt: str, description: str = "", kbs: List[str] = None) -> Role:
    """Creates a new Agent Persona."""
        role_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        kbs_json = json.dumps(kbs or [])
        
        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO roles (id, name, description, system_prompt, knowledge_bases_json, memory_policy, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    (role_id, name, description, system_prompt, kbs_json, "scratchpad", now)
                )
            log.info(f"Created Role: {name}")
            return self.get_role(name)
        except sqlite3.IntegrityError:
            raise ValueError(f"Role '{name}' already exists.")

    @service_endpoint(
    inputs={"name_or_id": "str"},
    outputs={"role": "Optional[Role]"},
    description="Retrieves a role by Name or ID.",
    tags=["roles", "read"]
    )
    def get_role(self, name_or_id: str) -> Optional[Role]:
    """Retrieves a role by Name or ID."""
        with self._get_conn() as conn:
            # Try ID first
            row = conn.execute("SELECT * FROM roles WHERE id = ?", (name_or_id,)).fetchone()
            if not row:
                # Try Name
                row = conn.execute("SELECT * FROM roles WHERE name = ?", (name_or_id,)).fetchone()
            
            if not row: return None

            return Role(
                id=row['id'],
                name=row['name'],
                description=row['description'],
                system_prompt=row['system_prompt'],
                knowledge_bases=json.loads(row['knowledge_bases_json']),
                memory_policy=row['memory_policy'],
                created_at=row['created_at'] # Adapter might need datetime.fromisoformat if stored as str
            )

    @service_endpoint(
    inputs={},
    outputs={"roles": "List[Dict]"},
    description="Lists all available roles.",
    tags=["roles", "read"]
    )
    def list_roles(self) -> List[Dict]:
    with self._get_conn() as conn:
            rows = conn.execute("SELECT id, name, description FROM roles").fetchall()
            return [dict(r) for r in rows]

    @service_endpoint(
    inputs={"name": "str"},
    outputs={},
    description="Deletes a role by name.",
    tags=["roles", "delete"],
    side_effects=["db:write"]
    )
    def delete_role(self, name: str):
    with self._get_conn() as conn:
            conn.execute("DELETE FROM roles WHERE name = ?", (name,))
        log.info(f"Deleted Role: {name}")

# --- Independent Test Block ---
if __name__ == "__main__":
import os
if DB_PATH.exists(): os.remove(DB_PATH)
    
mgr = RoleManagerMS()
print("Service ready:", mgr)
    
    # 1. Create
    mgr.create_role(
        name="SeniorDev", 
        system_prompt="You are a senior Python developer. Prefer Clean Code principles.",
        description="Expert coding assistant",
        kbs=["python_docs", "project_repo"]
    )
    
    # 2. Retrieve
    role = mgr.get_role("SeniorDev")
    print(f"Role: {role.name}")
    print(f"Prompt: {role.system_prompt}")
    print(f"KBs: {role.knowledge_bases}")
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: _RoleManagerMS\app.py.bak
--------------------------------------------------------------------------------
import sqlite3
import json
import uuid
import logging
import datetime
from pathlib import Path
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path("roles.db")
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("RoleManager")
# ==============================================================================

class Role(BaseModel):
    id: str
    name: str
    description: Optional[str] = ""
    system_prompt: str
    knowledge_bases: List[str] = []
    memory_policy: str = "scratchpad" # or 'auto_commit'
    created_at: datetime.datetime

@service_metadata(
name="RoleManager",
version="1.0.0",
description="Manages Agent Personas (Roles), including System Prompts and Memory Settings.",
tags=["roles", "personas", "db"],
capabilities=["db:sqlite"]
)
class RoleManagerMS:
    """
The Casting Director: Manages Agent Personas (Roles).
Persists configuration for System Prompts, Attached KBs, and Memory Settings.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", DB_PATH)
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS roles (
                    id TEXT PRIMARY KEY,
                    name TEXT UNIQUE NOT NULL,
                    description TEXT,
                    system_prompt TEXT NOT NULL,
                    knowledge_bases_json TEXT,
                    memory_policy TEXT,
                    created_at TIMESTAMP
                )
            """)

    @service_endpoint(
    inputs={"name": "str", "system_prompt": "str", "description": "str", "kbs": "List[str]"},
    outputs={"role": "Role"},
    description="Creates a new Agent Persona.",
    tags=["roles", "create"],
    side_effects=["db:write"]
    )
    def create_role(self, name: str, system_prompt: str, description: str = "", kbs: List[str] = None) -> Role:
    """Creates a new Agent Persona."""
        role_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        kbs_json = json.dumps(kbs or [])
        
        try:
            with self._get_conn() as conn:
                conn.execute(
                    "INSERT INTO roles (id, name, description, system_prompt, knowledge_bases_json, memory_policy, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    (role_id, name, description, system_prompt, kbs_json, "scratchpad", now)
                )
            log.info(f"Created Role: {name}")
            return self.get_role(name)
        except sqlite3.IntegrityError:
            raise ValueError(f"Role '{name}' already exists.")

    @service_endpoint(
    inputs={"name_or_id": "str"},
    outputs={"role": "Optional[Role]"},
    description="Retrieves a role by Name or ID.",
    tags=["roles", "read"]
    )
    def get_role(self, name_or_id: str) -> Optional[Role]:
    """Retrieves a role by Name or ID."""
        with self._get_conn() as conn:
            # Try ID first
            row = conn.execute("SELECT * FROM roles WHERE id = ?", (name_or_id,)).fetchone()
            if not row:
                # Try Name
                row = conn.execute("SELECT * FROM roles WHERE name = ?", (name_or_id,)).fetchone()
            
            if not row: return None

            return Role(
                id=row['id'],
                name=row['name'],
                description=row['description'],
                system_prompt=row['system_prompt'],
                knowledge_bases=json.loads(row['knowledge_bases_json']),
                memory_policy=row['memory_policy'],
                created_at=row['created_at'] # Adapter might need datetime.fromisoformat if stored as str
            )

    @service_endpoint(
    inputs={},
    outputs={"roles": "List[Dict]"},
    description="Lists all available roles.",
    tags=["roles", "read"]
    )
    def list_roles(self) -> List[Dict]:
    with self._get_conn() as conn:
            rows = conn.execute("SELECT id, name, description FROM roles").fetchall()
            return [dict(r) for r in rows]

    @service_endpoint(
    inputs={"name": "str"},
    outputs={},
    description="Deletes a role by name.",
    tags=["roles", "delete"],
    side_effects=["db:write"]
    )
    def delete_role(self, name: str):
    with self._get_conn() as conn:
            conn.execute("DELETE FROM roles WHERE name = ?", (name,))
        log.info(f"Deleted Role: {name}")

# --- Independent Test Block ---
if __name__ == "__main__":
import os
if DB_PATH.exists(): os.remove(DB_PATH)
    
mgr = RoleManagerMS()
print("Service ready:", mgr)
    
    # 1. Create
    mgr.create_role(
        name="SeniorDev", 
        system_prompt="You are a senior Python developer. Prefer Clean Code principles.",
        description="Expert coding assistant",
        kbs=["python_docs", "project_repo"]
    )
    
    # 2. Retrieve
    role = mgr.get_role("SeniorDev")
    print(f"Role: {role.name}")
    print(f"Prompt: {role.system_prompt}")
    print(f"KBs: {role.knowledge_bases}")
    
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: _RoleManagerMS\requirements.txt
--------------------------------------------------------------------------------
pydantic
--------------------------------------------------------------------------------
FILE: _SandboxManagerMS\app.py
--------------------------------------------------------------------------------
import shutil
import hashlib
import os
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Default folders to ignore when syncing or diffing
DEFAULT_EXCLUDES = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "_sandbox", "_project_library"
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("SandboxMgr")
# ==============================================================================

class SandboxManagerMS:
    """
    The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
    Allows for safe experimentation, diffing, and atomic promotion of changes.
    """
    def __init__(self, live_path: str, sandbox_path: str):
        self.live_root = Path(live_path).resolve()
        self.sandbox_root = Path(sandbox_path).resolve()

    def init_sandbox(self, force: bool = False):
        """
        Creates or resets the sandbox by mirroring the live project.
        """
        if self.sandbox_root.exists():
            if not force:
                raise FileExistsError(f"Sandbox already exists at {self.sandbox_root}")
            log.info("Wiping existing sandbox...")
            shutil.rmtree(self.sandbox_root)
        
        log.info(f"Cloning {self.live_root} -> {self.sandbox_root}...")
        self._mirror_tree(self.live_root, self.sandbox_root)
        log.info("Sandbox initialized.")

    def reset_sandbox(self):
        """
        Discards all sandbox changes and re-syncs from live.
        """
        self.init_sandbox(force=True)

    def get_diff(self) -> Dict[str, List[str]]:
        Compares Sandbox vs Live. Returns added, modified, and deleted files.
        """
        sandbox_files = self._scan_files(self.sandbox_root)
        live_files = self._scan_files(self.live_root)
        
        sandbox_paths = set(sandbox_files.keys())
        live_paths = set(live_files.keys())

        # 1. Added: In sandbox but not in live
        added = sorted(list(sandbox_paths - live_paths))
        
        # 2. Deleted: In live but not in sandbox
        deleted = sorted(list(live_paths - sandbox_paths))
        
        # 3. Modified: In both, but hashes differ
        common = sandbox_paths.intersection(live_paths)
        modified = []
        for rel_path in common:
            if sandbox_files[rel_path] != live_files[rel_path]:
                modified.append(rel_path)
        modified.sort()

        return {
            "added": added,
            "modified": modified,
            "deleted": deleted
        }

    def promote_changes(self) -> Tuple[int, int, int]:
        """
        Applies changes from Sandbox to Live.
        Returns (added_count, modified_count, deleted_count).
        """
        diff = self.get_diff()
        
        # 1. Additions & Modifications (Copy file -> file)
        for rel_path in diff['added'] + diff['modified']:
            src = self.sandbox_root / rel_path
            dst = self.live_root / rel_path
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
            
        # 2. Deletions (Remove file)
        for rel_path in diff['deleted']:
            target = self.live_root / rel_path
            if target.exists():
                os.remove(target)
                
        log.info(f"Promoted: {len(diff['added'])} added, {len(diff['modified'])} modified, {len(diff['deleted'])} deleted.")
        return len(diff['added']), len(diff['modified']), len(diff['deleted'])

    # --- Internal Helpers ---

    def _mirror_tree(self, src_root: Path, dst_root: Path):
        """Recursive copy that respects the exclusion list."""
        if not dst_root.exists():
            dst_root.mkdir(parents=True, exist_ok=True)

        for item in src_root.iterdir():
            if item.name in DEFAULT_EXCLUDES:
                continue
                
            dst_path = dst_root / item.name
            
            if item.is_dir():
                self._mirror_tree(item, dst_path)
            else:
                shutil.copy2(item, dst_path)

    def _scan_files(self, root: Path) -> Dict[str, str]:
        """
        Scans directory and returns {relative_path: sha256_hash}.
        """
        file_map = {}
        if not root.exists():
            return {}
            
        for path in root.rglob("*"):
            if path.is_file() and not self._is_excluded(path, root):
                rel = str(path.relative_to(root)).replace("\\", "/")
                file_map[rel] = self._get_hash(path)
        return file_map

    def _is_excluded(self, path: Path, root: Path) -> bool:
        """Checks if any part of the path is in the exclusion list."""
        try:
            rel_parts = path.relative_to(root).parts
            return any(p in DEFAULT_EXCLUDES for p in rel_parts)
        except ValueError:
            return False

    def _get_hash(self, path: Path) -> str:
        """Fast SHA-256 for file content."""
        try:
            # Skip binary files if needed, or hash them too (hashing is safe)
            return hashlib.sha256(path.read_bytes()).hexdigest()
        except Exception:
            return "read_error"

# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup test environment
    base = Path("test_env")
    live = base / "live_project"
    box = base / "sandbox"
    
    if base.exists(): shutil.rmtree(base)
    live.mkdir(parents=True)
    
    # 1. Create Mock Live Project
    (live / "main.py").write_text("print('v1')")
    (live / "utils.py").write_text("def help(): pass")
    (live / "node_modules").mkdir() # Should be ignored
    (live / "node_modules" / "junk.js").write_text("junk")
    
    print("--- Initializing Sandbox ---")
    mgr = SandboxManagerMS(str(live), str(box))
    mgr.init_sandbox()
    
    # 2. Make Changes in Sandbox
print("\n--- Modifying Sandbox ---")
(box / "main.py").write_text("print('v2')")  # Modify
(box / "new_feature.py").write_text("print('new')")  # Add
os.remove(box / "utils.py")  # Delete

# 3. Check Diff
diff = mgr.get_diff()
print(f"Diff Analysis:\n Added: {diff['added']}\n Modified: {diff['modified']}\n Deleted: {diff['deleted']}")

# 4. Promote
print("\n--- Promoting Changes ---")
mgr.promote_changes()

# Verify Live
print(f"Live 'main.py' content: {(live / 'main.py').read_text()}")
print(f"Live 'utils.py' exists? {(live / 'utils.py').exists()})

# Cleanup
if base.exists(): shutil.rmtree(base)

--------------------------------------------------------------------------------
FILE: _SandboxManagerMS\app.py.bak
--------------------------------------------------------------------------------
import shutil
import hashlib
import os
import logging
from pathlib import Path
from typing import List, Dict, Set, Optional, Tuple

# ==============================================================================
# CONFIGURATION
# ==============================================================================
# Default folders to ignore when syncing or diffing
DEFAULT_EXCLUDES = {
    "node_modules", ".git", "__pycache__", ".venv", ".mypy_cache",
    "_logs", "dist", "build", ".vscode", ".idea", "_sandbox", "_project_library"
}
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("SandboxMgr")
# ==============================================================================

class SandboxManagerMS:
    """
    The Safety Harness: Manages a 'Sandbox' mirror of a 'Live' project.
    Allows for safe experimentation, diffing, and atomic promotion of changes.
    """
    def __init__(self, live_path: str, sandbox_path: str):
        self.live_root = Path(live_path).resolve()
        self.sandbox_root = Path(sandbox_path).resolve()

    def init_sandbox(self, force: bool = False):
        """
        Creates or resets the sandbox by mirroring the live project.
        """
        if self.sandbox_root.exists():
            if not force:
                raise FileExistsError(f"Sandbox already exists at {self.sandbox_root}")
            log.info("Wiping existing sandbox...")
            shutil.rmtree(self.sandbox_root)
        
        log.info(f"Cloning {self.live_root} -> {self.sandbox_root}...")
        self._mirror_tree(self.live_root, self.sandbox_root)
        log.info("Sandbox initialized.")

    def reset_sandbox(self):
        """
        Discards all sandbox changes and re-syncs from live.
        """
        self.init_sandbox(force=True)

    def get_diff(self) -> Dict[str, List[str]]:
        """
        Compares Sandbox vs Live. Returns added, modified, and deleted files.
        """
        sandbox_files = self._scan_files(self.sandbox_root)
        live_files = self._scan_files(self.live_root)
        
        sandbox_paths = set(sandbox_files.keys())
        live_paths = set(live_files.keys())

        # 1. Added: In sandbox but not in live
        added = sorted(list(sandbox_paths - live_paths))
        
        # 2. Deleted: In live but not in sandbox
        deleted = sorted(list(live_paths - sandbox_paths))
        
        # 3. Modified: In both, but hashes differ
        common = sandbox_paths.intersection(live_paths)
        modified = []
        for rel_path in common:
            if sandbox_files[rel_path] != live_files[rel_path]:
                modified.append(rel_path)
        modified.sort()

        return {
            "added": added,
            "modified": modified,
            "deleted": deleted
        }

    def promote_changes(self) -> Tuple[int, int, int]:
        """
        Applies changes from Sandbox to Live.
        Returns (added_count, modified_count, deleted_count).
        """
        diff = self.get_diff()
        
        # 1. Additions & Modifications (Copy file -> file)
        for rel_path in diff['added'] + diff['modified']:
            src = self.sandbox_root / rel_path
            dst = self.live_root / rel_path
            dst.parent.mkdir(parents=True, exist_ok=True)
            shutil.copy2(src, dst)
            
        # 2. Deletions (Remove file)
        for rel_path in diff['deleted']:
            target = self.live_root / rel_path
            if target.exists():
                os.remove(target)
                
        log.info(f"Promoted: {len(diff['added'])} added, {len(diff['modified'])} modified, {len(diff['deleted'])} deleted.")
        return len(diff['added']), len(diff['modified']), len(diff['deleted'])

    # --- Internal Helpers ---

    def _mirror_tree(self, src_root: Path, dst_root: Path):
        """Recursive copy that respects the exclusion list."""
        if not dst_root.exists():
            dst_root.mkdir(parents=True, exist_ok=True)

        for item in src_root.iterdir():
            if item.name in DEFAULT_EXCLUDES:
                continue
                
            dst_path = dst_root / item.name
            
            if item.is_dir():
                self._mirror_tree(item, dst_path)
            else:
                shutil.copy2(item, dst_path)

    def _scan_files(self, root: Path) -> Dict[str, str]:
        """
        Scans directory and returns {relative_path: sha256_hash}.
        """
        file_map = {}
        if not root.exists():
            return {}
            
        for path in root.rglob("*"):
            if path.is_file() and not self._is_excluded(path, root):
                rel = str(path.relative_to(root)).replace("\\", "/")
                file_map[rel] = self._get_hash(path)
        return file_map

    def _is_excluded(self, path: Path, root: Path) -> bool:
        """Checks if any part of the path is in the exclusion list."""
        try:
            rel_parts = path.relative_to(root).parts
            return any(p in DEFAULT_EXCLUDES for p in rel_parts)
        except ValueError:
            return False

    def _get_hash(self, path: Path) -> str:
        """Fast SHA-256 for file content."""
        try:
            # Skip binary files if needed, or hash them too (hashing is safe)
            return hashlib.sha256(path.read_bytes()).hexdigest()
        except Exception:
            return "read_error"

# --- Independent Test Block ---
if __name__ == "__main__":
    # Setup test environment
    base = Path("test_env")
    live = base / "live_project"
    box = base / "sandbox"
    
    if base.exists(): shutil.rmtree(base)
    live.mkdir(parents=True)
    
    # 1. Create Mock Live Project
    (live / "main.py").write_text("print('v1')")
    (live / "utils.py").write_text("def help(): pass")
    (live / "node_modules").mkdir() # Should be ignored
    (live / "node_modules" / "junk.js").write_text("junk")
    
    print("--- Initializing Sandbox ---")
    mgr = SandboxManagerMS(str(live), str(box))
    mgr.init_sandbox()
    
    # 2. Make Changes in Sandbox
    print("\n--- Modifying Sandbox ---")
    (box / "main.py").write_text("print('v2')") # Modify
    (box / "new_feature.py").write_text("print('new')") # Add
    os.remove(box / "utils.py") # Delete
    
    # 3. Check Diff
    diff = mgr.get_diff()
    print(f"Diff Analysis:\n Added: {diff['added']}\n Modified: {diff['modified']}\n Deleted: {diff['deleted']}")
    
    # 4. Promote
    print("\n--- Promoting Changes ---")
    mgr.promote_changes()
    
    # Verify Live
    print(f"Live 'main.py' content: {(live / 'main.py').read_text()}")
    print(f"Live 'utils.py' exists? {(live / 'utils.py').exists()}")
    
    # Cleanup
    if base.exists(): shutil.rmtree(base)
--------------------------------------------------------------------------------
FILE: _ScannerMS\app.py
--------------------------------------------------------------------------------
import os
import time
from typing import Dict, List, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="Scanner",
version="1.0.0",
description="Recursively scans directories, filters junk, and detects binaries.",
tags=["filesystem", "scanner", "tree"],
capabilities=["filesystem:read"]
)
class ScannerMS:
    """
The Scanner: Walks the file system, filters junk, and detects binary files.
Generates the tree structure used by the UI.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
        # Folders to completely ignore (Standard developer noise)
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage'
        }
        
        # Extensions that are explicitly binary/junk
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', '.svg', 
            '.zip', '.tar', '.gz', '.pdf', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }

    def is_binary(self, file_path: str) -> bool:
        """
        Determines if a file is binary using two heuristics:
        1. Extension check (Fast)
        2. Content check for null bytes (Accurate)
        """
        # 1. Fast Fail on Extension
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS:
            return True
            
        # 2. Content Inspection (Read first 1KB)
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                # Text files shouldn't contain null bytes
                if b'\x00' in chunk:
                    return True
        except (IOError, OSError):
            # If we can't read it, treat as binary/unsafe
            return True
            
        return False

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"tree": "Optional[Dict]"},
    description="Recursively scans a directory and returns a JSON-compatible tree.",
    tags=["filesystem", "scan"],
    side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Optional[Dict[str, Any]]:
    """
    Recursively scans a directory and returns a JSON-compatible tree.
    Returns None if path is invalid.
    """
        target = os.path.abspath(root_path)
        
        if not os.path.exists(target):
            return None
            
        if not os.path.isdir(target):
            # Handle single file case
            return self._create_node(target, is_dir=False)

        return self._scan_recursive(target)

    def _scan_recursive(self, current_path: str) -> Dict[str, Any]:
        """
        Internal recursive worker.
        """
        node = self._create_node(current_path, is_dir=True)
        node['children'] = []
        
        try:
            # os.scandir is faster than os.listdir as it returns file attributes
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                
                for entry in entries:
                    # Skip ignored directories
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS:
                        continue
                        
                    # Skip hidden files (dotfiles)
                    if entry.name.startswith('.'):
                        continue

                    if entry.is_dir():
                        child_node = self._scan_recursive(entry.path)
                        if child_node: # Only add if valid
                            node['children'].append(child_node)
                    else:
                        child_node = self._create_node(entry.path, is_dir=False)
                        node['children'].append(child_node)
                        
        except PermissionError:
            node['error'] = "Access Denied"
            
        return node

    def _create_node(self, path: str, is_dir: bool) -> Dict[str, Any]:
        """
        Standardizes the node structure for the UI.
        """
        name = os.path.basename(path)
        node = {
            'text': name,
            'path': path,
            'type': 'folder' if is_dir else 'file',
            'checked': False, # UI State
        }
        
        if not is_dir:
            if self.is_binary(path):
                node['type'] = 'binary'
return node

    @service_endpoint(
    inputs={"tree_node": "Dict"},
    outputs={"files": "List[str]"},
    description="Flattens a tree node into a list of file paths.",
    tags=["filesystem", "utility"],
    side_effects=[]
    )
    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
    """
    Helper to extract all valid file paths from a tree node 
    (e.g., when the user clicks 'Start Ingest').
    """
        files = []
        if tree_node['type'] == 'file':
            files.append(tree_node['path'])
        elif tree_node['type'] == 'folder' and 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files

# --- Independent Test Block ---
if __name__ == "__main__":
    scanner = ScannerMS()
    print("Service ready:", scanner)
    
    # Scan the current directory
    cwd = os.getcwd()
    print(f"Scanning: {cwd} ...")
    
    start_time = time.time()
    tree = scanner.scan_directory(cwd)
    duration = time.time() - start_time
    
    if tree:
        file_count = len(scanner.flatten_tree(tree))
        print(f"Scan complete in {duration:.4f}s")
        print(f"Found {file_count} files.")
# Print top level children to verify
        print("Top Level Structure:")
        for child in tree.get('children', [])[:5]:
            print(f" - [{child['type'].upper()}] {child['text']}")
    else:
        print("Scan failed or path invalid.")

--------------------------------------------------------------------------------
FILE: _ScannerMS\app.py.bak
--------------------------------------------------------------------------------
import os
import time
from typing import Dict, List, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="Scanner",
version="1.0.0",
description="Recursively scans directories, filters junk, and detects binaries.",
tags=["filesystem", "scanner", "tree"],
capabilities=["filesystem:read"]
)
class ScannerMS:
    """
The Scanner: Walks the file system, filters junk, and detects binary files.
Generates the tree structure used by the UI.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
        # Folders to completely ignore (Standard developer noise)
        self.IGNORE_DIRS = {
            '.git', '__pycache__', 'node_modules', 'venv', '.env', 
            '.idea', '.vscode', 'dist', 'build', 'coverage'
        }
        
        # Extensions that are explicitly binary/junk
        self.BINARY_EXTENSIONS = {
            '.pyc', '.pyd', '.exe', '.dll', '.so', '.dylib', '.class', 
            '.jpg', '.jpeg', '.png', '.gif', '.ico', '.svg', 
            '.zip', '.tar', '.gz', '.pdf', '.docx', '.xlsx',
            '.db', '.sqlite', '.sqlite3'
        }

    def is_binary(self, file_path: str) -> bool:
        """
        Determines if a file is binary using two heuristics:
        1. Extension check (Fast)
        2. Content check for null bytes (Accurate)
        """
        # 1. Fast Fail on Extension
        _, ext = os.path.splitext(file_path)
        if ext.lower() in self.BINARY_EXTENSIONS:
            return True
            
        # 2. Content Inspection (Read first 1KB)
        try:
            with open(file_path, 'rb') as f:
                chunk = f.read(1024)
                # Text files shouldn't contain null bytes
                if b'\x00' in chunk:
                    return True
        except (IOError, OSError):
            # If we can't read it, treat as binary/unsafe
            return True
            
        return False

    @service_endpoint(
    inputs={"root_path": "str"},
    outputs={"tree": "Optional[Dict]"},
    description="Recursively scans a directory and returns a JSON-compatible tree.",
    tags=["filesystem", "scan"],
    side_effects=["filesystem:read"]
    )
    def scan_directory(self, root_path: str) -> Optional[Dict[str, Any]]:
    """
    Recursively scans a directory and returns a JSON-compatible tree.
    Returns None if path is invalid.
    """
        target = os.path.abspath(root_path)
        
        if not os.path.exists(target):
            return None
            
        if not os.path.isdir(target):
            # Handle single file case
            return self._create_node(target, is_dir=False)

        return self._scan_recursive(target)

    def _scan_recursive(self, current_path: str) -> Dict[str, Any]:
        """
        Internal recursive worker.
        """
        node = self._create_node(current_path, is_dir=True)
        node['children'] = []
        
        try:
            # os.scandir is faster than os.listdir as it returns file attributes
            with os.scandir(current_path) as it:
                entries = sorted(it, key=lambda e: (not e.is_dir(), e.name.lower()))
                
                for entry in entries:
                    # Skip ignored directories
                    if entry.is_dir() and entry.name in self.IGNORE_DIRS:
                        continue
                        
                    # Skip hidden files (dotfiles)
                    if entry.name.startswith('.'):
                        continue

                    if entry.is_dir():
                        child_node = self._scan_recursive(entry.path)
                        if child_node: # Only add if valid
                            node['children'].append(child_node)
                    else:
                        child_node = self._create_node(entry.path, is_dir=False)
                        node['children'].append(child_node)
                        
        except PermissionError:
            node['error'] = "Access Denied"
            
        return node

    def _create_node(self, path: str, is_dir: bool) -> Dict[str, Any]:
        """
        Standardizes the node structure for the UI.
        """
        name = os.path.basename(path)
        node = {
            'text': name,
            'path': path,
            'type': 'folder' if is_dir else 'file',
            'checked': False, # UI State
        }
        
        if not is_dir:
            if self.is_binary(path):
                node['type'] = 'binary'
                
        return node

    @service_endpoint(
    inputs={"tree_node": "Dict"},
    outputs={"files": "List[str]"},
    description="Flattens a tree node into a list of file paths.",
    tags=["filesystem", "utility"],
    side_effects=[]
    )
    def flatten_tree(self, tree_node: Dict[str, Any]) -> List[str]:
    """
    Helper to extract all valid file paths from a tree node 
    (e.g., when the user clicks 'Start Ingest').
    """
        files = []
        if tree_node['type'] == 'file':
            files.append(tree_node['path'])
        elif tree_node['type'] == 'folder' and 'children' in tree_node:
            for child in tree_node['children']:
                files.extend(self.flatten_tree(child))
        return files

# --- Independent Test Block ---
if __name__ == "__main__":
scanner = ScannerMS()
print("Service ready:", scanner)
    
    # Scan the current directory
    cwd = os.getcwd()
    print(f"Scanning: {cwd} ...")
    
    start_time = time.time()
    tree = scanner.scan_directory(cwd)
    duration = time.time() - start_time
    
    if tree:
        file_count = len(scanner.flatten_tree(tree))
        print(f"Scan complete in {duration:.4f}s")
        print(f"Found {file_count} files.")
        
        # Print top level children to verify
        print("Top Level Structure:")
        for child in tree.get('children', [])[:5]:
            print(f" - [{child['type'].upper()}] {child['text']}")
    else:
        print("Scan failed or path invalid.")

--------------------------------------------------------------------------------
FILE: _SearchEngineMS\app.py
--------------------------------------------------------------------------------
import sqlite3
import json
import struct
import requests
import os
from typing import List, Dict, Any, Optional

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    
    Architecture:
    1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
    2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
    3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).
    """

    def __init__(self, model_name: str = "phi3:mini-128k"):
        self.model = model_name

    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict]:
        """
        Main entry point. Returns a list of results sorted by relevance.
        """
        if not os.path.exists(db_path):
            return []

        conn = sqlite3.connect(db_path)
        # Enable sqlite-vec extension if needed, though standard connect might miss it 
        # depending on system install. For now, we assume the DB is pre-populated 
        # and standard SQL queries work if the extension is loaded globally or unnecessary 
        # for simple selects (standard SQLite can read vec0 tables usually, just not query them efficiently without ext).
        # Note: If sqlite-vec is not loaded, the vec0 MATCH queries below will fail.
        # We try to load it here just in case.
        conn.enable_load_extension(True)
        try:
            import sqlite_vec
            sqlite_vec.load(conn)
        except:
            print("Warning: sqlite_vec not loaded in Search Engine. Vector search may fail.")

        cursor = conn.cursor()

        # 1. Vectorize the User Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails
            return self._keyword_search_only(cursor, query, limit)

        # Pack vector for sqlite-vec (Float32 Little Endian)
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)

        # 2. HYBRID QUERY (The "Magic" SQL)
        # We use CTEs to get top 50 from Vector and top 50 from Keyword, then merge.
        sql = """
        WITH 
        vec_matches AS (
            SELECT rowid, distance,
            row_number() OVER (ORDER BY distance) as rank
            FROM knowledge_vectors
            WHERE embedding MATCH ? 
            AND k = 50
        ),
        fts_matches AS (
            SELECT rowid, rank as fts_score,
            row_number() OVER (ORDER BY rank) as rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT 50
        )
        SELECT 
            kc.file_path,
            kc.content,
            (
                -- RRF Formula: 1 / (k + rank)
                COALESCE(1.0 / (60 + v.rank), 0.0) +
                COALESCE(1.0 / (60 + f.rank), 0.0)
            ) as rrf_score
        FROM knowledge_chunks kc
        LEFT JOIN vec_matches v ON kc.id = v.rowid
        LEFT JOIN fts_matches f ON kc.id = f.rowid
        WHERE v.rowid IS NOT NULL OR f.rowid IS NOT NULL
        ORDER BY rrf_score DESC
        LIMIT ?;
        """

        try:
            # Escape quotes for FTS
            fts_query = f'"{query}"' 
            rows = cursor.execute(sql, (vec_bytes, fts_query, limit)).fetchall()
        except sqlite3.OperationalError as e:
            print(f"Search Error (likely missing sqlite-vec): {e}")
            return []

        results = []
        for r in rows:
            path, content, score = r
            snippet = self._extract_snippet(content, query)
            results.append({
                "path": path,
                "score": round(score, 4),
                "snippet": snippet,
                "full_content": content # Keeping this for "Reconstruct" later
            })

        conn.close()
        return results

    def _keyword_search_only(self, cursor, query: str, limit: int) -> List[Dict]:
        """Fallback if embeddings are offline."""
        sql = """
            SELECT file_path, content
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        rows = cursor.execute(sql, (f'"{query}"', limit)).fetchall()
        return [{
            "path": r[0], 
            "score": 0.0, 
            "snippet": self._extract_snippet(r[1], query),
            "full_content": r[1]
        } for r in rows]

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        """Call Ollama to get the vector for the search query."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

    def _extract_snippet(self, content: str, query: str) -> str:
        """Finds the best window of text around the keyword."""
        lower_content = content.lower()
        lower_query = query.lower().split()[0] # Take first word for simple centering
        
        idx = lower_content.find(lower_query)
        if idx == -1:
            return content[:200].replace('\n', ' ') + "..."
            
        start = max(0, idx - 60)
        end = min(len(content), idx + 140)
        snippet = content[start:end].replace('\n', ' ')
        return f"...{snippet}..."

# --- Independent Test Block ---
if __name__ == "__main__":
    # Note: Requires a real DB path to work
    print("Initializing Search Engine...")
    engine = SearchEngineMS()
    # Test would go here

--------------------------------------------------------------------------------
FILE: _SearchEngineMS\app.py.bak
--------------------------------------------------------------------------------
import sqlite3
import json
import struct
import requests
import os
from typing import List, Dict, Any, Optional

# Configuration
OLLAMA_API_URL = "http://localhost:11434/api"

class SearchEngineMS:
    """
    The Oracle: Performs Hybrid Search (Vector Similarity + Keyword Matching).
    
    Architecture:
    1. Vector Search: Uses sqlite-vec (vec0) for fast nearest neighbor search.
    2. Keyword Search: Uses SQLite FTS5 for BM25-style text matching.
    3. Reranking: Combines scores using Reciprocal Rank Fusion (RRF).
    """

    def __init__(self, model_name: str = "phi3:mini-128k"):
        self.model = model_name

    def search(self, db_path: str, query: str, limit: int = 10) -> List[Dict]:
        """
        Main entry point. Returns a list of results sorted by relevance.
        """
        if not os.path.exists(db_path):
            return []

        conn = sqlite3.connect(db_path)
        # Enable sqlite-vec extension if needed, though standard connect might miss it 
        # depending on system install. For now, we assume the DB is pre-populated 
        # and standard SQL queries work if the extension is loaded globally or unnecessary 
        # for simple selects (standard SQLite can read vec0 tables usually, just not query them efficiently without ext).
        # Note: If sqlite-vec is not loaded, the vec0 MATCH queries below will fail.
        # We try to load it here just in case.
        conn.enable_load_extension(True)
        try:
            import sqlite_vec
            sqlite_vec.load(conn)
        except:
            print("Warning: sqlite_vec not loaded in Search Engine. Vector search may fail.")

        cursor = conn.cursor()

        # 1. Vectorize the User Query
        query_vec = self._get_query_embedding(query)
        if not query_vec:
            # Fallback to keyword only if embedding fails
            return self._keyword_search_only(cursor, query, limit)

        # Pack vector for sqlite-vec (Float32 Little Endian)
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)

        # 2. HYBRID QUERY (The "Magic" SQL)
        # We use CTEs to get top 50 from Vector and top 50 from Keyword, then merge.
        sql = """
        WITH 
        vec_matches AS (
            SELECT rowid, distance,
            row_number() OVER (ORDER BY distance) as rank
            FROM knowledge_vectors
            WHERE embedding MATCH ? 
            AND k = 50
        ),
        fts_matches AS (
            SELECT rowid, rank as fts_score,
            row_number() OVER (ORDER BY rank) as rank
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT 50
        )
        SELECT 
            kc.file_path,
            kc.content,
            (
                -- RRF Formula: 1 / (k + rank)
                COALESCE(1.0 / (60 + v.rank), 0.0) +
                COALESCE(1.0 / (60 + f.rank), 0.0)
            ) as rrf_score
        FROM knowledge_chunks kc
        LEFT JOIN vec_matches v ON kc.id = v.rowid
        LEFT JOIN fts_matches f ON kc.id = f.rowid
        WHERE v.rowid IS NOT NULL OR f.rowid IS NOT NULL
        ORDER BY rrf_score DESC
        LIMIT ?;
        """

        try:
            # Escape quotes for FTS
            fts_query = f'"{query}"' 
            rows = cursor.execute(sql, (vec_bytes, fts_query, limit)).fetchall()
        except sqlite3.OperationalError as e:
            print(f"Search Error (likely missing sqlite-vec): {e}")
            return []

        results = []
        for r in rows:
            path, content, score = r
            snippet = self._extract_snippet(content, query)
            results.append({
                "path": path,
                "score": round(score, 4),
                "snippet": snippet,
                "full_content": content # Keeping this for "Reconstruct" later
            })

        conn.close()
        return results

    def _keyword_search_only(self, cursor, query: str, limit: int) -> List[Dict]:
        """Fallback if embeddings are offline."""
        sql = """
            SELECT file_path, content
            FROM documents_fts
            WHERE documents_fts MATCH ?
            ORDER BY rank
            LIMIT ?
        """
        rows = cursor.execute(sql, (f'"{query}"', limit)).fetchall()
        return [{
            "path": r[0], 
            "score": 0.0, 
            "snippet": self._extract_snippet(r[1], query),
            "full_content": r[1]
        } for r in rows]

    def _get_query_embedding(self, text: str) -> Optional[List[float]]:
        """Call Ollama to get the vector for the search query."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=5
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except:
            return None
        return None

    def _extract_snippet(self, content: str, query: str) -> str:
        """Finds the best window of text around the keyword."""
        lower_content = content.lower()
        lower_query = query.lower().split()[0] # Take first word for simple centering
        
        idx = lower_content.find(lower_query)
        if idx == -1:
            return content[:200].replace('\n', ' ') + "..."
            
        start = max(0, idx - 60)
        end = min(len(content), idx + 140)
        snippet = content[start:end].replace('\n', ' ')
        return f"...{snippet}..."

# --- Independent Test Block ---
if __name__ == "__main__":
    # Note: Requires a real DB path to work
    print("Initializing Search Engine...")
    engine = SearchEngineMS()
    # Test would go here
--------------------------------------------------------------------------------
FILE: _SearchEngineMS\requirements.txt
--------------------------------------------------------------------------------
requests
sqlite-vec
--------------------------------------------------------------------------------
FILE: _ServiceRegistryMS\app.py
--------------------------------------------------------------------------------
import ast
import json
import uuid
import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("ServiceRegistry")
# ==============================================================================

@service_metadata(
name="ServiceRegistry",
version="1.0.0",
description="Scans a library of Python microservices and generates standardized JSON Service Tokens.",
tags=["introspection", "registry", "parsing"],
capabilities=["filesystem:read", "filesystem:write"]
)
class ServiceRegistryMS:
    """
The Tokenizer (v2): Scans a library of Python microservices and generates
standardized JSON 'Service Tokens'.
Feature: Hybrid AST/Regex parsing for maximum robustness.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
    self.config = config or {}
    self.root = Path(self.config.get("root_path", ".")).resolve()
    self.registry = []

    @service_endpoint(
    inputs={"save_to": "str"},
    outputs={"registry": "List[Dict]"},
    description="Scans the file system for microservices and builds a registry.",
    tags=["introspection", "scan"],
    side_effects=["filesystem:read", "filesystem:write"]
    )
    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict]:
    log.info(f"Scanning for microservices in: {self.root}")
        
        # 1. Walk directories
        for item in self.root.iterdir():
            if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                self._process_folder(item)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            log.info(f"✅ Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            log.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_folder(self, folder: Path):
        # Find the main .py file (usually matches folder name, or is the only .py file)
        candidates = list(folder.glob("*.py"))
        for file in candidates:
            if file.name.startswith("__"): continue
            
            token = self._tokenize_file(file)
            if token:
                self.registry.append(token)
                log.info(f"  + Tokenized: {token['name']}")
                break 

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source = f.read()
            
            # Attempt 1: Strict AST Parsing (The "Right" Way)
            try:
                return self._ast_parse(source, file_path)
            except Exception:
                # Attempt 2: Regex Fallback (The "Survival" Way)
                return self._regex_parse(source, file_path)
                
        except Exception as e:
            log.warning(f"  - Failed to read {file_path.name}: {e}")
            return None

    def _ast_parse(self, source: str, file_path: Path):
tree = ast.parse(source)
        target_class = None
        
        # Find class ending in 'MS'
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name.endswith("MS"):
                target_class = node
                break
        
        if not target_class: return None

        # Extract Metadata
        return self._build_token(
            name=target_class.name,
            doc=ast.get_docstring(target_class) or "",
            methods=[
                (n.name, [a.arg for a in n.args.args if a.arg != 'self'], ast.get_docstring(n) or "")
                for n in target_class.body if isinstance(n, ast.FunctionDef) and not n.name.startswith("_")
            ],
            deps=self._extract_ast_imports(tree),
            file_path=file_path
        )

    def _regex_parse(self, source: str, file_path: Path):
        # Find class definition
        class_match = re.search(r'class\s+(\w+MS)', source)
        if not class_match: return None
        name = class_match.group(1)
        
        # Find methods (def name(args):)
        methods = []
        for match in re.finditer(r'def\s+(\w+)\s*\((.*?)\):', source):
            m_name = match.group(1)
            if not m_name.startswith("_"):
                args = [a.strip().split(':')[0] for a in match.group(2).split(',') if a.strip() != 'self']
                methods.append((m_name, args, "Regex extracted"))
                
        return self._build_token(name, "Parsed via Regex", methods, [], file_path)

    def _build_token(self, name, doc, methods, deps, file_path):
        # Generate deterministic ID
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, name).hex[:8].upper()}"
        
        method_dict = {
            m_name: {"args": m_args, "doc": m_doc.strip()} 
            for m_name, m_args, m_doc in methods
        }

        return {
            "token_id": token_id,
            "name": name,
            "path": str(file_path.relative_to(self.root)).replace('\\', '/'),
            "description": doc.strip(),
            "methods": method_dict,
            "dependencies": sorted(deps)
        }

    def _extract_ast_imports(self, tree):
        deps = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: deps.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: deps.add(node.module.split('.')[0])
        return list(deps)

if __name__ == "__main__":
svc = ServiceRegistryMS()
print("Service ready:", svc)
svc.scan()

--------------------------------------------------------------------------------
FILE: _ServiceRegistryMS\app.py.bak
--------------------------------------------------------------------------------
import ast
import json
import uuid
import os
import re
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
OUTPUT_FILE = "registry.json"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("ServiceRegistry")
# ==============================================================================

@service_metadata(
name="ServiceRegistry",
version="1.0.0",
description="Scans a library of Python microservices and generates standardized JSON Service Tokens.",
tags=["introspection", "registry", "parsing"],
capabilities=["filesystem:read", "filesystem:write"]
)
class ServiceRegistryMS:
    """
The Tokenizer (v2): Scans a library of Python microservices and generates
standardized JSON 'Service Tokens'.
Feature: Hybrid AST/Regex parsing for maximum robustness.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.root = Path(self.config.get("root_path", ".")).resolve()
self.registry = []

    @service_endpoint(
    inputs={"save_to": "str"},
    outputs={"registry": "List[Dict]"},
    description="Scans the file system for microservices and builds a registry.",
    tags=["introspection", "scan"],
    side_effects=["filesystem:read", "filesystem:write"]
    )
    def scan(self, save_to: str = OUTPUT_FILE) -> List[Dict]:
    log.info(f"Scanning for microservices in: {self.root}")
        
        # 1. Walk directories
        for item in self.root.iterdir():
            if item.is_dir() and item.name.startswith("_") and item.name.endswith("MS"):
                self._process_folder(item)
        
        # 2. Save Registry
        try:
            with open(save_to, "w", encoding="utf-8") as f:
                json.dump(self.registry, f, indent=2)
            log.info(f"✅ Registry built. Found {len(self.registry)} services. Saved to {save_to}")
        except Exception as e:
            log.error(f"Failed to save registry: {e}")
            
        return self.registry

    def _process_folder(self, folder: Path):
        # Find the main .py file (usually matches folder name, or is the only .py file)
        candidates = list(folder.glob("*.py"))
        for file in candidates:
            if file.name.startswith("__"): continue
            
            token = self._tokenize_file(file)
            if token:
                self.registry.append(token)
                log.info(f"  + Tokenized: {token['name']}")
                break 

    def _tokenize_file(self, file_path: Path) -> Optional[Dict[str, Any]]:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                source = f.read()
            
            # Attempt 1: Strict AST Parsing (The "Right" Way)
            try:
                return self._ast_parse(source, file_path)
            except Exception:
                # Attempt 2: Regex Fallback (The "Survival" Way)
                return self._regex_parse(source, file_path)
                
        except Exception as e:
            log.warning(f"  - Failed to read {file_path.name}: {e}")
            return None

    def _ast_parse(self, source: str, file_path: Path):
        tree = ast.parse(source)
        target_class = None
        
        # Find class ending in 'MS'
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name.endswith("MS"):
                target_class = node
                break
        
        if not target_class: return None

        # Extract Metadata
        return self._build_token(
            name=target_class.name,
            doc=ast.get_docstring(target_class) or "",
            methods=[
                (n.name, [a.arg for a in n.args.args if a.arg != 'self'], ast.get_docstring(n) or "")
                for n in target_class.body if isinstance(n, ast.FunctionDef) and not n.name.startswith("_")
            ],
            deps=self._extract_ast_imports(tree),
            file_path=file_path
        )

    def _regex_parse(self, source: str, file_path: Path):
        # Find class definition
        class_match = re.search(r'class\s+(\w+MS)', source)
        if not class_match: return None
        name = class_match.group(1)
        
        # Find methods (def name(args):)
        methods = []
        for match in re.finditer(r'def\s+(\w+)\s*\((.*?)\):', source):
            m_name = match.group(1)
            if not m_name.startswith("_"):
                args = [a.strip().split(':')[0] for a in match.group(2).split(',') if a.strip() != 'self']
                methods.append((m_name, args, "Regex extracted"))
                
        return self._build_token(name, "Parsed via Regex", methods, [], file_path)

    def _build_token(self, name, doc, methods, deps, file_path):
        # Generate deterministic ID
        namespace = uuid.uuid5(uuid.NAMESPACE_DNS, "microservice.library")
        token_id = f"MS_{uuid.uuid5(namespace, name).hex[:8].upper()}"
        
        method_dict = {
            m_name: {"args": m_args, "doc": m_doc.strip()} 
            for m_name, m_args, m_doc in methods
        }

        return {
            "token_id": token_id,
            "name": name,
            "path": str(file_path.relative_to(self.root)).replace('\\', '/'),
            "description": doc.strip(),
            "methods": method_dict,
            "dependencies": sorted(deps)
        }

    def _extract_ast_imports(self, tree):
        deps = set()
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for n in node.names: deps.add(n.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module: deps.add(node.module.split('.')[0])
        return list(deps)

if __name__ == "__main__":
svc = ServiceRegistryMS()
print("Service ready:", svc)
svc.scan()

--------------------------------------------------------------------------------
FILE: _SmartChunkerMS\app.py
--------------------------------------------------------------------------------
import re
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SmartChunker",
version="1.0.0",
description="Recursively splits text respecting natural boundaries (paragraphs, sentences).",
tags=["chunking", "nlp", "text-processing"],
capabilities=["compute"]
)
class SmartChunkerMS:
    """
The Editor: A 'Recursive' text splitter.
It respects the natural structure of text (Paragraphs -> Sentences -> Words)
rather than just hacking it apart by character count.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
# Separators in order of preference.
        # 1. Double newline (Paragraph break)
        # 2. Single newline (Line break / List item)
        # 3. Sentence endings (Period, Question, Exclamation + Space)
        # 4. Space (Word break)
        # 5. Empty string (Hard character cut)
        self.separators = ["\n\n", "\n", "(?<=[.?!])\s+", " ", ""]

    @service_endpoint(
    inputs={"text": "str", "max_size": "int", "overlap": "int"},
    outputs={"chunks": "List[str]"},
    description="Recursively chunks text while trying to keep related content together.",
    tags=["chunking", "compute"],
    side_effects=[]
    )
    def chunk(self, text: str, max_size: int = 1000, overlap: int = 100) -> List[str]:
    """
    Recursively chunks text while trying to keep related content together.
    """
        return self._recursive_split(text, self.separators, max_size, overlap)

    def _recursive_split(self, text: str, separators: List[str], max_size: int, overlap: int) -> List[str]:
        final_chunks = []
        
        # 1. Base Case: If the text fits, return it
        if len(text) <= max_size:
            return [text]
        
        # 2. Edge Case: No more separators, forced hard split
        if not separators:
            return self._hard_split(text, max_size, overlap)

        # 3. Recursive Step: Try to split by the current separator
        current_sep = separators[0]
        next_separators = separators[1:]
        
        # Regex split to keep delimiters if possible (logic varies by regex complexity)
        # For simple string splits like \n\n, we just split.
        if len(current_sep) > 1 and "(" in current_sep: 
            # It's a regex lookbehind (sentence splitter), use re.split
            splits = re.split(current_sep, text)
        else:
            splits = text.split(current_sep)

        # Now we have a list of smaller pieces. We need to merge them back together
        # until they fill the 'max_size' bucket, then start a new bucket.
        current_doc = []
        current_length = 0
        
        for split in splits:
            if not split: continue
            
            # If a single split is STILL too big, recurse deeper on it
            if len(split) > max_size:
                # If we have stuff in the buffer, flush it first
                if current_doc:
                    final_chunks.append(current_sep.join(current_doc))
                    current_doc = []
                    current_length = 0
                
                # Recurse on the big chunk using the NEXT separator
                sub_chunks = self._recursive_split(split, next_separators, max_size, overlap)
                final_chunks.extend(sub_chunks)
                continue

            # Check if adding this split would overflow
            if current_length + len(split) + len(current_sep) > max_size:
                # Flush the current buffer
                doc_text = current_sep.join(current_doc)
                final_chunks.append(doc_text)
# Start new buffer with overlap logic?
                # For simplicity in recursion, we often just start fresh or carry over 
                # a small tail if we implemented a rolling window here.
                # To keep this "Pure logic" simple, we start fresh with the current split.
                current_doc = [split]
                current_length = len(split)
            else:
                # Add to buffer
                current_doc.append(split)
                current_length += len(split) + len(current_sep)

        # Flush remaining
        if current_doc:
            final_chunks.append(current_sep.join(current_doc))

        return final_chunks

    def _hard_split(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Last resort: naive character sliding window."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
    chunker = SmartChunkerMS()
    print("Service ready:", chunker)
    
    # Example: A technical document with structure
    doc = """
    # Intro to AI
    Artificial Intelligence is great. It helps us code.
    
    ## How it works
    1. Ingestion: Reading data.
    2. Processing: Thinking about data.
    
    This is a very long paragraph that effectively serves as a stress test for the sentence splitter. It should hopefully not break in the middle of a thought! We want to keep sentences whole.
    """
    
    print("--- Testing Smart Chunking (Max 60 chars) ---")
    # We set max_size very small to force it to use the sentence/word splitters
    chunks = chunker.chunk(doc, max_size=60, overlap=0)
    
    for i, c in enumerate(chunks):
        print(f"[{i}] {repr(c)}")

--------------------------------------------------------------------------------
FILE: _SmartChunkerMS\app.py.bak
--------------------------------------------------------------------------------
import re
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SmartChunker",
version="1.0.0",
description="Recursively splits text respecting natural boundaries (paragraphs, sentences).",
tags=["chunking", "nlp", "text-processing"],
capabilities=["compute"]
)
class SmartChunkerMS:
    """
The Editor: A 'Recursive' text splitter.
It respects the natural structure of text (Paragraphs -> Sentences -> Words)
rather than just hacking it apart by character count.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
# Separators in order of preference.
        # 1. Double newline (Paragraph break)
        # 2. Single newline (Line break / List item)
        # 3. Sentence endings (Period, Question, Exclamation + Space)
        # 4. Space (Word break)
        # 5. Empty string (Hard character cut)
        self.separators = ["\n\n", "\n", "(?<=[.?!])\s+", " ", ""]

    @service_endpoint(
    inputs={"text": "str", "max_size": "int", "overlap": "int"},
    outputs={"chunks": "List[str]"},
    description="Recursively chunks text while trying to keep related content together.",
    tags=["chunking", "compute"],
    side_effects=[]
    )
    def chunk(self, text: str, max_size: int = 1000, overlap: int = 100) -> List[str]:
    """
    Recursively chunks text while trying to keep related content together.
    """
        return self._recursive_split(text, self.separators, max_size, overlap)

    def _recursive_split(self, text: str, separators: List[str], max_size: int, overlap: int) -> List[str]:
        final_chunks = []
        
        # 1. Base Case: If the text fits, return it
        if len(text) <= max_size:
            return [text]
        
        # 2. Edge Case: No more separators, forced hard split
        if not separators:
            return self._hard_split(text, max_size, overlap)

        # 3. Recursive Step: Try to split by the current separator
        current_sep = separators[0]
        next_separators = separators[1:]
        
        # Regex split to keep delimiters if possible (logic varies by regex complexity)
        # For simple string splits like \n\n, we just split.
        if len(current_sep) > 1 and "(" in current_sep: 
            # It's a regex lookbehind (sentence splitter), use re.split
            splits = re.split(current_sep, text)
        else:
            splits = text.split(current_sep)

        # Now we have a list of smaller pieces. We need to merge them back together
        # until they fill the 'max_size' bucket, then start a new bucket.
        current_doc = []
        current_length = 0
        
        for split in splits:
            if not split: continue
            
            # If a single split is STILL too big, recurse deeper on it
            if len(split) > max_size:
                # If we have stuff in the buffer, flush it first
                if current_doc:
                    final_chunks.append(current_sep.join(current_doc))
                    current_doc = []
                    current_length = 0
                
                # Recurse on the big chunk using the NEXT separator
                sub_chunks = self._recursive_split(split, next_separators, max_size, overlap)
                final_chunks.extend(sub_chunks)
                continue

            # Check if adding this split would overflow
            if current_length + len(split) + len(current_sep) > max_size:
                # Flush the current buffer
                doc_text = current_sep.join(current_doc)
                final_chunks.append(doc_text)
                
                # Start new buffer with overlap logic?
                # For simplicity in recursion, we often just start fresh or carry over 
                # a small tail if we implemented a rolling window here.
                # To keep this "Pure logic" simple, we start fresh with the current split.
                current_doc = [split]
                current_length = len(split)
            else:
                # Add to buffer
                current_doc.append(split)
                current_length += len(split) + len(current_sep)

        # Flush remaining
        if current_doc:
            final_chunks.append(current_sep.join(current_doc))

        return final_chunks

    def _hard_split(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """Last resort: naive character sliding window."""
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunks.append(text[start:end])
            start += chunk_size - overlap
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = SmartChunkerMS()
print("Service ready:", chunker)
    
    # Example: A technical document with structure
    doc = """
    # Intro to AI
    Artificial Intelligence is great. It helps us code.
    
    ## How it works
    1. Ingestion: Reading data.
    2. Processing: Thinking about data.
    
    This is a very long paragraph that effectively serves as a stress test for the sentence splitter. It should hopefully not break in the middle of a thought! We want to keep sentences whole.
    """
    
    print("--- Testing Smart Chunking (Max 60 chars) ---")
    # We set max_size very small to force it to use the sentence/word splitters
    chunks = chunker.chunk(doc, max_size=60, overlap=0)
    
    for i, c in enumerate(chunks):
        print(f"[{i}] {repr(c)}")

--------------------------------------------------------------------------------
FILE: _SpinnerThingyMaBobberMS\app.py
--------------------------------------------------------------------------------
import tkinter as tk
import math
import colorsys
import time
from typing import Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SpinnerTHINGYMABOBBER",
version="1.0.0",
description="Interactive visual spinner widget for OBS/UI overlays.",
tags=["ui", "widget", "visuals"],
capabilities=["ui:gui"]
)
class SpinnerTHINGYMABOBBERMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.root = tk.Tk()
self.root.title("OBS Interactive Spinner")
        self.root.configure(bg="black")
        
        # Default size
        self.root.geometry("600x600")
        
        # Canvas for drawing
        self.canvas = tk.Canvas(
            self.root, 
            bg="black", 
            highlightthickness=0
        )
        self.canvas.pack(fill="both", expand=True)

        # --- STATE VARIABLES ---
        self.angle_1 = 0
        self.angle_2 = 0
        self.angle_3 = 0
        self.hue = 0
        
        # Text Input State
        self.user_text = "PROCESSING"
        self.cursor_visible = True
        self.last_cursor_toggle = time.time()
        
        # Bind keyboard events to the window
        self.root.bind("<Key>", self.handle_keypress)
        
        # Start animation
        self.animate()

        @service_endpoint(
@service_endpoint(
    inputs={},
    outputs={},
    description="Launches the GUI main loop.",
    tags=["ui", "execution"],
    mode="sync",
    side_effects=["ui:block"]
)
def launch(self):
    self.root.mainloop()

def handle_keypress(self, event):
    # Handle Backspace
    if event.keysym == "BackSpace":
        self.user_text = self.user_text[:-1]
    # Handle Escape (Reset to default)
    elif event.keysym == "Escape":
        self.user_text = "PROCESSING"
    # Ignore special keys (Shift, Ctrl, Alt, F-keys, etc.)
    elif len(event.char) == 1 and ord(event.char) >= 32:
        # Limit length to prevent chaos (optional, but 20 is a safe max)
        if len(self.user_text) < 25: 
            self.user_text += event.char.upper()

def get_neon_color(self, offset=0):
    h = (self.hue + offset) % 1.0
    r, g, b = colorsys.hsv_to_rgb(h, 1.0, 1.0)
    return f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}'

def draw_arc(self, cx, cy, radius, width, start, extent, color):
    x0 = cx - radius
    y0 = cy - radius
    x1 = cx + radius
    y1 = cy + radius
    
    self.canvas.create_arc(
        x0, y0, x1, y1,
        start=start, extent=extent,
        outline=color, width=width, style="arc"
    )
def animate(self):
    self.canvas.delete("all")
    
    # Window Dimensions
    w = self.canvas.winfo_width()
    h = self.canvas.winfo_height()
    
    if w < 10 or h < 10:
        self.root.after(50, self.animate)
        return

    cx, cy = w / 2, h / 2
    base_size = min(w, h) / 2
    
    # Update Hue
    self.hue += 0.005
    if self.hue > 1: self.hue = 0
    c1 = self.get_neon_color(0.0)
    c2 = self.get_neon_color(0.3)
    c3 = self.get_neon_color(0.6)

    # --- RINGS ---
        
    # Ring 1
    r1 = base_size * 0.85
    self.angle_1 -= 3
    for i in range(3):
        self.draw_arc(cx, cy, r1, base_size*0.08, self.angle_1 + (i*120), 80, c1)

    # Ring 2
    r2 = base_size * 0.65
    self.angle_2 += 5
    self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2, 160, c2)
    self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2 + 180, 160, c2)

    # Ring 3
    r3 = base_size * 0.45
    self.angle_3 -= 8
    self.draw_arc(cx, cy, r3, base_size*0.04, self.angle_3, 300, c3)
        # --- TEXT LOGIC ---
        
        # Toggle cursor every 0.5 seconds
        if time.time() - self.last_cursor_toggle > 0.5:
            self.cursor_visible = not self.cursor_visible
            self.last_cursor_toggle = time.time()
            
        display_text = self.user_text + ("_" if self.cursor_visible else " ")

        # Dynamic Font Scaling
        # We start with a base size (0.15 of window).
        # If text is long (> 8 chars), we shrink it proportionally so it fits.
        text_len = max(len(self.user_text), 1)
        scaling_factor = 1.0
        if text_len > 8:
            scaling_factor = 8 / text_len
            
        font_size = int(base_size * 0.15 * scaling_factor)
        # Ensure font doesn't vanish
        font_size = max(font_size, 10) 

        self.canvas.create_text(
            cx, cy, 
            text=display_text, 
            fill="white", 
            font=("Courier", font_size, "bold")
        )

        self.root.after(30, self.animate)

if __name__ == "__main__":
svc = SpinnerTHINGYMABOBBERMS()
svc.launch()

--------------------------------------------------------------------------------
FILE: _SpinnerThingyMaBobberMS\app.py.bak
--------------------------------------------------------------------------------
import tkinter as tk
import math
import colorsys
import time
from typing import Optional, Dict, Any
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SpinnerTHINGYMABOBBER",
version="1.0.0",
description="Interactive visual spinner widget for OBS/UI overlays.",
tags=["ui", "widget", "visuals"],
capabilities=["ui:gui"]
)
class SpinnerTHINGYMABOBBERMS:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.root = tk.Tk()
self.root.title("OBS Interactive Spinner")
        self.root.configure(bg="black")
        
        # Default size
        self.root.geometry("600x600")
        
        # Canvas for drawing
        self.canvas = tk.Canvas(
            self.root, 
            bg="black", 
            highlightthickness=0
        )
        self.canvas.pack(fill="both", expand=True)

        # --- STATE VARIABLES ---
        self.angle_1 = 0
        self.angle_2 = 0
        self.angle_3 = 0
        self.hue = 0
        
        # Text Input State
        self.user_text = "PROCESSING"
        self.cursor_visible = True
        self.last_cursor_toggle = time.time()
        
        # Bind keyboard events to the window
        self.root.bind("<Key>", self.handle_keypress)
        
        # Start animation
        self.animate()

        @service_endpoint(
        inputs={},
        outputs={},
        description="Launches the GUI main loop.",
        tags=["ui", "execution"],
        mode="sync",
        side_effects=["ui:block"]
        )
        def launch(self):
        self.root.mainloop()

        def handle_keypress(self, event):
        # Handle Backspace
        if event.keysym == "BackSpace":
            self.user_text = self.user_text[:-1]
        # Handle Escape (Reset to default)
        elif event.keysym == "Escape":
            self.user_text = "PROCESSING"
        # Ignore special keys (Shift, Ctrl, Alt, F-keys, etc.)
        elif len(event.char) == 1 and ord(event.char) >= 32:
            # Limit length to prevent chaos (optional, but 20 is a safe max)
            if len(self.user_text) < 25: 
                self.user_text += event.char.upper()

    def get_neon_color(self, offset=0):
        h = (self.hue + offset) % 1.0
        r, g, b = colorsys.hsv_to_rgb(h, 1.0, 1.0)
        return f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}'

    def draw_arc(self, cx, cy, radius, width, start, extent, color):
        x0 = cx - radius
        y0 = cy - radius
        x1 = cx + radius
        y1 = cy + radius
        
        self.canvas.create_arc(
            x0, y0, x1, y1,
            start=start, extent=extent,
            outline=color, width=width, style="arc"
        )

    def animate(self):
        self.canvas.delete("all")
        
        # Window Dimensions
        w = self.canvas.winfo_width()
        h = self.canvas.winfo_height()
        
        if w < 10 or h < 10:
            self.root.after(50, self.animate)
            return

        cx, cy = w / 2, h / 2
        base_size = min(w, h) / 2
        
        # Update Hue
        self.hue += 0.005
        if self.hue > 1: self.hue = 0
        c1 = self.get_neon_color(0.0)
        c2 = self.get_neon_color(0.3)
        c3 = self.get_neon_color(0.6)

        # --- RINGS ---
        
        # Ring 1
        r1 = base_size * 0.85
        self.angle_1 -= 3
        for i in range(3):
            self.draw_arc(cx, cy, r1, base_size*0.08, self.angle_1 + (i*120), 80, c1)

        # Ring 2
        r2 = base_size * 0.65
        self.angle_2 += 5
        self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2, 160, c2)
        self.draw_arc(cx, cy, r2, base_size*0.05, self.angle_2 + 180, 160, c2)

        # Ring 3
        r3 = base_size * 0.45
        self.angle_3 -= 8
        self.draw_arc(cx, cy, r3, base_size*0.04, self.angle_3, 300, c3)

        # --- TEXT LOGIC ---
        
        # Toggle cursor every 0.5 seconds
        if time.time() - self.last_cursor_toggle > 0.5:
            self.cursor_visible = not self.cursor_visible
            self.last_cursor_toggle = time.time()
            
        display_text = self.user_text + ("_" if self.cursor_visible else " ")

        # Dynamic Font Scaling
        # We start with a base size (0.15 of window).
        # If text is long (> 8 chars), we shrink it proportionally so it fits.
        text_len = max(len(self.user_text), 1)
        scaling_factor = 1.0
        if text_len > 8:
            scaling_factor = 8 / text_len
            
        font_size = int(base_size * 0.15 * scaling_factor)
        # Ensure font doesn't vanish
        font_size = max(font_size, 10) 

        self.canvas.create_text(
            cx, cy, 
            text=display_text, 
            fill="white", 
            font=("Courier", font_size, "bold")
        )

        self.root.after(30, self.animate)

if __name__ == "__main__":
svc = SpinnerTHINGYMABOBBERMS()
svc.launch()

--------------------------------------------------------------------------------
FILE: _SysInspectorMS\app.py
--------------------------------------------------------------------------------
import platform
import subprocess
import sys
import datetime
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SysInspector",
version="1.0.0",
description="Gathers hardware and environment statistics via shell commands.",
tags=["system", "audit", "hardware"],
capabilities=["os:shell", "compute"]
)
class SysInspectorMS:
    """
The Auditor: Gathers hardware and environment statistics.
Supports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={},
outputs={"report": "str"},
description="Runs the full audit and returns a formatted string report.",
tags=["system", "report"],
side_effects=["os:read"]
)
def generate_report(self) -> str:
        """
        Runs the full audit and returns a formatted string report.
        """
        system_os = platform.system()
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = [
            f"System Audit Report",
            f"Generated: {timestamp}",
            f"OS: {system_os} {platform.release()} ({platform.machine()})",
            "-" * 40,
            ""
        ]

        # 1. Hardware Section
        report.append("--- Hardware Information ---")
        if system_os == "Windows":
            report.extend(self._audit_windows())
        elif system_os == "Linux":
            report.extend(self._audit_linux())
        elif system_os == "Darwin":
            report.extend(self._audit_mac())
        else:
            report.append("Unsupported Operating System for detailed hardware audit.")

        # 2. Software Section
        report.append("\n--- Software Environment ---")
        report.append(f"Python Version: {platform.python_version()}")
        report.append(f"Python Executable: {sys.executable}")
        
        return "\n".join(report)

    def _run_cmd(self, cmd: str) -> str:
        """Helper to run shell commands safely."""
        try:
            # shell=True is often required for piped commands, specifically on Windows/Linux
            result = subprocess.run(
                cmd, 
                text=True, 
                capture_output=True, 
                check=False, 
                shell=True, 
                timeout=5
            )
            if result.returncode == 0 and result.stdout:
                return result.stdout.strip()
            elif result.stderr:
                return f"[Cmd Error]: {result.stderr.strip()}"
            return "[No Output]"
        except Exception as e:
            return f"[Execution Error]: {e}"

    # --- OS Specific Implementations ---

    def _audit_windows(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("wmic cpu get name"))
        # GPU
        data.append("GPU: " + self._run_cmd("wmic path win32_videocontroller get name"))
        # RAM
        try:
            mem_str = self._run_cmd("wmic computersystem get totalphysicalmemory").splitlines()[-1]
            mem_bytes = int(mem_str)
            data.append(f"Memory: {mem_bytes / (1024**3):.2f} GB")
        except:
            data.append("Memory: Could not retrieve total physical memory.")
        # Disk
        data.append("\nDisks:")
        data.append(self._run_cmd("wmic diskdrive get model,size"))
        return data

    def _audit_linux(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("lscpu | grep 'Model name'"))
        # GPU (Requires lspci, usually in pciutils)
        data.append("GPU: " + self._run_cmd("lspci | grep -i vga"))
        # RAM
        data.append("Memory:\n" + self._run_cmd("free -h"))
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("lsblk -o NAME,SIZE,MODEL"))
        return data

    def _audit_mac(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("sysctl -n machdep.cpu.brand_string"))
        # GPU
        data.append("GPU:\n" + self._run_cmd("system_profiler SPDisplaysDataType | grep -E 'Chipset Model|VRAM'"))
        # RAM
        data.append("Memory Details:\n" + self._run_cmd("system_profiler SPMemoryDataType | grep -E 'Size|Type|Speed'"))
        # RAM Total
        try:
            mem_bytes = int(self._run_cmd('sysctl -n hw.memsize'))
            data.append(f"Total Memory: {mem_bytes / (1024**3):.2f} GB")
        except: 
            pass
        # Disk
data.append("\nDisks:\n" + self._run_cmd("diskutil list physical"))
        return data

# --- Independent Test Block ---
if __name__ == "__main__":
    inspector = SysInspectorMS()
    print("Service ready:", inspector)
    print("Running System Inspector...")
    print("\n" + inspector.generate_report())

--------------------------------------------------------------------------------
FILE: _SysInspectorMS\app.py.bak
--------------------------------------------------------------------------------
import platform
import subprocess
import sys
import datetime
from typing import Any, Dict, List, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="SysInspector",
version="1.0.0",
description="Gathers hardware and environment statistics via shell commands.",
tags=["system", "audit", "hardware"],
capabilities=["os:shell", "compute"]
)
class SysInspectorMS:
    """
The Auditor: Gathers hardware and environment statistics.
Supports: Windows (WMIC), Linux (lscpu/lspci), and macOS (sysctl/system_profiler).
"""

def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={},
outputs={"report": "str"},
description="Runs the full audit and returns a formatted string report.",
tags=["system", "report"],
side_effects=["os:read"]
)
def generate_report(self) -> str:
        """
        Runs the full audit and returns a formatted string report.
        """
        system_os = platform.system()
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = [
            f"System Audit Report",
            f"Generated: {timestamp}",
            f"OS: {system_os} {platform.release()} ({platform.machine()})",
            "-" * 40,
            ""
        ]

        # 1. Hardware Section
        report.append("--- Hardware Information ---")
        if system_os == "Windows":
            report.extend(self._audit_windows())
        elif system_os == "Linux":
            report.extend(self._audit_linux())
        elif system_os == "Darwin":
            report.extend(self._audit_mac())
        else:
            report.append("Unsupported Operating System for detailed hardware audit.")

        # 2. Software Section
        report.append("\n--- Software Environment ---")
        report.append(f"Python Version: {platform.python_version()}")
        report.append(f"Python Executable: {sys.executable}")
        
        return "\n".join(report)

    def _run_cmd(self, cmd: str) -> str:
        """Helper to run shell commands safely."""
        try:
            # shell=True is often required for piped commands, specifically on Windows/Linux
            result = subprocess.run(
                cmd, 
                text=True, 
                capture_output=True, 
                check=False, 
                shell=True, 
                timeout=5
            )
            if result.returncode == 0 and result.stdout:
                return result.stdout.strip()
            elif result.stderr:
                return f"[Cmd Error]: {result.stderr.strip()}"
            return "[No Output]"
        except Exception as e:
            return f"[Execution Error]: {e}"

    # --- OS Specific Implementations ---

    def _audit_windows(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("wmic cpu get name"))
        # GPU
        data.append("GPU: " + self._run_cmd("wmic path win32_videocontroller get name"))
        # RAM
        try:
            mem_str = self._run_cmd("wmic computersystem get totalphysicalmemory").splitlines()[-1]
            mem_bytes = int(mem_str)
            data.append(f"Memory: {mem_bytes / (1024**3):.2f} GB")
        except:
            data.append("Memory: Could not retrieve total physical memory.")
        # Disk
        data.append("\nDisks:")
        data.append(self._run_cmd("wmic diskdrive get model,size"))
        return data

    def _audit_linux(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("lscpu | grep 'Model name'"))
        # GPU (Requires lspci, usually in pciutils)
        data.append("GPU: " + self._run_cmd("lspci | grep -i vga"))
        # RAM
        data.append("Memory:\n" + self._run_cmd("free -h"))
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("lsblk -o NAME,SIZE,MODEL"))
        return data

    def _audit_mac(self) -> list[str]:
        data = []
        # CPU
        data.append("CPU: " + self._run_cmd("sysctl -n machdep.cpu.brand_string"))
        # GPU
        data.append("GPU:\n" + self._run_cmd("system_profiler SPDisplaysDataType | grep -E 'Chipset Model|VRAM'"))
        # RAM
        data.append("Memory Details:\n" + self._run_cmd("system_profiler SPMemoryDataType | grep -E 'Size|Type|Speed'"))
        # RAM Total
        try:
            mem_bytes = int(self._run_cmd('sysctl -n hw.memsize'))
            data.append(f"Total Memory: {mem_bytes / (1024**3):.2f} GB")
        except: 
            pass
        # Disk
        data.append("\nDisks:\n" + self._run_cmd("diskutil list physical"))
        return data

# --- Independent Test Block ---
if __name__ == "__main__":
inspector = SysInspectorMS()
print("Service ready:", inspector)
print("Running System Inspector...")
    print("\n" + inspector.generate_report())

--------------------------------------------------------------------------------
FILE: _TasklistVaultMS\app.py
--------------------------------------------------------------------------------
import sqlite3
import uuid
import logging
import datetime
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "task_vault.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("TaskVault")

TaskStatus = Literal["Pending", "Running", "Complete", "Error", "Awaiting-Approval"]
# ==============================================================================

@service_metadata(
name="TaskVault",
version="1.0.0",
description="Persistent SQLite engine for hierarchical task management.",
tags=["tasks", "db", "project-management"],
capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class TaskVaultMS:
    """
The Taskmaster: A persistent SQLite engine for hierarchical task management.
Supports infinite nesting of sub-tasks and status tracking.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", DB_PATH)
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. Task Lists (The containers)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_lists (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP
                )
            """)
            # 2. Tasks (The items, supporting hierarchy via parent_id)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id TEXT PRIMARY KEY,
                    list_id TEXT NOT NULL,
                    parent_id TEXT,
                    content TEXT NOT NULL,
                    status TEXT DEFAULT 'Pending',
                    result TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    FOREIGN KEY(list_id) REFERENCES task_lists(id) ON DELETE CASCADE,
                    FOREIGN KEY(parent_id) REFERENCES tasks(id) ON DELETE CASCADE
                )
            """)

    # --- List Management ---

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"list_id": "str"},
    description="Creates a new task list and returns its ID.",
    tags=["tasks", "create"],
    side_effects=["db:write"]
    )
    def create_list(self, name: str) -> str:
    """Creates a new task list and returns its ID."""
        list_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO task_lists (id, name, created_at) VALUES (?, ?, ?)",
                (list_id, name, now)
            )
        log.info(f"Created Task List: '{name}' ({list_id})")
        return list_id

    @service_endpoint(
    inputs={},
    outputs={"lists": "List[Dict]"},
    description="Returns metadata for all task lists.",
    tags=["tasks", "read"],
    side_effects=["db:read"]
    )
    def get_lists(self) -> List[Dict]:
    """Returns metadata for all task lists."""
        with self._get_conn() as conn:
            rows = conn.execute("SELECT * FROM task_lists ORDER BY created_at DESC").fetchall()
            return [dict(r) for r in rows]

    # --- Task Management ---

    @service_endpoint(
    inputs={"list_id": "str", "content": "str", "parent_id": "Optional[str]"},
    outputs={"task_id": "str"},
    description="Adds a task (or sub-task) to a list.",
    tags=["tasks", "write"],
    side_effects=["db:write"]
    )
    def add_task(self, list_id: str, content: str, parent_id: Optional[str] = None) -> str:
    """Adds a task (or sub-task) to a list."""
        task_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                """INSERT INTO tasks (id, list_id, parent_id, content, status, created_at, updated_at) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (task_id, list_id, parent_id, content, "Pending", now, now)
            )
        return task_id

    @service_endpoint(
    inputs={"task_id": "str", "content": "str", "status": "str", "result": "str"},
    outputs={},
    description="Updates a task's details.",
    tags=["tasks", "update"],
    side_effects=["db:write"]
    )
    def update_task(self, task_id: str, content: str = None, status: TaskStatus = None, result: str = None):
    """Updates a task's details."""
        updates = []
        params = []
        
        if content:
            updates.append("content = ?")
            params.append(content)
        if status:
            updates.append("status = ?")
            params.append(status)
        if result:
            updates.append("result = ?")
            params.append(result)
            
        if not updates: return

        updates.append("updated_at = ?")
        params.append(datetime.datetime.utcnow())
        params.append(task_id)

        sql = f"UPDATE tasks SET {', '.join(updates)} WHERE id = ?"
        
        with self._get_conn() as conn:
            conn.execute(sql, params)
        log.info(f"Updated task {task_id}")

    # --- Tree Reconstruction ---

    @service_endpoint(
    inputs={"list_id": "str"},
    outputs={"tree": "Dict[str, Any]"},
    description="Fetches a list and reconstructs the full hierarchy of tasks.",
    tags=["tasks", "read"],
    side_effects=["db:read"]
    )
    def get_full_tree(self, list_id: str) -> Dict[str, Any]:
    """
    Fetches a list and reconstructs the full hierarchy of tasks.
    """
        with self._get_conn() as conn:
# 1. Get List Info
            list_row = conn.execute("SELECT * FROM task_lists WHERE id = ?", (list_id,)).fetchone()
            if not list_row: return None
            
            # 2. Get All Tasks
            task_rows = conn.execute("SELECT * FROM tasks WHERE list_id = ?", (list_id,)).fetchall()
            
        # 3. Build Adjacency Map
        tasks_by_id = {}
        for r in task_rows:
            t = dict(r)
            t['sub_tasks'] = [] # Prepare children container
            tasks_by_id[t['id']] = t

        # 4. Link Parents and Children
        root_tasks = []
        for t_id, task in tasks_by_id.items():
            parent_id = task['parent_id']
            if parent_id and parent_id in tasks_by_id:
                tasks_by_id[parent_id]['sub_tasks'].append(task)
            else:
                root_tasks.append(task)

        return {
            "id": list_row['id'],
            "name": list_row['name'],
            "tasks": root_tasks
        }

    @service_endpoint(
    inputs={"list_id": "str"},
    outputs={},
    description="Deletes a task list and all its tasks.",
    tags=["tasks", "delete"],
    side_effects=["db:write"]
    )
    def delete_list(self, list_id: str):
        with self._get_conn() as conn:
            conn.execute("DELETE FROM task_lists WHERE id = ?", (list_id,))
        log.info(f"Deleted list {list_id}")
# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    vault = TaskVaultMS()
    print("Service ready:", vault)
    
    # 1. Create a Plan
    plan_id = vault.create_list("System Upgrade Plan")
    
    # 2. Add Root Tasks
    t1 = vault.add_task(plan_id, "Backup Database")
    t2 = vault.add_task(plan_id, "Update Server")
    
    # 3. Add Sub-Tasks
    t2_1 = vault.add_task(plan_id, "Stop Services", parent_id=t2)
    t2_2 = vault.add_task(plan_id, "Run Installer", parent_id=t2)
    
    # 4. Update Status
    vault.update_task(t1, status="Complete", result="Backup saved to /tmp/bk.tar")
    vault.update_task(t2_1, status="Running")
    
    # 5. Render Tree
    tree = vault.get_full_tree(plan_id)
    print(f"\n--- {tree['name']} ---")
    
    def print_node(node, indent=0):
        status_icon = "✓" if node['status'] == 'Complete' else "○"
        print(f"{'  '*indent}{status_icon} {node['content']} [{node['status']}]")
        for child in node['sub_tasks']:
            print_node(child, indent + 1)

    for task in tree['tasks']:
        print_node(task)
        
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)

--------------------------------------------------------------------------------
FILE: _TasklistVaultMS\app.py.bak
--------------------------------------------------------------------------------
import sqlite3
import uuid
import logging
import datetime
import json
from pathlib import Path
from typing import List, Optional, Dict, Any, Literal
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
DB_PATH = Path(__file__).parent / "task_vault.db"
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("TaskVault")

TaskStatus = Literal["Pending", "Running", "Complete", "Error", "Awaiting-Approval"]
# ==============================================================================

@service_metadata(
name="TaskVault",
version="1.0.0",
description="Persistent SQLite engine for hierarchical task management.",
tags=["tasks", "db", "project-management"],
capabilities=["db:sqlite", "filesystem:read", "filesystem:write"]
)
class TaskVaultMS:
    """
The Taskmaster: A persistent SQLite engine for hierarchical task management.
Supports infinite nesting of sub-tasks and status tracking.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.db_path = self.config.get("db_path", DB_PATH)
self._init_db()

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _init_db(self):
        with self._get_conn() as conn:
            # 1. Task Lists (The containers)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS task_lists (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    created_at TIMESTAMP
                )
            """)
            # 2. Tasks (The items, supporting hierarchy via parent_id)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS tasks (
                    id TEXT PRIMARY KEY,
                    list_id TEXT NOT NULL,
                    parent_id TEXT,
                    content TEXT NOT NULL,
                    status TEXT DEFAULT 'Pending',
                    result TEXT,
                    created_at TIMESTAMP,
                    updated_at TIMESTAMP,
                    FOREIGN KEY(list_id) REFERENCES task_lists(id) ON DELETE CASCADE,
                    FOREIGN KEY(parent_id) REFERENCES tasks(id) ON DELETE CASCADE
                )
            """)

    # --- List Management ---

    @service_endpoint(
    inputs={"name": "str"},
    outputs={"list_id": "str"},
    description="Creates a new task list and returns its ID.",
    tags=["tasks", "create"],
    side_effects=["db:write"]
    )
    def create_list(self, name: str) -> str:
    """Creates a new task list and returns its ID."""
        list_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                "INSERT INTO task_lists (id, name, created_at) VALUES (?, ?, ?)",
                (list_id, name, now)
            )
        log.info(f"Created Task List: '{name}' ({list_id})")
        return list_id

    @service_endpoint(
    inputs={},
    outputs={"lists": "List[Dict]"},
    description="Returns metadata for all task lists.",
    tags=["tasks", "read"],
    side_effects=["db:read"]
    )
    def get_lists(self) -> List[Dict]:
    """Returns metadata for all task lists."""
        with self._get_conn() as conn:
            rows = conn.execute("SELECT * FROM task_lists ORDER BY created_at DESC").fetchall()
            return [dict(r) for r in rows]

    # --- Task Management ---

    @service_endpoint(
    inputs={"list_id": "str", "content": "str", "parent_id": "Optional[str]"},
    outputs={"task_id": "str"},
    description="Adds a task (or sub-task) to a list.",
    tags=["tasks", "write"],
    side_effects=["db:write"]
    )
    def add_task(self, list_id: str, content: str, parent_id: Optional[str] = None) -> str:
    """Adds a task (or sub-task) to a list."""
        task_id = str(uuid.uuid4())
        now = datetime.datetime.utcnow()
        with self._get_conn() as conn:
            conn.execute(
                """INSERT INTO tasks (id, list_id, parent_id, content, status, created_at, updated_at) 
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (task_id, list_id, parent_id, content, "Pending", now, now)
            )
        return task_id

    @service_endpoint(
    inputs={"task_id": "str", "content": "str", "status": "str", "result": "str"},
    outputs={},
    description="Updates a task's details.",
    tags=["tasks", "update"],
    side_effects=["db:write"]
    )
    def update_task(self, task_id: str, content: str = None, status: TaskStatus = None, result: str = None):
    """Updates a task's details."""
        updates = []
        params = []
        
        if content:
            updates.append("content = ?")
            params.append(content)
        if status:
            updates.append("status = ?")
            params.append(status)
        if result:
            updates.append("result = ?")
            params.append(result)
            
        if not updates: return

        updates.append("updated_at = ?")
        params.append(datetime.datetime.utcnow())
        params.append(task_id)

        sql = f"UPDATE tasks SET {', '.join(updates)} WHERE id = ?"
        
        with self._get_conn() as conn:
            conn.execute(sql, params)
        log.info(f"Updated task {task_id}")

    # --- Tree Reconstruction ---

    @service_endpoint(
    inputs={"list_id": "str"},
    outputs={"tree": "Dict[str, Any]"},
    description="Fetches a list and reconstructs the full hierarchy of tasks.",
    tags=["tasks", "read"],
    side_effects=["db:read"]
    )
    def get_full_tree(self, list_id: str) -> Dict[str, Any]:
    """
    Fetches a list and reconstructs the full hierarchy of tasks.
    """
        with self._get_conn() as conn:
            # 1. Get List Info
            list_row = conn.execute("SELECT * FROM task_lists WHERE id = ?", (list_id,)).fetchone()
            if not list_row: return None
            
            # 2. Get All Tasks
            task_rows = conn.execute("SELECT * FROM tasks WHERE list_id = ?", (list_id,)).fetchall()
            
        # 3. Build Adjacency Map
        tasks_by_id = {}
        for r in task_rows:
            t = dict(r)
            t['sub_tasks'] = [] # Prepare children container
            tasks_by_id[t['id']] = t

        # 4. Link Parents and Children
        root_tasks = []
        for t_id, task in tasks_by_id.items():
            parent_id = task['parent_id']
            if parent_id and parent_id in tasks_by_id:
                tasks_by_id[parent_id]['sub_tasks'].append(task)
            else:
                root_tasks.append(task)

        return {
            "id": list_row['id'],
            "name": list_row['name'],
            "tasks": root_tasks
        }

    @service_endpoint(
    inputs={"list_id": "str"},
    outputs={},
    description="Deletes a task list and all its tasks.",
    tags=["tasks", "delete"],
    side_effects=["db:write"]
    )
    def delete_list(self, list_id: str):
    with self._get_conn() as conn:
            conn.execute("DELETE FROM task_lists WHERE id = ?", (list_id,))
        log.info(f"Deleted list {list_id}")

# --- Independent Test Block ---
if __name__ == "__main__":
    import os
    if DB_PATH.exists(): os.remove(DB_PATH)
    
    vault = TaskVaultMS()
    print("Service ready:", vault)
    
    # 1. Create a Plan
    plan_id = vault.create_list("System Upgrade Plan")
    
    # 2. Add Root Tasks
    t1 = vault.add_task(plan_id, "Backup Database")
    t2 = vault.add_task(plan_id, "Update Server")
    
    # 3. Add Sub-Tasks
    t2_1 = vault.add_task(plan_id, "Stop Services", parent_id=t2)
    t2_2 = vault.add_task(plan_id, "Run Installer", parent_id=t2)
    
    # 4. Update Status
    vault.update_task(t1, status="Complete", result="Backup saved to /tmp/bk.tar")
    vault.update_task(t2_1, status="Running")
    
    # 5. Render Tree
    tree = vault.get_full_tree(plan_id)
    print(f"\n--- {tree['name']} ---")
    
    def print_node(node, indent=0):
        status_icon = "✓" if node['status'] == 'Complete' else "○"
        print(f"{'  '*indent}{status_icon} {node['content']} [{node['status']}]")
        for child in node['sub_tasks']:
            print_node(child, indent + 1)

    for task in tree['tasks']:
        print_node(task)
        
    # Cleanup
    if DB_PATH.exists(): os.remove(DB_PATH)


--------------------------------------------------------------------------------
FILE: _TextChunkerMS\app.py
--------------------------------------------------------------------------------
from typing import Any, Dict, List, Optional, Tuple
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="TextChunker",
version="1.0.0",
description="Splits text into chunks using various strategies (chars, lines).",
tags=["chunking", "nlp", "rag"],
capabilities=["compute"]
)
class TextChunkerMS:
    """
The Butcher: A unified service for splitting text into digestible chunks
for RAG (Retrieval Augmented Generation).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@staticmethod
@service_endpoint(
inputs={"text": "str", "chunk_size": "int", "chunk_overlap": "int"},
outputs={"chunks": "List[str]"},
description="Standard sliding window split by character count.",
tags=["chunking", "chars"],
side_effects=[]
)
    def chunk_by_chars(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """
        Standard Sliding Window. Best for prose/documentation.
        Splits purely by character count.
        """
        if chunk_size <= 0: raise ValueError("chunk_size must be positive")
        
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            # Advance start, backing up by overlap
            start += chunk_size - chunk_overlap
            
        return chunks

    @staticmethod
    @service_endpoint(
    inputs={"text": "str", "max_lines": "int", "max_chars": "int"},
    outputs={"chunks": "List[Dict]"},
    description="Line-preserving chunker, best for code.",
    tags=["chunking", "lines", "code"],
    side_effects=[]
    )
    def chunk_by_lines(text: str, max_lines: int = 200, max_chars: int = 4000) -> List[Dict[str, Any]]:
    """
    Line-Preserving Chunker. Best for Code.
    Respects line boundaries and returns metadata about line numbers.
    """
        lines = text.splitlines()
        chunks = []
        start = 0
        
        while start < len(lines):
            end = min(start + max_lines, len(lines))
            chunk_str = "\n".join(lines[start:end])
            
            # If too big, shrink window (back off)
            while len(chunk_str) > max_chars and end > start + 1:
                end -= 1
                chunk_str = "\n".join(lines[start:end])
            
            chunks.append({
                "text": chunk_str,
                "start_line": start + 1,
                "end_line": end
            })
            start = end
            
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = TextChunkerMS()
print("Service ready:", chunker)
    
    # 1. Prose Test
    print("--- Prose Chunking ---")
    lorem = "A" * 100 # 100 chars
    result = chunker.chunk_by_chars(lorem, chunk_size=40, chunk_overlap=10)
for i, c in enumerate(result):
        print(f"Chunk {i}: len={len(c)}")

    # 2. Code Test
    print("\n--- Code Chunking ---")
    code = "\n".join([f"print('Line {i}')" for i in range(1, 10)])
    # Force splits small for testing
    result_code = chunker.chunk_by_lines(code, max_lines=3, max_chars=100)
    for i, c in enumerate(result_code):
        print(f"Chunk {i}: Lines {c['start_line']}-{c['end_line']}")

--------------------------------------------------------------------------------
FILE: _TextChunkerMS\app.py.bak
--------------------------------------------------------------------------------
from typing import Any, Dict, List, Optional, Tuple
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="TextChunker",
version="1.0.0",
description="Splits text into chunks using various strategies (chars, lines).",
tags=["chunking", "nlp", "rag"],
capabilities=["compute"]
)
class TextChunkerMS:
    """
The Butcher: A unified service for splitting text into digestible chunks
for RAG (Retrieval Augmented Generation).
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@staticmethod
@service_endpoint(
inputs={"text": "str", "chunk_size": "int", "chunk_overlap": "int"},
outputs={"chunks": "List[str]"},
description="Standard sliding window split by character count.",
tags=["chunking", "chars"],
side_effects=[]
)
    def chunk_by_chars(text: str, chunk_size: int = 500, chunk_overlap: int = 50) -> List[str]:
        """
        Standard Sliding Window. Best for prose/documentation.
        Splits purely by character count.
        """
        if chunk_size <= 0: raise ValueError("chunk_size must be positive")
        
        chunks = []
        start = 0
        text_length = len(text)

        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            # Advance start, backing up by overlap
            start += chunk_size - chunk_overlap
            
        return chunks

    @staticmethod
    @service_endpoint(
    inputs={"text": "str", "max_lines": "int", "max_chars": "int"},
    outputs={"chunks": "List[Dict]"},
    description="Line-preserving chunker, best for code.",
    tags=["chunking", "lines", "code"],
    side_effects=[]
    )
    def chunk_by_lines(text: str, max_lines: int = 200, max_chars: int = 4000) -> List[Dict[str, Any]]:
    """
    Line-Preserving Chunker. Best for Code.
    Respects line boundaries and returns metadata about line numbers.
    """
        lines = text.splitlines()
        chunks = []
        start = 0
        
        while start < len(lines):
            end = min(start + max_lines, len(lines))
            chunk_str = "\n".join(lines[start:end])
            
            # If too big, shrink window (back off)
            while len(chunk_str) > max_chars and end > start + 1:
                end -= 1
                chunk_str = "\n".join(lines[start:end])
            
            chunks.append({
                "text": chunk_str,
                "start_line": start + 1,
                "end_line": end
            })
            start = end
            
        return chunks

# --- Independent Test Block ---
if __name__ == "__main__":
chunker = TextChunkerMS()
print("Service ready:", chunker)
    
    # 1. Prose Test
    print("--- Prose Chunking ---")
    lorem = "A" * 100 # 100 chars
    result = chunker.chunk_by_chars(lorem, chunk_size=40, chunk_overlap=10)
    for i, c in enumerate(result):
        print(f"Chunk {i}: len={len(c)}")

    # 2. Code Test
    print("\n--- Code Chunking ---")
    code = "\n".join([f"print('Line {i}')" for i in range(1, 10)])
    # Force splits small for testing
    result_code = chunker.chunk_by_lines(code, max_lines=3, max_chars=100)
    for i, c in enumerate(result_code):
        print(f"Chunk {i}: Lines {c['start_line']}-{c['end_line']}")

--------------------------------------------------------------------------------
FILE: _ThoughtStreamMS\app.py
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import datetime
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="ThoughtStream",
version="1.0.0",
description="A UI widget for displaying a stream of AI thoughts/logs.",
tags=["ui", "stream", "logs", "widget"],
capabilities=["ui:gui"]
)
class ThoughtStreamMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
        
        # Header
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        # The Stream Area (Canvas allows for custom drawing like sparklines)
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340) # Fixed width like React
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    @service_endpoint(
    inputs={"filename": "str", "chunk_id": "int", "content": "str", "vector_preview": "List[float]", "color": "str"},
    outputs={},
    description="Adds a new thought bubble to the visual stream.",
    tags=["ui", "update"],
    side_effects=["ui:update"]
    )
    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
    """
    Mimics the 'InspectorFrame' from your React code.
    """
# Bubble Container
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        # Header: File + Timestamp
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        header_lbl = tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", 
                              fg="#007ACC", bg="#1a1a25", font=("Consolas", 8))
        header_lbl.pack(anchor="w", padx=5, pady=2)
        
        # Content Snippet
        snippet = content[:400] + "..." if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", 
                               font=("Consolas", 8), justify="left", wraplength=300)
        content_lbl.pack(fill="x", padx=5, pady=2)
        
        # Vector Sparkline (The Custom Draw)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        
        bar_w = w / len(vector) if len(vector) > 0 else 0
        
        for i, val in enumerate(vector):
            # Normalize -1..1 to 0..1 for height
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            
            # Draw bar
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")

# --- Usage Example ---
if __name__ == "__main__":
    root = tk.Tk()
    root.geometry("400x600")
    
    stream = ThoughtStreamMS({"parent": root})
    print("Service ready:", stream)
    stream.pack(fill="both", expand=True)
    
    # Simulate an incoming "Microservice" event
    import random
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("ExplorerView.tsx", 1, "import React from 'react'...", fake_vector, "#FF00FF")
    
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _ThoughtStreamMS\app.py.bak
--------------------------------------------------------------------------------
import tkinter as tk
from tkinter import ttk
import datetime
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

@service_metadata(
name="ThoughtStream",
version="1.0.0",
description="A UI widget for displaying a stream of AI thoughts/logs.",
tags=["ui", "stream", "logs", "widget"],
capabilities=["ui:gui"]
)
class ThoughtStreamMS(ttk.Frame):
    def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
        
        # Header
        self.header = ttk.Label(self, text="NEURAL INSPECTOR", font=("Consolas", 10, "bold"))
        self.header.pack(fill="x", padx=5, pady=5)
        
        # The Stream Area (Canvas allows for custom drawing like sparklines)
        self.canvas = tk.Canvas(self, bg="#13131f", highlightthickness=0)
        self.scrollbar = ttk.Scrollbar(self, orient="vertical", command=self.canvas.yview)
        self.scrollable_frame = tk.Frame(self.canvas, bg="#13131f")
        
        self.scrollable_frame.bind(
            "<Configure>",
            lambda e: self.canvas.configure(scrollregion=self.canvas.bbox("all"))
        )
        
        self.canvas.create_window((0, 0), window=self.scrollable_frame, anchor="nw", width=340) # Fixed width like React
        self.canvas.configure(yscrollcommand=self.scrollbar.set)
        
        self.canvas.pack(side="left", fill="both", expand=True)
        self.scrollbar.pack(side="right", fill="y")

    @service_endpoint(
    inputs={"filename": "str", "chunk_id": "int", "content": "str", "vector_preview": "List[float]", "color": "str"},
    outputs={},
    description="Adds a new thought bubble to the visual stream.",
    tags=["ui", "update"],
    side_effects=["ui:update"]
    )
    def add_thought_bubble(self, filename, chunk_id, content, vector_preview, color):
    """
    Mimics the 'InspectorFrame' from your React code.
    """
        # Bubble Container
        bubble = tk.Frame(self.scrollable_frame, bg="#1a1a25", highlightbackground="#444", highlightthickness=1)
        bubble.pack(fill="x", padx=5, pady=5)
        
        # Header: File + Timestamp
        ts = datetime.datetime.now().strftime("%H:%M:%S")
        header_lbl = tk.Label(bubble, text=f"{filename} #{chunk_id} [{ts}]", 
                              fg="#007ACC", bg="#1a1a25", font=("Consolas", 8))
        header_lbl.pack(anchor="w", padx=5, pady=2)
        
        # Content Snippet
        snippet = content[:400] + "..." if len(content) > 400 else content
        content_lbl = tk.Label(bubble, text=snippet, fg="#ccc", bg="#10101a", 
                               font=("Consolas", 8), justify="left", wraplength=300)
        content_lbl.pack(fill="x", padx=5, pady=2)
        
        # Vector Sparkline (The Custom Draw)
        self._draw_sparkline(bubble, vector_preview, color)

    def _draw_sparkline(self, parent, vector, color):
        """
        Recreates the 'vector_preview' visual from React using a micro-canvas.
        """
        h = 30
        w = 300
        cv = tk.Canvas(parent, height=h, width=w, bg="#1a1a25", highlightthickness=0)
        cv.pack(padx=5, pady=2)
        
        bar_w = w / len(vector) if len(vector) > 0 else 0
        
        for i, val in enumerate(vector):
            # Normalize -1..1 to 0..1 for height
            mag = abs(val) 
            bar_h = mag * h
            x0 = i * bar_w
            y0 = h - bar_h
            x1 = x0 + bar_w
            y1 = h
            
            # Draw bar
            cv.create_rectangle(x0, y0, x1, y1, fill=color, outline="")

# --- Usage Example ---
if __name__ == "__main__":
    root = tk.Tk()
    root.geometry("400x600")
    
    stream = ThoughtStreamMS({"parent": root})
    print("Service ready:", stream)
    stream.pack(fill="both", expand=True)
    
    # Simulate an incoming "Microservice" event
    import random
    fake_vector = [random.uniform(-1, 1) for _ in range(20)]
    stream.add_thought_bubble("ExplorerView.tsx", 1, "import React from 'react'...", fake_vector, "#FF00FF")
    
    root.mainloop()

--------------------------------------------------------------------------------
FILE: _TkinterUniButtonMS\README.md
--------------------------------------------------------------------------------
TO USE:
# In your main app file
from components import UnifiedButtonGroup # assuming you saved the class there

def my_validation_logic():
    # do pandas stuff, etc
    pass

def my_apply_logic():
    # do database stuff
    pass

# Drop the button group into your GUI
my_buttons = UnifiedButtonGroup(
    parent=my_frame, 
    on_validate=my_validation_logic, 
    on_apply=my_apply_logic
)
my_buttons.pack()
--------------------------------------------------------------------------------
FILE: _TkinterUniButtonMS\src\app.py
--------------------------------------------------------------------------------
## Locking Dual Button - Instructions
# TO USE:
# In your main app file
# from components import UnifiedButtonGroup # assuming you saved the class there
# 
# def my_validation_logic():
#     # do pandas stuff, etc
#     pass
# 
# def my_apply_logic():
#    # do database stuff
#     pass
# 
# Drop the button group into your GUI
# my_buttons = UnifiedButtonGroup(
#     parent=my_frame, 
#     on_validate=my_validation_logic, 
#     on_apply=my_apply_logic
# )
# my_buttons.pack()
## 

import tkinter as tk
from dataclasses import dataclass, field
from typing import Any, Dict, Optional
from microservice_std_lib import service_metadata, service_endpoint

@dataclass
class ButtonConfig:
    text: str
    command: callable
    bg_color: str
    active_bg_color: str
    fg_color: str = "#FFFFFF"

@dataclass
class LinkConfig:
    """Configuration for the 'Linked' state (The Trap)"""
    trap_bg: str = "#7C3AED"    # Deep Purple
    btn_bg: str = "#8B5CF6"     # Lighter Purple
    text_color: str = "#FFFFFF"

@service_metadata(
name="LockingDualBtn",
version="1.0.0",
description="A unified button group (Left/Right/Link) where linking merges the actions.",
tags=["ui", "widget", "button"],
capabilities=["ui:gui"]
)
class LockingDualBtnMS(tk.Frame):
"""
A generic button group that can merge ANY two actions.
Pass the visual/functional definitions in via the config objects.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
parent = self.config.get("parent")
super().__init__(parent)
        
self.left_cfg = self.config.get("left_btn")
self.right_cfg = self.config.get("right_btn")
self.link_cfg = self.config.get("link_config") or LinkConfig()
        
        self.is_linked = False
        self.default_bg = parent.cget("bg") # Fallback to parent background

        self._setup_ui()
        self._update_state()

    def _setup_ui(self):
        self.config(padx=4, pady=4)
        
        common_style = {"relief": "flat", "font": ("Segoe UI", 10, "bold"), "bd": 0, "cursor": "hand2"}

        # 1. Left Button (Generic)
        self.btn_left = tk.Button(self, command=lambda: self._execute("left"), **common_style)
        self.btn_left.pack(side="left", fill="y", padx=(0, 2))

        # 2. Link Toggle (The Chain)
        self.btn_link = tk.Button(self, text="&", width=3, command=self._toggle_link, **common_style)
        self.btn_link.pack(side="left", fill="y", padx=(0, 2))

        # 3. Right Button (Generic)
        self.btn_right = tk.Button(self, command=lambda: self._execute("right"), **common_style)
        self.btn_right.pack(side="left", fill="y")

    def _toggle_link(self):
        self.is_linked = not self.is_linked
        self._update_state()

    def _update_state(self):
        if self.is_linked:
            # --- LINKED STATE (The Trap) ---
            self.config(bg=self.link_cfg.trap_bg)
            
            # Both buttons look identical in the "Trap"
            for btn in (self.btn_left, self.btn_right, self.btn_link):
                btn.config(bg=self.link_cfg.btn_bg, fg=self.link_cfg.text_color, activebackground=self.link_cfg.trap_bg)
            
            # Keep original text
            self.btn_left.config(text=self.left_cfg.text)
            self.btn_right.config(text=self.right_cfg.text)

        else:
            # --- INDEPENDENT STATE ---
            try: self.config(bg=self.default_bg)
            except: self.config(bg="#f0f0f0") 

            # Restore Left Button
            self.btn_left.config(
                text=self.left_cfg.text, 
                bg=self.left_cfg.bg_color, 
                fg=self.left_cfg.fg_color,
                activebackground=self.left_cfg.active_bg_color
            )

            # Restore Right Button
            self.btn_right.config(
                text=self.right_cfg.text, 
                bg=self.right_cfg.bg_color, 
                fg=self.right_cfg.fg_color,
                activebackground=self.right_cfg.active_bg_color
            )

            # Restore Link Button (Neutral Gray)
            self.btn_link.config(bg="#E5E7EB", fg="#374151", activebackground="#D1D5DB")

    def _execute(self, source):
        if self.is_linked:
            # Chain them: Left then Right
            self.left_cfg.command()
            self.right_cfg.command()
        else:
        if source == "left": self.left_cfg.command()
        elif source == "right": self.right_cfg.command()

        if __name__ == "__main__":
        root = tk.Tk()
        btn1 = ButtonConfig("Save", lambda: print("Save"), "#444", "#555")
        btn2 = ButtonConfig("Run", lambda: print("Run"), "#444", "#555")
        svc = LockingDualBtnMS({"parent": root, "left_btn": btn1, "right_btn": btn2})
        print("Service ready:", svc)
        svc.pack(pady=20)
        root.mainloop()

--------------------------------------------------------------------------------
FILE: _TreeMapperMS\app.py
--------------------------------------------------------------------------------
import os
from pathlib import Path
from typing import Any, Dict, List, Set, Optional
from microservice_std_lib import service_metadata, service_endpoint
import datetime

# ==============================================================================
# USER CONFIGURATION: DEFAULT EXCLUSIONS
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    '.venv', 'env', 'venv', 'dist', 'build', '.DS_Store'
}
# ==============================================================================

@service_metadata(
name="TreeMapper",
version="1.0.0",
description="Generates ASCII-art style directory maps of the file system.",
tags=["filesystem", "map", "visualization"],
capabilities=["filesystem:read"]
)
class TreeMapperMS:
    """
The Cartographer: Generates ASCII-art style directory maps.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"root_path": "str", "additional_exclusions": "Set[str]", "use_default_exclusions": "bool"},
outputs={"tree_map": "str"},
description="Generates an ASCII tree map of the directory.",
tags=["filesystem", "visualization"],
side_effects=["filesystem:read"]
)
def generate_tree(self, 
                      root_path: str, 
                      additional_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> str:
        
        start_path = Path(root_path).resolve()
        if not start_path.exists(): return f"Error: Path '{root_path}' does not exist."

        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_EXCLUDES)
        if additional_exclusions:
            exclusions.update(additional_exclusions)

        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lines = [
            f"Project Map: {start_path.name}",
            f"Generated: {timestamp}",
            "-" * 40,
            f"📁 {start_path.name}/"
        ]

        self._walk(start_path, "", lines, exclusions)
        return "\n".join(lines)

    def _walk(self, directory: Path, prefix: str, lines: List[str], exclusions: Set[str]):
        try:
            children = sorted(
                [p for p in directory.iterdir() if p.name not in exclusions],
                key=lambda x: (x.is_file(), x.name.lower())
            )
        except PermissionError:
            lines.append(f"{prefix}└── 🚫 [Permission Denied]")
            return

        count = len(children)
        for index, path in enumerate(children):
            is_last = (index == count - 1)
            connector = "└── " if is_last else "├── "
            
            if path.is_dir():
                lines.append(f"{prefix}{connector}📁 {path.name}/")
                extension = "    " if is_last else "│   "
                self._walk(path, prefix + extension, lines, exclusions)
            else:
                lines.append(f"{prefix}{connector}📄 {path.name}")

if __name__ == "__main__":
svc = TreeMapperMS()
print("Service ready:", svc)

--------------------------------------------------------------------------------
FILE: _TreeMapperMS\app.py.bak
--------------------------------------------------------------------------------
import os
from pathlib import Path
from typing import Any, Dict, List, Set, Optional
from microservice_std_lib import service_metadata, service_endpoint
import datetime

# ==============================================================================
# USER CONFIGURATION: DEFAULT EXCLUSIONS
# ==============================================================================
DEFAULT_EXCLUDES = {
    '.git', '__pycache__', '.idea', '.vscode', 'node_modules', 
    '.venv', 'env', 'venv', 'dist', 'build', '.DS_Store'
}
# ==============================================================================

@service_metadata(
name="TreeMapper",
version="1.0.0",
description="Generates ASCII-art style directory maps of the file system.",
tags=["filesystem", "map", "visualization"],
capabilities=["filesystem:read"]
)
class TreeMapperMS:
    """
The Cartographer: Generates ASCII-art style directory maps.
"""
    
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"root_path": "str", "additional_exclusions": "Set[str]", "use_default_exclusions": "bool"},
outputs={"tree_map": "str"},
description="Generates an ASCII tree map of the directory.",
tags=["filesystem", "visualization"],
side_effects=["filesystem:read"]
)
def generate_tree(self, 
                      root_path: str, 
                      additional_exclusions: Optional[Set[str]] = None,
                      use_default_exclusions: bool = True) -> str:
        
        start_path = Path(root_path).resolve()
        if not start_path.exists(): return f"Error: Path '{root_path}' does not exist."

        exclusions = set()
        if use_default_exclusions:
            exclusions.update(DEFAULT_EXCLUDES)
        if additional_exclusions:
            exclusions.update(additional_exclusions)

        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        lines = [
            f"Project Map: {start_path.name}",
            f"Generated: {timestamp}",
            "-" * 40,
            f"📁 {start_path.name}/"
        ]

        self._walk(start_path, "", lines, exclusions)
        return "\n".join(lines)

    def _walk(self, directory: Path, prefix: str, lines: List[str], exclusions: Set[str]):
        try:
            children = sorted(
                [p for p in directory.iterdir() if p.name not in exclusions],
                key=lambda x: (x.is_file(), x.name.lower())
            )
        except PermissionError:
            lines.append(f"{prefix}└── 🚫 [Permission Denied]")
            return

        count = len(children)
        for index, path in enumerate(children):
            is_last = (index == count - 1)
            connector = "└── " if is_last else "├── "
            
            if path.is_dir():
                lines.append(f"{prefix}{connector}📁 {path.name}/")
                extension = "    " if is_last else "│   "
                self._walk(path, prefix + extension, lines, exclusions)
            else:
                lines.append(f"{prefix}{connector}📄 {path.name}")

if __name__ == "__main__":
svc = TreeMapperMS()
print("Service ready:", svc)

--------------------------------------------------------------------------------
FILE: _VectorFactoryMS\app.py
--------------------------------------------------------------------------------
import os
import uuid
import logging
import shutil
from typing import List, Dict, Any, Optional, Protocol, Union
from pathlib import Path
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("VectorFactory")
# ==============================================================================

# --- Interface Definition ---

class VectorStore(Protocol):
    """The contract that all vector backends must fulfill."""
    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]) -> None:
        ...
    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        ...
    def count(self) -> int:
        ...
    def clear(self) -> None:
        ...

# --- Implementation 1: FAISS (Local, Fast, RAM-heavy) ---

class FaissVectorStore:
    def __init__(self, index_path: str, dimension: int):
        import numpy as np
        import faiss # Lazy import
        self.np = np
        self.faiss = faiss
        
        self.index_path = index_path
        self.dim = dimension
        self.metadata_store = []
        
        # Load or Create
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
            # Load metadata (simple JSON sidecar for this implementation)
            meta_path = index_path + ".meta.json"
            if os.path.exists(meta_path):
                import json
                with open(meta_path, 'r') as f:
                    self.metadata_store = json.load(f)
        else:
            self.index = faiss.IndexFlatL2(dimension)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        
        vecs = self.np.array(embeddings).astype("float32")
        self.index.add(vecs)
        self.metadata_store.extend(metadatas)
        self._save()

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        if self.index.ntotal == 0: return []
        
        q_vec = self.np.array([query_vector]).astype("float32")
        distances, indices = self.index.search(q_vec, k)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and idx < len(self.metadata_store):
                entry = self.metadata_store[idx].copy()
                entry['score'] = float(dist) # FAISS returns L2 distance (lower is better)
                results.append(entry)
        return results

    def count(self) -> int:
        return self.index.ntotal

    def clear(self):
        self.index.reset()
        self.metadata_store = []
        self._save()

    def _save(self):
        self.faiss.write_index(self.index, self.index_path)
        import json
        with open(self.index_path + ".meta.json", 'w') as f:
            json.dump(self.metadata_store, f)

# --- Implementation 2: ChromaDB (Persistent, Feature-rich) ---

class ChromaVectorStore:
    def __init__(self, persist_dir: str, collection_name: str):
        import chromadb # Lazy import
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(collection_name)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        # Chroma requires unique IDs
        ids = [str(uuid.uuid4()) for _ in embeddings]
        
        # Ensure metadata is flat (Chroma limitation on nested dicts)
        clean_metas = [{k: str(v) if isinstance(v, (list, dict)) else v for k, v in m.items()} for m in metadatas]
        
        # Chroma expects 'documents' usually, but we handle logic upstream. 
        # We pass empty strings for 'documents' if purely vector-based, 
        # or map content from metadata if available.
        docs = [m.get("content", "") for m in metadatas]

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            metadatas=clean_metas,
            documents=docs
        )

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        results = self.collection.query(
            query_embeddings=[query_vector],
            n_results=k
        )
        
        output = []
        if not results['ids']: return []

        # Unpack Chroma's columnar response format
        for i in range(len(results['ids'][0])):
            entry = results['metadatas'][0][i].copy()
            entry['score'] = results['distances'][0][i]
            entry['id'] = results['ids'][0][i]
            output.append(entry)
        return output

    def count(self) -> int:
        return self.collection.count()

    def clear(self):
        # Chroma doesn't have a truncate command, so we delete the collection
        name = self.collection.name
        self.client.delete_collection(name)
        self.collection = self.client.get_or_create_collection(name)

# --- The Factory ---

@service_metadata(
name="VectorFactory",
version="1.0.0",
description="Factory for creating VectorStore instances (FAISS, Chroma).",
tags=["vector", "factory", "db"],
capabilities=["filesystem:read", "filesystem:write"]
)
class VectorFactoryMS:
    """
The Switchboard: Returns the appropriate VectorStore implementation
based on configuration.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"backend": "str", "config": "Dict"},
outputs={"store": "VectorStore"},
description="Creates and returns a configured VectorStore instance.",
tags=["vector", "create"],
side_effects=[]
)
def create(self, backend: str, config: Dict[str, Any]) -> VectorStore:
"""
:param backend: 'faiss' or 'chroma'
        :param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)
        """
        log.info(f"Initializing Vector Store: {backend.upper()}")
        
        if backend == "faiss":
            path = config.get("path", "vector_index.bin")
            dim = config.get("dim", 384)
            return FaissVectorStore(path, dim)
            
        elif backend == "chroma":
            path = config.get("path", "./chroma_db")
            name = config.get("collection", "default_collection")
            return ChromaVectorStore(path, name)
            
        else:
            raise ValueError(f"Unknown backend: {backend}")

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing VectorFactoryMS ---")
    
    # 1. Mock Data (dim=4 for simplicity)
    mock_vec = [0.1, 0.2, 0.3, 0.4]
    mock_meta = {"text": "Hello World", "source": "test"}
    
    # 2. Test FAISS
    print("\n[Testing FAISS]")
    factory = VectorFactoryMS()
    print("Service ready:", factory)
    try:
    faiss_store = factory.create("faiss", {"path": "test_faiss.index", "dim": 4})
        faiss_store.add([mock_vec], [mock_meta])
        print(f"Count: {faiss_store.count()}")
        res = faiss_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("test_faiss.index"): os.remove("test_faiss.index")
        if os.path.exists("test_faiss.index.meta.json"): os.remove("test_faiss.index.meta.json")
    except ImportError:
        print("Skipping FAISS test (library not installed)")
# 3. Test Chroma
print("\n[Testing Chroma]")
try:
    chroma_store = factory.create("chroma", {"path": "./test_chroma_db", "collection": "test_col"})
    chroma_store.add([mock_vec], [mock_meta])
    print(f"Count: {chroma_store.count()}")
    res = chroma_store.search(mock_vec, 1)
    print(f"Search Result: {res[0]['text']}")
    # Cleanup
    if os.path.exists("./test_chroma_db"): shutil.rmtree("./test_chroma_db")
except ImportError:
    print("Skipping Chroma test (library not installed)")
except Exception as e:
    print(f"Chroma Error: {e}")

--------------------------------------------------------------------------------
FILE: _VectorFactoryMS\app.py.bak
--------------------------------------------------------------------------------
import os
import uuid
import logging
import shutil
from typing import List, Dict, Any, Optional, Protocol, Union
from pathlib import Path
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("VectorFactory")
# ==============================================================================

# --- Interface Definition ---

class VectorStore(Protocol):
    """The contract that all vector backends must fulfill."""
    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]) -> None:
        ...
    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        ...
    def count(self) -> int:
        ...
    def clear(self) -> None:
        ...

# --- Implementation 1: FAISS (Local, Fast, RAM-heavy) ---

class FaissVectorStore:
    def __init__(self, index_path: str, dimension: int):
        import numpy as np
        import faiss # Lazy import
        self.np = np
        self.faiss = faiss
        
        self.index_path = index_path
        self.dim = dimension
        self.metadata_store = []
        
        # Load or Create
        if os.path.exists(index_path):
            self.index = faiss.read_index(index_path)
            # Load metadata (simple JSON sidecar for this implementation)
            meta_path = index_path + ".meta.json"
            if os.path.exists(meta_path):
                import json
                with open(meta_path, 'r') as f:
                    self.metadata_store = json.load(f)
        else:
            self.index = faiss.IndexFlatL2(dimension)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        
        vecs = self.np.array(embeddings).astype("float32")
        self.index.add(vecs)
        self.metadata_store.extend(metadatas)
        self._save()

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        if self.index.ntotal == 0: return []
        
        q_vec = self.np.array([query_vector]).astype("float32")
        distances, indices = self.index.search(q_vec, k)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx != -1 and idx < len(self.metadata_store):
                entry = self.metadata_store[idx].copy()
                entry['score'] = float(dist) # FAISS returns L2 distance (lower is better)
                results.append(entry)
        return results

    def count(self) -> int:
        return self.index.ntotal

    def clear(self):
        self.index.reset()
        self.metadata_store = []
        self._save()

    def _save(self):
        self.faiss.write_index(self.index, self.index_path)
        import json
        with open(self.index_path + ".meta.json", 'w') as f:
            json.dump(self.metadata_store, f)

# --- Implementation 2: ChromaDB (Persistent, Feature-rich) ---

class ChromaVectorStore:
    def __init__(self, persist_dir: str, collection_name: str):
        import chromadb # Lazy import
        self.client = chromadb.PersistentClient(path=persist_dir)
        self.collection = self.client.get_or_create_collection(collection_name)

    def add(self, embeddings: List[List[float]], metadatas: List[Dict[str, Any]]):
        if not embeddings: return
        # Chroma requires unique IDs
        ids = [str(uuid.uuid4()) for _ in embeddings]
        
        # Ensure metadata is flat (Chroma limitation on nested dicts)
        clean_metas = [{k: str(v) if isinstance(v, (list, dict)) else v for k, v in m.items()} for m in metadatas]
        
        # Chroma expects 'documents' usually, but we handle logic upstream. 
        # We pass empty strings for 'documents' if purely vector-based, 
        # or map content from metadata if available.
        docs = [m.get("content", "") for m in metadatas]

        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            metadatas=clean_metas,
            documents=docs
        )

    def search(self, query_vector: List[float], k: int) -> List[Dict[str, Any]]:
        results = self.collection.query(
            query_embeddings=[query_vector],
            n_results=k
        )
        
        output = []
        if not results['ids']: return []

        # Unpack Chroma's columnar response format
        for i in range(len(results['ids'][0])):
            entry = results['metadatas'][0][i].copy()
            entry['score'] = results['distances'][0][i]
            entry['id'] = results['ids'][0][i]
            output.append(entry)
        return output

    def count(self) -> int:
        return self.collection.count()

    def clear(self):
        # Chroma doesn't have a truncate command, so we delete the collection
        name = self.collection.name
        self.client.delete_collection(name)
        self.collection = self.client.get_or_create_collection(name)

# --- The Factory ---

@service_metadata(
name="VectorFactory",
version="1.0.0",
description="Factory for creating VectorStore instances (FAISS, Chroma).",
tags=["vector", "factory", "db"],
capabilities=["filesystem:read", "filesystem:write"]
)
class VectorFactoryMS:
    """
The Switchboard: Returns the appropriate VectorStore implementation
based on configuration.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}

@service_endpoint(
inputs={"backend": "str", "config": "Dict"},
outputs={"store": "VectorStore"},
description="Creates and returns a configured VectorStore instance.",
tags=["vector", "create"],
side_effects=[]
)
def create(self, backend: str, config: Dict[str, Any]) -> VectorStore:
"""
:param backend: 'faiss' or 'chroma'
        :param config: Dict containing 'path', 'dim' (for FAISS), or 'collection' (for Chroma)
        """
        log.info(f"Initializing Vector Store: {backend.upper()}")
        
        if backend == "faiss":
            path = config.get("path", "vector_index.bin")
            dim = config.get("dim", 384)
            return FaissVectorStore(path, dim)
            
        elif backend == "chroma":
            path = config.get("path", "./chroma_db")
            name = config.get("collection", "default_collection")
            return ChromaVectorStore(path, name)
            
        else:
            raise ValueError(f"Unknown backend: {backend}")

# --- Independent Test Block ---
if __name__ == "__main__":
    print("--- Testing VectorFactoryMS ---")
    
    # 1. Mock Data (dim=4 for simplicity)
    mock_vec = [0.1, 0.2, 0.3, 0.4]
    mock_meta = {"text": "Hello World", "source": "test"}
    
    # 2. Test FAISS
    print("\n[Testing FAISS]")
    factory = VectorFactoryMS()
    print("Service ready:", factory)
    try:
    faiss_store = factory.create("faiss", {"path": "test_faiss.index", "dim": 4})
        faiss_store.add([mock_vec], [mock_meta])
        print(f"Count: {faiss_store.count()}")
        res = faiss_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("test_faiss.index"): os.remove("test_faiss.index")
        if os.path.exists("test_faiss.index.meta.json"): os.remove("test_faiss.index.meta.json")
    except ImportError:
        print("Skipping FAISS test (library not installed)")

    # 3. Test Chroma
    print("\n[Testing Chroma]")
    try:
    chroma_store = factory.create("chroma", {"path": "./test_chroma_db", "collection": "test_col"})
        chroma_store.add([mock_vec], [mock_meta])
        print(f"Count: {chroma_store.count()}")
        res = chroma_store.search(mock_vec, 1)
        print(f"Search Result: {res[0]['text']}")
        # Cleanup
        if os.path.exists("./test_chroma_db"): shutil.rmtree("./test_chroma_db")
    except ImportError:
        print("Skipping Chroma test (library not installed)")
    except Exception as e:
        print(f"Chroma Error: {e}")

--------------------------------------------------------------------------------
FILE: _VectorFactoryMS\requirements.txt
--------------------------------------------------------------------------------
pip install chromadb faiss-cpu numpy
--------------------------------------------------------------------------------
FILE: _WebScraperMS\app.py
--------------------------------------------------------------------------------
import httpx
import logging
import asyncio
from typing import Optional, Dict, Any, List
from readability import Document
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
TIMEOUT_SECONDS = 15.0

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("WebScraper")
# ==============================================================================

@service_metadata(
name="WebScraper",
version="1.0.0",
description="Fetches URLs and extracts main content using Readability (stripping ads/nav).",
tags=["scraper", "web", "readability"],
capabilities=["network:outbound", "compute"]
)
class WebScraperMS:
    """
The Reader: Fetches URLs and extracts the main content using Readability.
Strips ads, navbars, and boilerplate to return clean text for LLMs.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.headers = {"User-Agent": USER_AGENT}

    @service_endpoint(
    inputs={"url": "str"},
    outputs={"data": "Dict[str, Any]"},
    description="Fetches and cleans a URL.",
    tags=["scraper", "read"],
    side_effects=["network:outbound"]
    )
    def scrape(self, url: str) -> Dict[str, Any]:
    """
    Synchronous wrapper for fetching and cleaning a URL.
    Returns: {
            "url": str,
            "title": str,
            "content": str (The main body text),
            "html": str (The raw HTML of the main content area)
        }
        """
return asyncio.run(self._scrape_async(url))

    async def _scrape_async(self, url: str) -> Dict[str, Any]:
        log.info(f"Fetching: {url}")
        
        async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=TIMEOUT_SECONDS) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                log.error(f"HTTP Error {e.response.status_code}: {e}")
                raise
            except httpx.RequestError as e:
                log.error(f"Request failed: {e}")
                raise

        # Parse with Readability
        try:
            doc = Document(response.text)
            title = doc.title()
            # Summary() returns the HTML of the main content area
            clean_html = doc.summary() 
            
            # Convert HTML content to plain text for the LLM
            # (Simple strip tags implementation, for better results use BeautifulSoup)
            clean_text = self._strip_tags(clean_html)
            
            log.info(f"Successfully scraped '{title}' ({len(clean_text)} chars)")
            
            return {
                "url": url,
                "title": title,
                "content": clean_text,
                "html": clean_html
            }
        except Exception as e:
            log.error(f"Parsing failed: {e}")
            raise

    def _strip_tags(self, html: str) -> str:
def _strip_tags(self, html: str) -> str:
        """
        Removes HTML tags to leave only the readable text.
        """
        import re
        # Remove scripts and styles
        html = re.sub(r'<(script|style).*?>.*?</\1>', '', html, flags=re.DOTALL)
        # Remove tags
        text = re.sub(r'<[^>]+>', ' ', html)
        # Collapse whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# --- Independent Test Block ---
if __name__ == "__main__":
    scraper = WebScraperMS()
    print("Service ready:", scraper)
    
    # Test URL (Example: Python's PEP 8)
    target_url = "https://peps.python.org/pep-0008/"
    
    print(f"--- Scraping {target_url} ---")
    try:
        data = scraper.scrape(target_url)
        print(f"\nTitle: {data['title']}")
        print(f"Content Preview:\n{data['content'][:500]}...")
        print(f"\nTotal Length: {len(data['content'])} characters")
    except Exception as e:
        print(f"Scrape failed: {e}")

--------------------------------------------------------------------------------
FILE: _WebScraperMS\app.py.bak
--------------------------------------------------------------------------------
import httpx
import logging
import asyncio
from typing import Optional, Dict, Any, List
from readability import Document
from microservice_std_lib import service_metadata, service_endpoint

# ==============================================================================
# CONFIGURATION
# ==============================================================================
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
TIMEOUT_SECONDS = 15.0

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
log = logging.getLogger("WebScraper")
# ==============================================================================

@service_metadata(
name="WebScraper",
version="1.0.0",
description="Fetches URLs and extracts main content using Readability (stripping ads/nav).",
tags=["scraper", "web", "readability"],
capabilities=["network:outbound", "compute"]
)
class WebScraperMS:
    """
The Reader: Fetches URLs and extracts the main content using Readability.
Strips ads, navbars, and boilerplate to return clean text for LLMs.
"""
def __init__(self, config: Optional[Dict[str, Any]] = None):
self.config = config or {}
self.headers = {"User-Agent": USER_AGENT}

    @service_endpoint(
    inputs={"url": "str"},
    outputs={"data": "Dict[str, Any]"},
    description="Fetches and cleans a URL.",
    tags=["scraper", "read"],
    side_effects=["network:outbound"]
    )
    def scrape(self, url: str) -> Dict[str, Any]:
    """
    Synchronous wrapper for fetching and cleaning a URL.
    Returns: {
            "url": str,
            "title": str,
            "content": str (The main body text),
            "html": str (The raw HTML of the main content area)
        }
        """
        return asyncio.run(self._scrape_async(url))

    async def _scrape_async(self, url: str) -> Dict[str, Any]:
        log.info(f"Fetching: {url}")
        
        async with httpx.AsyncClient(headers=self.headers, follow_redirects=True, timeout=TIMEOUT_SECONDS) as client:
            try:
                response = await client.get(url)
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                log.error(f"HTTP Error {e.response.status_code}: {e}")
                raise
            except httpx.RequestError as e:
                log.error(f"Request failed: {e}")
                raise

        # Parse with Readability
        try:
            doc = Document(response.text)
            title = doc.title()
            # Summary() returns the HTML of the main content area
            clean_html = doc.summary() 
            
            # Convert HTML content to plain text for the LLM
            # (Simple strip tags implementation, for better results use BeautifulSoup)
            clean_text = self._strip_tags(clean_html)
            
            log.info(f"Successfully scraped '{title}' ({len(clean_text)} chars)")
            
            return {
                "url": url,
                "title": title,
                "content": clean_text,
                "html": clean_html
            }
        except Exception as e:
            log.error(f"Parsing failed: {e}")
            raise

    def _strip_tags(self, html: str) -> str:
        """
        Removes HTML tags to leave only the readable text.
        Note: A robust production version should use BeautifulSoup4.
        """
        import re
        # Remove scripts and styles
        html = re.sub(r'<(script|style).*?>.*?</\1>', '', html, flags=re.DOTALL)
        # Remove tags
        text = re.sub(r'<[^>]+>', ' ', html)
        # Collapse whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

# --- Independent Test Block ---
if __name__ == "__main__":
scraper = WebScraperMS()
print("Service ready:", scraper)
    
    # Test URL (Example: Python's PEP 8)
    target_url = "https://peps.python.org/pep-0008/"
    
    print(f"--- Scraping {target_url} ---")
    try:
        data = scraper.scrape(target_url)
        print(f"\nTitle: {data['title']}")
        print(f"Content Preview:\n{data['content'][:500]}...")
        print(f"\nTotal Length: {len(data['content'])} characters")
    except Exception as e:
        print(f"Scrape failed: {e}")

--------------------------------------------------------------------------------
FILE: _WebScraperMS\requirements.txt
--------------------------------------------------------------------------------
pip install httpx readability-lxml