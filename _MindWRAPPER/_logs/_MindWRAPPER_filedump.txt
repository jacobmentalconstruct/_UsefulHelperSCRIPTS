Dump: C:\Users\jacob\Documents\_UsefulHelperSCRIPTS\_MindWRAPPER


--------------------------------------------------------------------------------
FILE: LICENSE.md
--------------------------------------------------------------------------------
MIT License

Copyright (c) 2025 Jacob Lambert

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

--------------------------------------------------------------------------------
FILE: README.md
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: requirements.txt
--------------------------------------------------------------------------------
# --- Critical UI/Graphing ---
pygame-ce>=2.3.0
Pillow>=10.0.0

# --- Networking/AI ---
requests>=2.30.0

# --- Database Extensions ---
# (Ensure your Python environment supports installing this, 
# otherwise you may need to manually place the DLL/SO)
sqlite-vec>=0.1.0

# --- Document Processing ---
pypdf>=3.0.0
beautifulsoup4>=4.12.0
--------------------------------------------------------------------------------
FILE: setup_env.bat
--------------------------------------------------------------------------------
@echo off
echo [SYSTEM] Initializing new project environment...

:: 1. Create the venv if it doesn't exist
if not exist .venv (
    echo [SYSTEM] Creating .venv...
    py -m venv .venv
)

:: 2. Upgrade pip and install requirements
echo [SYSTEM] Installing dependencies...
.venv\Scripts\python.exe -m pip install --upgrade pip
if exist requirements.txt (
    .venv\Scripts\pip install -r requirements.txt
)

echo.
echo [SUCCESS] Environment ready!
echo You can now open this folder in VS Code or launch via scripts_menu.py
pause
--------------------------------------------------------------------------------
FILE: src\app.py
--------------------------------------------------------------------------------
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
== _MindWRAPPER Core ==
The Central Nervous System.
Mounts the Microservices (Intake, Refinery, Neural, Cartridge) and 
exposes them via both a GUI (Workbench) and a CLI (Headless).
"""

import sys
import argparse
import sqlite3
import time
import tkinter as tk
from tkinter import ttk, messagebox, scrolledtext
from pathlib import Path
import threading

# --- MICROSERVICE IMPORTS ---
sys.path.append(str(Path(__file__).parent))

from microservices.base_service import BaseService
from microservices.neural_service import NeuralService
from microservices.cartridge_service import CartridgeService
from microservices.intake_service import IntakeService
from microservices.refinery_service import RefineryService

# --- CONFIGURATION ---
DEFAULT_DB_PATH = "./cortex_dbs/mindwrapper.db"

# ==============================================================================
#  THE ENGINE ROOM (Controller)
# ==============================================================================
class MindWrapperController(BaseService):
    """
    The 'Backend' Controller. 
    It owns the services so the GUI doesn't have to manage them directly.
    """
    def __init__(self, db_path: str = DEFAULT_DB_PATH):
        super().__init__("MindController")
        
        self.log_info(f"Mounting Services at {db_path}...")
        
        # 1. Mount Services
        self.neural = NeuralService(max_workers=8) 
        self.cartridge = CartridgeService(db_path)
        self.intake = IntakeService(self.cartridge)
        self.refinery = RefineryService(self.cartridge, self.neural)
        
        self.neural_online = self.neural.check_connection()
        if not self.neural_online:
            self.log_error("Ollama is OFFLINE. AI features disabled.")

class DatabaseView(tk.Frame):
    """
    The Inspector.
    Provides a raw SQL view of the Cartridge internals.
    """
    def __init__(self, parent, controller):
        super().__init__(parent, bg="#1e1e2f")
        self.ctrl = controller
        
        # --- Toolbar ---
        toolbar = tk.Frame(self, bg="#171725", pady=5, padx=5)
        toolbar.pack(fill="x")
        
        tk.Label(toolbar, text="TABLE:", bg="#171725", fg="gray", font=("Arial", 8, "bold")).pack(side="left")
        self.table_var = tk.StringVar()
        self.table_combo = ttk.Combobox(toolbar, textvariable=self.table_var, width=20, state="readonly")
        self.table_combo.pack(side="left", padx=5)
        self.table_combo.bind("<<ComboboxSelected>>", self.refresh_data)

        tk.Label(toolbar, text="FILTER:", bg="#171725", fg="gray", font=("Arial", 8, "bold")).pack(side="left", padx=(15, 5))
        self.search_entry = tk.Entry(toolbar, bg="#2d2d44", fg="white", insertbackground="white")
        self.search_entry.pack(side="left", fill="x", expand=True)
        self.search_entry.bind("<Return>", self.refresh_data)
        
        tk.Button(toolbar, text="↻ REFRESH", bg="#444", fg="white", relief="flat", command=self.refresh_tables).pack(side="right", padx=5)

        # --- Data Grid ---
        self.tree_frame = tk.Frame(self, bg="#1e1e2f")
        self.tree_frame.pack(fill="both", expand=True, padx=5, pady=5)
        
        self.tree = ttk.Treeview(self.tree_frame, show="headings", selectmode="browse")
        vsb = ttk.Scrollbar(self.tree_frame, orient="vertical", command=self.tree.yview)
        hsb = ttk.Scrollbar(self.tree_frame, orient="horizontal", command=self.tree.xview)
        self.tree.configure(yscrollcommand=vsb.set, xscrollcommand=hsb.set)
        
        self.tree.grid(column=0, row=0, sticky='nsew')
        vsb.grid(column=1, row=0, sticky='ns')
        hsb.grid(column=0, row=1, sticky='ew')
        
        self.tree_frame.grid_columnconfigure(0, weight=1)
        self.tree_frame.grid_rowconfigure(0, weight=1)
        
        self.tree.bind("<Double-1>", self.on_double_click)
        
        # Initial Load
        self.refresh_tables()

    def refresh_tables(self):
        """Auto-discover tables in the DB."""
        try:
            conn = self.ctrl.cartridge._get_conn()
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = [r[0] for r in cursor.fetchall() if not r[0].startswith('sqlite_')]
            conn.close()
            
            self.table_combo['values'] = tables
            if tables:
                if not self.table_var.get() or self.table_var.get() not in tables:
                    self.table_combo.set("files") # Default
                self.refresh_data()
        except Exception as e:
            print(f"DB Error: {e}")

    def refresh_data(self, event=None):
        table = self.table_var.get()
        if not table: return
        
        query_filter = self.search_entry.get().strip()
        self.tree.delete(*self.tree.get_children())
        
        try:
            conn = self.ctrl.cartridge._get_conn()
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            # 1. Get Columns
            cursor.execute(f"PRAGMA table_info({table})")
            columns = [r[1] for r in cursor.fetchall()]
            
            # 2. Build Query
            sql = f"SELECT * FROM {table}"
            params = []
            
            if query_filter:
                # Smart Filter: Search in all text-like columns
                text_cols = [c for c in columns if 'id' in c or 'path' in c or 'content' in c or 'metadata' in c]
                if text_cols:
                    conditions = " OR ".join([f"{col} LIKE ?" for col in text_cols])
                    sql += f" WHERE {conditions}"
                    params = [f"%{query_filter}%" for _ in text_cols]
            
            sql += " LIMIT 100"
            
            # 3. Populate Tree
            self.tree['columns'] = columns
            for col in columns:
                self.tree.heading(col, text=col)
                self.tree.column(col, width=120)
                
            cursor.execute(sql, params)
            for row in cursor.fetchall():
                # Truncate long values for display
                values = []
                for val in row:
                    s = str(val)
                    if len(s) > 50: s = s[:50] + "..."
                    values.append(s)
                self.tree.insert("", "end", values=values, tags=(str(row[0]) if 'id' in row.keys() else '',))
                
            conn.close()
        except Exception as e:
            print(f"Data Fetch Error: {e}")

    def on_double_click(self, event):
        """Pop up a viewer for full cell content."""
        item = self.tree.identify_row(event.y)
        if not item: return
        
        # We need to fetch the REAL data, not the truncated view
        values = self.tree.item(item, 'values')
        
        top = tk.Toplevel(self)
        top.title("Cell Inspector")
        top.geometry("600x400")
        top.configure(bg="#252526")
        
        txt = scrolledtext.ScrolledText(top, bg="#1e1e2f", fg="#e0e0e0", font=("Consolas", 10))
        txt.pack(fill="both", expand=True)
        
        # Pretty print the row
        display_text = ""
        headers = self.tree['columns']
        for i, val in enumerate(values):
            header = headers[i] if i < len(headers) else f"Col {i}"
            display_text += f"[{header}]:\n{val}\n\n{'-'*40}\n\n"
            
        txt.insert("1.0", display_text)

# ==============================================================================
#  THE WORKBENCH (GUI)
# ==============================================================================
class WorkbenchApp(tk.Tk):
    def __init__(self, controller: MindWrapperController):
        super().__init__()
        self.ctrl = controller
        self.title("_MindWRAPPER Workbench v2.0")
        self.geometry("1000x700")
        self.configure(bg="#1e1e2f")
        
        self._setup_ui()
        
    def _setup_ui(self):
        # --- HEADER ---
        header = tk.Frame(self, bg="#101018", pady=10)
        header.pack(fill="x")
        
        status_color = "#4caf50" if self.ctrl.neural_online else "#f44336"
        tk.Label(header, text="MindWRAPPER", bg="#101018", fg="white", font=("Consolas", 16, "bold")).pack(side="left", padx=20)
        tk.Label(header, text=f"Neural Link: {'ONLINE' if self.ctrl.neural_online else 'OFFLINE'}", bg="#101018", fg=status_color).pack(side="right", padx=20)

        # --- TABS ---
        style = ttk.Style()
        style.theme_use('clam')
        style.configure("TNotebook", background="#1e1e2f", borderwidth=0)
        style.configure("TNotebook.Tab", background="#2d2d44", foreground="white", padding=[15, 5])
        style.map("TNotebook.Tab", background=[("selected", "#007ACC")])

        self.notebook = ttk.Notebook(self)
        self.notebook.pack(fill="both", expand=True, padx=10, pady=10)
        
        self._init_intake_tab()
        self._init_refinery_tab()
        self._init_inspector_tab()

    def _init_intake_tab(self):
        tab = tk.Frame(self.notebook, bg="#1e1e2f")
        self.notebook.add(tab, text="  INTAKE (Vacuum)  ")
        
        # Path Entry
        row1 = tk.Frame(tab, bg="#1e1e2f", pady=20)
        row1.pack(fill="x")
        tk.Label(row1, text="TARGET PATH:", bg="#1e1e2f", fg="gray").pack(side="left", padx=10)
        self.ent_path = tk.Entry(row1, bg="#2d2d44", fg="white", width=50)
        self.ent_path.insert(0, "./src") # Default
        self.ent_path.pack(side="left", fill="x", expand=True, padx=10)
        
        # Buttons
        btn_scan = tk.Button(row1, text="RUN VACUUM", bg="#007ACC", fg="white", relief="flat", padx=15, command=self._run_vacuum)
        btn_scan.pack(side="left", padx=10)
        
        # Log Output
        self.log_intake = scrolledtext.ScrolledText(tab, bg="#101018", fg="#00ff00", font=("Consolas", 9))
        self.log_intake.pack(fill="both", expand=True, padx=10, pady=10)

    def _init_refinery_tab(self):
        tab = tk.Frame(self.notebook, bg="#1e1e2f")
        self.notebook.add(tab, text="  REFINERY (Enrich)  ")
        
        ctrl_panel = tk.Frame(tab, bg="#1e1e2f", pady=20)
        ctrl_panel.pack(fill="x")
        
        self.lbl_pending = tk.Label(ctrl_panel, text="Pending Files: ?", bg="#1e1e2f", fg="yellow", font=("Arial", 12))
        self.lbl_pending.pack(side="left", padx=20)
        
        btn_refresh = tk.Button(ctrl_panel, text="↻ Refresh", bg="#444", fg="white", relief="flat", command=self._refresh_stats)
        btn_refresh.pack(side="left", padx=5)
        
        btn_process = tk.Button(ctrl_panel, text="RUN NIGHT SHIFT", bg="#E02080", fg="white", relief="flat", padx=15, command=self._run_refinery)
        btn_process.pack(side="right", padx=20)
        
        self.log_refinery = scrolledtext.ScrolledText(tab, bg="#101018", fg="#00ccff", font=("Consolas", 9))
        self.log_refinery.pack(fill="both", expand=True, padx=10, pady=10)
        
        # Initial Load
        self.after(500, self._refresh_stats)

    def _init_inspector_tab(self):
        tab = tk.Frame(self.notebook, bg="#1e1e2f")
        self.notebook.add(tab, text="  CARTRIDGE INSPECTOR  ")
        
        # Initialize the DatabaseView
        # We pass self.ctrl so it can access the CartridgeService
        inspector = DatabaseView(tab, self.ctrl)
        inspector.pack(fill="both", expand=True)

    # --- ACTIONS ---

    def _log(self, widget, msg):
        widget.insert(tk.END, msg + "\n")
        widget.see(tk.END)

    def _run_vacuum(self):
        path = self.ent_path.get()
        self._log(self.log_intake, f"--- Starting Scan: {path} ---")
        
        def worker():
            stats = self.ctrl.intake.scan_directory(path)
            self._log(self.log_intake, f"Scan Complete.\n{stats}")
            self.after(0, self._refresh_stats) # Refresh stats on main thread
            
        threading.Thread(target=worker, daemon=True).start()

    def _refresh_stats(self):
        try:
            pending = self.ctrl.cartridge.get_pending_files(limit=9999)
            count = len(pending)
            self.lbl_pending.config(text=f"Pending Files: {count}")
        except:
            self.lbl_pending.config(text="Pending Files: Error")

    def _run_refinery(self):
        self._log(self.log_refinery, "--- Starting Refinery ---")
        
        def worker():
            total = 0
            while True:
                # Process in small batches to keep UI updating
                count = self.ctrl.refinery.process_pending_files(batch_size=5)
                if count == 0:
                    break
                total += count
                self._log(self.log_refinery, f"Enriched {count} files...")
                self.after(0, self._refresh_stats)
            
            self._log(self.log_refinery, f"--- Refinery Finished. Total: {total} ---")

        threading.Thread(target=worker, daemon=True).start()

# ==============================================================================
#  ENTRY POINT
# ==============================================================================
def main():
    parser = argparse.ArgumentParser(description="_MindWRAPPER")
    parser.add_argument("--headless", action="store_true", help="Run in CLI mode")
    parser.add_argument("command", nargs="?", help="ingest/refine (only for headless)")
    parser.add_argument("path", nargs="?", help="Path for ingest")
    args = parser.parse_args()

    # Initialize Backend
    controller = MindWrapperController(DEFAULT_DB_PATH)

    if args.headless:
        # --- CLI MODE ---
        print("running in HEADLESS mode...")
        if args.command == "ingest" and args.path:
            controller.intake.scan_directory(args.path)
        elif args.command == "refine":
            # Simple loop
            while controller.refinery.process_pending_files(10) > 0:
                print("Processing batch...")
        else:
            print("Invalid Headless Arguments. Use: --headless ingest <path> OR --headless refine")
    else:
        # --- GUI MODE ---
        app = WorkbenchApp(controller)
        app.mainloop()

if __name__ == "__main__":
    main()

--------------------------------------------------------------------------------
FILE: src\__init__.py
--------------------------------------------------------------------------------

--------------------------------------------------------------------------------
FILE: src\microservices\base_service.py
--------------------------------------------------------------------------------
import logging
import sys

class BaseService:
    """
    Standard base class for all _NeoCORTEX microservices.
    Provides unified logging and error handling.
    """
    def __init__(self, service_name: str):
        self.service_name = service_name
        self.log = logging.getLogger(service_name)
        
        # Configure logging if not already set up
        if not self.log.handlers:
            handler = logging.StreamHandler(sys.stdout)
            formatter = logging.Formatter('%(asctime)s [%(name)s] %(levelname)s: %(message)s', datefmt='%H:%M:%S')
            handler.setFormatter(formatter)
            self.log.addHandler(handler)
            self.log.setLevel(logging.INFO)

    def log_info(self, msg: str):
        self.log.info(msg)

    def log_error(self, msg: str):
        self.log.error(msg)

--------------------------------------------------------------------------------
FILE: src\microservices\cartridge_service.py
--------------------------------------------------------------------------------
import sqlite3
import json
import time
import os
from pathlib import Path
from typing import Dict, Any, Optional, List
from .base_service import BaseService

class CartridgeService(BaseService):
    """
    The Storage Manager.
    Manages the SQLite Cartridge format, including BLOB storage and ELT Status tracking.
    """
    
    def __init__(self, db_path: str):
        super().__init__("CartridgeService")
        self.db_path = Path(db_path)
        self._init_db()

    def _init_db(self):
        """Initializes the ELT Schema (Extract, Load, Transform)."""
        # Ensure parent folder exists
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        
        conn = self._get_conn()
        cursor = conn.cursor()
        
        # 1. Manifest (Metadata about the Cartridge itself)
        cursor.execute("CREATE TABLE IF NOT EXISTS manifest (key TEXT PRIMARY KEY, value TEXT)")
        
        # 2. Files (The Core Artifacts)
        # Note the 'blob_data' for binaries and 'status' for the workflow
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS files (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                path TEXT UNIQUE NOT NULL,      -- Virtual path (e.g. src/app.py)
                content TEXT,                   -- Text content (if applicable)
                blob_data BLOB,                 -- Binary content (PDFs, Images)
                
                status TEXT DEFAULT 'RAW',      -- RAW, PENDING_TRIAGE, ENRICHED, ERROR
                mime_type TEXT,                 -- text/plain, application/pdf
                
                metadata TEXT DEFAULT '{}',     -- JSON Tags from the Neural Service
                last_updated TIMESTAMP
            )
        """)

        # 3. Chunks (Vectors)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS chunks (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                file_id INTEGER,
                chunk_index INTEGER,
                content TEXT,
                embedding BLOB,
                FOREIGN KEY(file_id) REFERENCES files(id)
            )
        """)
        
        conn.commit()
        conn.close()

    def _get_conn(self):
        return sqlite3.connect(self.db_path)

    def store_raw_file(self, path: str, content: str = None, blob: bytes = None, mime_type: str = "text/plain"):
        """
        The 'Vacuum' method. Dumps raw data into the DB.
        """
        conn = self._get_conn()
        cursor = conn.cursor()
        try:
            cursor.execute("""
                INSERT OR REPLACE INTO files (path, content, blob_data, mime_type, status, last_updated)
                VALUES (?, ?, ?, ?, 'RAW', ?)
            """, (path, content, blob, mime_type, time.time()))
            conn.commit()
            return True
        except Exception as e:
            self.log_error(f"Failed to store {path}: {e}")
            return False
        finally:
            conn.close()

    def get_pending_files(self, limit: int = 100) -> List[Dict]:
        """Fetches files waiting for the Refinery."""
        conn = self._get_conn()
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        rows = cursor.execute(
            "SELECT id, path, content, mime_type FROM files WHERE status = 'RAW' LIMIT ?", 
            (limit,)
        ).fetchall()
        
        conn.close()
        return [dict(row) for row in rows]

    def update_file_status(self, file_id: int, status: str, metadata: Dict = None):
        """Promotes a file's status (e.g., RAW -> ENRICHED)."""
        conn = self._get_conn()
        cursor = conn.cursor()
        
        if metadata:
            meta_json = json.dumps(metadata)
            cursor.execute("UPDATE files SET status = ?, metadata = ? WHERE id = ?", (status, meta_json, file_id))
        else:
            cursor.execute("UPDATE files SET status = ? WHERE id = ?", (status, file_id))
            
        conn.commit()
        conn.close()

--------------------------------------------------------------------------------
FILE: src\microservices\intake_service.py
--------------------------------------------------------------------------------
import os
import mimetypes
from pathlib import Path
from typing import List, Dict
from .base_service import BaseService
from .cartridge_service import CartridgeService

class IntakeService(BaseService):
    """
    The Vacuum.
    Crawls local directories and ingests raw files into the Cartridge.
    Performs NO analysis, only storage.
    """

    # Files to absolutely ignore
    IGNORE_DIRS = {'.git', '__pycache__', 'node_modules', '.venv', 'dist', 'build', '.idea', '.vscode'}
    IGNORE_EXTS = {'.pyc', '.pyd', '.db', '.sqlite', '.sqlite3'}

    def __init__(self, cartridge_service: CartridgeService):
        super().__init__("IntakeService")
        self.cartridge = cartridge_service

    def scan_directory(self, root_path: str) -> Dict[str, int]:
        """
        Recursively walks the directory and stores everything in the DB.
        Returns a report of what it did.
        """
        root = Path(root_path).resolve()
        stats = {"text": 0, "binary": 0, "skipped": 0, "total": 0}

        self.log_info(f"Starting scan of {root}...")

        for current_dir, dirs, files in os.walk(root):
            # In-place modification of dirs to prune IGNORED directories
            dirs[:] = [d for d in dirs if d not in self.IGNORE_DIRS]

            for filename in files:
                file_path = Path(current_dir) / filename
                
                # Check extension blacklist
                if file_path.suffix.lower() in self.IGNORE_EXTS:
                    stats["skipped"] += 1
                    continue

                # Calculate the "Virtual Path" (relative to project root)
                try:
                    vfs_path = file_path.relative_to(root).as_posix()
                except ValueError:
                    vfs_path = filename

                # Attempt Ingestion
                if self._ingest_file(file_path, vfs_path, stats):
                    stats["total"] += 1
                else:
                    stats["skipped"] += 1

        self.log_info(f"Scan Complete. {stats}")
        return stats

    def _ingest_file(self, real_path: Path, vfs_path: str, stats: Dict) -> bool:
        """Reads the file and pushes to CartridgeService."""
        mime_type, _ = mimetypes.guess_type(real_path)
        if not mime_type:
            mime_type = "application/octet-stream"

        # Strategy: Try to read as Text first. If that fails, treat as Binary.
        content = None
        blob = None
        
        try:
            # Try Text
            with open(real_path, 'r', encoding='utf-8') as f:
                content = f.read()
            stats["text"] += 1
        except UnicodeDecodeError:
            # Fallback to Binary
            try:
                with open(real_path, 'rb') as f:
                    blob = f.read()
                stats["binary"] += 1
                # Ensure mime reflects binary if we guessed wrong
                if mime_type.startswith("text"): 
                    mime_type = "application/octet-stream"
            except Exception as e:
                self.log_error(f"Access denied or read error {vfs_path}: {e}")
                return False

        # Store in DB
        return self.cartridge.store_raw_file(
            path=vfs_path,
            content=content,
            blob=blob,
            mime_type=mime_type
        )

--------------------------------------------------------------------------------
FILE: src\microservices\neural_service.py
--------------------------------------------------------------------------------
import requests
import json
import concurrent.futures
from typing import Optional, Dict, Any, List
from .base_service import BaseService

# Configuration constants
OLLAMA_API_URL = "http://localhost:11434/api"

class NeuralService(BaseService):
    """
    Manages connections to the Neural Backend (Ollama).
    Handles Tiering (CPU vs GPU) and Parallelism.
    """
    
    # Define our tiers based on the 'Bake' script we ran earlier
    MODELS = {
        "fast": "qwen2.5-coder:1.5b-cpu",     # The Roughneck (CPU)
        "smart": "qwen2.5:3b-cpu",            # The Supervisor (CPU)
        "architect": "qwen2.5:7b",            # The Big Brain (GPU)
        "embed": "mxbai-embed-large:latest-cpu" # The Embedder (CPU)
    }

    def __init__(self, max_workers: int = 4):
        super().__init__("NeuralService")
        self.max_workers = max_workers
        self.log_info(f"Neural Link Active. CPU Workers: {max_workers}")

    def check_connection(self) -> bool:
        """Pings Ollama to see if it's alive."""
        try:
            requests.get(f"{OLLAMA_API_URL}/tags", timeout=2)
            return True
        except requests.RequestException:
            self.log_error("Ollama connection failed. Is 'ollama serve' running?")
            return False

    def get_embedding(self, text: str) -> Optional[List[float]]:
        """Generates a vector using the CPU embedder."""
        try:
            res = requests.post(
                f"{OLLAMA_API_URL}/embeddings",
                json={"model": self.MODELS["embed"], "prompt": text},
                timeout=30
            )
            if res.status_code == 200:
                return res.json().get("embedding")
        except Exception as e:
            self.log_error(f"Embedding failed: {e}")
        return None

    def request_inference(self, prompt: str, tier: str = "fast", format_json: bool = False) -> str:
        """
        Synchronous inference request.
        tier: 'fast' (1.5b-cpu), 'smart' (3b-cpu), or 'architect' (7b-gpu)
        """
        model = self.MODELS.get(tier, self.MODELS["fast"])
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False
        }
        if format_json:
            payload["format"] = "json"

        try:
            res = requests.post(f"{OLLAMA_API_URL}/generate", json=payload, timeout=60)
            if res.status_code == 200:
                return res.json().get("response", "").strip()
        except Exception as e:
            self.log_error(f"Inference ({tier}) failed: {e}")
        return ""

    def process_parallel(self, items: List[Any], worker_func) -> List[Any]:
        """
        Helper to run a function across many items using the ThreadPool.
        Useful for batch ingestion.
        """
        results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # worker_func should take a single item and return a result
            futures = {executor.submit(worker_func, item): item for item in items}
            for future in concurrent.futures.as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    self.log_error(f"Worker task failed: {e}")
        return results

--------------------------------------------------------------------------------
FILE: src\microservices\refinery_service.py
--------------------------------------------------------------------------------
import json
from typing import Dict, Any, Optional
from .base_service import BaseService
from .cartridge_service import CartridgeService
from .neural_service import NeuralService
from .semantic_chunker import SemanticChunker

class RefineryService(BaseService):
    """
    The Night Shift (v2).
    Chunks code, generates vectors, and enriches metadata.
    """

    def __init__(self, cartridge: CartridgeService, neural: NeuralService):
        super().__init__("RefineryService")
        self.cartridge = cartridge
        self.neural = neural
        self.chunker = SemanticChunker()

    def process_pending_files(self, batch_size: int = 10) -> int:
        pending = self.cartridge.get_pending_files(limit=batch_size)
        if not pending: return 0

        self.log_info(f"Refining {len(pending)} files (Chunking + Vectorizing)...")
        results = self.neural.process_parallel(pending, self._process_single_file)
        return len(results)

    def _process_single_file(self, file_row: Dict) -> bool:
        file_id = file_row["id"]
        path = file_row["path"]
        content = file_row["content"]
        
        if not content:
            # Binary file? Mark processed but don't chunk
            self.cartridge.update_file_status(file_id, "SKIPPED_BINARY")
            return True

        try:
            # 1. Semantic Chunking
            chunks = self.chunker.chunk_file(content, path)
            
            # 2. Vectorization Loop
            conn = self.cartridge._get_conn()
            cursor = conn.cursor()
            
            for i, chunk in enumerate(chunks):
                # Get Vector from CPU Embedder
                vector = self.neural.get_embedding(chunk.content)
                vec_blob = json.dumps(vector).encode('utf-8') if vector else None
                
                cursor.execute("""
                    INSERT INTO chunks (file_id, chunk_index, content, embedding, name, type, start_line, end_line)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (file_id, i, chunk.content, vec_blob, chunk.name, chunk.type, chunk.start_line, chunk.end_line))
            
            conn.commit()
            conn.close()

            # 3. Metadata Enrichment (The 1.5b-coder pass)
            meta = {}
            if path.endswith(".py"):
                meta = self._analyze_python(content)
            
            self.cartridge.update_file_status(file_id, "ENRICHED", metadata=meta)
            return True

        except Exception as e:
            self.log_error(f"Refinery failed on {path}: {e}")
            self.cartridge.update_file_status(file_id, "ERROR", metadata={"error": str(e)})
            return False

    def _analyze_python(self, content: str) -> Dict:
        """Ask 1.5b-coder for a structural summary."""
        prompt = f"""
        Analyze this Python code. Return JSON with:
        - "summary": "One sentence description"
        - "complexity": "Low/Medium/High"
        
        Code:
        {content[:2000]}
        """
        response = self.neural.request_inference(prompt, tier="fast", format_json=True)
        try: return json.loads(response)
        except: return {"summary": "Analysis failed"}

--------------------------------------------------------------------------------
FILE: src\microservices\search_service.py
--------------------------------------------------------------------------------
import sqlite3
import json
import struct
import math
from typing import List, Dict, Optional
from .base_service import BaseService
from .neural_service import NeuralService

class SearchService(BaseService):
    """
    The Oracle.
    Performs Hybrid Search (Vector + Keyword) to find knowledge.
    Includes robust fallback to pure Python math if C++ extensions fail.
    """

    def __init__(self, db_path: str, neural: NeuralService):
        super().__init__("SearchService")
        self.db_path = db_path
        self.neural = neural
        self.vec_extension_loaded = False

    def _get_conn(self):
        conn = sqlite3.connect(self.db_path)
        # Try to load sqlite-vec for speed
        try:
            conn.enable_load_extension(True)
            import sqlite_vec
            sqlite_vec.load(conn)
            self.vec_extension_loaded = True
        except:
            self.vec_extension_loaded = False
        return conn

    def search(self, query: str, limit: int = 15) -> List[Dict]:
        """
        Main Entry Point: Hybrid Search.
        """
        # 1. Get Vector for the Query
        query_vec = self.neural.get_embedding(query)
        
        conn = self._get_conn()
        results = []
        
        try:
            if query_vec:
                if self.vec_extension_loaded:
                    results = self._search_fast_vec(conn, query_vec, limit)
                else:
                    results = self._search_python_vec(conn, query_vec, limit)
            else:
                self.log_info("Embedding failed/skipped. Falling back to Keyword Search.")
                results = self._search_keyword(conn, query, limit)
        finally:
            conn.close()
            
        return results

    def _search_fast_vec(self, conn, query_vec, limit):
        """C++ Accelerated Search."""
        cursor = conn.cursor()
        vec_bytes = struct.pack(f'{len(query_vec)}f', *query_vec)
        
        sql = """
            SELECT f.path, c.content, vec_distance_cosine(c.embedding, ?) as distance
            FROM chunks c
            JOIN files f ON c.file_id = f.id
            WHERE c.embedding IS NOT NULL
            ORDER BY distance ASC
            LIMIT ?
        """
        rows = cursor.execute(sql, (vec_bytes, limit)).fetchall()
        return self._format_results(rows, score_is_distance=True)

    def _search_python_vec(self, conn, query_vec, limit):
        """Python Fallback Search (Slow but Reliable)."""
        cursor = conn.cursor()
        cursor.execute("SELECT c.id, f.path, c.content, c.embedding FROM chunks c JOIN files f ON c.file_id = f.id")
        
        candidates = []
        for row in cursor.fetchall():
            _, path, content, blob = row
            if not blob: continue
            
            try:
                # Handle JSON or Binary blobs
                try: vec = json.loads(blob)
                except: vec = struct.unpack(f'{len(query_vec)}f', blob)
                
                score = self._cosine_similarity(query_vec, vec)
                candidates.append((score, path, content))
            except: continue

        candidates.sort(key=lambda x: x[0], reverse=True)
        return [{"path": c[1], "score": c[0], "content": c[2]} for c in candidates[:limit]]

    def _search_keyword(self, conn, query, limit):
        """Old-school LIKE search."""
        cursor = conn.cursor()
        cursor.execute("SELECT path, content FROM files WHERE content LIKE ? LIMIT ?", (f'%{query}%', limit))
        return [{"path": r[0], "score": 1.0, "content": r[1]} for r in cursor.fetchall()]

    def _cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1, v2))
        norm1 = math.sqrt(sum(a*a for a in v1))
        norm2 = math.sqrt(sum(b*b for b in v2))
        return dot / (norm1 * norm2) if norm1 and norm2 else 0.0

    def _format_results(self, rows, score_is_distance=False):
        formatted = []
        for r in rows:
            score = 1 - r[2] if score_is_distance else r[2]
            formatted.append({"path": r[0], "score": score, "content": r[1]})
        return formatted

--------------------------------------------------------------------------------
FILE: src\microservices\semantic_chunker.py
--------------------------------------------------------------------------------
import ast
from dataclasses import dataclass
from typing import List

@dataclass
class CodeChunk:
    name: str          # e.g., "class AuthMS"
    type: str          # "class", "function", "text"
    content: str       # The raw source
    start_line: int
    end_line: int
    docstring: str = "" # Captured separately for high-quality RAG

class SemanticChunker:
    """
    Intelligent Code Splitter.
    Parses source code into logical units (Classes, Functions) 
    rather than arbitrary text windows.
    """
    
    def chunk_file(self, content: str, filename: str) -> List[CodeChunk]:
        if filename.endswith(".py"):
            return self._chunk_python(content)
        return self._chunk_generic(content)

    def _chunk_python(self, source: str) -> List[CodeChunk]:
        chunks = []
        try:
            tree = ast.parse(source)
            lines = source.splitlines(keepends=True)
            
            def get_segment(node):
                start = node.lineno - 1
                end = node.end_lineno if hasattr(node, 'end_lineno') else start + 1
                return "".join(lines[start:end]), start + 1, end

            for node in tree.body:
                if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"def {node.name}", type="function", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))
                elif isinstance(node, ast.ClassDef):
                    text, s, e = get_segment(node)
                    doc = ast.get_docstring(node) or ""
                    chunks.append(CodeChunk(
                        name=f"class {node.name}", type="class", 
                        content=text, start_line=s, end_line=e, docstring=doc
                    ))

            # Fallback: If no classes/functions found (e.g., script file), treat as generic
            if not chunks:
                return self._chunk_generic(source)
                
        except SyntaxError:
            return self._chunk_generic(source)
            
        return chunks

    def _chunk_generic(self, text: str, window_size: int = 1500) -> List[CodeChunk]:
        """Sliding window for non-code files."""
        chunks = []
        lines = text.splitlines(keepends=True)
        current_chunk = []
        current_size = 0
        chunk_idx = 1
        start_line = 1
        
        for i, line in enumerate(lines):
            current_chunk.append(line)
            current_size += len(line)
            
            if current_size >= window_size:
                chunks.append(CodeChunk(
                    name=f"Chunk {chunk_idx}", type="text_block",
                    content="".join(current_chunk), start_line=start_line, end_line=i + 1
                ))
                current_chunk = []
                current_size = 0
                chunk_idx += 1
                start_line = i + 2
                
        if current_chunk:
            chunks.append(CodeChunk(
                name=f"Chunk {chunk_idx}", type="text_block",
                content="".join(current_chunk), start_line=start_line, end_line=len(lines)
            ))
            
        return chunks

--------------------------------------------------------------------------------
FILE: src\microservices\__init__.py
--------------------------------------------------------------------------------

